# Comparing `tmp/flax-0.6.9.tar.gz` & `tmp/flax-0.7.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flax-0.6.9.tar", last modified: Tue Apr 18 21:02:47 2023, max compression
+gzip compressed data, was "flax-0.7.0.tar", last modified: Sat Jul  8 01:21:39 2023, max compression
```

## Comparing `flax-0.6.9.tar` & `flax-0.7.0.tar`

### file list

```diff
@@ -1,416 +1,420 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.017206 flax-0.6.9/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.929206 flax-0.6.9/.github/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.929206 flax-0.6.9/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 runner    (1001) docker     (123)      792 2023-04-18 21:02:27.000000 flax-0.6.9/.github/ISSUE_TEMPLATE/bug_report.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.933206 flax-0.6.9/.github/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-04-18 21:02:27.000000 flax-0.6.9/.github/analytics/README.md
--rw-r--r--   0 runner    (1001) docker     (123)    13763 2023-04-18 21:02:27.000000 flax-0.6.9/.github/analytics/get_repo_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1677 2023-04-18 21:02:27.000000 flax-0.6.9/.github/analytics/issue_activity_since_date.gql
--rw-r--r--   0 runner    (1001) docker     (123)     2016 2023-04-18 21:02:27.000000 flax-0.6.9/.github/analytics/pr_data_query.gql
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-04-18 21:02:27.000000 flax-0.6.9/.github/analytics/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-04-18 21:02:27.000000 flax-0.6.9/.github/pull_request_template.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.933206 flax-0.6.9/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (123)     6155 2023-04-18 21:02:27.000000 flax-0.6.9/.github/workflows/build.yml
--rw-r--r--   0 runner    (1001) docker     (123)      834 2023-04-18 21:02:27.000000 flax-0.6.9/.github/workflows/pythonpublish.yml
--rw-r--r--   0 runner    (1001) docker     (123)      138 2023-04-18 21:02:27.000000 flax-0.6.9/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      786 2023-04-18 21:02:27.000000 flax-0.6.9/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-04-18 21:02:27.000000 flax-0.6.9/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (123)      293 2023-04-18 21:02:27.000000 flax-0.6.9/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (123)    18804 2023-04-18 21:02:27.000000 flax-0.6.9/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (123)    11309 2023-04-18 21:02:27.000000 flax-0.6.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     8610 2023-04-18 21:02:47.017206 flax-0.6.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     7938 2023-04-18 21:02:27.000000 flax-0.6.9/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-04-18 21:02:27.000000 flax-0.6.9/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.933206 flax-0.6.9/dev/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.933206 flax-0.6.9/dev/.devcontainer/
--rw-r--r--   0 runner    (1001) docker     (123)     2643 2023-04-18 21:02:27.000000 flax-0.6.9/dev/.devcontainer/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-04-18 21:02:27.000000 flax-0.6.9/dev/.devcontainer/devcontainer.json
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-04-18 21:02:27.000000 flax-0.6.9/dev/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     4515 2023-04-18 21:02:27.000000 flax-0.6.9/dev/update_requirements.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.937206 flax-0.6.9/docs/
--rw-r--r--   0 runner    (1001) docker     (123)       18 2023-04-18 21:02:27.000000 flax-0.6.9/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      634 2023-04-18 21:02:27.000000 flax-0.6.9/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)     5359 2023-04-18 21:02:27.000000 flax-0.6.9/docs/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.941207 flax-0.6.9/docs/_ext/
--rw-r--r--   0 runner    (1001) docker     (123)     4251 2023-04-18 21:02:27.000000 flax-0.6.9/docs/_ext/codediff.py
--rw-r--r--   0 runner    (1001) docker     (123)     3389 2023-04-18 21:02:27.000000 flax-0.6.9/docs/_ext/codediff_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.917207 flax-0.6.9/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.941207 flax-0.6.9/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (123)      309 2023-04-18 21:02:27.000000 flax-0.6.9/docs/_static/css/flax_theme.css
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.917207 flax-0.6.9/docs/_templates/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.941207 flax-0.6.9/docs/_templates/autosummary/
--rw-r--r--   0 runner    (1001) docker     (123)      462 2023-04-18 21:02:27.000000 flax-0.6.9/docs/_templates/autosummary/flax_module.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.941207 flax-0.6.9/docs/advanced_topics/
--rw-r--r--   0 runner    (1001) docker     (123)     4077 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/arguments.md
--rw-r--r--   0 runner    (1001) docker     (123)    10351 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/contributing.md
--rw-r--r--   0 runner    (1001) docker     (123)     9296 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/convert_pytorch_to_flax.rst
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18092 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/lift.md
--rw-r--r--   0 runner    (1001) docker     (123)     5856 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/linen_design_principles.rst
--rw-r--r--   0 runner    (1001) docker     (123)    17132 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/linen_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)    21980 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/module_lifecycle.rst
--rw-r--r--   0 runner    (1001) docker     (123)    10621 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/optax_update_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1148 2023-04-18 21:02:27.000000 flax-0.6.9/docs/advanced_topics/philosophy.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.945206 flax-0.6.9/docs/api_reference/
--rw-r--r--   0 runner    (1001) docker     (123)      118 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.config.rst
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.core.frozen_dict.rst
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.errors.rst
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.jax_utils.rst
--rw-r--r--   0 runner    (1001) docker     (123)     4205 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.linen.rst
--rw-r--r--   0 runner    (1001) docker     (123)      480 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.serialization.rst
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.struct.rst
--rw-r--r--   0 runner    (1001) docker     (123)      275 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.traceback_util.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.training.rst
--rw-r--r--   0 runner    (1001) docker     (123)      798 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/flax.traverse_util.rst
--rw-r--r--   0 runner    (1001) docker     (123)      244 2023-04-18 21:02:27.000000 flax-0.6.9/docs/api_reference/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)     5061 2023-04-18 21:02:27.000000 flax-0.6.9/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)     7338 2023-04-18 21:02:27.000000 flax-0.6.9/docs/conf_sphinx_patch.py
--rw-r--r--   0 runner    (1001) docker     (123)    11920 2023-04-18 21:02:27.000000 flax-0.6.9/docs/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.945206 flax-0.6.9/docs/developer_notes/
--rw-r--r--   0 runner    (1001) docker     (123)      153 2023-04-18 21:02:27.000000 flax-0.6.9/docs/developer_notes/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-04-18 21:02:27.000000 flax-0.6.9/docs/developer_notes/lift.md
--rw-r--r--   0 runner    (1001) docker     (123)    21998 2023-04-18 21:02:27.000000 flax-0.6.9/docs/developer_notes/module_lifecycle.rst
--rw-r--r--   0 runner    (1001) docker     (123)      184 2023-04-18 21:02:27.000000 flax-0.6.9/docs/examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     4925 2023-04-18 21:02:27.000000 flax-0.6.9/docs/examples_community_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     4422 2023-04-18 21:02:27.000000 flax-0.6.9/docs/examples_core_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)    22578 2023-04-18 21:02:27.000000 flax-0.6.9/docs/examples_google_research_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     2028 2023-04-18 21:02:27.000000 flax-0.6.9/docs/examples_repositories_that_use_flax.rst
--rw-r--r--   0 runner    (1001) docker     (123)    20991 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flax.png
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.949206 flax-0.6.9/docs/flip/
--rw-r--r--   0 runner    (1001) docker     (123)      648 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/0000-template.md
--rw-r--r--   0 runner    (1001) docker     (123)    17256 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/1009-optimizer-api.md
--rw-r--r--   0 runner    (1001) docker     (123)     8189 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/1777-default-dtype.md
--rw-r--r--   0 runner    (1001) docker     (123)    10422 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/2434-general-metadata.md
--rw-r--r--   0 runner    (1001) docker     (123)     4099 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/2974-kw-only-dataclasses.md
--rw-r--r--   0 runner    (1001) docker     (123)     1404 2023-04-18 21:02:27.000000 flax-0.6.9/docs/flip/README.md
--rw-r--r--   0 runner    (1001) docker     (123)   102609 2023-04-18 21:02:27.000000 flax-0.6.9/docs/getting_started.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    16367 2023-04-18 21:02:27.000000 flax-0.6.9/docs/getting_started.md
--rw-r--r--   0 runner    (1001) docker     (123)     6679 2023-04-18 21:02:27.000000 flax-0.6.9/docs/glossary.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.957206 flax-0.6.9/docs/guides/
--rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/arguments.md
--rw-r--r--   0 runner    (1001) docker     (123)     9180 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/batch_norm.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9296 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/convert_pytorch_to_flax.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11073 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/dropout.rst
--rw-r--r--   0 runner    (1001) docker     (123)    10467 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/ensembling.rst
--rw-r--r--   0 runner    (1001) docker     (123)    10080 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/extracting_intermediates.rst
--rw-r--r--   0 runner    (1001) docker     (123)    40409 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/flax_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    22504 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/flax_basics.md
--rw-r--r--   0 runner    (1001) docker     (123)    38147 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/flax_on_pjit.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    22791 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/flax_on_pjit.md
--rw-r--r--   0 runner    (1001) docker     (123)     7442 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/full_eval.rst
--rw-r--r--   0 runner    (1001) docker     (123)      265 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)      176 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_converting_and_upgrading.rst
--rw-r--r--   0 runner    (1001) docker     (123)       80 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_data_preprocessing.rst
--rw-r--r--   0 runner    (1001) docker     (123)      201 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_flax_fundamentals.rst
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_model_inspection.rst
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_parallel_training.rst
--rw-r--r--   0 runner    (1001) docker     (123)      152 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/index_training_techniques.rst
--rw-r--r--   0 runner    (1001) docker     (123)    38261 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/jax_for_the_impatient.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    21301 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/jax_for_the_impatient.md
--rw-r--r--   0 runner    (1001) docker     (123)    17180 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/linen_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8175 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/lr_schedule.rst
--rw-r--r--   0 runner    (1001) docker     (123)     7142 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/model_surgery.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/model_surgery.md
--rw-r--r--   0 runner    (1001) docker     (123)    10621 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/optax_update_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9077 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/orbax_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     3129 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/setup_or_nncompact.rst
--rw-r--r--   0 runner    (1001) docker     (123)     6040 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/state_params.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11473 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/transfer_learning.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     7992 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/transfer_learning.md
--rw-r--r--   0 runner    (1001) docker     (123)    53165 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/use_checkpointing.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    26620 2023-04-18 21:02:27.000000 flax-0.6.9/docs/guides/use_checkpointing.md
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-04-18 21:02:27.000000 flax-0.6.9/docs/howtos.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8274 2023-04-18 21:02:27.000000 flax-0.6.9/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)      596 2023-04-18 21:02:27.000000 flax-0.6.9/docs/installation.md
--rw-r--r--   0 runner    (1001) docker     (123)     8090 2023-04-18 21:02:27.000000 flax-0.6.9/docs/mission.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.961206 flax-0.6.9/docs/notebooks/
--rw-r--r--   0 runner    (1001) docker     (123)     7377 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/flax_sharp_bits.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     5927 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/flax_sharp_bits.md
--rw-r--r--   0 runner    (1001) docker     (123)    18559 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/full_eval.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/full_eval.md
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    40238 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/linen_intro.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    21869 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/linen_intro.md
--rw-r--r--   0 runner    (1001) docker     (123)    21338 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/optax_update_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    11407 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/optax_update_guide.md
--rw-r--r--   0 runner    (1001) docker     (123)    13947 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/orbax_upgrade_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     8587 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/orbax_upgrade_guide.md
--rw-r--r--   0 runner    (1001) docker     (123)     9512 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/state_params.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     6687 2023-04-18 21:02:27.000000 flax-0.6.9/docs/notebooks/state_params.md
--rw-r--r--   0 runner    (1001) docker     (123)     2510 2023-04-18 21:02:27.000000 flax-0.6.9/docs/overview.md
--rw-r--r--   0 runner    (1001) docker     (123)     7576 2023-04-18 21:02:27.000000 flax-0.6.9/docs/philosophy.md
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-04-18 21:02:27.000000 flax-0.6.9/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.961206 flax-0.6.9/examples/
--rw-r--r--   0 runner    (1001) docker     (123)      743 2023-04-18 21:02:27.000000 flax-0.6.9/examples/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.961206 flax-0.6.9/examples/cloud/
--rw-r--r--   0 runner    (1001) docker     (123)     4635 2023-04-18 21:02:27.000000 flax-0.6.9/examples/cloud/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     8965 2023-04-18 21:02:27.000000 flax-0.6.9/examples/cloud/launch_gce.py
--rw-r--r--   0 runner    (1001) docker     (123)     1650 2023-04-18 21:02:27.000000 flax-0.6.9/examples/cloud/startup_script.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.965206 flax-0.6.9/examples/imagenet/
--rw-r--r--   0 runner    (1001) docker     (123)     9388 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.965206 flax-0.6.9/examples/imagenet/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     2192 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/configs/fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     1670 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/configs/tpu.py
--rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/configs/v100_x8.py
--rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/configs/v100_x8_mixed_precision.py
--rw-r--r--   0 runner    (1001) docker     (123)   294628 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/imagenet.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     3360 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/imagenet_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     2235 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/imagenet_fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     8207 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2175 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/main.py
--rw-r--r--   0 runner    (1001) docker     (123)     4529 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     1971 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    12701 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-04-18 21:02:27.000000 flax-0.6.9/examples/imagenet/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.969206 flax-0.6.9/examples/linen_design_test/
--rw-r--r--   0 runner    (1001) docker     (123)     6470 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/attention_simple.py
--rw-r--r--   0 runner    (1001) docker     (123)     3215 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/dense.py
--rw-r--r--   0 runner    (1001) docker     (123)     1346 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/linear_regression.py
--rw-r--r--   0 runner    (1001) docker     (123)     2385 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/mlp_explicit.py
--rw-r--r--   0 runner    (1001) docker     (123)     1996 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/mlp_inline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1926 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/mlp_lazy.py
--rw-r--r--   0 runner    (1001) docker     (123)     1540 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/tied_autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2270 2023-04-18 21:02:27.000000 flax-0.6.9/examples/linen_design_test/weight_std.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.973206 flax-0.6.9/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.973206 flax-0.6.9/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     3443 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)    12699 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2240 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    12495 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     4969 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5618 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    20239 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-04-18 21:02:27.000000 flax-0.6.9/examples/lm1b/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.973206 flax-0.6.9/examples/mnist/
--rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.973206 flax-0.6.9/examples/mnist/configs/
--rw-r--r--   0 runner    (1001) docker     (123)      912 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    99011 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/mnist.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     2366 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/mnist_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)      298 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     5301 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     2226 2023-04-18 21:02:27.000000 flax-0.6.9/examples/mnist/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.977206 flax-0.6.9/examples/nlp_seq/
--rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     8009 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3868 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6643 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/models.py
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    14128 2023-04-18 21:02:27.000000 flax-0.6.9/examples/nlp_seq/train.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.981206 flax-0.6.9/examples/ogbg_molpcba/
--rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.981206 flax-0.6.9/examples/ogbg_molpcba/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     1551 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/configs/default_graph_net.py
--rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/configs/hparam_sweep.py
--rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/configs/test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8256 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2554 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/main.py
--rw-r--r--   0 runner    (1001) docker     (123)     6891 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     5223 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)  1111228 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/ogbg_molpcba.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     4841 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)      328 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    13830 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/train.py
--rw-r--r--   0 runner    (1001) docker     (123)    12218 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ogbg_molpcba/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.985206 flax-0.6.9/examples/ppo/
--rw-r--r--   0 runner    (1001) docker     (123)     2484 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     2593 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/agent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.985206 flax-0.6.9/examples/ppo/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1954 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     2477 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/env_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2252 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    13175 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/ppo_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)     5281 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/ppo_lib_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/ppo_main.py
--rw-r--r--   0 runner    (1001) docker     (123)      147 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     8670 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/seed_rl_atari_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (123)     1895 2023-04-18 21:02:27.000000 flax-0.6.9/examples/ppo/test_episodes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.985206 flax-0.6.9/examples/seq2seq/
--rw-r--r--   0 runner    (1001) docker     (123)      913 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     5558 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     4359 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/models.py
--rw-r--r--   0 runner    (1001) docker     (123)       65 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    25399 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/seq2seq.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     7095 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3269 2023-04-18 21:02:27.000000 flax-0.6.9/examples/seq2seq/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.989206 flax-0.6.9/examples/sst2/
--rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/README.md
--rwxr-xr-x   0 runner    (1001) docker     (123)     2031 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/build_vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.989206 flax-0.6.9/examples/sst2/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/configs/default.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    10057 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3528 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2168 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    14275 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3578 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     9117 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/sst2.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/train_test.py
--rw-r--r--   0 runner    (1001) docker     (123)   117898 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/vocab.txt
--rwxr-xr-x   0 runner    (1001) docker     (123)     4421 2023-04-18 21:02:27.000000 flax-0.6.9/examples/sst2/vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.993206 flax-0.6.9/examples/vae/
--rw-r--r--   0 runner    (1001) docker     (123)      594 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     2152 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/reconstruction.png
--rw-r--r--   0 runner    (1001) docker     (123)      113 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.993206 flax-0.6.9/examples/vae/results/
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/results/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)    43139 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/sample.png
--rw-r--r--   0 runner    (1001) docker     (123)     5654 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3133 2023-04-18 21:02:27.000000 flax-0.6.9/examples/vae/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.997206 flax-0.6.9/examples/wmt/
--rw-r--r--   0 runner    (1001) docker     (123)     6107 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     7394 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/bleu.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:46.997206 flax-0.6.9/examples/wmt/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     3482 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)    15122 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/decode.py
--rw-r--r--   0 runner    (1001) docker     (123)    13174 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3172 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2216 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    19064 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/models.py
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     5619 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    23635 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-04-18 21:02:27.000000 flax-0.6.9/examples/wmt/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.001206 flax-0.6.9/flax/
--rw-r--r--   0 runner    (1001) docker     (123)      908 2023-04-18 21:02:27.000000 flax-0.6.9/flax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4168 2023-04-18 21:02:27.000000 flax-0.6.9/flax/configurations.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.005206 flax-0.6.9/flax/core/
--rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/axes_scan.py
--rw-r--r--   0 runner    (1001) docker     (123)    10412 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/flax_functional_engine.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9418 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/frozen_dict.py
--rw-r--r--   0 runner    (1001) docker     (123)    56619 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/lift.py
--rw-r--r--   0 runner    (1001) docker     (123)    11617 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/meta.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.005206 flax-0.6.9/flax/core/nn/
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18626 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)    12538 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     7164 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1503 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (123)     2539 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/partial_eval.py
--rw-r--r--   0 runner    (1001) docker     (123)    35706 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/scope.py
--rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/tracers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-04-18 21:02:27.000000 flax-0.6.9/flax/core/variables.py
--rw-r--r--   0 runner    (1001) docker     (123)    29793 2023-04-18 21:02:27.000000 flax-0.6.9/flax/errors.py
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-04-18 21:02:27.000000 flax-0.6.9/flax/ids.py
--rw-r--r--   0 runner    (1001) docker     (123)     5316 2023-04-18 21:02:27.000000 flax-0.6.9/flax/io.py
--rw-r--r--   0 runner    (1001) docker     (123)    11483 2023-04-18 21:02:27.000000 flax-0.6.9/flax/jax_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.009206 flax-0.6.9/flax/linen/
--rw-r--r--   0 runner    (1001) docker     (123)     2195 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     4014 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/activation.py
--rw-r--r--   0 runner    (1001) docker     (123)    19157 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     2891 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/combinators.py
--rw-r--r--   0 runner    (1001) docker     (123)     3383 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/dotgetter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/dtypes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.009206 flax-0.6.9/flax/linen/experimental/
--rw-r--r--   0 runner    (1001) docker     (123)    11542 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/experimental/layers_with_named_axes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2619 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/initializers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7528 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/kw_only_dataclasses.py
--rw-r--r--   0 runner    (1001) docker     (123)    32979 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)    86133 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/module.py
--rw-r--r--   0 runner    (1001) docker     (123)    21305 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)    19763 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/partitioning.py
--rw-r--r--   0 runner    (1001) docker     (123)     5480 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)    36844 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/recurrent.py
--rw-r--r--   0 runner    (1001) docker     (123)    11368 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/spmd.py
--rw-r--r--   0 runner    (1001) docker     (123)     2741 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (123)    19162 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/summary.py
--rw-r--r--   0 runner    (1001) docker     (123)    59698 2023-04-18 21:02:27.000000 flax-0.6.9/flax/linen/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.009206 flax-0.6.9/flax/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      582 2023-04-18 21:02:27.000000 flax-0.6.9/flax/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7726 2023-04-18 21:02:27.000000 flax-0.6.9/flax/metrics/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.009206 flax-0.6.9/flax/oss/
--rw-r--r--   0 runner    (1001) docker     (123)      443 2023-04-18 21:02:27.000000 flax-0.6.9/flax/oss/ .git-blame-ignore-revs
--rw-r--r--   0 runner    (1001) docker     (123)       58 2023-04-18 21:02:27.000000 flax-0.6.9/flax/py.typed
--rw-r--r--   0 runner    (1001) docker     (123)    14471 2023-04-18 21:02:27.000000 flax-0.6.9/flax/serialization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7332 2023-04-18 21:02:27.000000 flax-0.6.9/flax/struct.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.009206 flax-0.6.9/flax/testing/
--rw-r--r--   0 runner    (1001) docker     (123)      647 2023-04-18 21:02:27.000000 flax-0.6.9/flax/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-04-18 21:02:27.000000 flax-0.6.9/flax/testing/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-04-18 21:02:27.000000 flax-0.6.9/flax/traceback_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.013206 flax-0.6.9/flax/training/
--rw-r--r--   0 runner    (1001) docker     (123)      613 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    43229 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (123)     3678 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/dynamic_scale.py
--rw-r--r--   0 runner    (1001) docker     (123)     2718 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/lr_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/orbax_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2971 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/prefetch_iterator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-04-18 21:02:27.000000 flax-0.6.9/flax/training/train_state.py
--rw-r--r--   0 runner    (1001) docker     (123)    13401 2023-04-18 21:02:27.000000 flax-0.6.9/flax/traverse_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      651 2023-04-18 21:02:27.000000 flax-0.6.9/flax/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.001206 flax-0.6.9/flax.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     8610 2023-04-18 21:02:46.000000 flax-0.6.9/flax.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    10788 2023-04-18 21:02:46.000000 flax-0.6.9/flax.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-18 21:02:46.000000 flax-0.6.9/flax.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-04-18 21:02:46.000000 flax-0.6.9/flax.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        5 2023-04-18 21:02:46.000000 flax-0.6.9/flax.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.013206 flax-0.6.9/images/
--rw-r--r--   0 runner    (1001) docker     (123)    80407 2023-04-18 21:02:27.000000 flax-0.6.9/images/flax_logo.png
--rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-04-18 21:02:27.000000 flax-0.6.9/images/flax_logo.svg
--rw-r--r--   0 runner    (1001) docker     (123)    15137 2023-04-18 21:02:27.000000 flax-0.6.9/images/flax_logo_250px.png
--rw-r--r--   0 runner    (1001) docker     (123)    29095 2023-04-18 21:02:27.000000 flax-0.6.9/images/flax_logo_500px.png
--rw-r--r--   0 runner    (1001) docker     (123)    14116 2023-04-18 21:02:27.000000 flax-0.6.9/pylintrc
--rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-04-18 21:02:27.000000 flax-0.6.9/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-04-18 21:02:47.017206 flax-0.6.9/setup.cfg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.013206 flax-0.6.9/tests/
--rw-r--r--   0 runner    (1001) docker     (123)    17587 2023-04-18 21:02:27.000000 flax-0.6.9/tests/checkpoints_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-04-18 21:02:27.000000 flax-0.6.9/tests/colab_tpu_jax_version.ipynb
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.013206 flax-0.6.9/tests/core/
--rw-r--r--   0 runner    (1001) docker     (123)     5087 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/core_frozen_dict_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7924 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/core_lift_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/core_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9113 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/core_scope_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.017206 flax-0.6.9/tests/core/design/
--rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_auto_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2815 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_big_resnets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_custom_vjp_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4457 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_dense_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2630 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_flow_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4694 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_resnet_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2541 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_scan_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2172 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_tied_autoencoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_vmap_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-04-18 21:02:27.000000 flax-0.6.9/tests/core/design/core_weight_std_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      887 2023-04-18 21:02:27.000000 flax-0.6.9/tests/download_dataset_metadata.sh
--rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-04-18 21:02:27.000000 flax-0.6.9/tests/early_stopping_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-04-18 21:02:27.000000 flax-0.6.9/tests/import_test.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     8234 2023-04-18 21:02:27.000000 flax-0.6.9/tests/io_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3473 2023-04-18 21:02:27.000000 flax-0.6.9/tests/jax_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-18 21:02:47.017206 flax-0.6.9/tests/linen/
--rw-r--r--   0 runner    (1001) docker     (123)     3598 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/dotgetter_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1992 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/initializers_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4355 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/kw_only_dataclasses_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_activation_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5370 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_combinators_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1590 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_dtypes_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    35451 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_linear_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6242 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    61569 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_module_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    16085 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_recurrent_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    16028 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    53644 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/linen_transforms_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    17800 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/partitioning_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    18442 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/summary_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2403 2023-04-18 21:02:27.000000 flax-0.6.9/tests/linen/toplevel_test.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3960 2023-04-18 21:02:27.000000 flax-0.6.9/tests/run_all_tests.sh
--rw-r--r--   0 runner    (1001) docker     (123)    15319 2023-04-18 21:02:27.000000 flax-0.6.9/tests/serialization_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2371 2023-04-18 21:02:27.000000 flax-0.6.9/tests/struct_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    14163 2023-04-18 21:02:27.000000 flax-0.6.9/tests/tensorboard_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6027 2023-04-18 21:02:27.000000 flax-0.6.9/tests/traceback_util_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10482 2023-04-18 21:02:27.000000 flax-0.6.9/tests/traverse_util_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.768545 flax-0.7.0/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (123)      792 2023-07-08 01:21:24.000000 flax-0.7.0/.github/ISSUE_TEMPLATE/bug_report.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)    13763 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/get_repo_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1677 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/issue_activity_since_date.gql
+-rw-r--r--   0 runner    (1001) docker     (123)     2016 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/pr_data_query.gql
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-07-08 01:21:24.000000 flax-0.7.0/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)     6177 2023-07-08 01:21:24.000000 flax-0.7.0/.github/workflows/build.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      834 2023-07-08 01:21:24.000000 flax-0.7.0/.github/workflows/pythonpublish.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      138 2023-07-08 01:21:24.000000 flax-0.7.0/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      786 2023-07-08 01:21:24.000000 flax-0.7.0/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-07-08 01:21:24.000000 flax-0.7.0/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      293 2023-07-08 01:21:24.000000 flax-0.7.0/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (123)    19306 2023-07-08 01:21:24.000000 flax-0.7.0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (123)    11309 2023-07-08 01:21:24.000000 flax-0.7.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-07-08 01:21:39.768545 flax-0.7.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     7930 2023-07-08 01:21:24.000000 flax-0.7.0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-08 01:21:24.000000 flax-0.7.0/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/dev/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/dev/.devcontainer/
+-rw-r--r--   0 runner    (1001) docker     (123)     2643 2023-07-08 01:21:24.000000 flax-0.7.0/dev/.devcontainer/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-07-08 01:21:24.000000 flax-0.7.0/dev/.devcontainer/devcontainer.json
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-08 01:21:24.000000 flax-0.7.0/dev/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4515 2023-07-08 01:21:24.000000 flax-0.7.0/dev/update_requirements.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)       18 2023-07-08 01:21:24.000000 flax-0.7.0/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      634 2023-07-08 01:21:24.000000 flax-0.7.0/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)     5359 2023-07-08 01:21:24.000000 flax-0.7.0/docs/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_ext/
+-rw-r--r--   0 runner    (1001) docker     (123)     4251 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/codediff.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3389 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/codediff_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/flax_module.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.708545 flax-0.7.0/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (123)      309 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_static/css/flax_theme.css
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.708545 flax-0.7.0/docs/_templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_templates/autosummary/
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_templates/autosummary/flax_module.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/api_reference/
+-rw-r--r--   0 runner    (1001) docker     (123)      130 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.config.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.core.frozen_dict.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.errors.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.jax_utils.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/api_reference/flax.linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     1143 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/activation_functions.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/decorators.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      350 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      232 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/init_apply.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2220 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/layers.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      278 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/module.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/profiling.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      931 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/spmd.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      566 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/transformations.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      213 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/variable.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      480 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.serialization.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.struct.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      275 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.traceback_util.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.training.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      798 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.traverse_util.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      250 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5080 2023-07-08 01:21:24.000000 flax-0.7.0/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7338 2023-07-08 01:21:24.000000 flax-0.7.0/docs/conf_sphinx_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11920 2023-07-08 01:21:24.000000 flax-0.7.0/docs/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/developer_notes/
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/lift.md
+-rw-r--r--   0 runner    (1001) docker     (123)    22018 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/module_lifecycle.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_community_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4422 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_core_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    22579 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_google_research_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2028 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_repositories_that_use_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    20991 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flax.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/flip/
+-rw-r--r--   0 runner    (1001) docker     (123)      648 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/0000-template.md
+-rw-r--r--   0 runner    (1001) docker     (123)    17256 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/1009-optimizer-api.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8189 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/1777-default-dtype.md
+-rw-r--r--   0 runner    (1001) docker     (123)    10424 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/2434-general-metadata.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4099 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/2974-kw-only-dataclasses.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1404 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)   102681 2023-07-08 01:21:24.000000 flax-0.7.0/docs/getting_started.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    16394 2023-07-08 01:21:24.000000 flax-0.7.0/docs/getting_started.md
+-rw-r--r--   0 runner    (1001) docker     (123)     6661 2023-07-08 01:21:24.000000 flax-0.7.0/docs/glossary.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/docs/guides/
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/arguments.md
+-rw-r--r--   0 runner    (1001) docker     (123)     9149 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/batch_norm.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9329 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/convert_pytorch_to_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11050 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/dropout.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10488 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/ensembling.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    12735 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/extracting_intermediates.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    40409 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    22504 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_basics.md
+-rw-r--r--   0 runner    (1001) docker     (123)    63884 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_on_pjit.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    24655 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_on_pjit.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7442 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/full_eval.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    20294 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/haiku_migration_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      265 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      225 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_converting_and_upgrading.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       80 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_data_preprocessing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_flax_fundamentals.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_model_inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_parallel_training.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      152 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_training_techniques.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    38261 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/jax_for_the_impatient.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    21301 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/jax_for_the_impatient.md
+-rw-r--r--   0 runner    (1001) docker     (123)    17271 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/linen_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8175 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/lr_schedule.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     7142 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/model_surgery.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/model_surgery.md
+-rw-r--r--   0 runner    (1001) docker     (123)    10621 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/optax_update_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9077 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/orbax_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3984 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/regular_dict_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6129 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/rnncell_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3125 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/setup_or_nncompact.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6076 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/state_params.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11487 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/transfer_learning.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8006 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/transfer_learning.md
+-rw-r--r--   0 runner    (1001) docker     (123)    53165 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/use_checkpointing.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    26620 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/use_checkpointing.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8273 2023-07-08 01:21:24.000000 flax-0.7.0/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8090 2023-07-08 01:21:24.000000 flax-0.7.0/docs/mission.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/docs/notebooks/
+-rw-r--r--   0 runner    (1001) docker     (123)     7391 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/flax_sharp_bits.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     5941 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/flax_sharp_bits.md
+-rw-r--r--   0 runner    (1001) docker     (123)    18559 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/full_eval.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/full_eval.md
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    40759 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/linen_intro.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    21840 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/linen_intro.md
+-rw-r--r--   0 runner    (1001) docker     (123)    21338 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/optax_update_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    11407 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/optax_update_guide.md
+-rw-r--r--   0 runner    (1001) docker     (123)    13947 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/orbax_upgrade_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8587 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/orbax_upgrade_guide.md
+-rw-r--r--   0 runner    (1001) docker     (123)     9512 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/state_params.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     6687 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/state_params.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2510 2023-07-08 01:21:24.000000 flax-0.7.0/docs/overview.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7604 2023-07-08 01:21:24.000000 flax-0.7.0/docs/philosophy.md
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-07-08 01:21:24.000000 flax-0.7.0/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)      743 2023-07-08 01:21:24.000000 flax-0.7.0/examples/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/examples/cloud/
+-rw-r--r--   0 runner    (1001) docker     (123)     4635 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8965 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/launch_gce.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1650 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/startup_script.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/imagenet/
+-rw-r--r--   0 runner    (1001) docker     (123)     9387 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/imagenet/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     2192 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1670 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/tpu.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/v100_x8.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/v100_x8_mixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (123)   294627 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     3360 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2235 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet_fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8204 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2175 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4529 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1971 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    12701 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/linen_design_test/
+-rw-r--r--   0 runner    (1001) docker     (123)     6470 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/attention_simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/dense.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1346 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/linear_regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2385 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_explicit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1996 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_inline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1926 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_lazy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1540 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/tied_autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2270 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/weight_std.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     3443 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12698 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2240 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12495 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     4969 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5618 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20239 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/mnist/
+-rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/mnist/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)      912 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    99011 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/mnist.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     2366 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/mnist_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      298 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     5301 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2226 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/nlp_seq/
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8010 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3868 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6643 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)       60 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    14128 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/train.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ogbg_molpcba/
+-rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ogbg_molpcba/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1551 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/default_graph_net.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/hparam_sweep.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8256 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2554 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6891 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5223 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)  1111228 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     4841 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      329 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    13830 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12218 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ppo/
+-rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2593 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/agent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ppo/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1954 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/env_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2252 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13175 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5281 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_lib_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)      147 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     8715 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/seed_rl_atari_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1895 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/test_episodes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/seq2seq/
+-rw-r--r--   0 runner    (1001) docker     (123)      913 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     5558 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4361 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    25401 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/seq2seq.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     7095 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3269 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/sst2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/README.md
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2031 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/build_vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/sst2/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/configs/default.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10057 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3528 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2168 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14299 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3558 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      156 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     9117 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/sst2.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)   117898 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/vocab.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4421 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/vae/
+-rw-r--r--   0 runner    (1001) docker     (123)      593 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1458 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1775 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2152 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/reconstruction.png
+-rw-r--r--   0 runner    (1001) docker     (123)      113 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/vae/results/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/results/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)    43139 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/sample.png
+-rw-r--r--   0 runner    (1001) docker     (123)     4574 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/examples/wmt/
+-rw-r--r--   0 runner    (1001) docker     (123)     6106 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7394 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/bleu.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/examples/wmt/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     3482 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15118 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/decode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13174 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3172 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2216 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19064 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)      398 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     5619 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23635 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/flax/
+-rw-r--r--   0 runner    (1001) docker     (123)      908 2023-07-08 01:21:24.000000 flax-0.7.0/flax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4062 2023-07-08 01:21:24.000000 flax-0.7.0/flax/configurations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/axes_scan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10412 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/flax_functional_engine.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9418 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/frozen_dict.py
+-rw-r--r--   0 runner    (1001) docker     (123)    56616 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/lift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11617 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/meta.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax/core/nn/
+-rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18626 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12538 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7164 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1503 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2539 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/partial_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35812 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/scope.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/tracers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/variables.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29794 2023-07-08 01:21:24.000000 flax-0.7.0/flax/errors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-07-08 01:21:24.000000 flax-0.7.0/flax/ids.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5316 2023-07-08 01:21:24.000000 flax-0.7.0/flax/io.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11483 2023-07-08 01:21:24.000000 flax-0.7.0/flax/jax_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4014 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2559 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19211 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3236 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/combinators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/dtypes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/linen/experimental/
+-rw-r--r--   0 runner    (1001) docker     (123)    11542 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/experimental/layers_with_named_axes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2619 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7528 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/kw_only_dataclasses.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32979 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)    87710 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/module.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21102 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19763 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5480 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38078 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/recurrent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11368 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19162 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)    59683 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      582 2023-07-08 01:21:24.000000 flax-0.7.0/flax/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7727 2023-07-08 01:21:24.000000 flax-0.7.0/flax/metrics/tensorboard.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/oss/
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-07-08 01:21:24.000000 flax-0.7.0/flax/oss/ .git-blame-ignore-revs
+-rw-r--r--   0 runner    (1001) docker     (123)       58 2023-07-08 01:21:24.000000 flax-0.7.0/flax/py.typed
+-rw-r--r--   0 runner    (1001) docker     (123)    14472 2023-07-08 01:21:24.000000 flax-0.7.0/flax/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7332 2023-07-08 01:21:24.000000 flax-0.7.0/flax/struct.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-08 01:21:24.000000 flax-0.7.0/flax/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-07-08 01:21:24.000000 flax-0.7.0/flax/testing/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-07-08 01:21:24.000000 flax-0.7.0/flax/traceback_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/flax/training/
+-rw-r--r--   0 runner    (1001) docker     (123)      613 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43475 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3681 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5843 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/dynamic_scale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2718 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/lr_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2722 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/orbax_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2971 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/prefetch_iterator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/train_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13392 2023-07-08 01:21:24.000000 flax-0.7.0/flax/traverse_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      651 2023-07-08 01:21:24.000000 flax-0.7.0/flax/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    11022 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        5 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/images/
+-rw-r--r--   0 runner    (1001) docker     (123)    80407 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    15137 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo_250px.png
+-rw-r--r--   0 runner    (1001) docker     (123)    29095 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo_500px.png
+-rw-r--r--   0 runner    (1001) docker     (123)    14116 2023-07-08 01:21:24.000000 flax-0.7.0/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (123)     4607 2023-07-08 01:21:24.000000 flax-0.7.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-08 01:21:39.768545 flax-0.7.0/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)    17587 2023-07-08 01:21:24.000000 flax-0.7.0/tests/checkpoints_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-08 01:21:24.000000 flax-0.7.0/tests/colab_tpu_jax_version.ipynb
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/tests/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     5087 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_frozen_dict_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8005 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_lift_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9408 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_scope_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.764545 flax-0.7.0/tests/core/design/
+-rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_auto_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2815 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_big_resnets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_custom_vjp_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4457 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_dense_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2630 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_flow_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4694 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_resnet_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2541 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_scan_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2172 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_tied_autoencoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_vmap_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_weight_std_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      887 2023-07-08 01:21:24.000000 flax-0.7.0/tests/download_dataset_metadata.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-07-08 01:21:24.000000 flax-0.7.0/tests/early_stopping_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-07-08 01:21:24.000000 flax-0.7.0/tests/import_test.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8234 2023-07-08 01:21:24.000000 flax-0.7.0/tests/io_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3473 2023-07-08 01:21:24.000000 flax-0.7.0/tests/jax_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.764545 flax-0.7.0/tests/linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     1992 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/initializers_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4355 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/kw_only_dataclasses_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_activation_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7748 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5370 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_combinators_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1590 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_dtypes_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35451 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_linear_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6355 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    65434 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_module_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16404 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_recurrent_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16481 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    54451 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_transforms_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17800 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/partitioning_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18567 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/summary_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2403 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/toplevel_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3960 2023-07-08 01:21:24.000000 flax-0.7.0/tests/run_all_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (123)    15319 2023-07-08 01:21:24.000000 flax-0.7.0/tests/serialization_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2371 2023-07-08 01:21:24.000000 flax-0.7.0/tests/struct_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14163 2023-07-08 01:21:24.000000 flax-0.7.0/tests/tensorboard_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6027 2023-07-08 01:21:24.000000 flax-0.7.0/tests/traceback_util_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10482 2023-07-08 01:21:24.000000 flax-0.7.0/tests/traverse_util_test.py
```

### Comparing `flax-0.6.9/.github/ISSUE_TEMPLATE/bug_report.md` & `flax-0.7.0/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/analytics/README.md` & `flax-0.7.0/.github/analytics/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/analytics/get_repo_metrics.py` & `flax-0.7.0/.github/analytics/get_repo_metrics.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/analytics/issue_activity_since_date.gql` & `flax-0.7.0/.github/analytics/issue_activity_since_date.gql`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/analytics/pr_data_query.gql` & `flax-0.7.0/.github/analytics/pr_data_query.gql`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/pull_request_template.md` & `flax-0.7.0/.github/pull_request_template.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.github/workflows/build.yml` & `flax-0.7.0/.github/workflows/build.yml`

 * *Files 3% similar despite different names*

```diff
@@ -23,15 +23,15 @@
         with:
           access_token: ${{ github.token }}
   pre-commit:
     name: Test pre-commit hooks
     runs-on: ubuntu-latest
     steps:
       - uses: actions/checkout@v3
-      - name: Set up Python 3.8
+      - name: Set up Python
         uses: actions/setup-python@v4
         with:
           python-version: 3.9
       - uses: pre-commit/action@v2.0.3
   commit-count:
     name: Check commit count
     runs-on: ubuntu-latest
@@ -50,24 +50,24 @@
         git fetch origin --unshallow $GITHUB_REF:commit-count
         git fetch origin main
         diff=$(git rev-list --count origin/main...commit-count)
         # $GITHUB_REF adds an additional commit to the commit tree, so $diff is
         # one too high when executing this as a Github Action.
         if (( $diff > 6)); then
           echo "ERROR! More than 5 commits in PR -- please squash your commits."
-          url=https://flax.readthedocs.io/en/latest/contributing.html#too-many-commits-in-a-pr
+          url=https://flax.readthedocs.io/en/latest/contributing.html#too-many-commits-in-a-pull-request
           echo "See $url for help on how to resolve this."
           exit 1
         fi
   test-import:
     name: Test import standalone
     runs-on: ubuntu-latest
     strategy:
       matrix:
-        python-version: ['3.8', '3.9', '3.10']
+        python-version: ['3.9', '3.10', '3.11']
     steps:
     - uses: actions/checkout@v3
     - name: Set up Python ${{ matrix.python-version }}
       uses: actions/setup-python@v4
       with:
         python-version: ${{ matrix.python-version }}
     - name: Install standalone dependencies only
@@ -78,23 +78,23 @@
         python -c "import flax"
   tests:
     name: Run Tests
     needs: [cancel-previous, pre-commit, commit-count, test-import]
     runs-on: ubuntu-20.04-16core
     strategy:
       matrix:
-        python-version: ['3.8', '3.9', '3.10']
+        python-version: ['3.9', '3.10', '3.11']
         test-type: [doctest, pytest, pytype, mypy]
         exclude:
           - test-type: pytype
-            python-version: 3.8
+            python-version: '3.11'
           - test-type: pytype
-            python-version: 3.10
+            python-version: '3.10'
           - test-type: mypy
-            python-version: 3.8
+            python-version: '3.11'
     steps:
     - uses: actions/checkout@v3
     - name: Set up Python ${{ matrix.python-version }}
       id: setup_python
       uses: actions/setup-python@v4
       with:
         python-version: ${{ matrix.python-version }}
@@ -102,15 +102,15 @@
       id: date_key
       run: echo "DATE=$(date +%j)" >> $GITHUB_OUTPUT
     - name: Cached virtual environment
       id: venv_cache
       uses: actions/cache@v3
       with:
         path: venv
-        key: pip-${{ steps.setup_python.outputs.python-version }}-${{ steps.date_key.outputs.DATE }}-${{ hashFiles('**/requirements.txt', 'setup.py') }}
+        key: pip-${{ steps.setup_python.outputs.python-version }}-${{ steps.date_key.outputs.DATE }}-${{ hashFiles('**/requirements.txt', 'pyproject.toml') }}
     - name: Install Dependencies for cache
       if: steps.venv_cache.outputs.cache-hit != 'true'
       run: |
         if [ -d "venv" ]; then rm -rf venv; fi
         python3 -m venv venv
         venv/bin/python3 -m pip install .[all]
         venv/bin/python3 -m pip install .[testing]
```

### Comparing `flax-0.6.9/.github/workflows/pythonpublish.yml` & `flax-0.7.0/.github/workflows/pythonpublish.yml`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.pre-commit-config.yaml` & `flax-0.7.0/.pre-commit-config.yaml`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/.readthedocs.yml` & `flax-0.7.0/.readthedocs.yml`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 # .readthedocs.yml
 # Read the Docs configuration file
 # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details
 
 # Required
 version: 2
 
+build:
+  os: ubuntu-22.04
+  tools:
+    python: "3.9"
+
 # Build documentation in the docs/ directory with Sphinx
 sphinx:
   configuration: docs/conf.py
 
 # Optionally build your docs in additional formats such as PDF and ePub
 formats:
   - htmlzip
   - epub
   - pdf
 
 # Optionally set the version of Python and requirements required to build your docs
 python:
-  version: 3.8
   install:
     - requirements: docs/requirements.txt
```

### Comparing `flax-0.6.9/CHANGELOG.md` & `flax-0.7.0/CHANGELOG.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 Changelog
 ----------
 
 vNext
 ------
 (Add your change to a random empty line to avoid merge conflicts)
 -
-- Rudimentary quantization support: some layers can be parametrized with custom dot_general and conv_general_dilated.
 -
 -
 -
 -
 -
 -
 -
@@ -20,14 +19,33 @@
 -
 -
 -
 -
 -
 -
 
+0.7.0
+-----
+- RNNCellBase refactor.
+
+0.6.11
+-----
+- Set Orbax-as-backend to be the default checkpointing method.
+- Fix setup trigger issue under sharing and transforms.
+- Add collection to self.scope.reserve(name, col) so that sow works with the same name in different collections.
+- Minor improvements for Sequential.
+- Improve the error message in MultiHeadDotProductAttention.
+- Allow manually specifying the rng key for Dropout.
+- RNN refactor.
+- fixed missing separator for rng fold in.
+
+0.6.10
+-----
+- Rudimentary quantization support: some layers can be parametrized with custom dot_general and conv_general_dilated.
+
 0.6.9
 -----
 - Depend on `orbax-checkpoint` package instead of `orbax`.
 - Refactored setup scripts to `project.toml`.
 - Added pretty_repr utility fn.
 - Fix get_partition_spec on replicated array.
 - Updates imagenet.ipynb to use GPU Colab runtime, and fixed config.
@@ -105,19 +123,19 @@
 - Added guides for: Flax the Sharp Bits, Checkpointing, Extracting Gradients
 - Improved existed documentation pages.
 - Improved errors, error messages and tests.
 - Removed codebase's trailing whitespaces.
 
 Bug fixes:
 - Fixes launch_gce.sh with imagenet example.
-- Properly report AttributeErrors from descriptors .
+- Properly report AttributeErrors from descriptors.
 - Fixes usages of `pmap`.
 - Return None if no _parent_ref is set.
 - Cap dynamic scale to float32 max.
-- no-op when double wrapping with struct.dataclass
+- no-op when double wrapping with struct.dataclass.
 - Allow variable_with_axes to have empty axes when axes is set to an empty tuple.
 - Don't create reference cycles among Modules.
 
 0.6.1
 -----
 - Adds axis_name and axis_index_groups to LayerNorm and GroupNorm. by @copybara-service in [#2402](https://github.com/google/flax/pull/2402)
 - Plumb spmd_axis_name through transforms.vmap through to JAX vmap by @copybara-service in [#2398](https://github.com/google/flax/pull/2398)
@@ -148,15 +166,15 @@
 Bug fixes:
 - Fixed variable aliasing in put_variable
 - Fixed missing passthrough of nn.scan unroll arg
 - Fixed the MNIST example
 
 0.5.2
 -----
-- Fixes missing PyYAML dependecy.
+- Fixes missing PyYAML dependency.
 
 0.5.1
 -----
 New features:
 - Added `nn.tabulate` and `Module.tabulate` to generate rich representations of the network structure.
 
 0.5.0
@@ -168,15 +186,15 @@
   complex numbers to their real component by default. Instead the complex dtype is preserved by default.
 
 
 Bug fixes:
 - Fix support for JAX's experimental_name_stack.
 
 Breaking changes:
-- In rare cases the dtype of a layer can change due to  [default dtype FLIP](https://github.com/google/flax/blob/main/docs/flip/1777-default-dtype.md). See the "Backward compatibility" section of the proposal for more information.
+- In rare cases the dtype of a layer can change due to [default dtype FLIP](https://github.com/google/flax/blob/main/docs/flip/1777-default-dtype.md). See the "Backward compatibility" section of the proposal for more information.
 
 0.4.2
 -----
 
 New features:
 - Add lifted conditional `nn.cond`.
 - Improved error messages: parameters not found, loading checkpoints.
@@ -305,15 +323,15 @@
 
 NOTE: You must now explicitly import `flax.nn` if you want to use the old
       pre-Linen `flax.nn.Module`.
 
 0.3.1
 ------
 
-Many improvements to Linen, and the old `flax.nn` is officially reprecated!
+Many improvements to Linen, and the old `flax.nn` is officially deprecated!
 
 Notably, there's a clean API for extracting intermediates from modules
 defined using `@nn.compact`, a more ergonomic API for using Batch Norm and Dropout in modules
 defined using `setup`, support for `MultiOptimizer` with Linen, and multiple safety, performance
 and error message improvements.
 
 Possible breaking changes:
@@ -323,28 +341,28 @@
    is enforced by raising a TypeError in `__setattr__` after `setup`.
  - Pytrees of dicts and lists are transformed into FrozenDict and tuples during
    attribute assignment.
    This avoids undetected submodules and inner state.
  - Bug Fix `flax.core.apply` and `Module.apply`. Now it returns a tuple
    containing the output and a frozen empty
    collection when `mutable` is specified as an empty list.
- - `broadcast_dims` is now a attribute to `Dropout` instead of a `__call__`
+ - `broadcast_dims` is now an attribute to `Dropout` instead of a `__call__`
    argument.
  - `use_running_average` and `deterministic` no longer have a default. They
    should be passed explicitly
  - Bug Fix `Scope.variable` mutability check, before a variable could only be
    initialized if the 'params' collection was mutable.
 
 Other Improvements:
  - Re-introduced the `lm1b` language modeling example
  - Recognizes batch free inputs in pooling layers. (for use with vmap)
  - Add Adadelta optimizer
  - Fully deprecate all "pre-Linen" `flax.nn` classes and methods.
  - Some Module arguments can now be passed either as dataclass attribute or
-   as argument to `__call__`. See [design note](https://flax.readthedocs.io/en/latest/design_notes/arguments.html)
+   as argument to `__call__`. See [design note](https://flax.readthedocs.io/en/latest/guides/arguments.html)
  - Add `sow` method to `Module` and `capture_intermediates` argument to `Module.apply`.
    See [howto](https://flax.readthedocs.io/en/latest/howtos/extracting_intermediates.html) for usage patterns.
  - Support passing in modules directly as attributes to other modules, and
    deal with them correctly both in top-level modules and in submodules.
  - Don't require the `variable` argument to `Module.apply` to be a FrozenDict
  - Add support for dict/FrozenDict when using `ModelParamTraversal`
    As a result `MultiOptimizer` can be used properly with linen modules.
```

### Comparing `flax-0.6.9/LICENSE` & `flax-0.7.0/LICENSE`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/PKG-INFO` & `flax-0.7.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.6.9
+Version: 0.7.0
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -18,15 +18,15 @@
 
 <div align="center">
 <img src="https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png" alt="logo"></img>
 </div>
 
 # Flax: A neural network library and ecosystem for JAX designed for flexibility
 
-![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/github/google/flax)](https://codecov.io/github/google/flax)
+![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/gh/google/flax)](https://codecov.io/gh/google/flax)
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
@@ -211,15 +211,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.6.9},
+  version = {0.7.0},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.6.9/README.md` & `flax-0.7.0/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 <div align="center">
 <img src="https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png" alt="logo"></img>
 </div>
 
 # Flax: A neural network library and ecosystem for JAX designed for flexibility
 
-![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/github/google/flax)](https://codecov.io/github/google/flax)
+![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/gh/google/flax)](https://codecov.io/gh/google/flax)
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
@@ -193,15 +193,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.6.9},
+  version = {0.7.0},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.6.9/dev/.devcontainer/Dockerfile` & `flax-0.7.0/dev/.devcontainer/Dockerfile`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/dev/.devcontainer/devcontainer.json` & `flax-0.7.0/dev/.devcontainer/devcontainer.json`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/dev/README.md` & `flax-0.7.0/dev/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/dev/update_requirements.py` & `flax-0.7.0/dev/update_requirements.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/Makefile` & `flax-0.7.0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/README.md` & `flax-0.7.0/docs/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/_ext/codediff.py` & `flax-0.7.0/docs/_ext/codediff.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/_ext/codediff_test.py` & `flax-0.7.0/docs/_ext/codediff_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/advanced_topics/arguments.md` & `flax-0.7.0/docs/guides/arguments.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-# Dealing with Module Arguments
+# Dealing with Flax Module arguments
 
 ## Introduction
 
-In Linen we can define `Module` arguments either as dataclass attributes or as arguments to methods (usually `__call__`).
+In Flax Linen we can define `Module` arguments either as dataclass attributes or as arguments to methods (usually `__call__`).
 Typically the distinction is clear:
 * Completely fixed properties, such as the choice of kernel initializer or number of output features, are hyperparameters and should be defined as dataclass attributes. Typically two Module instances with different hyperparamaters cannot share in a meaningful way.
 * Dynamic properties, such as input data and top-level "mode switches" like `train=True/False`, should be passed as arguments to `__call__` or another method.
 
 Some cases are however less clear cut. Take for example the `Dropout` module.
 We have a number of clear hyperparameters:
```

### Comparing `flax-0.6.9/docs/advanced_topics/contributing.md` & `flax-0.7.0/docs/contributing.md`

 * *Files 18% similar despite different names*

```diff
@@ -1,66 +1,70 @@
 # How to contribute
 
-Everyone can contribute to Flax, and we value everyone's contributions.
+Everyone can contribute to Flax, and the Flax development team values everyone's contributions!
 You can contribute in many more ways than just writing code. Answering questions
-on our [Discussions page](https://github.com/google/flax/discussions), helping
-each other, and improving our documentation are extremely valuable to our
+on the [Flax GitHub Discussions page](https://github.com/google/flax/discussions), helping
+each other, and improving Flax documentation are extremely valuable to the Flax
 ecosystem.
 
-We also appreciate if you spread the word, for instance by starring our GitHub
-repo, or referencing Flax in blog posts of projects that used it.
+We also appreciate if you spread the word, for instance by starring the [Flax GitHub repository](https://github.com/google/flax),
+or referencing Flax in blog posts of projects that used it.
 
 This project follows
 [Google's Open Source Community Guidelines](https://opensource.google/conduct/).
 
 ## Ways to contribute
 
 We welcome pull requests (PRs), in particular for those issues
 [marked as PR-ready](https://github.com/google/flax/issues?q=is%3Aopen+is%3Aissue+label%3A%22Status%3A+pull+requests+welcome%22).
 For other proposals, you should first open a GitHub Issue or a GitHub Discussion to
 start a conversation about your planned contribution.
 
-## Contributing code using Pull Requests
+## Contributing code using pull requests
 
-We do all of our development using git, so basic knowledge is assumed.
+The Flax development team performs all development using [Git](https://git-scm.com/). To contribute,
+you should have basic knowledge of [Git](https://git-scm.com/) and [GitHub](https://docs.github.com).
+(You can learn how to set up Git by following Git's official
+[Getting Started - First-Time Git Setup](https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup)
+and GitHub's [Set Up Git](https://docs.github.com/en/get-started/quickstart/set-up-git) guides.)
 
-Follow these steps to contribute code:
+To contribute code to Flax on GitHub, follow these steps:
 
-### Create a Pull Request in your own branch
+### To create a pull request from a fork
 
-1. Fork the Flax repository by clicking the 'Fork' button on the
-   [repository page](http://www.github.com/google/flax). This creates a copy
-   of the Flax repository in your own account.
+1. Using GitHub's web UI, fork the Flax repository by clicking the 'Fork' button on the
+   [`github.com/google/flax` repository page](http://www.github.com/google/flax). This creates a
+   fork (a copy) of the Flax repository in your own GitHub.
 
-2. Install [Python >=3.6](https://www.python.org/downloads/).
+   Reference: [Creating a pull request from a fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork).
+
+2. Install [Python >=3.7](https://www.python.org/downloads/).
 
 3. (Optional) Create a virtual environment or a Docker container. See
    [`dev/README.md`](https://github.com/google/flax/blob/main/dev/README.md)
    for details on how to set up a Docker Container. To set up a virtual environment,
    run the following:
 
    ```bash
    python3 -m virtualenv env
    . env/bin/activate
    ```
 
    This ensures all your dependencies are installed in this environment.
 
-4. Clone your local forked Flax repo, then install the required packages with [PyPi](https://pip.pypa.io/en/stable/cli/pip_install/).
-   This enables you to immediately test the code after modifying it:
+4. Clone your local forked Flax repo with `git clone`. Then, install the required packages
+   with [PyPi](https://pip.pypa.io/en/stable/cli/pip_install/). This enables you to immediately
+   test the code after modifying it:
 
    ```bash
    git clone https://github.com/YOUR_USERNAME/flax
    cd flax
-   pip install -e .
-   pip install ".[testing]"
+   pip install -e .[all]
+   pip install -e .[testing]
    pip install -r docs/requirements.txt
-   # install in editable mode again because docs/requirements.txt
-   # reinstalls project in non-editable mode
-   pip install -e .
    ```
 
 5. Set up pre-commit hooks, this will run some automated checks during each `git` commit and
    possibly update some files that require changes.
 
    ```bash
    pip install pre-commit
@@ -71,175 +75,186 @@
    changes.
 
    ```bash
    git remote add upstream http://www.github.com/google/flax
    ```
 
 
-7. Create a branch where you will develop from:
+7. Create a branch, such as `my_development_branch`, you will develop from:
 
    ```bash
-   git checkout -b name-of-change
+   git checkout -b my_development_branch
    ```
 
 8. Implement your changes using your favorite editor (we recommend
    [Visual Studio Code](https://code.visualstudio.com/)).
 
    Make sure the tests pass by running the following command from the top of
    the repository:
 
    ```bash
    ./tests/run_all_tests.sh
    ```
 
-9.  Once your change is done, create a commit as follows
-   ([how to write a commit message](https://chris.beams.io/posts/git-commit/)):
+9. Once you finish making changes, don't forget to create commits
+   ([learn how to write a commit message](https://chris.beams.io/posts/git-commit/)):
 
    ```bash
    git add file1.py file2.py ...
+   # or use `git add .` to add all changed files
    git commit -m "Your commit message"
    ```
 
-   Then sync your code with the main repo:
+   Then sync your code with the main repository:
 
    ```bash
    git rebase upstream/main
    ```
 
-10. Finally push your commit on your development branch and create a remote
-   branch in your fork that you can use to create a Pull Request form:
+10. Finally, push your commit on your `my_development_branch`, and create a remote
+   branch in your fork that you can use to create a pull request from:
 
    ```bash
-   git push --set-upstream origin name-of-change
+   git push --set-upstream origin my_development_branch
    ```
 
-   After running the command, you should see a GitHub link in your terminal output that you can click on to create a Pull Request.
-   If you do not see this link in the terminal after doing a `git push`, go to the GitHub web UI; there should be a button there that lets you turn the commit into a Pull Request yourself.
+   After running the command, you should get a GitHub link in your (VS Code) terminal output for creating a pull request.
+   If you don't receive a link after `git push`, use the [GitHub web UI](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request?tool=webui) to create a pull request.
 
-11. Make sure your PR passes the
-   [PR checklist](https://github.com/google/flax/blob/main/.github/pull_request_template.md#checklist).
-   If so, create a Pull Request from the Flax repository and send it for review.
+11. Make sure your pull request passes the
+   [Flax PR checklist](https://github.com/google/flax/blob/main/.github/pull_request_template.md#checklist).
+   If so, create a pull request from the Flax repository and send it for review.
    Consult [GitHub Help](https://help.github.com/articles/about-pull-requests/)
-   for more information on using Pull Requests.
+   for more information on using pull requests.
+
+You can learn more in GitHub's [Creating a pull request from a fork
+](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork). documentation.
 
-### Update notebooks
+### Updating Jupyter Notebooks
 
 We use [jupytext](https://jupytext.readthedocs.io/) to maintain two synced copies of docs
 in `docs/notebooks`: one in the Jupyter Notebook (`.ipynb`) format, and one in Markdown (`.md`).
 
 The former can be opened and executed directly in [Google Colab](https://colab.research.google.com/).
 Markdown makes it easier to track changes/diffs within version control and, for example, GitHub
 web UI, since `.ipynb` files are based on JSON.
 
-**NOTE**: If your notebook contains a cell that uses `pip` to install a package
-you must add a `skip-execution` tag to that cell so `myst-nb` will skip the cell
-when testing the notebooks.
-
 #### Editing Jupyter Notebooks (`.ipynb`)
 
 For making large changes that substantially modify code and outputs, it's recommended to edit
 the notebooks in [Jupyter](https://jupyter.org/install) or in [Colab](https://colab.research.google.com/).
 
 If you choose to work in Colab, go to **File** and click **Upload notebook**, then pick your file.
 After loading it into Colab and editing it, make sure you run the cells, and that there aren't any errors.
 Click on **Runtime**, then select **Run all**. After you finish, click **File** > **Download** > **Download ipynb**.
 You may also want to test that the file executes properly by using `sphinx-build`, as explained above.
 
+After you make changes in your Jupyter Notebook, follow the steps _Syncing notebooks_ below.
+
 #### Editing Markdown files (`.md`)
 
 For making smaller changes to the text content of the notebooks, it is easiest to edit the
 `.md` versions using a text editor.
 
+After you make changes in your Markdown file, follow the steps _Syncing notebooks_ below.
+
 #### Syncing notebooks
 
 After editing either the `.ipynb` or `.md` versions of the docs, sync the two versions
 using [jupytext](https://jupytext.readthedocs.io/) by running `jupytext --sync` on the updated
-notebooks
+notebooks.
 
-First, make sure you have jupytext (version 1.13.8) installed. The jupytext version should match
-the one specified in [.pre-commit-config.yaml](https://github.com/google/flax/blob/main/.pre-commit-config.yaml).
+First, make sure you have jupytext installed. The jupytext version should match
+the one specified in [.pre-commit-config.yaml](https://github.com/google/flax/blob/main/.pre-commit-config.yaml)
+(currently, it is v1.13.8).
 
-```
+```bash
 pip install jupytext==1.13.8
 ```
 
-Then, if you worked on a Jupyter Notebook document, sync the contents with its Markdown-equivalent
+Then, after you have made your changes in the Jupyter Notebook, sync the contents with its Markdown-equivalent
 file by running the following command:
 
-```
+```bash
 jupytext --sync path/to/the/file.ipynb
 ```
 
 Similarly, to sync your Markdown file with its Jupyter Notebook version, run:
 
-```
+```bash
 jupytext --sync path/to/the/file.md
 ```
 
-To check that the `.md` and `.ipynb` files are properly synced, you can also use the
-[pre-commit](https://pre-commit.com/) framework to perform the same checks used
-in the GitHub CI:
+Note that if you receive an error, and it is the first time you worked in a Jupyter Notebook, you may need
+to (re)create a synced copy of the document (which is explained in detail in _Creating new notebooks_ section below):
 
+```bash
+jupytext --set-formats ipynb,md:myst path/to/the/notebook.ipynb
 ```
+
+Once you're finished with syncing the `.md` and `.ipynb` files, you can check that they are properly synced using the
+[pre-commit](https://pre-commit.com/) framework to perform the same checks used
+in the Flax GitHub CI:
+
+```bash
 git add docs -u  # pre-commit runs on files in git staging.
 pre-commit run jupytext
 ```
 
 #### Creating new notebooks
 
 If you are adding a new Jupyter Notebook to the documentation, you can use `jupytext --set-formats`.
 It can set up both the Jupyter Notebook (`.ipynb`) and Markdown (`.md`) versions of the file:
 
-```
+```bash
 jupytext --set-formats ipynb,md:myst path/to/the/notebook.ipynb
 ```
 
 This works by adding a `"jupytext"` metadata field to the notebook file which specifies the
 desired formats. The `jupytext --sync` command can then recognize them when invoked.
 
 After you make changes in your file(s), follow the steps from the _Syncing notebooks_
 section above to keep the contents of both Markdown and Jupyter Notebook files in sync.
 
-#### Notebooks within the sphinx build
+#### Notebooks within the Sphinx build
 
 Some of the notebooks are built automatically as part of the pre-submit checks and
 as part of the [Read the Docs](https://flax.readthedocs.io/en/latest) build.
 The build will fail if cells raise errors. If the errors are intentional, you can either catch them,
 or tag the cell with `raises-exceptions` metadata ([example PR](https://github.com/google/jax/pull/2402/files)).
 You have to add this metadata by hand in the `.ipynb` file. It will be preserved when somebody else
 re-saves the notebook.
 
 We exclude some notebooks from the build because, for example, they contain long computations.
 See `exclude_patterns` in [`conf.py`](https://github.com/google/flax/blob/main/docs/conf.py).
 
-### Updating the Pull Request contents
+### Updating the pull request contents
 
-Every Pull Request should ideally be limited to just one commit, so if you have multiple commits please squash them.
+Every pull request should ideally be limited to just one commit, so if you have multiple commits please squash them.
 
-Assuming you now have only one commit in your Pull Request, and want to add changes requested during review:
+Assuming you now have only one commit in your pull request, and want to add changes requested during review:
 
 1. Make the changes locally in your editor.
 2. Run `git commit -a --amend`. This updates the commit contents and allows you to edit the commit message.
 3. At this point, `git push` alone will result in an error. Instead, use `git push --force`.
 4. Check that it's done: The changes to your commit should be immediately reflected in the Github web UI.
 
 ## Troubleshooting
 
-### Too many commits in a PR
-
-If your PR has too many commits associated with it, then our build process may
-fail with an error message. This is because of two reasons:
+### Too many commits in a pull request
 
-* We prefer to keep our commit history clean.
+If your PR has too many commits associated with it (for example, more than five),
+you need to squash them. Otherwise, the Flax docs build process may fail with an
+error message. This is because of the following reasons:
 
-* Our source sync process will fail if our commit tree is too large.
+* There are more than five commits in your pull request; and
+* The Flax source sync process fails when the commit tree is too large.
 
-If you encounter this error message, you should squash your commits. To
-rebase your branch to `main` and create a new commit containing all your
-changes, run the following command:
+To squash your commits, you can rebase your branch to `main` and create a new
+commit containing all your changes, run the following command:
 
 ```bash
 git rebase main && git reset --soft main && git commit
 ```
 
 This will apply all your changes to the main branch. Note that if you had to
 resolve any conflicts while working on your change (for instance, you did a
```

### Comparing `flax-0.6.9/docs/advanced_topics/convert_pytorch_to_flax.rst` & `flax-0.7.0/docs/guides/convert_pytorch_to_flax.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-Convert PyTorch Models to Flax
+Convert PyTorch models to Flax
 ==============================
 
 .. testsetup::
 
   import numpy as np
   import jax
   from jax import random, numpy as jnp
@@ -38,15 +38,15 @@
   j_fc = nn.Dense(features=4)
   j_out = j_fc.apply(variables, x)
 
   t_x = torch.from_numpy(np.array(x))
   t_out = t_fc(t_x)
   t_out = t_out.detach().cpu().numpy()
 
-  np.testing.assert_almost_equal(j_out, t_out)
+  np.testing.assert_almost_equal(j_out, t_out, decimal=6)
 
 
 Convolutions
 --------------------------------
 
 Let's now look at 2D convolutions. PyTorch uses the NCHW format and Flax uses NHWC.
 Consequently, the kernels will have different shapes. The kernel in PyTorch has shape [outC, inC, kH, kW]
@@ -201,15 +201,15 @@
 
   # [N, H, W, C] -> [N, C, H, W]
   t_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))
   t_out = t_bn(t_x)
   # [N, C, H, W] -> [N, H, W, C]
   t_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))
 
-  np.testing.assert_almost_equal(j_out, t_out)
+  np.testing.assert_almost_equal(j_out, t_out, decimal=6)
 
 
 
 Average Pooling
 --------------------------------
 
 ``torch.nn.AvgPool2d`` and |nn.avg_pool()|_ are compatible when using default parameters.
@@ -249,15 +249,15 @@
 
   # [N, H, W, C] -> [N, C, H, W]
   t_x = torch.from_numpy(np.transpose(np.array(x), (0, 3, 1, 2)))
   t_out = t_pool(t_x)
   # [N, C, H, W] -> [N, H, W, C]
   t_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))
 
-  np.testing.assert_almost_equal(j_out, t_out)
+  np.testing.assert_almost_equal(j_out, t_out, decimal=6)
 
 
 
 Transposed Convolutions
 --------------------------------
 
 ``torch.nn.ConvTranspose2d`` and |nn.ConvTranspose|_ are not compatible.
```

### Comparing `flax-0.6.9/docs/advanced_topics/lift.md` & `flax-0.7.0/docs/developer_notes/lift.md`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-# Lifted Transformations
+# Lifted transformations
 
  Advanced topic 
 
-This design note explains the underlying implementation of `flax.linen.transform`, which enables JAX transformations inside `Module`s.
+This design note explains the underlying implementation of `flax.linen.transform`, which enables JAX transformations inside Flax `Module`s.
 
 
 ## Introduction
 
 JAX uses a functional API meaning that it only guarantees correct behavior when using functions without side effects ([JAX docs](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#differences-from-numpy)).
 Typically, these side effects are the result of mutating an object that lives outside the function.
 
@@ -291,15 +291,15 @@
 | while_loop |  | Carry variables cannot be initialized inside the while_loop body. |
 | cond |  | Variable initialization / mutation must structurally match across branches. |
 | switch |  | Variable initialization / mutation must structurally match across branches. |
 | pmap |  |  |
 | xmap |  |  |
 
 References:
-- [Linen transforms documentation](https://flax.readthedocs.io/en/latest/flax.linen.html#module-flax.linen.transforms).
+- [Linen transforms documentation](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html)
 - [Linen transforms source code](https://github.com/google/flax/blob/main/flax/linen/transforms.py)
 - [Core lifting source code](https://github.com/google/flax/blob/main/flax/core/lift.py)
 
 ### Linen examples
 
 Going back to our original example, we can now use `nn.vmap` to simplify our implementation:
```

### Comparing `flax-0.6.9/docs/advanced_topics/linen_design_principles.rst` & `flax-0.7.0/docs/philosophy.md`

 * *Files 26% similar despite different names*

```diff
@@ -1,101 +1,121 @@
-Linen Design Principles
-=======================
+# The Flax philosophy
 
-Flax is a neural network library built on JAX that has been adopted by a
+In no particular order:
+
+* Library code should be easy to read and understand.
+* Prefer duplicating code over a bad abstraction.
+* Generally, prefer duplicating code over adding options to functions.
+* Comment-driven design: If it's hard to document your code, consider
+  changing the design.
+* Unit test-driven design: If it's hard to test your code, consider
+  changing the design.
+* People start projects by copying an existing implementation  make
+  base implementations excellent.
+* If we expose an abstraction to our developers, we own the mental
+  overhead.
+* Developer-facing functional programming abstractions confuse some users,
+  expose them where the benefit is high.
+* "Read the manual" is not an appropriate response to developer confusion.
+  The framework should guide developers
+  towards good solutions, such as through assertions and error messages.
+* An unhelpful error message is a bug.
+* "Debugging is twice as hard as writing the code in the first
+  place. Therefore, if you write the code as cleverly as possible, you
+  are, by definition, not smart enough to debug it."  Brian Kernighan
+
+## Design principles
+
+Flax is a neural network library built on [JAX](https://jax.readthedocs.io) that has been adopted by a
 growing set of users, most notably in the JAX submissions for the MLPerf
 0.7 benchmark. Our experience over the last year (and many conversations
 with users and JAX core devs) has guided a redesign of the API called
-Linen in response to the following basic design questions.
+[Linen](https://github.com/google/flax/blob/main/flax/linen/README.md) ([`flax.linen`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/index.html)) in response to the following basic design questions.
 
-How does a neural network library benefit from being built on JAX and leverage JAXs unique strengths?
-------------------------------------------------------------------------------------------------------
+### How does a neural network library benefit from being built on JAX and leverage JAXs unique strengths?
 
 The world already has TensorFlow and PyTorch, and theres little need to
 build a clone of either. We believe that the composable
 function-transformation approach that JAX takes opens up new frontiers
 for making neural net code more maintainable, more scalable and more
 performant than existing libraries. While we strive to offer an API
 familiar to those experienced with Keras/Sonnet/PyTorch, Linen is
 fundamentally a functional system for defining neural nets in JAX. Just
 a few examples of what we believe a JAX-targeted library can enable:
 
--  write models as single-example code and introduce batching
-   automatically with vmap
--  automatically handle ragged batches in NLP and other masking issues
--  create efficient compile-time and runtime models by utilizing
-   rematerialized scan for massive conv-nets.
--  remove memory headaches by enabling easy rematerialization,
-   reversibility, and model-parallel data sharding.
+- Write models as single-example code and introduce batching
+  automatically with [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html).
+- Automatically handle ragged batches in NLP and other masking issues.
+- Create efficient compile-time and runtime models by utilizing
+  rematerialized `scan` for massive convolutional networks.
+- Remove memory headaches by enabling easy rematerialization,
+  reversibility, and model-parallel data sharding.
 
-How does one interoperate with JAX transformations?
----------------------------------------------------
+### How does one interoperate with JAX transformations?
 
-Arguably the entire point of a neural net library is to offer an
+Arguably, the entire point of a neural net library is to offer an
 implicit variable management API to save the user from having to
 manually thread thousands of variables through a complex tree of
 functions. However, JAX operates on pure functions. To handle both
 current and future JAX transforms (configured and composed in any way),
 Linen Modules are directly functionalized, that is, automatically cast
 in-place as explicit functions of the form:
 
-.. math:: f(v_{in}, x) \rightarrow v_{out}, y
+$$f \left( v_{in}, x \right) \rightarrow v_{out}, y$$
 
-Where :math:`v_{in}` is the variable collections and PRNG state used by
-the model, :math:`v_{out}` the mutated output variable collections,
-:math:`x` the input data and :math:`y` the output data. Applying JAX
+Where $v_{in}$ is the variable collections and [PRNG](https://jax.readthedocs.io/en/latest/jep/263-prng.html) state used by
+the model, $v_{out}$ the mutated output variable collections,
+$x$ the input data and $y$ the output data. Applying JAX
 transformations then simply reduces to specifying any argument-specific
 transform options to the various variable collections and PRNG state.
-This unleashes the flexibility and strength of JAX transformations  for
+This unleashes the flexibility and strength of [JAX transformations](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)  for
 example, one can achieve either device-parallel training or per-device
-ensembling by using ``pmap`` in different ways, without any explicit
-library support. Moreover, **within Modules**, we expose lightweight
-wrappers around the complex JAX transforms such as ``vmap`` and ``scan``
+ensembling by using [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) in different ways, without any explicit
+library support. Moreover, **within [Modules](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)**, we expose lightweight
+wrappers around the complex JAX transforms such as [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) and [`jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html)
 that annotate how each variable collection is to be transformed by JAX.
 Importantly, we handle the nontrivial cases of creating new variables
 and transformed variables under mapping and loop transforms correctly
 for initialization and application.
 
-How are parameters represented, and how do we handle general differentiable algorithms that update stateful variables?
-------------------------------------------------------------------------------------------------------------------------
+### How are parameters represented, and how do we handle general differentiable algorithms that update stateful variables?
 
 We follow the JAX functional conventions of storing data in pytrees:
 JAX arrays contained in nested tuples, lists, dictionaries. Because
 researchers inevitably manually interact with this data, we use nested
 dictionaries with meaningful default keys and offer several utilities
 (traversals, etc.) for handling them directly. Linen uses an accelerated
 version of a Python frozen dictionary that caches its JAX-flattened form
-to speed up jitted function call overheads.
+to speed up [`jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html)ted function call overheads.
 
 Flax generalizes the operation of a neural net by allowing models to
 accept collections of several different kinds: parameters, batch-norm
 stats, autoregressive caches, debug information, fine-grained
 hyperparameters, etc. Each collection is stored in a nested dictionary
 of the same structure as the model. Importantly, we do *not* conflate
 these various kinds under the single vague rubric of state, but keep
 different logical types of variables separate that can be treated
 differently under JAX transformations and under mutations (e.g.training
 vs prediction). Similarly, we allow for multiple separate named PRNG
-chains inside Modules for separate treatment of randomness for different
+chains inside [Modules](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module) for separate treatment of randomness for different
 applications such as initialization, dropout, sampling, etc.
 
 At every stage the data associated with a neural net is not kept in a
 custom object hierarchy, but left in an explicit, Python and JAX native
 form that is easy to introspect and modify. Users have utilized this to
 map TF and PyTorch checkpoints to Flax, to implement submodel-specific
 loss terms, and to perform fast model surgery, etc. For saving this
 data, most Flax examples store these nested dictionaries via the
 efficient msgpack binary format  but as variables are simply Python
 dicts, you can use any (non-JAX-aware) serialization library directly.
 
-How does one interoperate with purely functional JAX code?
-----------------------------------------------------------
+### How does one interoperate with purely functional JAX code?
 
 To be broadly useful to the JAX ecosystem, users shouldnt need to
 heavily refactor their code in order to add trainability for a given
-numerical task. The library should not get in the way. Utilizing
-purely functional code from within Linen is trivial: Module
+numerical task. _The library should not get in the way._ Utilizing
+purely functional code from within Linen is trivial: [Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)
 implementations are just JAX code with named variables. Using Linen
-modules inside otherwise purely functional code can be as simple as
-using a single top-level module transformation to allow initialization
+Modules inside otherwise purely functional code can be as simple as
+using a single top-level Module transformation to allow initialization
 and pure application of any JAX program that might contain various
 trainable sections.
```

### Comparing `flax-0.6.9/docs/advanced_topics/linen_upgrade_guide.rst` & `flax-0.7.0/docs/guides/linen_upgrade_guide.rst`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-Upgrading my Codebase to Linen
+Upgrading my codebase to Linen
 ==============================
 
 As of Flax v0.4.0, ``flax.nn`` no longer exists, and is replaced with the new
 Linen API at ``flax.linen``. If your codebase is still using the old API, you
 can use this upgrade guide to upgrade it to Linen.
 
 .. testsetup::
@@ -21,16 +21,16 @@
   PRNGKey = Any
   Shape = Tuple[int, ...]
   Dtype = Any
   Array = Any
 
   default_kernel_init = initializers.lecun_normal()
 
-Defining Simple Modules
---------------------------------
+Defining simple Flax Modules
+----------------------------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   from flax import nn
@@ -69,15 +69,15 @@
       y = jnp.dot(inputs, kernel)
       if self.use_bias:
         bias = self.param(
           'bias', self.bias_init, (self.features,))  # [5] #!
         y = y + bias
       return y
 
-1. Replace from ``flax import nn`` with from ``flax import linen as nn``.
+1. Replace ``from flax import nn`` with ``from flax import linen as nn``.
 
 2. Move arguments to ``apply`` into dataclass attributes. Add type annotations
    (or use type ``Any`` to bypass).
 
 3. Rename method ``apply`` to ``__call__`` and (optionally) wrap with
    |@compact|_. Methods wrapped in |@compact|_ can define submodules directly
    within the method (like in old Flax). You can only wrap a single method with
@@ -87,16 +87,16 @@
 4. Access dataclass attributes values by ``self.<attr>`` inside methods, e.g.
    ``self.features``.
 
 5. Move shape to the end of the arguments to |self.param|_ (initializer functions
    can take arbitrary argument lists).
 
 
-Using Modules inside other Modules
---------------------------------
+Using Flax Modules inside other Modules
+---------------------------------------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   class Encoder(nn.Module):
@@ -172,15 +172,15 @@
 
 3. We don't use |@compact|_ here because we're not defining any inline
    submodules (all submodules are defined in setup).
 
 4. Define additional methods just like in regular Python.
 
 ``Module.partial`` inside other modules
---------------------------------
+---------------------------------------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   # no import #!
@@ -266,32 +266,24 @@
     variables = CNN().init(rng, jnp.ones([1, 28, 28, 1]))  # [2] #!
     params = variables['params']  # [3] #!
     tx = optax.sgd(config.learning_rate, config.momentum)  # [4] #!
     return train_state.TrainState.create(
         apply_fn=CNN.apply, params=params, tx=tx)
 
 
-
-
-
-
-
-
-
-
   def loss_fn(params):
     logits = CNN().apply({'params': params}, batch['image'])  # [5] #!
     one_hot = jax.nn.one_hot(batch['label'], 10)
     loss = jnp.mean(optax.softmax_cross_entropy(logits=logits,
                                                 labels=one_hot))
     return loss, logits
 
 
 1. We no longer use the ``Model`` abstraction -- instead we pass parameters
-   around directly, usually encapsulated in a `Train State`_ object, which can
+   around directly, usually encapsulated in a `TrainState`_ object, which can
    directly be passed to JAX transformations.
 
 2. To compute initial parameters, construct a module instance and call |init|_
    or |init_with_output|_. We haven't ported over ``init_by_shape`` because this
    function did some magic we did not like (it evaluated the function by shape.
    but returned real values anyway). Therefore, you should now pass concrete
    values to the initializer functions, and you can optimize the initialization
@@ -300,22 +292,22 @@
 
 3. Linen generalizes parameters into variables. Parameters are one
    "collection" of variables. Variables are nested dicts, where the top-level
    keys reflect the different variable collections, of which "param" is one of.
    See the `Variables documentation`_ for more details.
 
 4. We recommend using Optax optimizers. See our separate HOWTO called
-   `Upgrading my Codebase to Optax`_ for more details.
+   `Upgrading my codebase to Optax`_ for more details.
 
 5. To make predictions with your model, make an instance at the top level (this
    is free -- just a wrapper around constructor attributes) and call the
    ``apply`` method (which will call ``__call__`` internally).
 
 Non-trainable variables ("state"): Use within Modules
---------------------------------
+-----------------------------------------------------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   class BatchNorm(nn.Module):
@@ -339,15 +331,15 @@
 The first argument is the name of the variable collection ("param" is the only
 variable collection that's always available). Some colllections may be treated
 as mutable, and others as immutable at top-level training code (see next section
 for details). Flax also lets you treat each variable collection differently when
 using JAX transformations inside modules.
 
 Non-trainable variables ("state"): Top-level training code patterns
---------------------------------
+-------------------------------------------------------------------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   # initial params and state
@@ -362,29 +354,26 @@
   def loss_fn(model, model_state):
     with nn.stateful(model_state) as new_model_state:
       logits = model(batch['image'])
     # [...]
 
 
 
-
   # reads immutable batch statistics during evaluation
   def eval_step(model, model_state, batch):
   with nn.stateful(model_state, mutable=False):
       logits = model(batch['image'], train=False)
     return compute_metrics(logits, batch['label'])
   ---
   # initial variables ({"param": ..., "batch_stats": ...})
   def initial_variables(key, init_batch):
     return ResNet().init(key, init_batch)  # [1] #!
 
 
 
-
-
   # updates batch statistics during training
   def loss_fn(params, batch_stats):
     variables = {'params': params, 'batch_stats': batch_stats}  # [2] #!
     logits, new_variables = ResNet(train=true).apply(
       variables, batch['image'], mutable=['batch_stats'])  # [3] #!
     new_batch_stats = new_variables['batch_stats']
     # [...]
@@ -394,29 +383,29 @@
   def eval_step(params, batch_stats, batch):
     variables = {'params': params, 'batch_stats': batch_stats}
     logits = ResNet(train=False).apply(
       variables, batch['image'], mutable=False)  # [4] #!
     return compute_metrics(logits, batch['label'])
 
 1. |init|_ returns a variable dict, e.g. ``{"param": ..., "batch_stats": ...}``
-   (see `Variable documentation`_).
+   (see `Variables documentation`_).
 
 2. Combine the different variable collections into a variable dict.
 
 3. During training, the ``batch_stats`` variable collection changes. Since we
    specify that in the mutable argument, the return value from ``module.apply``
    becomes an ordered pair of ``output, new_variables``.
 
 4. During evaluation, we want to raise an error if we're accidentally applying
    Batch Norm in training mode. By passing ``mutable=False`` into
    ``module.apply`` we enforce that. Since no variables are mutated, the return
    value is once again just the output.
 
 Loading pre-Linen checkpoints
---------------------------------
+-----------------------------
 
 While most Linen modules should be able to use pre-Linen weights without any
 modification, there is one catch: In pre-Linen API submodules were numbered
 incrementally, independent of the submodule class. With Linen this behavior has
 changed to keep separate submodule counts per module class.
 
 In pre-Linen, params have the following structure:
@@ -426,15 +415,15 @@
 In Linen this is instead:
 
 ``{'Conv_0': { ... }, 'Dense_0': { ... } }``
 
 TODO: Add an example here how to load a new ``TrainState`` object.
 
 Randomness
---------------------------------
+----------
 
 .. codediff::
   :title_left: Old Flax
   :title_right: Linen
   :sync:
 
   def dropout(inputs, rate, deterministic=False):
@@ -467,58 +456,58 @@
           mask, inputs / keep_prob, jnp.zeros_like(inputs))
 
 
   def loss_fn(params, dropout_rng):
     logits = Transformer().apply(
       {'params': params}, inputs, rngs={'dropout': dropout_rng})  # [2] #!
 
-1. RNGs in Linen have "kinds" -- in this case "dropout". Different kinds can be
-   treated different in JAX transformations (for example -- do you want the same
-   dropout mask for each timestep in a sequence model or a different one?)
+1. RNGs in Linen have "kinds" -- in this case ``'dropout'``. Different kinds can
+   be treated different in JAX transformations (for example, do you want the
+   same dropout mask for each timestep in a sequence model or a different one?)
 
 2. Instead of using the ``nn.stochastic`` context manager, you pass in RNGs
    explicitly to ``module.apply``. During evaluation you wouldn't pass any RNGs
    -- then if you accidentally use dropout in non-deterministic mode,
    ``self.make_rng('dropout')`` would raise an error.
 
 
-Lifted Transforms
---------------------------------
+Lifted transformations
+----------------------
 
 In Linen, rather than using JAX transformation directly, we are using
 "lifted transforms", which are JAX transformations applied to Flax Modules.
 
-For more information, please see the design note on `Lifted Transformations`_.
+For more information, please see the design note on `Lifted transformations`_.
 
 TODO: Given an example of ``jax.scan_in_dim`` (pre-Linen) vs. ``nn.scan``
 (Linen).
 
-.. _`Should I use setup or nn.compact?`: https://flax.readthedocs.io/en/latest/design_notes/setup_or_nncompact.html
-.. _`Variables documentation`: https://flax.readthedocs.io/en/latest/flax.linen.html#module-flax.core.variables
+.. _`Should I use setup or nn.compact?`: https://flax.readthedocs.io/en/latest/guides/setup_or_nncompact.html
+.. _`Variables documentation`: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html
 .. _`TrainState`: https://flax.readthedocs.io/en/latest/flax.training.html#train-state
-.. _`Upgrading my Codebase to Optax`: https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html
-.. _`Lifted Transformations`: https://flax.readthedocs.io/en/latest/design_notes/lift.html
+.. _`Upgrading my codebase to Optax`: https://flax.readthedocs.io/en/latest/guides/optax_update_guide.html
+.. _`Lifted transformations`: https://flax.readthedocs.io/en/latest/developer_notes/lift.html
 
 
 .. |@compact| replace:: ``@compact``
-.. _@compact: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.compact
+.. _@compact: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/decorators.html#flax.linen.compact
 
 .. |init| replace:: ``init``
-.. _init: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.init
+.. _init: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init
 
 .. |init_with_output| replace:: ``init_with_output``
-.. _init_with_output: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.init_with_output
+.. _init_with_output: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init_with_output
 
 .. |jax.jit| replace:: ``jax.jit``
 .. _jax.jit: https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit
 
 .. |self.param| replace:: ``self.param``
-.. _self.param: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.param
+.. _self.param: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.param
 
 .. |setup| replace:: ``setup``
-.. _setup: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.setup
+.. _setup: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.setup
 
 .. |@flax.struct.dataclass| replace:: ``@flax.struct.dataclass``
 .. _@flax.struct.dataclass: https://flax.readthedocs.io/en/latest/flax.struct.html#flax.struct.dataclass
 
 .. |checkpoints.convert_pre_linen()| replace:: ``checkpoints.convert_pre_linen()``
 .. _checkpoints.convert_pre_linen(): https://flax.readthedocs.io/en/latest/flax.training.html#flax.training.checkpoints.convert_pre_linen
```

### Comparing `flax-0.6.9/docs/advanced_topics/module_lifecycle.rst` & `flax-0.7.0/docs/developer_notes/module_lifecycle.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-The Module lifecycle
-######################
+The Flax Module lifecycle
+#########################
 
 .. testsetup::
 
   from typing import Any, Callable, Iterable
   import flax
   from flax import linen as nn
   from jax import random
   import jax
 
 
-This design note is intended for users who are already familiar with linen Modules but want to understand more about the design principles behind the abstraction. This note should give you a good understanding of the assumptions and guarantees the Module API is built upon. If you have no practical experience with Modules yet, check out the `MNIST Tutorial <https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html>`_.
+This design note is intended for users who are already familiar with Flax Linen Modules but want to understand more about the design principles behind the abstraction. This note should give you a good understanding of the assumptions and guarantees the Module API is built upon. If you have no practical experience with Modules yet, check out the `Getting started notebook <https://flax.readthedocs.io/en/latest/getting_started.html>`_.
 
-linen Modules offer a Pythonic abstracton on top of Flax core. The `Module <https://flax.readthedocs.io/en/latest/flax.linen.html#module>`_ abstraction allows you to create classes that have state, parameters and randomness on top of JAX. This is a practical guide to the design and behavior of the ``Module`` class. By the end, you should feel comfortable to go off the beaten track and use Modules in new ways.
+Flax Linen Modules offer a Pythonic abstraction on top of Flax core. The `Module <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html>`_ abstraction allows you to create classes that have state, parameters and randomness on top of JAX. This is a practical guide to the design and behavior of the ``Module`` class. By the end, you should feel comfortable to go off the beaten track and use Modules in new ways.
 
 
 Overview
 ***********
 
 Definition
 =============
@@ -61,15 +61,15 @@
   x = jax.numpy.ones((1, 2))
   variables = mlp.init(random.PRNGKey(0), x)
   y = mlp.apply(variables, x)
 
 
 First, we construct an instance of ``MLP`` and pass the construction attributes. Note that construction here is different from what you might expect if you are not used to Functional Programming patterns. The ``MLP`` constructor does not actually create variables or any internal state whatsoever. It's best to think of it as a specification or template of the Module that contains functionality but no data.
 
-Let's take a closer look at initialization. Surprisingly, there actually is no seperate initialization path in Flax. Calling ``init`` is just a special case of ``apply``, which you can also write as:
+Let's take a closer look at initialization. Surprisingly, there actually is no separate initialization path in Flax. Calling ``init`` is just a special case of ``apply``, which you can also write as:
 
 
 .. testcode::
 
   # equivalent to: variables = mlp.init(random.PRNGKey(0), x)
   _, variables = mlp.apply({}, x, rngs={"params": random.PRNGKey(0)}, mutable=True)
 
@@ -98,15 +98,15 @@
    #. Returns the output of ``mlp_copy.__call__()`` and optionally the variable collections that were specified as mutable using the keyword argument ``mutable=``.
 
 Notice that the lifecycle includes cloning the Module instance. This is done to ensure that ``apply`` can be treated as a pure function (i.e., if you pass the same arguments in, it will return the same outputs). You will learn about this in more detail later in the  :ref:`Top-level Modules` section.
 
 Variables
 ==========
 
-The word variable is ubiquitous in programming and math. However, it's important to have a good understanding of what variables are in the context of JAX and Flax. Inside Flax Modules, `variables <https://flax.readthedocs.io/en/latest/flax.linen.html#module-flax.core.variables>`_ act like you expect from Python. They are initialized once, read, and perhaps even updated every so often. However, JAX has no concept of variables. Instead, values are stored in arrays similar to NumPy arrays - with one important difference: they are immutable.
+The word variable is ubiquitous in programming and math. However, it's important to have a good understanding of what variables are in the context of JAX and Flax. Inside Flax Modules, `variables <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`_ act like you expect from Python. They are initialized once, read, and perhaps even updated every so often. However, JAX has no concept of variables. Instead, values are stored in arrays similar to NumPy arrays - with one important difference: they are immutable.
 
 The ``init`` and ``apply`` methods return the variables as a nested dictionary with string keys and JAX arrays at the leaves. At the top level each key corresponds to a variable collection. Inside each collection the nested dict structure corresponds with the ``Module`` hierarchy. The variable dict is immutable and therefore really just a snapshot of state the variables are in. When ``apply`` is called again, the variable dict is passed as an argument. Such that the variables are in the same state as when the previous ``init`` / ``apply`` call finished.
 
 
 .. note::
    Module fields are declared using the `field_name: TypeHint` syntax (same as dataclasses). Without a type hint, an attribute is considered a static property of the class. In case you cannot specify the type you can use ``typing.Any`` as a wildcard type.
 
@@ -126,15 +126,15 @@
     @nn.compact
     def __call__(self, x):
       a = nn.Dense(self.hidden_size)(x)
       h = nn.relu(a)
       return nn.Dense(self.out_size)(h)
 
 
-A compact ``Module`` is similar in spirit to a function. It offers a concise notation and restricts external interaction to the inputs and return values of the function. In this case the concise notation might make it easier for others to understand what the Module does. There is no need to jump back and forth between the ``setup`` and ``__call__`` method to understand what the submodules are doing. Instead, simply reading the ``__call__`` method from top to bottom once should provide a concise overview. This can make a significant difference if you are implementing complex Modules with many hyperparameters. See `setup or compact <https://flax.readthedocs.io/en/latest/design_notes/setup_or_nncompact.html>`_ for a practical guide on decding between setup and compact.
+A compact ``Module`` is similar in spirit to a function. It offers a concise notation and restricts external interaction to the inputs and return values of the function. In this case the concise notation might make it easier for others to understand what the Module does. There is no need to jump back and forth between the ``setup`` and ``__call__`` method to understand what the submodules are doing. Instead, simply reading the ``__call__`` method from top to bottom once should provide a concise overview. This can make a significant difference if you are implementing complex Modules with many hyperparameters. See `setup or compact <https://flax.readthedocs.io/en/latest/guides/setup_or_nncompact.html>`_ for a practical guide on deciding between setup and compact.
 
 Another benefit of defining submodules and/or variables inline is that you can add arguments to your method when constructing variables. The most common example of this is using shape information to determine the shape of a parameter like this:
 
 
 .. testcode::
 
   class CompactScaledMLP(nn.Module):
@@ -173,15 +173,15 @@
     def __call__(self, x, mode):
       if mode == "encode":
         return nn.Dense(features=8)(x)
       elif mode == "decode":
         return nn.Dense(features=4)(x)
 
 
-The above Module will break because either the encoder or decoder path will construct a Module named "Dense_0". This means the two Modules will share parameters which is not intented here. Actually, the two Modules cannot share parameters because they each have a different number of features.
+The above Module will break because either the encoder or decoder path will construct a Module named "Dense_0". This means the two Modules will share parameters which is not intended here. Actually, the two Modules cannot share parameters because they each have a different number of features.
 
 This problem can be solved in various ways:
  - Provide explicit names
  - create the modules in ``setup``
  - or move the constructor out of the control flow.
 
 The latter is done as follows:
@@ -238,15 +238,15 @@
 ===============================================
 
 To make this approach work reliably we need well-defined cloning behavior. Rather than relying on a complex nested cloning procedure like Python's ``deepcopy``, Flax enforces that a ``Module`` is exactly defined by its construction arguments. Therefore cloning a Module reduces to calling the constructor with its original construction arguments. Because ``Module`` acts as an immutable dataclass, the construction arguments are mapped directly to instance attributes. Non-construction attributes that are computed in ``setup`` or ``__post_init__`` should also depend only on the construciton arguments to ensure a well-defined clone.
 
 Bind
 ===============================================
 
-Sometimes it's useful to have a bound, top-level Module without having to wrap the code in a function. For example: to interact with a Module inside a Jupyter notebook. The `bind <https://flax.readthedocs.io/en/latest/flax.linen.html?highlight=bind#flax.linen.Module.bind>`_ method returns a bound clone with an unlimited lifetime. The downside of this is that you cannot combine it with JAX transformations or integrate it into a vanilla JAX codebase that expects stateless code. For example, `Optax <https://github.com/deepmind/optax>`_ can optimze a Pytree of parameters but it cannot directly optimize a bound ``Module`` instance created with ``.bind`` (because that's not a Pytree). Thus, you cannot combine the ``bind`` API with a functional optimizer API like Optax.
+Sometimes it's useful to have a bound, top-level Module without having to wrap the code in a function. For example: to interact with a Module inside a Jupyter notebook. The `bind <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.bind>`_ method returns a bound clone with an unlimited lifetime. The downside of this is that you cannot combine it with JAX transformations or integrate it into a vanilla JAX codebase that expects stateless code. For example, `Optax <https://github.com/deepmind/optax>`_ can optimize a Pytree of parameters but it cannot directly optimize a bound ``Module`` instance created with ``.bind`` (because that's not a Pytree). Thus, you cannot combine the ``bind`` API with a functional optimizer API like Optax.
 
 
 Setup
 **********
 
 The ``setup`` method is often used like the constructor hook (``__init__``) in normal Python classes. However, for more advanced use cases it's good to realize that it is not quite the same as a constructor.
 
@@ -258,15 +258,15 @@
 
     def setup(self):
       self.foo = nn.Dense(2)
 
   mdl = TopLevelAccess()
   assert not hasattr(mdl, "foo")  # foo is not defined because setup is not called
 
-The ``setup`` method is not called immediately after the ``Module`` becomes bound but only when you interact with the ``Module`` instance (e.g.: call a method or access an attribute). This should not impact the behavior of a ``Module`` but the lazy execution does sometimes affect log statements and stack traces during debugging. The section on functionalization will explain why we need ``setup`` to be lazy in the first place.
+The ``setup`` method is not called immediately after the ``Module`` becomes bound but only when you interact with the ``Module`` instance (e.g.: call a method or access an attribute). This should not impact the behavior of a ``Module`` but the lazy execution does sometimes affect log statements and stack traces during debugging. The section on :ref:`Functionalization` will explain why we need ``setup`` to be lazy in the first place.
 
 
 Functionalization
 ******************
 
 So far we had a pure ``apply`` function that is typically transformed with some JAX transformations and inside ``apply`` we have a stateful Module instance to work with. In other words: Outside of a Module we are in a functional world where we have the power of JAX's functional transformations and inside the Module we get the power of Flax's stateful variables and PRNG sequence, and the ``apply`` method is our bridge between these two worlds.
 
@@ -282,15 +282,15 @@
 
    #. Call the user code ``fn``
 
    #. Collect the updated variables and rng and return it together with the original return values from ``fn``
 
 #. Update the original state with the updated state returned from the transformation.
 
-A more in depth explanation of functionalization and lifting can be found in the `Lifted Transformation <https://flax.readthedocs.io/en/latest/design_notes/lift.html>`_ design note.
+A more in depth explanation of functionalization and lifting can be found in the `Lifted Transformation <https://flax.readthedocs.io/en/latest/developer_notes/lift.html>`_ design note.
 
 Practical consequences
 ==========================
 
 For the most part functionalization is something that is handled automatically for you. Still there are some constraints that you must take into account. Most importantly, Flax only handles the stateful primitives (Linen variables and RNGs) and not arbitrary stateful Python code. Most importantly: You cannot close over stateful objects and ``Module`` objects because they are invisible to Flax's internals (and to JAX in general).
 
 
@@ -310,15 +310,15 @@
     def inner(self, x, fn):
       for i in range(3):
         x = fn(x)
       return x
 
 Here ``inner`` takes a function that closes over a Module instance. In this example, that works fine because we are not transforming the inner method with a lifted transformation. Most methods are not transformed but it is good to know how to make Module methods transformable.
 
-The main obstacle for transformability are types that JAX does not recognize. JAX only understands `Pytree <https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html>`_ arguments. That's arbitrarily nested Python containers (dict, list, tuple) of (Jax) numpy ndarrays and Python numbers/bools. Flax allows to define dataclasses which are Pytree compatible using the `flax.struct <https://flax.readthedocs.io/en/latest/flax.struct.html>`_ API.
+The main obstacle for transformability are types that JAX does not recognize. JAX only understands `Pytree <https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html>`_ arguments; i.e. arbitrarily nested Python containers (dict, list, tuple) of (Jax) numpy ndarrays and Python numbers/bools. Flax allows to define dataclasses which are Pytree compatible using the `flax.struct <https://flax.readthedocs.io/en/latest/flax.struct.html>`_ API.
 
 Function closure is the most common way to accidentally hide a JAX array or Linen Module from a transformation. There is however an easy workaround if you want to pass closures that are also compatible with JAX and Linen transformations:
 
 
 .. testcode::
 
   class Partial(flax.struct.PyTreeNode):
@@ -359,15 +359,15 @@
 Future work
 *************
 
 
 Setup for unbound Modules
 ===========================
 
-The current Module abstraction is particularly restrictive when it comes to initializing fields after construction. In the current Module API, the ``setup`` method is the place to initialize the fields of  the Module instance. Because ``setup`` is only called on a bound Module, the full Module API is available inside ``setup``, including variable declaration. However, oftentimes we don't actually require any stateful API's to initialize a field. In fact, most commonly we simply want to declare a submodule. More importantly, it's often useful to inspect submodules for debugging or to partially run the model. Consider for example:
+The current Module abstraction is particularly restrictive when it comes to initializing fields after construction. In the current Module API, the ``setup`` method is the place to initialize the fields of the Module instance. Because ``setup`` is only called on a bound Module, the full Module API is available inside ``setup``, including variable declaration. However, oftentimes we don't actually require any stateful API's to initialize a field. In fact, most commonly we simply want to declare a submodule. More importantly, it's often useful to inspect submodules for debugging or to partially run the model. Consider for example:
 
 
 .. testcode::
 
   class AutoEncoder(nn.Module):
     def setup(self):
       self.encoder = Encoder(...)
```

### Comparing `flax-0.6.9/docs/advanced_topics/optax_update_guide.rst` & `flax-0.7.0/docs/guides/optax_update_guide.rst`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 .. image:: https://colab.research.google.com/assets/colab-badge.svg
    :target: https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/optax_update_guide.ipynb
 
-Upgrading my Codebase to Optax
+Upgrading my codebase to Optax
 ==============================
 
 We have proposed to replace :py:mod:`flax.optim` with `Optax
 <https://optax.readthedocs.io>`_ in 2021 with `FLIP #1009
 <https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md>`_ and
 the Flax optimizers have been removed in v0.6.0 - this guide is targeted
 towards :py:mod:`flax.optim` users to help them update their code to Optax.
```

### Comparing `flax-0.6.9/docs/api_reference/flax.training.rst` & `flax-0.7.0/docs/api_reference/flax.training.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/api_reference/flax.traverse_util.rst` & `flax-0.7.0/docs/api_reference/flax.traverse_util.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/conf.py` & `flax-0.7.0/docs/conf.py`

 * *Files 1% similar despite different names*

```diff
@@ -56,14 +56,15 @@
     'sphinx.ext.doctest',
     'sphinx.ext.intersphinx',
     'sphinx.ext.mathjax',
     'sphinx.ext.napoleon',
     'sphinx.ext.viewcode',
     'myst_nb',
     'codediff',
+    'flax_module',
     'sphinx_design',
 ]
 
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates']
 
 # List of patterns, relative to source directory, that match files and
```

### Comparing `flax-0.6.9/docs/conf_sphinx_patch.py` & `flax-0.7.0/docs/conf_sphinx_patch.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/developer_notes/module_lifecycle.rst` & `flax-0.7.0/docs/guides/haiku_migration_guide.rst`

 * *Files 25% similar despite different names*

```diff
@@ -1,381 +1,716 @@
-The Flax Module lifecycle
-#########################
 
-.. testsetup::
-
-  from typing import Any, Callable, Iterable
-  import flax
-  from flax import linen as nn
-  from jax import random
-  import jax
-
-
-This design note is intended for users who are already familiar with Flax Linen Modules but want to understand more about the design principles behind the abstraction. This note should give you a good understanding of the assumptions and guarantees the Module API is built upon. If you have no practical experience with Modules yet, check out the `Getting started notebook <https://flax.readthedocs.io/en/latest/getting_started.html>`_.
-
-Flax Linen Modules offer a Pythonic abstracton on top of Flax core. The `Module <https://flax.readthedocs.io/en/latest/flax.linen.html#module>`_ abstraction allows you to create classes that have state, parameters and randomness on top of JAX. This is a practical guide to the design and behavior of the ``Module`` class. By the end, you should feel comfortable to go off the beaten track and use Modules in new ways.
-
-
-Overview
-***********
-
-Definition
-=============
-
-Let's start with a high-level overview of the Module lifecycle. First, define a simple Module:
-
-
-.. testcode::
-
-  class MLP(nn.Module):
-    # 1. Attribute annotations
-    hidden_size: int
-    out_size: int
-
-    # 2. The ``setup`` method
-    def setup(self):
-      self.hidden = nn.Dense(self.hidden_size)
-      self.out = nn.Dense(self.out_size)
-
-    # 3. User methods
-    def __call__(self, x):
-      a = self.hidden(x)
-      h = nn.relu(a)
-      return self.out(h)
-
-
-This Module consists of:
-
-#. **Attribute annotations**, defined as `dataclass <https://docs.python.org/3/library/dataclasses.html>`_ fields. These annotations automatically define a constructor.
-#. **The ``setup`` method**, which creates submodules and assigns them to attributes.
-#. **User methods**. By convention, most Modules have just one  ``__call__`` method, but you can define multiple methods or use different method names.
-
-Construction/initialization
-=============================
-
-Now we want to construct and use the ``MLP`` Module:
-
-
-.. testcode::
-
-  mlp = MLP(hidden_size=5, out_size=3)
-  x = jax.numpy.ones((1, 2))
-  variables = mlp.init(random.PRNGKey(0), x)
-  y = mlp.apply(variables, x)
-
-
-First, we construct an instance of ``MLP`` and pass the construction attributes. Note that construction here is different from what you might expect if you are not used to Functional Programming patterns. The ``MLP`` constructor does not actually create variables or any internal state whatsoever. It's best to think of it as a specification or template of the Module that contains functionality but no data.
-
-Let's take a closer look at initialization. Surprisingly, there actually is no seperate initialization path in Flax. Calling ``init`` is just a special case of ``apply``, which you can also write as:
-
-
-.. testcode::
-
-  # equivalent to: variables = mlp.init(random.PRNGKey(0), x)
-  _, variables = mlp.apply({}, x, rngs={"params": random.PRNGKey(0)}, mutable=True)
-
-
-Thus, ``init`` is nothing more than a wrapper around ``apply`` where:
-
-#. We call a Module without any initial variables (an empty dict).
-#. A PRNG generator named ``"params"`` is always passed for randomly initializing parameters (using the parameter initialization function).
-#. All variable collections are set to mutable (``mutable=True``). When a collection is mutable, existing variables can be updated and new variables can be created. Thus, inside ``init`` variables can be initialized in any variable collection and they are all added to the returned variable dictionary.
-
-Lifecycle
-=============
-
-
-Now that you have learned about ``init`` being a special case of ``apply``, let's look at ``.apply(...)`` in more detail. In fact, most of the complexity of Modules resides in the ``apply`` method. The "Module lifecycle" consists of constructing and ``apply``-ing a Module. We can summarize the Module lifecycle as follows:
-
-
-#. We construct ``mlp = MLP(hidden_size=5, out_size=3)``, such that ``mlp.hidden_size=5`` and ``mlp.out_size=3``.
-
-#. Then, call ``mlp.apply``, which:
-
-   #. Makes a clone of ``mlp``, let's call it ``mlp_copy``.
-
-   #. Calls ``mlp_copy.setup()``.
-
-   #. Returns the output of ``mlp_copy.__call__()`` and optionally the variable collections that were specified as mutable using the keyword argument ``mutable=``.
-
-Notice that the lifecycle includes cloning the Module instance. This is done to ensure that ``apply`` can be treated as a pure function (i.e., if you pass the same arguments in, it will return the same outputs). You will learn about this in more detail later in the  :ref:`Top-level Modules` section.
-
-Variables
+Migrating from Haiku to Flax
 ==========
 
-The word variable is ubiquitous in programming and math. However, it's important to have a good understanding of what variables are in the context of JAX and Flax. Inside Flax Modules, `variables <https://flax.readthedocs.io/en/latest/flax.linen.html#module-flax.core.variables>`_ act like you expect from Python. They are initialized once, read, and perhaps even updated every so often. However, JAX has no concept of variables. Instead, values are stored in arrays similar to NumPy arrays - with one important difference: they are immutable.
-
-The ``init`` and ``apply`` methods return the variables as a nested dictionary with string keys and JAX arrays at the leaves. At the top level each key corresponds to a variable collection. Inside each collection the nested dict structure corresponds with the ``Module`` hierarchy. The variable dict is immutable and therefore really just a snapshot of state the variables are in. When ``apply`` is called again, the variable dict is passed as an argument. Such that the variables are in the same state as when the previous ``init`` / ``apply`` call finished.
+This guide will walk through the process of migrating Haiku models to Flax,
+and highlight the differences between the two libraries.
 
+.. testsetup::
 
-.. note::
-   Module fields are declared using the `field_name: TypeHint` syntax (same as dataclasses). Without a type hint, an attribute is considered a static property of the class. In case you cannot specify the type you can use ``typing.Any`` as a wildcard type.
+  import jax
+  import jax.numpy as jnp
+  from jax.random import PRNGKey
+  import optax
+  import flax.linen as nn
+
+Basic Example
+-----------------
+
+To create custom Modules you subclass from a ``Module`` base class in
+both Haiku and Flax. However, Haiku classes use a regular ``__init__`` method
+whereas Flax classes are ``dataclasses``, meaning you define some class
+attributes that are used to automatically generate a constructor. Also,
+all Flax Modules accept a ``name`` argument without needing to define it,
+whereas in Haiku ``name`` must be explicitly defined in the constructor
+signature and passed to the superclass constructor.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  import haiku as hk
+
+  class Block(hk.Module):
+    def __init__(self, features: int, name=None):
+      super().__init__(name=name)
+      self.features = features
+
+    def __call__(self, x, training: bool):
+      x = hk.Linear(self.features)(x)
+      x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)
+      x = jax.nn.relu(x)
+      return x
 
+  class Model(hk.Module):
+    def __init__(self, dmid: int, dout: int, name=None):
+      super().__init__(name=name)
+      self.dmid = dmid
+      self.dout = dout
+
+    def __call__(self, x, training: bool):
+      x = Block(self.dmid)(x, training)
+      x = hk.Linear(self.dout)(x)
+      return x
 
-Compact Modules
-******************
+  ---
 
-Linen provides an alternative API for defining modules more compactly. This is especially useful for the common case where the Module consists of only one method that uses parameters and/or sub-modules. Using the compact API the MLP can be rewritten as follows:
+  import flax.linen as nn
 
+  class Block(nn.Module):
+    features: int
 
-.. testcode::
-
-  class CompactMLP(nn.Module):
-    hidden_size: int
-    out_size: int
 
     @nn.compact
-    def __call__(self, x):
-      a = nn.Dense(self.hidden_size)(x)
-      h = nn.relu(a)
-      return nn.Dense(self.out_size)(h)
-
-
-A compact ``Module`` is similar in spirit to a function. It offers a concise notation and restricts external interaction to the inputs and return values of the function. In this case the concise notation might make it easier for others to understand what the Module does. There is no need to jump back and forth between the ``setup`` and ``__call__`` method to understand what the submodules are doing. Instead, simply reading the ``__call__`` method from top to bottom once should provide a concise overview. This can make a significant difference if you are implementing complex Modules with many hyperparameters. See `setup or compact <https://flax.readthedocs.io/en/latest/design_notes/setup_or_nncompact.html>`_ for a practical guide on decding between setup and compact.
+    def __call__(self, x, training: bool):
+      x = nn.Dense(self.features)(x)
+      x = nn.Dropout(0.5, deterministic=not training)(x)
+      x = jax.nn.relu(x)
+      return x
 
-Another benefit of defining submodules and/or variables inline is that you can add arguments to your method when constructing variables. The most common example of this is using shape information to determine the shape of a parameter like this:
+  class Model(nn.Module):
+    dmid: int
+    dout: int
 
 
-.. testcode::
+    @nn.compact
+    def __call__(self, x, training: bool):
+      x = Block(self.dmid)(x, training)
+      x = nn.Dense(self.dout)(x)
+      return x
 
-  class CompactScaledMLP(nn.Module):
-    hidden_size: int
-    out_size: int
+The ``__call__`` method looks very similar in both libraries, however, in Flax
+you have to use the ``@nn.compact`` decorator in order to be able to define
+submodules inline. In Haiku, this is the default behavior.
+
+Now, a place where Haiku and Flax differ substantially is in how you construct
+the model. In Haiku, you use ``hk.transform`` over a function
+that calls your Module, ``transform`` will return an object with ``init``
+and ``apply`` methods. In Flax, you simply instantiate your Module.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def forward(x, training: bool):
+    return Model(256, 10)(x, training)
+
+  model = hk.transform(forward)
+
+  ---
+
+  ...
+
+
+  model = Model(256, 10)
+
+To get the model parameters in both libraries you use the ``init`` method
+with a ``PRNGKey`` plus some inputs to run the model. The main difference here is
+that Flax returns a mapping from collection names to nested array dictionaries,
+``params`` is just one of these possible collections. In Haiku, you get the ``params``
+structure directly.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  sample_x = jax.numpy.ones((1, 784))
+  params = model.init(
+    PRNGKey(0),
+    sample_x, training=False # <== inputs
+  )
+  ...
+
+  ---
+
+  sample_x = jax.numpy.ones((1, 784))
+  variables = model.init(
+    PRNGKey(0),
+    sample_x, training=False # <== inputs
+  )
+  params = variables["params"]
+
+One very important thing to note is that in Flax the parameters structure is
+hierarchical, with one level per nested module and a final level for the
+parameter name.
+In Haiku the parameters structure is a python dictionary with a two level hierarchy:
+the fully qualified module name mapping to the parameter name. The module name
+consists of a ``/`` separated string path of all the nested Modules.
+
+.. tab-set::
+
+  .. tab-item:: Haiku
+    :sync: Haiku
+
+    .. code-block:: python
+
+      ...
+      {
+        'model/block/linear': {
+          'b': (256,),
+          'w': (784, 256),
+        },
+        'model/linear': {
+          'b': (10,),
+          'w': (256, 10),
+        }
+      }
+      ...
+
+
+  .. tab-item:: Flax
+    :sync: Flax
+
+    .. code-block:: python
+
+      FrozenDict({
+        Block_0: {
+          Dense_0: {
+            bias: (256,),
+            kernel: (784, 256),
+          },
+        },
+        Dense_0: {
+          bias: (10,),
+          kernel: (256, 10),
+        },
+      })
+
+During training in both frameworks you pass the parameters structure to the
+``apply`` method to run the forward pass. Since we are using dropout, in
+both cases we must provide a ``key`` to ``apply`` in order to generate
+the random dropout masks.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def train_step(key, params, inputs, labels):
+    def loss_fn(params):
+        logits = model.apply(
+          params,
+          key,
+          inputs, training=True # <== inputs
+        )
+        return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
+
+    grads = jax.grad(loss_fn)(params)
+    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+
+    return params
+
+  ---
+
+  def train_step(key, params, inputs, labels):
+    def loss_fn(params):
+        logits = model.apply(
+          {'params': params},
+          inputs, training=True, # <== inputs
+          rngs={'dropout': key}
+        )
+        return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
 
-    @nn.compact
-    def __call__(self, x):
-      scale = self.param("scale", nn.initializers.ones_init(), x.shape[-1:])
-      x *= scale[None]
-      a = nn.Dense(self.hidden_size)(x)
-      h = nn.relu(a)
-      return nn.Dense(self.out_size)(h)
+    grads = jax.grad(loss_fn)(params)
+    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
+    return params
 
 .. testcode::
   :hide:
 
-  mdl = CompactScaledMLP(hidden_size=4, out_size=5)
-  x = jax.numpy.ones((3, 2))
-  vars = mdl.init(random.PRNGKey(0), x)
-  assert vars["params"]["scale"].shape == (2,)
+  train_step(PRNGKey(0), params, sample_x, jnp.ones((1,), dtype=jnp.int32))
 
-Many of the standard Linen Modules like ``nn.Dense`` use shape inference already to avoid the need to specify input shapes (like the number of input features to a Dense layer).
-
-Compact control flow
-=====================
+The most notable differences is that in Flax you have to
+pass the parameters inside a dictionary with a ``params`` key, and the
+PRNGKey inside a dictionary with a ``dropout`` key. This is because in Flax
+you can have many types of model state and random state. In Haiku, you
+just pass the parameters and the PRNGKey directly.
+
+Handling State
+-----------------
+
+Now let's see how mutable state is handled in both libraries. We will take
+the same model as before, but now we will replace Dropout with BatchNorm.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  class Block(hk.Module):
+    def __init__(self, features: int, name=None):
+      super().__init__(name=name)
+      self.features = features
+
+    def __call__(self, x, training: bool):
+      x = hk.Linear(self.features)(x)
+      x = hk.BatchNorm(
+        create_scale=True, create_offset=True, decay_rate=0.99
+      )(x, is_training=training)
+      x = jax.nn.relu(x)
+      return x
 
-The order in which you define submodules determines the name of a submodule if none is provided explicitly (using the ``name=`` keyword argument passed to the Module's constructor). Because the ``name`` determines how parameters are mapped to submodules, you must be careful about mixing control flow with auto-generated names. Using control flow can change the order or remove certain submodules altogether. This is useful in case a submodule should only exist depending on some construction argument. However, when control flow depends on the input arguments to the Module, you should be careful. For example, the following Module will break:
+  ---
 
+  class Block(nn.Module):
+    features: int
 
-.. testcode::
 
-  class WrongModule(nn.Module):
     @nn.compact
-    def __call__(self, x, mode):
-      if mode == "encode":
-        return nn.Dense(features=8)(x)
-      elif mode == "decode":
-        return nn.Dense(features=4)(x)
-
+    def __call__(self, x, training: bool):
+      x = nn.Dense(self.features)(x)
+      x = nn.BatchNorm(
+        momentum=0.99
+      )(x, use_running_average=not training)
+      x = jax.nn.relu(x)
+      return x
 
-The above Module will break because either the encoder or decoder path will construct a Module named "Dense_0". This means the two Modules will share parameters which is not intented here. Actually, the two Modules cannot share parameters because they each have a different number of features.
+The code is very similar in this case as both libraries provide a BatchNorm
+layer. The most notable difference is that Haiku uses ``is_training`` to
+control whether or not to update the running statistics, whereas Flax uses
+``use_running_average`` for the same purpose.
+
+To instantiate a stateful model in Haiku you use ``hk.transform_with_state``,
+which changes the signature for ``init`` and ``apply`` to accept and return
+state. As before, in Flax you construct the Module directly.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def forward(x, training: bool):
+    return Model(256, 10)(x, training)
+
+  model = hk.transform_with_state(forward)
+
+  ---
+
+  ...
+
+
+  model = Model(256, 10)
+
+
+To initialize both the parameters and state you just call the ``init`` method
+as before. However, in Haiku you now get ``state`` as a second return value, and
+in Flax you get a new ``batch_stats`` collection in the ``variables`` dictionary.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  sample_x = jax.numpy.ones((1, 784))
+  params, state = model.init(
+    PRNGKey(0),
+    sample_x, training=True # <== inputs
+  )
+  ...
+
+  ---
+
+  sample_x = jax.numpy.ones((1, 784))
+  variables = model.init(
+    PRNGKey(0),
+    sample_x, training=False # <== inputs
+  )
+  params, batch_stats = variables["params"], variables["batch_stats"]
+
+
+In general, in Flax you might find other state collections in the ``variables``
+dictionary such as ``cache`` for auto-regressive transformers models,
+``intermediates`` for intermediate values added using ``Module.sow``, or other
+collection names defined by custom layers. Haiku only makes a distinction
+between ``params`` (variables which do not change while running ``apply``) and
+``state`` (variables which can change while running ``apply``).
+
+Now, training looks very similar in both frameworks as you use the same
+``apply`` method to run the forward pass. In Haiku, now pass the ``state``
+as the second argument to ``apply``, and get the new state as the second
+return value. In Flax, you instead add ``batch_stats`` as a new key to the
+input dictionary, and get the ``updates`` variables dictionary as the second
+return value.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def train_step(params, state, inputs, labels):
+    def loss_fn(params):
+      logits, new_state = model.apply(
+        params, state,
+        None, # <== rng
+        inputs, training=True # <== inputs
+      )
+      loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
+      return loss, new_state
+
+    grads, new_state = jax.grad(loss_fn, has_aux=True)(params)
+    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+
+    return params, new_state
+  ---
+
+  def train_step(params, batch_stats, inputs, labels):
+    def loss_fn(params):
+      logits, updates = model.apply(
+        {'params': params, 'batch_stats': batch_stats},
+        inputs, training=True, # <== inputs
+        mutable='batch_stats',
+      )
+      loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
+      return loss, updates["batch_stats"]
 
-This problem can be solved in various ways:
- - Provide explicit names
- - create the modules in ``setup``
- - or move the constructor out of the control flow.
+    grads, batch_stats = jax.grad(loss_fn, has_aux=True)(params)
+    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
-The latter is done as follows:
-
-.. testcode::
-
-  class CorrectModule(nn.Module):
-    @nn.compact
-    def __call__(self, x, mode):
-      encoder = nn.Dense(8)
-      decoder = nn.Dense(4)
-      if mode == "encode":
-        return encoder(x)
-      elif mode == "decode":
-        return decoder(x)
+    return params, batch_stats
 
 .. testcode::
   :hide:
 
-  def init_fn(mdl):
-    x = jax.numpy.ones((3, 2))
-    z = mdl(x, "encode")
-    return mdl(z, "decode")
-
-  mdl = CorrectModule()
-  vars = nn.init(init_fn, mdl)(random.PRNGKey(0))
-  assert vars["params"]["Dense_0"]["kernel"].shape == (2, 8)
-  assert vars["params"]["Dense_1"]["kernel"].shape == (8, 4)
-
-
-In the above example the construction order is fixed. After construction the submodules can be used in an arbitrary order.
-
-.. note::
-   compact modules show a strong resemblance to `React hooks <https://reactjs.org/docs/hooks-custom.html>`_.
-
-
-Top-level Modules
-*****************
-
-When a Module instance is created at the "top-level", it will be in an "unbound" state - that is, it has no variables attached. "Top-level" means it is not constructed as a sub-Module inside another Module class. Apart from calling ``init`` and ``apply``, there is not much you can do with an unbound Module. Note also that ``setup`` is not called on unbound Modules, so you can only access the construction arguments. Refer to the :ref:`Future work` section to learn how this might change in the future.
-
-Why are top-level Modules always unbound?
-===============================================
+  train_step(params, batch_stats, sample_x, jnp.ones((1,), dtype=jnp.int32))
 
-When we call ``apply``, a copy of the top-level Module is created which will actually hold the variables and PRNG sequences. This stateful, "bound", clone only exists while we are executing the apply method. The reason for this is that if you create a stateful object and destroy it before the apply function returns, the ``apply`` function itself behaves like a pure function. A pure function has two constraints:
+One major difference is that in Flax a state collection can be mutable or immutable.
+During ``init`` all collections are mutable by default, however, during ``apply``
+you have to explicitly specify which collections are mutable. In this example,
+we specify that ``batch_stats`` is mutable. Here a single string is passed but a list
+can also be given if there are more mutable collections. If this is not done an
+error will be raised at runtime when trying to mutate ``batch_stats``.
+Also, when ``mutable`` is anything other than ``False``, the ``updates``
+dictionary is returned as the second return value of ``apply``, else only the
+model output is returned.
+Haiku makes the mutable/immutable distinction through having ``params``
+(immutable) and ``state`` (mutable) and using either ``hk.transform`` or
+``hk.transform_with_state``
+
+Using Multiple Methods
+-----------------------
+
+In this section we will take a look at how to use multiple methods in Haiku and Flax.
+As an example, we will implement an auto-encoder model with three methods:
+``encode``, ``decode``, and ``__call__``.
+
+In Haiku, we can just define the submodules that ``encode`` and ``decode`` need
+directly in ``__init__``, in this case each will just use a ``Linear`` layer.
+In Flax, we will define an ``encoder`` and a ``decoder`` Module ahead of time
+in ``setup``, and use them in the ``encode`` and ``decode`` respectively.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  class AutoEncoder(hk.Module):
+
+
+    def __init__(self, embed_dim: int, output_dim: int, name=None):
+      super().__init__(name=name)
+      self.encoder = hk.Linear(embed_dim, name="encoder")
+      self.decoder = hk.Linear(output_dim, name="decoder")
 
-#. If you put the same arguments in, it will return the same outputs
-#. It does not change anything outside the function. This means you cannot manipulate stateful objects that are accessible outside the pure function.
+    def encode(self, x):
+      return self.encoder(x)
 
+    def decode(self, x):
+      return self.decoder(x)
 
-Pure functions have many advantages but when using JAX they are often essential. For example, most code requires compilation using ``jax.jit`` to be fast and once you created a Module you probably want to optimize its parameters using ``jax.grad``. However, these APIs expect a pure function and don't work on stateful bound ``Module`` instances directly. Moreover, pure functions allow for flexible interoperability with other libraries. For example, We recommend `Optax <https://github.com/deepmind/optax>`_ for optimizing parameters. The optimizers in Optax expect and return a PyTree of JAX arrays to optimize, just like the ``apply`` function of a Linen Module.
+    def __call__(self, x):
+      x = self.encode(x)
+      x = self.decode(x)
+      return x
 
-Cloning
-===============================================
+  ---
 
-To make this approach work reliably we need well-defined cloning behavior. Rather than relying on a complex nested cloning procedure like Python's ``deepcopy``, Flax enforces that a ``Module`` is exactly defined by its construction arguments. Therefore cloning a Module reduces to calling the constructor with its original construction arguments. Because ``Module`` acts as an immutable dataclass, the construction arguments are mapped directly to instance attributes. Non-construction attributes that are computed in ``setup`` or ``__post_init__`` should also depend only on the construciton arguments to ensure a well-defined clone.
+  class AutoEncoder(nn.Module):
+    embed_dim: int
+    output_dim: int
 
-Bind
-===============================================
+    def setup(self):
+      self.encoder = nn.Dense(self.embed_dim)
+      self.decoder = nn.Dense(self.output_dim)
 
-Sometimes it's useful to have a bound, top-level Module without having to wrap the code in a function. For example: to interact with a Module inside a Jupyter notebook. The `bind <https://flax.readthedocs.io/en/latest/flax.linen.html?highlight=bind#flax.linen.Module.bind>`_ method returns a bound clone with an unlimited lifetime. The downside of this is that you cannot combine it with JAX transformations or integrate it into a vanilla JAX codebase that expects stateless code. For example, `Optax <https://github.com/deepmind/optax>`_ can optimze a Pytree of parameters but it cannot directly optimize a bound ``Module`` instance created with ``.bind`` (because that's not a Pytree). Thus, you cannot combine the ``bind`` API with a functional optimizer API like Optax.
+    def encode(self, x):
+      return self.encoder(x)
 
+    def decode(self, x):
+      return self.decoder(x)
 
-Setup
-**********
+    def __call__(self, x):
+      x = self.encode(x)
+      x = self.decode(x)
+      return x
 
-The ``setup`` method is often used like the constructor hook (``__init__``) in normal Python classes. However, for more advanced use cases it's good to realize that it is not quite the same as a constructor.
+Note that in Flax ``setup`` doesn't run after ``__init__``, instead it runs
+when ``init`` or ``apply`` are called.
 
-``setup`` is only called after a Module becomes bound. Normally, this is not an issue because most Modules are bound (almost) immediately (as part of ``init`` and ``apply``). Inside ``setup``, sub-modules become bound when they are assigned to an attribute. Inside an ``nn.compact`` decorated method, sub-modules are bound immediately when constructed. As explained in the previous section, top-level Modules are never bound and thus setup is not called when they are constructed. This means you cannot access attributes assigned in setup from an unbound, top-level module.
+Now, we want to be able to call any method from our ``AutoEncoder`` model. In Haiku we
+can define multiple ``apply`` methods for a module through ``hk.multi_transform``. The
+function passed to ``multi_transform`` defines how to initialize the module and which
+different apply methods to generate.
 
-.. testcode::
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
 
-  class TopLevelAccess(nn.Module):
+  def forward():
+    module = AutoEncoder(256, 784)
+    init = lambda x: module(x)
+    return init, (module.encode, module.decode)
 
-    def setup(self):
-      self.foo = nn.Dense(2)
+  model = hk.multi_transform(forward)
 
-  mdl = TopLevelAccess()
-  assert not hasattr(mdl, "foo")  # foo is not defined because setup is not called
+  ---
 
-The ``setup`` method is not called immediately after the ``Module`` becomes bound but only when you interact with the ``Module`` instance (e.g.: call a method or access an attribute). This should not impact the behavior of a ``Module`` but the lazy execution does sometimes affect log statements and stack traces during debugging. The section on functionalization will explain why we need ``setup`` to be lazy in the first place.
+  ...
 
 
-Functionalization
-******************
 
-So far we had a pure ``apply`` function that is typically transformed with some JAX transformations and inside ``apply`` we have a stateful Module instance to work with. In other words: Outside of a Module we are in a functional world where we have the power of JAX's functional transformations and inside the Module we get the power of Flax's stateful variables and PRNG sequence, and the ``apply`` method is our bridge between these two worlds.
 
-But what if we want to use JAX transformations **inside** Modules? The answer to this is functionalization.
+  model = AutoEncoder(256, 784)
 
-This procedure itself is tedious and error-prone but handled internally by Flax. At a high-level we can summarize it as follows. For a method ``fn`` defined within a Module:
 
-#. Collect the state (variables & PRNG sequences) of the Module(s) that should be available inside the JAX transformation and take a snapshot of it.
+To initialize the parameters of our model, ``init`` can be used to trigger the
+``__call__`` method, which uses both the ``encode`` and ``decode``
+method. This will create all the necessary parameters for the model.
 
-#. Call the JAX transformation with the original arguments and the collected state. Then inside the transformation:
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
 
-   #. Unpack the state and recreate the Modules
+  params = model.init(
+    PRNGKey(0),
+    x=jax.numpy.ones((1, 784)),
+  )
+  ...
 
-   #. Call the user code ``fn``
+  ---
 
-   #. Collect the updated variables and rng and return it together with the original return values from ``fn``
+  variables = model.init(
+    PRNGKey(0),
+    x=jax.numpy.ones((1, 784)),
+  )
+  params = variables["params"]
 
-#. Update the original state with the updated state returned from the transformation.
+This generates the following parameter structure.
 
-A more in depth explanation of functionalization and lifting can be found in the `Lifted Transformation <https://flax.readthedocs.io/en/latest/design_notes/lift.html>`_ design note.
+.. tab-set::
 
-Practical consequences
-==========================
+  .. tab-item:: Haiku
+    :sync: Haiku
 
-For the most part functionalization is something that is handled automatically for you. Still there are some constraints that you must take into account. Most importantly, Flax only handles the stateful primitives (Linen variables and RNGs) and not arbitrary stateful Python code. Most importantly: You cannot close over stateful objects and ``Module`` objects because they are invisible to Flax's internals (and to JAX in general).
+    .. code-block:: python
 
+      {
+          'auto_encoder/~/decoder': {
+              'b': (784,),
+              'w': (256, 784)
+          },
+          'auto_encoder/~/encoder': {
+              'b': (256,),
+              'w': (784, 256)
+          }
+      }
 
-.. testcode::
+  .. tab-item:: Flax
+    :sync: Flax
 
-  class Foo(nn.Module):
-    @nn.compact
-    def __call__(self, x):
-      dense = nn.Dense(x.shape[-1])
-      fn = lambda x: dense(x) + 1
-      # simply calling inner works fine
-      # return self.inner(x, fn)
-      # but applying a transformation doesn't:
-      vmap_inner = nn.vmap(Foo.inner, in_axes=0, variable_axes={"params": 0}, split_rngs={"params": True})
-      return vmap_inner(self, x, fn)
-
-    def inner(self, x, fn):
-      for i in range(3):
-        x = fn(x)
-      return x
+    .. code-block:: python
 
-Here ``inner`` takes a function that closes over a Module instance. In this example, that works fine because we are not transforming the inner method with a lifted transformation. Most methods are not transformed but it is good to know how to make Module methods transformable.
+      FrozenDict({
+          decoder: {
+              bias: (784,),
+              kernel: (256, 784),
+          },
+          encoder: {
+              bias: (256,),
+              kernel: (784, 256),
+          },
+      })
 
-The main obstacle for transformability are types that JAX does not recognize. JAX only understands `Pytree <https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html>`_ arguments. That's arbitrarily nested Python containers (dict, list, tuple) of (Jax) numpy ndarrays and Python numbers/bools. Flax allows to define dataclasses which are Pytree compatible using the `flax.struct <https://flax.readthedocs.io/en/latest/flax.struct.html>`_ API.
 
-Function closure is the most common way to accidentally hide a JAX array or Linen Module from a transformation. There is however an easy workaround if you want to pass closures that are also compatible with JAX and Linen transformations:
+Finally, let's explore how we can employ the ``apply`` function to invoke the ``encode`` method:
 
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
 
-.. testcode::
+  encode, decode = model.apply
+  z = encode(
+    params,
+    None, # <== rng
+    x=jax.numpy.ones((1, 784)),
 
-  class Partial(flax.struct.PyTreeNode):
-    fn: Callable = flax.struct.field(pytree_node=False)
-    args: Iterable[Any]
+  )
 
-    def __call__(self, *args, **kwargs):
-      return self.fn(*(tuple(self.args) + args), **kwargs)
+  ---
 
-  class Foo(nn.Module):
+  ...
+  z = model.apply(
+    {"params": params},
 
-    @nn.compact
-    def __call__(self, x):
-      dense = nn.Dense(x.shape[-1])
-      fn = lambda mdl, x: mdl(x) + 1
-      vmap_inner = nn.vmap(Foo.inner, in_axes=0, variable_axes={"params": 0}, split_rngs={"params": True})
-      return vmap_inner(self, x, Partial(fn, [dense]))
-
-    def inner(self, x, fn):
-      for i in range(3):
-        x = fn(x)
-      return x
+    x=jax.numpy.ones((1, 784)),
+    method="encode",
+  )
 
+Because the Haiku ``apply`` function is generated through
+``hk.multi_transform``, it's a tuple of two functions which we can unpack into
+an ``encode`` and ``decode`` function which correspond to the methods on the
+``AutoEncoder`` module. In Flax we call the ``encode`` method through passing
+the method name as a string.
+Another noteworthy distinction here is that in Haiku, ``rng`` needs to be
+explicitly passed, even though the module does not use any stochastic
+operations during ``apply``. In Flax this is not necessary. The Haiku ``rng``
+is set to ``None`` here, but you could also use ``hk.without_apply_rng`` on the
+``apply`` function to remove the ``rng`` argument.
 
-.. testcode::
-  :hide:
 
-  x = jax.numpy.ones((3, 2))
-  mdl = Foo()
-  vars = mdl.init(random.PRNGKey(0), x)
-  assert vars['params']['Dense_0']['kernel'].shape == (3, 2, 2)
+Lifted Transforms
+-----------------
 
+Both Flax and Haiku provide a set of transforms, which we will refer to as lifted transforms,
+that wrap JAX transformations in such a way that they can be used with Modules and sometimes
+provide additional functionality. In this section we will take a look at how to use the
+lifted version of ``scan`` in both Flax and Haiku to implement a simple RNN layer.
 
+To begin, we will first define a ``RNNCell`` module that will contain the logic for a single
+step of the RNN. We will also define a ``initial_state`` method that will be used to initialize
+the state (a.k.a. ``carry``) of the RNN. Like with ``jax.lax.scan``, the ``RNNCell.__call__``
+method will be a function that takes the carry and input, and returns the new
+carry and output. In this case, the carry and the output are the same.
 
-Here the closure is implemented using a Flax dataclass. The function itself is annotated with ``flax.struct.field(pytree_node=False)`` to indicate that it does not contain JAX Arrays or Linen Modules. The partially applied ``args`` on the other hand is treated as a pytree container. We rewrite the closure to use Partial. Now the inner method can be transformed using lifted transformations.
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
 
+  class RNNCell(hk.Module):
+    def __init__(self, hidden_size: int, name=None):
+      super().__init__(name=name)
+      self.hidden_size = hidden_size
 
-Future work
-*************
+    def __call__(self, carry, x):
+      x = jnp.concatenate([carry, x], axis=-1)
+      x = hk.Linear(self.hidden_size)(x)
+      x = jax.nn.relu(x)
+      return x, x
 
+    def initial_state(self, batch_size: int):
+      return jnp.zeros((batch_size, self.hidden_size))
 
-Setup for unbound Modules
-===========================
+  ---
 
-The current Module abstraction is particularly restrictive when it comes to initializing fields after construction. In the current Module API, the ``setup`` method is the place to initialize the fields of  the Module instance. Because ``setup`` is only called on a bound Module, the full Module API is available inside ``setup``, including variable declaration. However, oftentimes we don't actually require any stateful API's to initialize a field. In fact, most commonly we simply want to declare a submodule. More importantly, it's often useful to inspect submodules for debugging or to partially run the model. Consider for example:
+  class RNNCell(nn.Module):
+    hidden_size: int
 
 
-.. testcode::
+    @nn.compact
+    def __call__(self, carry, x):
+      x = jnp.concatenate([carry, x], axis=-1)
+      x = nn.Dense(self.hidden_size)(x)
+      x = jax.nn.relu(x)
+      return x, x
+
+    def initial_state(self, batch_size: int):
+      return jnp.zeros((batch_size, self.hidden_size))
+
+Next, we will define a ``RNN`` Module that will contain the logic for the entire RNN.
+In Haiku, we will first initialze the ``RNNCell``, then use it to construct the ``carry``,
+and finally use ``hk.scan`` to run the ``RNNCell`` over the input sequence. In Flax its
+done a bit differently, we will use ``nn.scan`` to define a new temporary type that wraps
+``RNNCell``. During this process we will also specify instruct ``nn.scan`` to broadcast
+the ``params`` collection (all steps share the same parameters) and to not split the
+``params`` rng stream (so all steps intialize with the same parameters), and finally
+we will specify that we want scan to run over the second axis of the input and stack
+the outputs along the second axis as well. We will then use this temporary type immediately
+to create an instance of the lifted ``RNNCell`` and use it to create the ``carry`` and
+the run the ``__call__`` method which will ``scan`` over the sequence.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  class RNN(hk.Module):
+    def __init__(self, hidden_size: int, name=None):
+      super().__init__(name=name)
+      self.hidden_size = hidden_size
 
-  class AutoEncoder(nn.Module):
-    def setup(self):
-      self.encoder = Encoder(...)
-      self.decoder = Decoder(...)
+    def __call__(self, x):
+      cell = RNNCell(self.hidden_size)
+      carry = cell.initial_state(x.shape[0])
+      carry, y = hk.scan(cell, carry, jnp.swapaxes(x, 1, 0))
+      y = jnp.swapaxes(y, 0, 1)
+      return y
 
+  ---
 
-Imagine we want to call just the decoder using `auto_encoder.decoder.apply(decoder_variables, x)`. With the current setup API this does not work because we must first bind the variables before setup is called and the decoder attribute is defined. Of course we can manually construct the Decoder Module with the same attributes as in setup but this is not ideal in many cases.
+  class RNN(nn.Module):
+    hidden_size: int
 
-There are two possible solutions to make this use case more ergonomic. First, setup could be made to run immediately after construction before it becomes bound. This means you can still create sub modules but you can no longer define or manipulate variables. Therefore, this would be a breaking change and it would require a new API for defining variables lazily
 
-Alternatively, an additional special method could be introduced that runs right away after Module construction and before it becomes bound. In this case, the ``setup`` method would preserve its original semantics.
+    @nn.compact
+    def __call__(self, x):
+      rnn = nn.scan(RNNCell, variable_broadcast='params', split_rngs={'params': False},
+                    in_axes=1, out_axes=1)(self.hidden_size)
+      carry = rnn.initial_state(x.shape[0])
+      carry, y = rnn(carry, x)
+      return y
+
+In general, the main difference between lifted transforms between Flax and Haiku is that
+in Haiku the lifted transforms don't operate over the state, that is, Haiku will handle the
+``params`` and ``state`` in such a way that it keeps the same shape inside and outside of the
+transform. In Flax, the lifted transforms can operate over both variable collections and rng
+streams, the user must define how different collections are treated by each transform
+according to the transform's semantics.
+
+Finally, let's quickly view how the ``RNN`` Module would be used in both Haiku and Flax.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def forward(x):
+    return RNN(64)(x)
+
+  model = hk.without_apply_rng(hk.transform(forward))
+
+  params = model.init(
+    PRNGKey(0),
+    x=jax.numpy.ones((3, 12, 32)),
+  )
+
+  y = model.apply(
+    params,
+    x=jax.numpy.ones((3, 12, 32)),
+  )
+
+  ---
+
+  ...
+
+
+  model = RNN(64)
+
+  variables = model.init(
+    PRNGKey(0),
+    x=jax.numpy.ones((3, 12, 32)),
+  )
+  params = variables['params']
+  y = model.apply(
+    {'params': params},
+    x=jax.numpy.ones((3, 12, 32)),
+  )
+
+The only notable change with respect to the examples in the previous sections is that
+this time around we used ``hk.without_apply_rng`` in Haiku so we didn't have to
+pass the ``rng`` argument as ``None`` to the ``apply`` method.
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `flax-0.6.9/docs/examples_community_examples.rst` & `flax-0.7.0/docs/examples_community_examples.rst`

 * *Files 1% similar despite different names*

```diff
@@ -87,20 +87,20 @@
 
 Contributing policy
 *******************
 
 If you are interested in adding a project to the Community Examples section, take the following
 into consideration:
 
-* **Code examples**: Examples for must contain a README that is helpful, clear, and explains
+* **Code examples**: Examples must contain a README that is helpful, clear, and explains
   how to run the code. The code itself should be easy to follow.
 * **Tutorials**: These docs should preferrably be a Jupyter Notebook format
   (refer to `Contributing <https://flax.readthedocs.io/en/latest/contributing.html>`__
   to learn how to convert a Jupyter Notebook into a Markdown file with `jupytext`).
-  Your tutorial should be well-written, and discuss/decsribe an interesting topic/task.
+  Your tutorial should be well-written, and discuss/describe an interesting topic/task.
   To avoid duplication, the content of these docs must be different from
   `existing docs on the Flax documentation site <https://flax.readthedocs.io/>`__
   or other community examples mentioned in this document.
 * **Models**: repositories with models ported to Flax must provide at least one of the following:
 
   * Metrics that are comparable to the original work when the model is trained to completion. Having
     available plots of the metric's history during training is highly encouraged.
```

### Comparing `flax-0.6.9/docs/examples_core_examples.rst` & `flax-0.7.0/docs/examples_core_examples.rst`

 * *Files 0% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 - :octicon:`mark-github;0.9em` `MNIST <https://github.com/google/flax/tree/main/examples/mnist/>`__ -
   `Interactive <https://colab.research.google.com/github/google/flax/blob/main/examples/mnist/mnist.ipynb>`__:
   Convolutional neural network for MNIST classification (featuring simple
   code).
 
 - :octicon:`mark-github;0.9em` `ImageNet <https://github.com/google/flax/tree/main/examples/imagenet/>`__ -
   `Interactive <https://colab.research.google.com/github/google/flax/blob/main/examples/imagenet/imagenet.ipynb>`__:
-  Resnet-50 on ImageNet with weight decay (featuring multi host SPMD, custom
+  Resnet-50 on ImageNet with weight decay (featuring multi-host SPMD, custom
   preprocessing, checkpointing, dynamic scaling, mixed precision).
 
 Reinforcement learning
 **********************
 
 - :octicon:`mark-github;0.9em` `Proximal Policy Optimization <https://github.com/google/flax/tree/main/examples/ppo/>`__:
   Learning to play Atari games (featuring single host SPMD, RL setup).
```

### Comparing `flax-0.6.9/docs/examples_google_research_examples.rst` & `flax-0.7.0/docs/examples_google_research_examples.rst`

 * *Files 0% similar despite different names*

```diff
@@ -40,15 +40,15 @@
 
 - `Code on GitHub <https://github.com/google-research/google-research/tree/master/coltran>`__
 
 - Research paper:
 
   - `Colorization Transformer <https://openreview.net/forum?id=5NA1PinlGFu>`__ (Kumar et al., 2020)
 
-    - *"We presented the Colorization Transformer (ColTran), an architecture that entirely relies on selfattention for image colorization. We introduce conditional transformer layers, a novel building block for conditional, generative models based on self-attention. Our ablations show the superiority of employing this mechanism over a number of different baselines. Finally, we demonstrate that ColTran can generate diverse, high-fidelity colorizations on ImageNet, which are largely indistinguishable from the ground-truth even for human raters."*
+    - *"We presented the Colorization Transformer (ColTran), an architecture that entirely relies on self-attention for image colorization. We introduce conditional transformer layers, a novel building block for conditional, generative models based on self-attention. Our ablations show the superiority of employing this mechanism over a number of different baselines. Finally, we demonstrate that ColTran can generate diverse, high-fidelity colorizations on ImageNet, which are largely indistinguishable from the ground-truth even for human raters."*
 
 Vision Transformer (ViT), MLP-Mixer Architectures *and* Big Vision
 ==================================================================
 
 - Code on GitHub:
 
   - `Vision Transformer and MLP-Mixer Architectures <https://github.com/google-research/vision_transformer>`__
```

### Comparing `flax-0.6.9/docs/examples_repositories_that_use_flax.rst` & `flax-0.7.0/docs/examples_repositories_that_use_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flax.png` & `flax-0.7.0/docs/flax.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flip/0000-template.md` & `flax-0.7.0/docs/flip/0000-template.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flip/1009-optimizer-api.md` & `flax-0.7.0/docs/flip/1009-optimizer-api.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flip/1777-default-dtype.md` & `flax-0.7.0/docs/flip/1777-default-dtype.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flip/2434-general-metadata.md` & `flax-0.7.0/docs/flip/2434-general-metadata.md`

 * *Files 1% similar despite different names*

```diff
@@ -126,17 +126,17 @@
 Calling ``jax.tree_map`` on a boxed value will simply map over the value in the box.
 The lifted transforms that need to handle metadata will call ``jax.tree_map(..., is_leaf=lambda x: isinstance(x, AxisMetadata))`` to find the AxisMetadata instances within a PyTree.
 
 Advantages of the boxing approach:
 1. Boxing can be used outside of Flax and metadata is automatically "inherited". For example, the optimizer state will
    have the same partitioning spec as the parameters, because the state is initialized using a ``jax.tree_map`` over the boxed parameters.
 2. Boxes are composable.
-3. Boxing avoids string manipulation and generally avoids having to handle additional auxilary collections like "param_axes" in the current
+3. Boxing avoids string manipulation and generally avoids having to handle additional auxiliary collections like "param_axes" in the current
    partitioning API.
-4. No need to lift metadata collections seperately.
+4. No need to lift metadata collections separately.
 
 
 Disadvantages:
 1. Adding the boxes changes the PyTree hierarchy and introduces dataclasses within the otherwise plain, nested dict of variables.
 3. Custom Pytree nodes have a small runtime overhead. It's hard to observe this in practise because JAX calls are async.
 
 
@@ -183,15 +183,15 @@
 Initializing a model that creates partitioned weights would result in the following variable structure:
 
 ```python
 variables = partitioned_dense.init(rng, jnp.ones((4,)))
 jax.tree_map(np.shape, variables)  # => {"params": {"kernel": Partitioned(value=(4, 8), names=(None, "data")), bias: (8,)}}
 ```
 
-The variable tree with metadata can be used to integrate with other libaries and APIs.
+The variable tree with metadata can be used to integrate with other libraries and APIs.
 For example, we can turn the ``Partitioned`` metadata into ``jax.pjit`` sharding annotations:
 
 ```python
 def to_sharding_spec(x):
   if isinstance(x, Partitioned):
     return PartitionSpec(*x.names)
   else:
```

### Comparing `flax-0.6.9/docs/flip/2974-kw-only-dataclasses.md` & `flax-0.7.0/docs/flip/2974-kw-only-dataclasses.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/flip/README.md` & `flax-0.7.0/docs/flip/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/getting_started.ipynb` & `flax-0.7.0/docs/getting_started.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9832529196046336%*

 * *Differences: {"'cells'": "{5: {'source': {insert: [(3, '[Flax "*

 * *            "Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html).\\n'), "*

 * *            '(7, '*

 * *            "'[`@compact`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/decorators.html#flax.linen.compact)\\n')], "*

 * *            "delete: [7, 3]}}, 7: {'source': {insert: [(2, 'Create an instance of the Flax Module "*

 * *            'and use the '*

 * *            '[`Module.tabulate`](https://flax.readthedocs.io/en/latest/api []*

```diff
@@ -101,19 +101,19 @@
             "metadata": {
                 "id": "7057395a"
             },
             "source": [
                 "## 3. Define network\n",
                 "\n",
                 "Create a convolutional neural network with the Linen API by subclassing\n",
-                "[Flax Module](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
+                "[Flax Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html).\n",
                 "Because the architecture in this example is relatively simple\u2014you're just\n",
                 "stacking layers\u2014you can define the inlined submodules directly within the\n",
                 "`__call__` method and wrap it with the\n",
-                "[`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)\n",
+                "[`@compact`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/decorators.html#flax.linen.compact)\n",
                 "decorator. To learn more about the Flax Linen `@compact` decorator, refer to the [`setup` vs `compact`](https://flax.readthedocs.io/en/latest/guides/setup_or_nncompact.html) guide."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 49,
             "id": "cbc079cd",
@@ -152,15 +152,15 @@
             "id": "hy7iRu7_zlx-",
             "metadata": {
                 "id": "hy7iRu7_zlx-"
             },
             "source": [
                 "### View model layers\n",
                 "\n",
-                "Create an instance of the Flax Module and use the [`Module.tabulate`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.tabulate) method to visualize a table of the model layers by passing an RNG key and template image input."
+                "Create an instance of the Flax Module and use the [`Module.tabulate`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.tabulate) method to visualize a table of the model layers by passing an RNG key and template image input."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 50,
             "id": "lDHfog81zLQa",
             "metadata": {
@@ -356,15 +356,15 @@
             },
             "source": [
                 "## 5. Training step\n",
                 "\n",
                 "A function that:\n",
                 "\n",
                 "- Evaluates the neural network given the parameters and a batch of input images\n",
-                "  with [`TrainState.apply_fn`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) (which contains the [`Module.apply`](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n",
+                "  with [`TrainState.apply_fn`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) (which contains the [`Module.apply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.apply)\n",
                 "  method (forward pass)).\n",
                 "- Computes the cross entropy loss, using the predefined [`optax.softmax_cross_entropy_with_integer_labels()`](https://optax.readthedocs.io/en/latest/api.html#optax.softmax_cross_entropy_with_integer_labels). Note that this function expects integer labels, so there is no need to convert labels to onehot encoding.\n",
                 "- Evaluates the gradient of the loss function using\n",
                 "  [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad).\n",
                 "- Applies a\n",
                 "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)\n",
                 "  of gradients to the optimizer to update the model's parameters.\n",
@@ -468,29 +468,30 @@
                 "num_epochs = 10\n",
                 "batch_size = 32\n",
                 "\n",
                 "train_ds, test_ds = get_datasets(num_epochs, batch_size)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "id": "809ae1a0",
             "metadata": {
                 "id": "809ae1a0"
             },
             "source": [
                 "## 8. Seed randomness\n",
                 "\n",
                 "- Set the TF random seed to ensure dataset shuffling (with `tf.data.Dataset.shuffle`) is reproducible.\n",
                 "- Get one\n",
                 "  [PRNGKey](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax.random.PRNGKey)\n",
                 "  and use it for parameter initialization. (Learn\n",
                 "  more about\n",
                 "  [JAX PRNG design](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)\n",
-                "  and [PRNG chains](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables).)"
+                "  and [PRNG chains](https://flax.readthedocs.io/en/latest/philosophy.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables).)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 58,
             "id": "xC4MFyBsfT-U",
             "metadata": {
@@ -848,13 +849,14 @@
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst",
             "main_language": "python"
         },
         "language_info": {
-            "name": "python"
+            "name": "python",
+            "version": "3.9.6"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 5
 }
```

### Comparing `flax-0.6.9/docs/getting_started.md` & `flax-0.7.0/docs/getting_started.md`

 * *Files 2% similar despite different names*

```diff
@@ -73,19 +73,19 @@
 ```
 
 +++ {"id": "7057395a"}
 
 ## 3. Define network
 
 Create a convolutional neural network with the Linen API by subclassing
-[Flax Module](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).
+[Flax Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html).
 Because the architecture in this example is relatively simpleyou're just
 stacking layersyou can define the inlined submodules directly within the
 `__call__` method and wrap it with the
-[`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)
+[`@compact`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/decorators.html#flax.linen.compact)
 decorator. To learn more about the Flax Linen `@compact` decorator, refer to the [`setup` vs `compact`](https://flax.readthedocs.io/en/latest/guides/setup_or_nncompact.html) guide.
 
 ```{code-cell}
 ---
 executionInfo:
   elapsed: 53
   status: ok
@@ -112,15 +112,15 @@
     return x
 ```
 
 +++ {"id": "hy7iRu7_zlx-"}
 
 ### View model layers
 
-Create an instance of the Flax Module and use the [`Module.tabulate`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.tabulate) method to visualize a table of the model layers by passing an RNG key and template image input.
+Create an instance of the Flax Module and use the [`Module.tabulate`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.tabulate) method to visualize a table of the model layers by passing an RNG key and template image input.
 
 ```{code-cell}
 ---
 executionInfo:
   elapsed: 103
   status: ok
   timestamp: 1673483483427
@@ -217,15 +217,15 @@
 +++ {"id": "a15de484"}
 
 ## 5. Training step
 
 A function that:
 
 - Evaluates the neural network given the parameters and a batch of input images
-  with [`TrainState.apply_fn`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) (which contains the [`Module.apply`](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)
+  with [`TrainState.apply_fn`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) (which contains the [`Module.apply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.apply)
   method (forward pass)).
 - Computes the cross entropy loss, using the predefined [`optax.softmax_cross_entropy_with_integer_labels()`](https://optax.readthedocs.io/en/latest/api.html#optax.softmax_cross_entropy_with_integer_labels). Note that this function expects integer labels, so there is no need to convert labels to onehot encoding.
 - Evaluates the gradient of the loss function using
   [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad).
 - Applies a
   [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)
   of gradients to the optimizer to update the model's parameters.
@@ -307,15 +307,15 @@
 
 - Set the TF random seed to ensure dataset shuffling (with `tf.data.Dataset.shuffle`) is reproducible.
 - Get one
   [PRNGKey](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax.random.PRNGKey)
   and use it for parameter initialization. (Learn
   more about
   [JAX PRNG design](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)
-  and [PRNG chains](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables).)
+  and [PRNG chains](https://flax.readthedocs.io/en/latest/philosophy.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables).)
 
 ```{code-cell}
 ---
 executionInfo:
   elapsed: 59
   status: ok
   timestamp: 1673483485268
```

### Comparing `flax-0.6.9/docs/glossary.rst` & `flax-0.7.0/docs/glossary.rst`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
       is created through regular Python object construction (e.g. `module = SomeModule(args...)`, it is in an *unbound* state. This means that only
       dataclass attributes are set, and no variables are bound to the module. When the pure
       functions :meth:`Module.init() <flax.linen.Module.init>`
       or :meth:`Module.apply() <flax.linen.Module.apply>`
       are called, Flax clones the Module and binds the variables to it, and the module's method code is
       executed in a locally bound state, allowing things like calling submodules directly without
       providing variables. For more details, refer to the
-      `module lifecycle <https://flax.readthedocs.io/en/latest/advanced_topics/module_lifecycle.html>`__.
+      `module lifecycle <https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html>`__.
 
     Compact / Non-compact Module
       Modules with a single method are able to declare submodules and variables inline by
       using the  :func:`@nn.compact <flax.linen.compact>` decorator.
       These are referred to as compact-style modules,
       whereas modules defining a :meth:`setup() <flax.linen.Module.setup>` method
       (usually but not always with multiple callable methods)
@@ -49,15 +49,15 @@
       Variables in Flax are initialized late, only when needed. That is, during normal
       execution of a module, if a requested variable name isnt found in the provided
       variable collection data, we call the initializer function to create it. This
       allows us to treat initialization and application under the same code-paths,
       simplifying the use of JAX transforms with layers.
 
     Lifted transformation
-      Refer to the `Flax docs <https://flax.readthedocs.io/en/latest/advanced_topics/lift.html>`__.
+      Refer to the `Flax docs <https://flax.readthedocs.io/en/latest/developer_notes/lift.html>`__.
 
     Module
       A dataclass allowing the definition and initialization of parameters in a
       referentially-transparent form. This is responsible for storing and updating variables
       and parameters within itself. Modules can be readily transformed into functions,
       allowing them to be trivially used with JAX transformations like `vmap` and `scan`.
 
@@ -70,39 +70,39 @@
       `PRNG <https://en.wikipedia.org/wiki/Pseudorandom_number_generator>`__
       key through :meth:`Module.make_rng() <flax.linen.Module.make_rng>`.
       These keys can be used to generate random numbers through
       `JAX's functional random number generators <https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html>`__.
       Having different RNG sequences (e.g. for "params" and "dropout") allows fine-grained
       control in a multi-host setup (e.g. initializing parameters identically on different
       hosts, but have different dropout masks) and treating these sequences differently when
-      `lifting transformations <https://flax.readthedocs.io/en/latest/advanced_topics/lift.html>`__.
+      `lifting transformations <https://flax.readthedocs.io/en/latest/developer_notes/lift.html>`__.
 
     Scope
       A container class for holding the variables and PRNG keys for each layer.
 
     Shape inference
       Modules do not need to specify the shape of the input array in their definitions.
       Flax upon initialization inspects the input array, and infers the correct shapes
       for parameters in the model.
 
     TrainState
       Refer to :class:`flax.training.train_state.TrainState`.
 
     Variable
-      The `weights / parameters / data / arrays <https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.core.variables.Variable>`__
+      The `weights / parameters / data / arrays <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html#flax.linen.Variable>`__
       residing in the leaves of :term:`variable collections<Variable collections>`.
       Variables are defined inside modules using :meth:`Module.variable() <flax.linen.Module.variable>`.
       A variable of collection "params" is simply called a param and can be set using
       :meth:`Module.param() <flax.linen.Module.param>`.
 
     Variable collections
       Entries in the variable dict, containing weights / parameters / data / arrays that
       are used by the model. params is the canonical collection in the variable dict.
       They are typically differentiable, updated by an outer SGD-like loop / optimizer,
       rather than modified directly by forward-pass code.
 
-    `Variable dictionary <https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.core.variables>`__
+    `Variable dictionary <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`__
       A dictionary containing :term:`variable collections<Variable collections>`.
       Each variable collection is a mapping from a string name
       (e.g., ":term:`params<Params / parameters>`" or "batch_stats") to a (possibly nested)
       dictionary with :term:`Variables<Variable>` as leaves, matching the submodule tree structure.
       Read more about pytrees and leaves in the `Jax docs <https://jax.readthedocs.io/en/latest/pytrees.html>`__.
```

### Comparing `flax-0.6.9/docs/guides/batch_norm.rst` & `flax-0.7.0/docs/guides/batch_norm.rst`

 * *Files 2% similar despite different names*

```diff
@@ -65,15 +65,15 @@
 
 The ``batch_stats`` collection
 ******************************
 
 In addition to the ``params`` collection, ``BatchNorm`` also adds a ``batch_stats`` collection
 that contains the running average of the batch statistics.
 
-Note: You can learn more in the ``flax.linen`` `variables <https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.core.variables>`__
+Note: You can learn more in the ``flax.linen`` `variables <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`__
 API documentation.
 
 The ``batch_stats`` collection must be extracted from the ``variables`` for later use.
 
 .. codediff::
   :title_left: No BatchNorm
   :title_right: With BatchNorm
@@ -102,25 +102,15 @@
 
 .. codediff::
   :title_left: No BatchNorm
   :title_right: With BatchNorm
   :sync:
 
   FrozenDict({
-
-
-
-
-
-
     'params': {
-
-
-
-
       'Dense_0': {
           'bias': (4,),
           'kernel': (3, 4),
       },
       'Dense_1': {
           'bias': (1,),
           'kernel': (4, 1),
@@ -167,15 +157,14 @@
   :title_left: No BatchNorm
   :title_right: With BatchNorm
   :sync:
 
   y = mlp.apply(
     {'params': params},
     x,
-
   )
   ...
 
   ---
   y, updates = mlp.apply( #!
     {'params': params, 'batch_stats': batch_stats}, #!
     x,
@@ -196,16 +185,14 @@
   :title_left: No BatchNorm
   :title_right: With BatchNorm
   :sync:
 
   from flax.training import train_state
 
 
-
-
   state = train_state.TrainState.create(
     apply_fn=mlp.apply,
     params=params,
 
     tx=optax.adam(1e-3),
   )
   ---
```

### Comparing `flax-0.6.9/docs/guides/dropout.rst` & `flax-0.7.0/docs/guides/dropout.rst`

 * *Files 1% similar despite different names*

```diff
@@ -56,15 +56,15 @@
 **********************************
 
 To create a model with dropout:
 
 * Subclass :meth:`flax.linen.Module`, and then use
   :meth:`flax.linen.Dropout` to add a dropout layer. Recall that
   :meth:`flax.linen.Module` is the
-  `base class for all neural network Modules <https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module>`__,
+  `base class for all neural network Modules <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html>`__,
   and all layers and models are subclassed from it.
 
 * In :meth:`flax.linen.Dropout`, the ``deterministic`` argument is required to
   be passed as a keyword argument, either:
 
   * When constructing the :meth:`flax.linen.Module`; or
   * When calling :meth:`flax.linen.init()` or :meth:`flax.linen.apply()` on a constructed ``Module``. (Refer to :meth:`flax.linen.module.merge_param` for more details.)
@@ -105,16 +105,14 @@
   class MyModel(nn.Module):
     num_neurons: int
 
     @nn.compact
     def __call__(self, x):
       x = nn.Dense(self.num_neurons)(x)
 
-
-
       return x
   ---
   class MyModel(nn.Module):
     num_neurons: int
 
     @nn.compact
     def __call__(self, x, training: bool): #!
@@ -128,15 +126,15 @@
 ********************
 
 After creating your model:
 
 * Instantiate the model.
 * Then, in the :meth:`flax.linen.init()` call, set ``training=False``.
 * Finally, extract the ``params`` from the
-  `variable dictionary <https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.core.variables>`__.
+  `variable dictionary <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`__.
 
 Here, the main difference between the code without Flax ``Dropout``
 and with ``Dropout`` is that the ``training`` (or ``train``) argument must be
 provided if you need dropout enabled.
 
 .. codediff::
   :title_left: No Dropout
@@ -200,17 +198,14 @@
 .. codediff::
   :title_left: No Dropout
   :title_right: With Dropout
   :sync:
 
   from flax.training import train_state
 
-
-
-
   state = train_state.TrainState.create(
     apply_fn=my_model.apply,
     params=params,
 
     tx=optax.adam(1e-3)
   )
   ---
```

### Comparing `flax-0.6.9/docs/guides/ensembling.rst` & `flax-0.7.0/docs/guides/ensembling.rst`

 * *Files 1% similar despite different names*

```diff
@@ -269,10 +269,10 @@
 
 .. |jax.jit()| replace:: ``jax.jit()``
 .. _jax.jit(): https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#To-JIT-or-not-to-JIT
 .. |jax.pmap()| replace:: ``jax.pmap()``
 .. _jax.pmap(): https://jax.readthedocs.io/en/latest/jax.html#jax.pmap
 .. |jax.lax.pmean()| replace:: ``jax.lax.pmean()``
 .. _jax.lax.pmean(): https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.pmean.html
-.. _Module.init: https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.init
+.. _Module.init: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init
 .. _`JIT mechanics: tracing and static variables`: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#JIT-mechanics:-tracing-and-static-variables
 .. _`MNIST example`: https://github.com/google/flax/blob/main/examples/mnist/train.py
```

### Comparing `flax-0.6.9/docs/guides/extracting_intermediates.rst` & `flax-0.7.0/docs/guides/extracting_intermediates.rst`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 Extracting intermediate values
 ==============================
 
-This pattern will show you how to extract intermediate values from a module.
+This guide will show you how to extract intermediate values from a module.
 Let's start with this simple CNN that uses :code:`nn.compact`.
 
 .. testsetup::
 
+  import flax
   import flax.linen as nn
   import jax
   import jax.numpy as jnp
   from flax.core import FrozenDict
   from typing import Sequence
 
   batch = jnp.ones((4, 32, 32, 3))
@@ -84,22 +85,22 @@
       x = nn.relu(x)
       x = nn.Dense(features=10)(x)
       x = nn.log_softmax(x)
       return x
 
 ``sow`` acts as a no-op when the variable collection is not mutable.
 Therefore, it works perfectly for debugging and optional tracking of intermediates.
-The 'intermediates' collection is also used by the ``capture_intermediates`` API (see final section).
+The 'intermediates' collection is also used by the ``capture_intermediates`` API (see the :ref:`Use ``capture_intermediates``` section).
 
 Note that, by default ``sow`` appends values every time it is called:
 
 * This is necessary because once instantiated, a module could be called multiple
   times in its parent module, and we want to catch all the sowed values.
-* So you want to make sure that you **do not** feed intermediate values back in
-  in ``variables``. Otherwise every call will increase the length of that tuple
+* Therefore you want to make sure that you **do not** feed intermediate values back
+  into ``variables``. Otherwise every call will increase the length of that tuple
   and trigger a recompile.
 * To override the default append behavior, specify ``init_fn`` and ``reduce_fn``
   - see :meth:`Module.sow() <flax.linen.Module.sow>`.
 
 .. testcode::
 
   class SowCNN2(nn.Module):
@@ -185,15 +186,16 @@
 
 
 Use ``capture_intermediates``
 -----------------------------
 
 Linen supports the capture of intermediate return values from submodules automatically without any code changes.
 This pattern should be considered the "sledge hammer" approach to capturing intermediates.
-As a debugging and inspection tool it is very useful but using the other patterns described in this howto.
+As a debugging and inspection tool it is very useful, but using the other patterns described in this guide
+will give you more fine-grained control over what intermediates you want to extract.
 
 In the following code example we check if any intermediate activations are non-finite (NaN or infinite):
 
 .. testcode::
 
   @jax.jit
   def init(key, x):
@@ -206,27 +208,94 @@
     intermediates = state['intermediates']
     fin = jax.tree_util.tree_map(lambda xs: jnp.all(jnp.isfinite(xs)), intermediates)
     return y, fin
 
   variables = init(jax.random.PRNGKey(0), batch)
   y, is_finite = predict(variables, batch)
   all_finite = all(jax.tree_util.tree_leaves(is_finite))
-  assert all_finite, "non finite intermediate detected!"
+  assert all_finite, "non-finite intermediate detected!"
 
 By default only the intermediates of ``__call__`` methods are collected.
-Alternatively, you can pass a custom filter based on the ``Module`` instance and the method name.
+Alternatively, you can pass a custom filter function based on the ``Module`` instance and the method name.
 
 .. testcode::
 
   filter_Dense = lambda mdl, method_name: isinstance(mdl, nn.Dense)
   filter_encodings = lambda mdl, method_name: method_name == "encode"
 
   y, state = CNN().apply(variables, batch, capture_intermediates=filter_Dense, mutable=["intermediates"])
   dense_intermediates = state['intermediates']
 
+Note that ``capture_intermediates`` will only apply to layers. You can use ``self.sow`` to manually store
+non-layer intermediates, but the filter function won't be applied to it.
+
+.. codediff::
+  :title_left: Capturing all layer intermediates
+  :title_right: Using filter function and ``self.sow()``
+
+  class Model(nn.Module):
+    @nn.compact
+    def __call__(self, x):
+      a = nn.Dense(4)(x) # Dense_0
+      b = nn.Dense(4)(x) # Dense_1
+      c = a + b # not a Flax layer, so won't be stored as an intermediate
+      d = nn.Dense(4)(c) # Dense_2
+      return d
+
+  @jax.jit
+  def init(key, x):
+    variables = Model().init(key, x)
+    return variables['params']
+
+  @jax.jit
+  def predict(params, x):
+    return Model().apply({"params": params}, x, capture_intermediates=True)
+
+  batch = jax.random.uniform(jax.random.PRNGKey(1), (1,3))
+  params = init(jax.random.PRNGKey(0), batch)
+  preds, feats = predict(params, batch)
+  feats # intermediate c in Model was not stored because it's not a Flax layer
+  ---
+  class Model(nn.Module):
+    @nn.compact
+    def __call__(self, x):
+      a = nn.Dense(4)(x) # Dense_0
+      b = nn.Dense(4)(x) # Dense_1
+      c = a + b
+      self.sow('intermediates', 'c', c) # store intermediate c #!
+      d = nn.Dense(4)(c) # Dense_2
+      return d
+
+  @jax.jit
+  def init(key, x):
+    variables = Model().init(key, x)
+    return variables['params']
+
+  @jax.jit
+  def predict(params, x):
+    # filter specifically for only the Dense_0 and Dense_2 layer #!
+    filter_fn = lambda mdl, method_name: isinstance(mdl.name, str) and (mdl.name in {'Dense_0', 'Dense_2'}) #!
+    return Model().apply({"params": params}, x, capture_intermediates=filter_fn) #!
+
+  batch = jax.random.uniform(jax.random.PRNGKey(1), (1,3))
+  params = init(jax.random.PRNGKey(0), batch)
+  preds, feats = predict(params, batch)
+  feats # intermediate c in Model is stored and isn't filtered out by the filter function #!
+
+To separate the intermediates extracted from ``self.sow`` from the intermediates extracted from ``capture_intermediates``,
+we can either define a separate collection like ``self.sow('sow_intermediates', 'c', c)``, or manually filter out
+the intermediates after calling ``.apply()``. For example:
+
+.. testcode::
+
+  flattened_dict = flax.traverse_util.flatten_dict(feats['intermediates'], sep='/')
+  flattened_dict['c']
+
+In terms of efficiency, as long as everything is jitted, then any intermediates you don't end up using
+should be optimized away by XLA.
 
 Use ``Sequential``
 ---------------------
 
 You could also define ``CNN`` using a simple implementation of a ``Sequential`` combinator (this is quite common in more stateful approaches). This may be useful
 for very simple models and gives you arbitrary model
 surgery. But it can be very limiting -- if you even want to add one conditional, you are
@@ -263,14 +332,15 @@
     variables = SeqCNN().init(key, x)
     return variables['params']
 
   @jax.jit
   def features(params, x):
     return Sequential(SeqCNN().layers[0:7]).apply({"params": params}, x)
 
+  batch = jnp.ones((1,28,28,1))
   params = init(jax.random.PRNGKey(0), batch)
   features(params, batch)
 
 Extracting gradients of intermediate values
 ===========================================
 For debugging purposes, it can be useful to extract the gradients of intermediate values.
 This can be done by using the :meth:`Module.perturb() <flax.linen.Module.perturb>` method over the desired values.
```

### Comparing `flax-0.6.9/docs/guides/flax_basics.ipynb` & `flax-0.7.0/docs/guides/flax_basics.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/flax_basics.md` & `flax-0.7.0/docs/guides/flax_basics.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/flax_on_pjit.ipynb` & `flax-0.7.0/docs/notebooks/linen_intro.ipynb`

 * *Files 21% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.790472706705179%*

 * *Differences: {"'cells'": "{0: {'metadata': {'id': 'C1QVJFlVsxcZ'}, 'source': {insert: [(0, '[![Open in "*

 * *            "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/linen_intro.ipynb)\\n'), "*

 * *            "(1, '[![Open On "*

 * *            "GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/linen_intro.ipynb)\\n'), "*

 * *            '(3, \'# Preface\\n\'),  []*

```diff
@@ -1,991 +1,1170 @@
 {
     "cells": [
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "2a9f78765c0c"
+                "id": "C1QVJFlVsxcZ"
             },
             "source": [
-                "# Scale up Flax Modules on multiple devices with `pjit`\n",
+                "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/linen_intro.ipynb)\n",
+                "[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/linen_intro.ipynb)\n",
                 "\n",
-                "This guide shows how to scale up [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html) on multiple devices and hosts using JAX's [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) and [`flax.linen.spmd`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.linen.spmd).\n",
+                "# Preface\n",
                 "\n",
-                "## Flax and `pjit`\n",
+                "<br>\n",
+                "<div style=\"font-variant: small-caps;\">CAVEAT PROGRAMMER</div>\n",
                 "\n",
-                "[`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) provides a way to automatically compile and scale up JAX computations. `pjit` has the following benefits:\n",
+                "The below is an alpha API preview and things might break.  The surface syntax of the features of the API are not fixed in stone, and we welcome feedback on any points."
+            ]
+        },
+        {
+            "attachments": {},
+            "cell_type": "markdown",
+            "metadata": {
+                "id": "23zkGDayszYI"
+            },
+            "source": [
+                "## Useful links\n",
                 "\n",
-                "* `pjit` has the similar interface of [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) and works as a decorator on a function that needs to be compiled.\n",
-                "* When using `pjit`, you can write code as if it runs on a single device, and `pjit` will automatically compile and run it on multiple devices using the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm. \n",
-                "* With `pjit` you can state how the input and output of your code is partitioned across devices, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.\n",
+                "\u27f6 [Slides](https://docs.google.com/presentation/d/1ngKWUwsSqAwPRvATG8sAxMzu9ujv4N__cKsUofdNno0/edit?usp=sharing) for the core ideas of the new Functional Core and Linen\n",
                 "\n",
-                "To learn more, refer to [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html) and [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html).\n",
+                "\u27f6 \"Design tests\" guided our design process. Many are available for [functional core](https://github.com/google/flax/tree/main/examples/core_design_test) and some for the [proposed Module abstraction](https://github.com/google/flax/tree/main/examples/linen_design_test/)\n",
                 "\n",
-                "Flax provides several functionalities that can help you use `pjit` on [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html), including:\n",
+                "\u27f6 Ported examples: [ImageNet](https://github.com/google/flax/tree/main/examples/imagenet) and [WMT](https://github.com/google/flax/tree/main/examples/wmt) (to the proposed Module abstraction). TODO: Port to functional core.\n",
                 "\n",
-                "1. An interface to specify partitions of your data when defining [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module).\n",
-                "2. Utility functions to generate the partition information that `pjit` requires to run.\n",
-                "3. An interface to customize your axis names called \"logical axis annotations\" to decouple both your Module code and partition plan to experiment with different partition layouts more easily."
+                "\u27f6 Our new [discussion forums](https://github.com/google/flax/discussions/)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "0fa8ccbf573a"
+                "id": "vGtC_5W4mQnY"
             },
             "source": [
-                "## Setup\n",
-                "\n",
-                "Install Flax from HEAD:"
+                "# Install and Import"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 15,
+            "execution_count": null,
             "metadata": {
-                "id": "867203db3bef",
+                "id": "HgRZ_G8wGcoB",
                 "tags": [
                     "skip-execution"
                 ]
             },
             "outputs": [],
             "source": [
-                "# Once Flax v0.6.4 is released, use `pip3 install flax`.\n",
-                "! pip3 install -qq \"git+https://github.com/google/flax.git@main#egg=flax\""
+                "# Install the newest JAXlib version.\n",
+                "!pip install --upgrade -q pip jax jaxlib\n",
+                "# Install Flax at head:\n",
+                "!pip install --upgrade -q git+https://github.com/google/flax.git"
             ]
         },
         {
+            "cell_type": "code",
+            "execution_count": 2,
+            "metadata": {
+                "id": "Kvx7GmavHZbD"
+            },
+            "outputs": [],
+            "source": [
+                "import functools\n",
+                "from typing import Any, Callable, Sequence, Optional\n",
+                "import jax\n",
+                "from jax import lax, random, numpy as jnp\n",
+                "from flax.core import freeze, unfreeze\n",
+                "from flax import linen as nn"
+            ]
+        },
+        {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "a9601432b448"
+                "id": "u86fYsrEfYow"
             },
             "source": [
-                "## Imports\n",
-                "\n",
-                "Import some necessary dependencies.\n",
-                "\n",
-                "**Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. Check out the [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html#setup) to learn more about emulating a multi-device TPU environment (in which case you should ignore running `os.environ`)."
+                "# Invoking Modules"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 16,
+            "attachments": {},
+            "cell_type": "markdown",
             "metadata": {
-                "id": "f8f42d1174e5"
+                "id": "nrVbFrh1ffve"
             },
-            "outputs": [],
             "source": [
-                "import os\n",
-                "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
+                "Let's instantiate a `Dense` layer.\n",
+                " - Modules are actually objects in this API, so we provide _constructor arguments_ when initializing the Module.  In this case, we only have to provide the output `features` dimension."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 17,
+            "execution_count": 3,
             "metadata": {
-                "id": "b8da40732f0b"
+                "id": "EcDH20Uufc-v"
             },
             "outputs": [],
             "source": [
-                "import functools\n",
-                "import numpy as np\n",
-                "import jax\n",
-                "\n",
-                "from jax import lax, random, numpy as jnp\n",
-                "\n",
-                "import flax\n",
-                "from flax import struct, traverse_util, linen as nn\n",
-                "from flax.linen import spmd # Flax Linen SPMD.\n",
-                "from flax.core import freeze, unfreeze\n",
-                "from flax.training import train_state, checkpoints\n",
-                "\n",
-                "import optax # Optax for common losses and optimizers. "
+                "model = nn.Dense(features=3)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "c0d280def897"
+                "id": "hL4NgtBwgI0S"
             },
             "source": [
-                "Next, import all the `pjit`-related libraries.\n",
-                "\n",
-                "> **Note:** [`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) is still in the experimental package of JAX, so there may be changes in the API in future.\n",
+                "We need to initialize the Module variables, these include the parameters of the Module as well as any other state variables.\n",
                 "\n",
-                "1. Start a 2x4 device mesh (8 devices)\u2014this is the same as the layout of [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).\n",
-                "2. Annotate each axis with a name. A typical way to annotate axis names is `('data', 'model')`, where:\n",
-                "  * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.\n",
-                "  * `'model'`: the mesh dimension used for sharding parameters of the model across devices."
+                "We call the `init` method on the instantiated Module.  If the Module `__call__` method has args `(self, *args, **kwargs)` then we call `init` with `(rngs, *args, **kwargs)` so in this case, just `(rng, input)`:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 18,
+            "execution_count": 4,
             "metadata": {
-                "id": "684fe9fe13a0"
+                "id": "Vjx0HWNcfa8h",
+                "outputId": "3adfaeaf-977e-4e82-8adf-d254fae6eb91"
             },
             "outputs": [
                 {
-                    "name": "stdout",
+                    "name": "stderr",
                     "output_type": "stream",
                     "text": [
-                        "[[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
-                        " [CpuDevice(id=4) CpuDevice(id=5) CpuDevice(id=6) CpuDevice(id=7)]]\n"
+                        "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
                     ]
                 },
                 {
                     "data": {
                         "text/plain": [
-                            "Mesh(device_ids=array([[0, 1, 2, 3],\n",
-                            "       [4, 5, 6, 7]]), axis_names=('data', 'model'))"
+                            "FrozenDict({\n",
+                            "    params: {\n",
+                            "        kernel: DeviceArray([[ 0.6503669 ,  0.8678979 ,  0.46042678],\n",
+                            "                     [ 0.05673932,  0.9909285 , -0.63536596],\n",
+                            "                     [ 0.76134115, -0.3250529 , -0.6522163 ],\n",
+                            "                     [-0.8243032 ,  0.4150194 ,  0.19405058]], dtype=float32),\n",
+                            "        bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
+                            "    },\n",
+                            "})"
                         ]
                     },
-                    "execution_count": 18,
-                    "metadata": {},
+                    "execution_count": 4,
+                    "metadata": {
+                        "tags": []
+                    },
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from jax.experimental.pjit import pjit, with_sharding_constraint\n",
-                "from jax.sharding import Mesh, PartitionSpec\n",
-                "from jax.experimental import mesh_utils\n",
+                "# Make RNG Keys and a fake input.\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
                 "\n",
-                "# Start a device mesh.\n",
-                "device_mesh = mesh_utils.create_device_mesh((2, 4))\n",
-                "print(device_mesh)\n",
-                "# Annotate each axis with a name.\n",
-                "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\n",
-                "mesh"
+                "# provide key and fake input to get initialized variables\n",
+                "init_variables = model.init(key2, x)\n",
+                "\n",
+                "init_variables"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "307d39db6d94"
+                "id": "ubFTzroGhErh"
             },
             "source": [
-                "## Define a layer\n",
+                "We call the `apply` method on the instantiated Module.  If the Module `__call__` method has args `(self, *args, **kwargs)` then we call `apply` with `(variables, *args, rngs=<RNGS>, mutable=<MUTABLEKINDS>, **kwargs)` where\n",
+                " - `<RNGS>` are the optional _call time_ RNGs for things like dropout. For simple Modules this is just a single key, but if your module has multiple __kinds__ of data, it's a dictionary of rng-keys per-kind, e.g. `{'params': key0, 'dropout': key1}` for a Module with dropout layers.\n",
+                " - `<MUTABLEKINDS>` is an optional list of names of __kinds__ that are expected to be mutated during the call. e.g. `['batch_stats']` for a layer updating batchnorm statistics.\n",
                 "\n",
-                "Before defining a model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`), which creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.\n",
-                "\n",
-                "To use this layer in `pjit` efficiently, apply the following APIs to annotate the parameters and intermediate variables correctly:\n",
-                "\n",
-                "1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating parameters `W1` and `W2`.\n",
-                "\n",
-                "2. Apply [`pjit.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern under `pjit` when the ideal constraint is known.\n",
-                "\n",
-                "  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because `pjit` will figure out the same sharding layout for `y` and `z` regardless."
+                "So in this case, just `(variables, input)`:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 19,
+            "execution_count": 5,
             "metadata": {
-                "id": "b74c049968dc"
+                "id": "R9QZ6EOBg5X8",
+                "outputId": "e8c389a6-29f3-4f93-97ea-703e85a8b811"
             },
-            "outputs": [],
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "DeviceArray([[ 0.5035518 ,  1.8548559 , -0.4270196 ],\n",
+                            "             [ 0.0279097 ,  0.5589246 , -0.43061775],\n",
+                            "             [ 0.35471284,  1.5741    , -0.3286552 ],\n",
+                            "             [ 0.5264864 ,  1.2928858 ,  0.10089308]], dtype=float32)"
+                        ]
+                    },
+                    "execution_count": 5,
+                    "metadata": {
+                        "tags": []
+                    },
+                    "output_type": "execute_result"
+                }
+            ],
             "source": [
-                "class DotReluDot(nn.Module):\n",
-                "  depth: int\n",
-                "  @nn.compact\n",
-                "  def __call__(self, x):\n",
-                "    W1 = self.param(\n",
-                "        'W1', \n",
-                "        nn.with_partitioning(nn.initializers.xavier_normal(), (None, 'model')),\n",
-                "        (x.shape[-1], self.depth))\n",
-                "\n",
-                "    y = jax.nn.relu(jnp.dot(x, W1))\n",
-                "    # Force a local sharding annotation.\n",
-                "    y = with_sharding_constraint(y, PartitionSpec('data', 'model'))\n",
-                "\n",
-                "    W2 = self.param(\n",
-                "        'W2', \n",
-                "        nn.with_partitioning(nn.initializers.xavier_normal(), ('model', None)),\n",
-                "        (self.depth, x.shape[-1]))\n",
-                "\n",
-                "    z = jnp.dot(y, W2)\n",
-                "    # Force a local sharding annotation.\n",
-                "    z = with_sharding_constraint(z, PartitionSpec('data', None))\n",
-                "\n",
-                "    # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.\n",
-                "    return z, None"
+                "y = model.apply(init_variables, x)\n",
+                "y"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "cbac5321c08e"
+                "id": "lNH06qc1hPrd"
             },
             "source": [
-                "Note that device axis names like `'data'`, `'model'` or `None` are passed into both `flax.linen.with_partitioning` and `pjit_with_sharding_constraint` API calls. This refers to how each dimension of this data should be sharded \u2014 either across one of the device mesh dimensions, or not sharded at all.\n",
-                "\n",
-                "For example:\n",
-                "\n",
-                "* When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:\n",
-                "\n",
-                "  * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.\n",
-                "  * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.\n",
-                "\n",
-                "* When you annotate the output `z` as `('data', None)`:\n",
-                "\n",
-                "  * The first dimension \u2014 the batch dimension \u2014 will be sharded over the `'data'` axis. This means half of the batch will be processed on devices `0-3` (first four devices), and another half on devices `4-7` (the remaining four devices).\n",
-                "  * The second dimension \u2014 the data depth dimension \u2014 will be replicated across all devices."
+                "Additional points:\n",
+                " - If you want to `init` or `apply` a Module using a method other than call, you need to provide the `method=` kwarg to `init` and `apply` to use it instead of the default `__call__`, e.g. `method='encode'`, `method='decode'` to apply the encode/decode methods of an autoencoder."
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "b8389c11af79"
+                "id": "jjsyiBjIYcAB"
             },
             "source": [
-                "## Define a model with `flax.linen.scan` lifted transformation\n",
-                "\n",
-                "This guide uses `flax.linen.scan` to demonstrate how [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations), such as `scan`, can work together with [JAX `pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html).\n",
-                "\n",
-                "Having created `DotReluDot`, define the `MLP` model (by subclassing `flax.linen.Module`) as multiple layers of `DotReluDot`.\n",
-                "\n",
-                "To replicate identical layers, you can either use `flax.linen.scan`, or a for-loop:\n",
-                "\n",
-                "* `flax.linen.scan` can offer faster compilation times.\n",
-                "* The for-loop can be faster on runtime.\n",
-                "\n",
-                "The code below shows how to apply both methods.\n",
-                "\n",
-                "**Note:** `flax.linen.scan` has another dimension for the parameters (the dimension over which `scan` is applied). You need to use the [`metadata_params`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html#flax.linen.scan) argument to annotate the partition of this dimension. Since the parameters inside your `DotReluDot` (a sub-`Module`) are already sharded along the `model` axis, you don't need to partition multiple layers across the `model` dimension here, and therefore you should denote it as `None`."
+                "# Defining Basic Modules"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 20,
+            "attachments": {},
+            "cell_type": "markdown",
             "metadata": {
-                "id": "a0ea0dcccbc3"
+                "id": "UvU7416Ti_lR"
             },
-            "outputs": [],
             "source": [
-                "class MLP(nn.Module):\n",
-                "  num_layers: int\n",
-                "  depth: int\n",
-                "  use_scan: bool\n",
-                "  @nn.compact\n",
-                "  def __call__(self, x):\n",
-                "    if self.use_scan:\n",
-                "      x, _ = nn.scan(DotReluDot, length=self.num_layers, \n",
-                "                     variable_axes={\"params\": 0},\n",
-                "                     split_rngs={\"params\": True},\n",
-                "                     metadata_params={nn.PARTITION_NAME: None}\n",
-                "                     )(self.depth)(x)\n",
-                "    else:\n",
-                "      for i in range(self.num_layers):\n",
-                "        x, _ = DotReluDot(self.depth)(x)\n",
-                "    return x"
+                "## Composing submodules"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "5b3abfef359d"
+                "id": "LkTy0hmJdE5G"
             },
             "source": [
-                "## Specify sharding (includes initialization and `TrainState` creation)\n",
-                "\n",
-                "Next, generate the [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) that `pjit` should receive as annotations of _input_ and _output_ data. `PartitionSpec` is a tuple of 2 axes (in a 2x4 mesh). To learn more, refer to [JAX-101: Introduction to `pjit`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html).\n",
-                "\n",
-                "### Specify the input\n",
-                "\n",
-                "For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `data`:"
+                "We support declaring modules in `setup()` that can still benefit from shape inference by using __Lazy Initialization__ that sets up variables the first time the Module is called."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 21,
+            "execution_count": 6,
             "metadata": {
-                "id": "4b8472d462f2"
+                "id": "qB6l-9EabOwH",
+                "outputId": "1a6c6a17-0b95-42c2-b5bf-b9ad80fd7758",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "PartitionSpec('data', None)"
-                        ]
-                    },
-                    "execution_count": 21,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameter shapes:\n",
+                        " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
+                        "output:\n",
+                        " [[ 4.2292815e-02 -4.3807115e-02  2.9323792e-02  6.5492536e-03\n",
+                        "  -1.7147182e-02]\n",
+                        " [ 1.2967804e-01 -1.4551792e-01  9.4432175e-02  1.2521386e-02\n",
+                        "  -4.5417294e-02]\n",
+                        " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
+                        "   0.0000000e+00]\n",
+                        " [ 9.3024090e-04  2.7864411e-05  2.4478839e-04  8.1344356e-04\n",
+                        "  -1.0110775e-03]]\n"
+                    ]
                 }
             ],
             "source": [
-                "x_spec = PartitionSpec('data', None)  # dimensions: (batch, length)\n",
-                "x_spec"
+                "class ExplicitMLP(nn.Module):\n",
+                "  features: Sequence[int]\n",
+                "\n",
+                "  def setup(self):\n",
+                "    # we automatically know what to do with lists, dicts of submodules\n",
+                "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
+                "    # for single submodules, we would just write:\n",
+                "    # self.layer1 = nn.Dense(feat1)\n",
+                "\n",
+                "  def __call__(self, inputs):\n",
+                "    x = inputs\n",
+                "    for i, lyr in enumerate(self.layers):\n",
+                "      x = lyr(x)\n",
+                "      if i != len(self.layers) - 1:\n",
+                "        x = nn.relu(x)\n",
+                "    return x\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = ExplicitMLP(features=[3,4,5])\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
+                "\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('output:\\n', y)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "06d134795ae1"
+                "id": "slwE6ULqc_t_"
             },
             "source": [
-                "### Generate a `PartitionSpec` for the output\n",
-                "\n",
-                "Next, generate a [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) for the _output_, you need to use some actual output as a reference.\n",
-                "\n",
-                "1. Instantiate a model.\n",
-                "2. Evaluate `model.init` abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).\n",
-                "3. Use [`flax.linen.get_partition_spec`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_partition_spec.html) to automatically generate the `PartitionSpec`.\n",
-                "\n",
-                "The code below shows how to get the output spec if you use `flax.training.train_state` to carry out your initialization and training steps, in which case your `pjit`ted function will output a `TrainState`. \n",
-                "\n",
-                "(In a simpler case, people might choose the variable dict as in `variables = model.init(k, x)` as their `pjit`ted function's output. That works too.)"
+                "Here we show the equivalent compact form of the MLP that declares the submodules inline using the `@compact` decorator."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 22,
+            "execution_count": 7,
             "metadata": {
-                "id": "8b913a2e57d3"
+                "id": "UPNGIr6wcGaw",
+                "outputId": "b3709789-e66e-4e20-f6b2-04022f8a62bb",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "TrainState(step=None, apply_fn=<bound method Module.apply of MLP(\n",
-                            "    # attributes\n",
-                            "    num_layers = 4\n",
-                            "    depth = 1024\n",
-                            "    use_scan = True\n",
-                            ")>, params=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
-                        ]
-                    },
-                    "execution_count": 22,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameter shapes:\n",
+                        " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
+                        "output:\n",
+                        " [[ 4.2292815e-02 -4.3807115e-02  2.9323792e-02  6.5492536e-03\n",
+                        "  -1.7147182e-02]\n",
+                        " [ 1.2967804e-01 -1.4551792e-01  9.4432175e-02  1.2521386e-02\n",
+                        "  -4.5417294e-02]\n",
+                        " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
+                        "   0.0000000e+00]\n",
+                        " [ 9.3024090e-04  2.7864411e-05  2.4478839e-04  8.1344356e-04\n",
+                        "  -1.0110775e-03]]\n"
+                    ]
                 }
             ],
             "source": [
-                "# MLP hyperparameters.\n",
-                "BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, True\n",
-                "# Create fake inputs.\n",
-                "x = jnp.ones((BATCH, DEPTH))\n",
-                "# Initialize a PRNG key.\n",
-                "k = random.PRNGKey(0)\n",
-                "\n",
-                "# Create an Optax optimizer.\n",
-                "optimizer = optax.adam(learning_rate=0.001)\n",
-                "# Instantiate the model.\n",
-                "model = MLP(LAYERS, DEPTH, USE_SCAN)\n",
+                "class SimpleMLP(nn.Module):\n",
+                "  features: Sequence[int]\n",
                 "\n",
-                "# A functional way of model initialization.\n",
-                "def init_fn(k, x, model, optimizer):\n",
-                "  variables = model.init(k, x) # Initialize the model.\n",
-                "  state = train_state.TrainState.create( # Create a `TrainState`.\n",
-                "    apply_fn=model.apply,\n",
-                "    params=variables['params'],\n",
-                "    tx=optimizer)\n",
-                "  return state\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs):\n",
+                "    x = inputs\n",
+                "    for i, feat in enumerate(self.features):\n",
+                "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
+                "      if i != len(self.features) - 1:\n",
+                "        x = nn.relu(x)\n",
+                "      # providing a name is optional though!\n",
+                "      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n",
+                "      # x = nn.Dense(feat)(x)\n",
+                "    return x\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = SimpleMLP(features=[3,4,5])\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
                 "\n",
-                "with mesh:\n",
-                "  # Create an abstract closure to wrap the function before feeding it in\n",
-                "  # because `jax.eval_shape` only takes pytrees as arguments`.\n",
-                "  abstract_variables = jax.eval_shape(\n",
-                "      functools.partial(init_fn, model=model, optimizer=optimizer), k, x)\n",
-                "# This `state_spec` has the same pytree structure as the output\n",
-                "# of the `init_fn`.\n",
-                "state_spec = nn.get_partition_spec(abstract_variables)\n",
-                "state_spec"
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('output:\\n', y)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "2ec24614050b"
+                "id": "b2OzKXYyjFSf"
             },
             "source": [
-                "## Apply `pjit` to compile the code\n",
+                "## Declaring and using variables"
+            ]
+        },
+        {
+            "attachments": {},
+            "cell_type": "markdown",
+            "metadata": {
+                "id": "uYwS5KbcmYIp"
+            },
+            "source": [
+                "Flax uses lazy initialization, which allows declared variables to be initialized only at the first site of their use, using whatever shape information is available a the local call site for shape inference.  Once a variable has been initialized, a reference to the data is kept for use in subsequent calls.\n",
+                "\n",
+                "For declaring parameters that aren't mutated inside the model, but rather by gradient descent, we use the syntax:\n",
+                "\n",
+                " `self.param(parameter_name, parameter_init_fn, *init_args)`\n",
                 "\n",
-                "Now you can apply JAX [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) to your `init_fn` in a similar fashion as [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) but with two extra arguments: `in_axis_resources` and `out_axis_resources`.\n",
+                "with arguments:\n",
+                " - `parameter_name` just the name, a string\n",
+                " - `parameter_init_fn` a function taking an RNG key and a variable number of other arguments, i.e. `fn(rng, *args)`. typically those in `nn.initializers` take an `rng` and a `shape` argument.\n",
+                " - the remaining arguments to feed to the init function when initializing.\n",
                 "\n",
-                "You need to add a `with mesh:` context when running a `pjit`ted function, so that it can refer to `mesh` (an instance of `jax.sharding.Mesh`) to allocate data on devices correctly."
+                "Again, we'll demonstrate declaring things inline as we typically do using the `@compact` decorator."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 23,
+            "execution_count": 8,
             "metadata": {
-                "id": "a298c5d03c0d"
+                "id": "7OACbTFHjMvl",
+                "outputId": "bc5cb1f2-c5e9-4159-d131-73247009e32f",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "TrainState(step=(), apply_fn=<bound method Module.apply of MLP(\n",
-                            "    # attributes\n",
-                            "    num_layers = 4\n",
-                            "    depth = 1024\n",
-                            "    use_scan = True\n",
-                            ")>, params=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
-                            "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=(), mu=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
-                            "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    ScanDotReluDot_0: {\n",
-                            "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
-                            "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
-                        ]
-                    },
-                    "execution_count": 23,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameters:\n",
+                        " FrozenDict({\n",
+                        "    params: {\n",
+                        "        kernel: DeviceArray([[ 0.6503669 ,  0.8678979 ,  0.46042678],\n",
+                        "                     [ 0.05673932,  0.9909285 , -0.63536596],\n",
+                        "                     [ 0.76134115, -0.3250529 , -0.6522163 ],\n",
+                        "                     [-0.8243032 ,  0.4150194 ,  0.19405058]], dtype=float32),\n",
+                        "        bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
+                        "    },\n",
+                        "})\n",
+                        "output:\n",
+                        " [[ 0.5035518   1.8548559  -0.4270196 ]\n",
+                        " [ 0.0279097   0.5589246  -0.43061775]\n",
+                        " [ 0.35471284  1.5741     -0.3286552 ]\n",
+                        " [ 0.5264864   1.2928858   0.10089308]]\n"
+                    ]
                 }
             ],
             "source": [
-                "pjit_init_fn = pjit(init_fn,\n",
-                "                    static_argnums=(2, 3),\n",
-                "                    in_axis_resources=(PartitionSpec(None), x_spec),  # PRNG key and x\n",
-                "                    out_axis_resources=state_spec\n",
-                "                    )\n",
-                "with mesh:\n",
-                "  initialized_state = pjit_init_fn(k, x, model, optimizer)\n",
-                "jax.tree_map(jnp.shape, initialized_state)"
+                "class SimpleDense(nn.Module):\n",
+                "  features: int\n",
+                "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
+                "  bias_init: Callable = nn.initializers.zeros_init()\n",
+                "\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs):\n",
+                "    kernel = self.param('kernel',\n",
+                "                        self.kernel_init,  # RNG passed implicitly.\n",
+                "                        (inputs.shape[-1], self.features))  # shape info.\n",
+                "    y = lax.dot_general(inputs, kernel,\n",
+                "                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n",
+                "    bias = self.param('bias', self.bias_init, (self.features,))\n",
+                "    y = y + bias\n",
+                "    return y\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = SimpleDense(features=3)\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
+                "\n",
+                "print('initialized parameters:\\n', init_variables)\n",
+                "print('output:\\n', y)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "8f74b009f11f"
+                "id": "KgEwkrkfdlt8"
             },
             "source": [
-                "## Inspect the Module output\n",
-                "\n",
-                "Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record metadata associated with it. You can access the raw `jax.Array` by adding `.value` or running `.unbox()`.\n",
-                "\n",
-                "You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of the JAX array, which gives a hint on the way it is partitioned."
+                "We can also declare variables in setup, though in doing so you can't take advantage of shape inference and have to provide explicit shape information at initialization.  The syntax is a little repetitive in this case right now, but we do force agreement of the assigned names."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 24,
+            "execution_count": 9,
             "metadata": {
-                "id": "19243982c892"
+                "id": "CE0CTLVvZ8Yn",
+                "outputId": "1e822bd8-7a08-4e80-e0e6-a86637c46772",
+                "tags": []
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "<class 'flax.core.meta.Partitioned'>\n",
-                        "<class 'jaxlib.xla_extension.Array'>\n",
-                        "(4, 1024, 1024)\n"
+                        "initialized parameters:\n",
+                        " FrozenDict({\n",
+                        "    params: {\n",
+                        "        kernel: DeviceArray([[ 0.6503669 ,  0.8678979 ,  0.46042678],\n",
+                        "                     [ 0.05673932,  0.9909285 , -0.63536596],\n",
+                        "                     [ 0.76134115, -0.3250529 , -0.6522163 ],\n",
+                        "                     [-0.8243032 ,  0.4150194 ,  0.19405058]], dtype=float32),\n",
+                        "        bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
+                        "    },\n",
+                        "})\n",
+                        "output:\n",
+                        " [[ 0.5035518   1.8548559  -0.4270196 ]\n",
+                        " [ 0.0279097   0.5589246  -0.43061775]\n",
+                        " [ 0.35471284  1.5741     -0.3286552 ]\n",
+                        " [ 0.5264864   1.2928858   0.10089308]]\n"
                     ]
                 }
             ],
             "source": [
-                "print(type(initialized_state.params['ScanDotReluDot_0']['W1']))\n",
-                "print(type(initialized_state.params['ScanDotReluDot_0']['W1'].value))\n",
-                "print(initialized_state.params['ScanDotReluDot_0']['W1'].value.shape)"
+                "class ExplicitDense(nn.Module):\n",
+                "  features_in: int  # <-- explicit input shape\n",
+                "  features: int\n",
+                "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
+                "  bias_init: Callable = nn.initializers.zeros_init()\n",
+                "\n",
+                "  def setup(self):\n",
+                "    self.kernel = self.param('kernel',\n",
+                "                             self.kernel_init,\n",
+                "                             (self.features_in, self.features))\n",
+                "    self.bias = self.param('bias', self.bias_init, (self.features,))\n",
+                "\n",
+                "  def __call__(self, inputs):\n",
+                "    y = lax.dot_general(inputs, self.kernel,\n",
+                "                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n",
+                "    y = y + self.bias\n",
+                "    return y\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = ExplicitDense(features_in=4, features=3)\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
+                "\n",
+                "print('initialized parameters:\\n', init_variables)\n",
+                "print('output:\\n', y)"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 25,
+            "attachments": {},
+            "cell_type": "markdown",
             "metadata": {
-                "id": "2067c419a826"
+                "id": "t4MVj1RBmxsZ"
             },
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "OpShardingSharding({devices=[1,1,4,2]0,4,1,5,2,6,3,7 last_tile_dim_replicate})\n"
-                    ]
-                }
-            ],
             "source": [
-                "print(initialized_state.params['ScanDotReluDot_0']['W1'].value.sharding)"
+                "## General Variables"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "273547d3ab89"
+                "id": "CJatarOTpByQ"
             },
             "source": [
-                "You can use [`jax.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays."
+                "For declaring generally mutable _variables_ that may be mutated inside the model we use the call:\n",
+                "\n",
+                " `self.variable(variable_kind, variable_name, variable_init_fn, *init_args)`\n",
+                "\n",
+                "with arguments:\n",
+                " - `variable_kind` the \"kind\" of state this variable is, i.e. the name of the nested-dict collection that this will be stored in inside the top Modules variables.  e.g. `batch_stats` for the moving statistics for a batch norm layer or `cache` for autoregressive cache data.  Note that parameters also have a kind, but they're set to the default `param` kind.\n",
+                " - `variable_name` just the name, a string\n",
+                " - `variable_init_fn` a function taking a variable number of other arguments, i.e. `fn(*args)`. Note that we __don't__ assume the need for an RNG, if you _do_ want an RNG, provide it via a `self.make_rng(variable_kind)` call in the provided arguments.\n",
+                " - the remaining arguments to feed to the init function when initializing.\n",
+                "\n",
+                "\u26a0\ufe0f Unlike parameters, we expect these to be mutated, so `self.variable` returns not a constant, but a _reference_ to the variable.  To __get__ the raw value, you'd write `myvariable.value` and to __set__ it `myvariable.value = new_value`."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 26,
+            "execution_count": 10,
             "metadata": {
-                "id": "29b3dae156a2"
+                "id": "u6_fbrW2XT5t",
+                "outputId": "2a8f5453-81b1-44dc-a431-d14b372c5710",
+                "tags": []
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "FrozenDict({\n",
-                        "    W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
-                        "    W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
+                        "initialized variables:\n",
+                        " FrozenDict({\n",
+                        "    counter: {\n",
+                        "        count: DeviceArray(0, dtype=int32),\n",
+                        "    },\n",
                         "})\n",
-                        "<class 'jaxlib.xla_extension.Array'>\n",
-                        "(4, 1024, 1024)\n"
+                        "mutated variables:\n",
+                        " FrozenDict({\n",
+                        "    counter: {\n",
+                        "        count: DeviceArray(1, dtype=int32),\n",
+                        "    },\n",
+                        "})\n",
+                        "output:\n",
+                        " 1\n"
                     ]
                 }
             ],
             "source": [
-                "diff = jax.tree_map(\n",
-                "    lambda a, b: a - b, \n",
-                "    initialized_state.params['ScanDotReluDot_0'], initialized_state.params['ScanDotReluDot_0'])\n",
-                "print(jax.tree_map(jnp.shape, diff))\n",
-                "diff_array = diff['W1'].unbox()\n",
-                "print(type(diff_array))\n",
-                "print(diff_array.shape)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {
-                "id": "f7e1ccb14c6b"
-            },
-            "source": [
-                "## Apply `pjit` to the train step and inference \n",
+                "class Counter(nn.Module):\n",
+                "  @nn.compact\n",
+                "  def __call__(self):\n",
+                "    # easy pattern to detect if we're initializing\n",
+                "    is_initialized = self.has_variable('counter', 'count')\n",
+                "    counter = self.variable('counter', 'count', lambda: jnp.zeros((), jnp.int32))\n",
+                "    if is_initialized:\n",
+                "      counter.value += 1\n",
+                "    return counter.value\n",
+                "\n",
+                "\n",
+                "key1 = random.PRNGKey(0)\n",
+                "\n",
+                "model = Counter()\n",
+                "init_variables = model.init(key1)\n",
+                "print('initialized variables:\\n', init_variables)\n",
                 "\n",
-                "Now, you create a `pjit`ted training step:"
+                "y, mutated_variables = model.apply(init_variables, mutable=['counter'])\n",
+                "\n",
+                "print('mutated variables:\\n', mutated_variables)\n",
+                "print('output:\\n', y)"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 27,
+            "attachments": {},
+            "cell_type": "markdown",
             "metadata": {
-                "id": "4e3cc300cfee"
+                "id": "VLxwg2aMxUmy"
             },
-            "outputs": [],
             "source": [
-                "def train_step(state, x):\n",
-                "  # A fake loss function.\n",
-                "  def loss_unrolled(params):\n",
-                "    y = model.apply({'params': params}, x)\n",
-                "    return y.sum()\n",
-                "  grad_fn = jax.grad(loss_unrolled)\n",
-                "  grads = grad_fn(state.params)\n",
-                "  state = state.apply_gradients(grads=grads)\n",
-                "  return state\n",
-                "\n",
-                "pjit_step_fn = pjit(train_step,\n",
-                "                    in_axis_resources=(state_spec, x_spec),  # input annotations\n",
-                "                    out_axis_resources=state_spec,           # output annotations\n",
-                "                    )\n",
-                "with mesh:\n",
-                "  new_state = pjit_step_fn(initialized_state, x)"
+                "## Another Mutability and RNGs Example"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "2bae79e2e71b"
+                "id": "NOARPIowyeXS"
             },
             "source": [
-                "Apply `pjit` to inference. Note that, similar to `jax.jit`, you can use a decorator like `@functools.partial(pjit, ...)` to directly compile your function."
+                "Let's make an artificial, goofy example that mixes differentiable parameters, stochastic layers, and mutable variables:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 28,
+            "execution_count": 11,
             "metadata": {
-                "id": "c9264a48b9ee"
+                "id": "BBrbcEdCnQ4o",
+                "outputId": "8f299a5c-74c8-476c-93fa-e5543901ec45",
+                "tags": []
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "<class 'jaxlib.xla_extension.Array'>\n",
-                        "float32\n",
-                        "(8, 1024)\n"
+                        "updated variables:\n",
+                        " FrozenDict({\n",
+                        "    params: {\n",
+                        "        Dense_0: {\n",
+                        "            kernel: DeviceArray([[ 0.6498898 , -0.5000124 ,  0.78573596],\n",
+                        "                         [-0.25609785, -0.7132329 ,  0.2500864 ],\n",
+                        "                         [-0.64630085,  0.39321756, -1.0203307 ],\n",
+                        "                         [ 0.38721725,  0.86828285,  0.10860055]], dtype=float32),\n",
+                        "            bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
+                        "        },\n",
+                        "        BatchNorm_0: {\n",
+                        "            scale: DeviceArray([1., 1., 1.], dtype=float32),\n",
+                        "            bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
+                        "        },\n",
+                        "    },\n",
+                        "    batch_stats: {\n",
+                        "        BatchNorm_0: {\n",
+                        "            mean: DeviceArray([ 0.00059601, -0.00103457,  0.00166948], dtype=float32),\n",
+                        "            var: DeviceArray([0.9907686, 0.9923046, 0.992195 ], dtype=float32),\n",
+                        "        },\n",
+                        "    },\n",
+                        "})\n",
+                        "initialized variable shapes:\n",
+                        " FrozenDict({\n",
+                        "    batch_stats: {\n",
+                        "        BatchNorm_0: {\n",
+                        "            mean: (3,),\n",
+                        "            var: (3,),\n",
+                        "        },\n",
+                        "    },\n",
+                        "    params: {\n",
+                        "        BatchNorm_0: {\n",
+                        "            bias: (3,),\n",
+                        "            scale: (3,),\n",
+                        "        },\n",
+                        "        Dense_0: {\n",
+                        "            bias: (3,),\n",
+                        "            kernel: (4, 3),\n",
+                        "        },\n",
+                        "    },\n",
+                        "})\n",
+                        "output:\n",
+                        " [[[-0.21496922  0.21550177 -0.35633382]\n",
+                        "  [-0.21496922 -2.0458      1.3015485 ]\n",
+                        "  [-0.21496922 -0.925116   -0.35633382]\n",
+                        "  [-0.6595459   0.21550177  0.3749205 ]]\n",
+                        "\n",
+                        " [[-0.21496922  1.642865   -0.35633382]\n",
+                        "  [-0.21496922  1.3094063  -0.88034123]\n",
+                        "  [ 2.5726683   0.21550177  0.34353197]\n",
+                        "  [-0.21496922  0.21550177  1.6778195 ]]\n",
+                        "\n",
+                        " [[-1.6060593   0.21550177 -1.9460517 ]\n",
+                        "  [ 1.4126908  -1.4898677   1.2790381 ]\n",
+                        "  [-0.21496922  0.21550177 -0.35633382]\n",
+                        "  [-0.21496922  0.21550177 -0.7251308 ]]]\n",
+                        "eval output:\n",
+                        " [[[ 3.2246590e-01  2.6108384e-02  4.4821960e-01]\n",
+                        "  [ 8.5726947e-02 -5.4385906e-01  3.8821870e-01]\n",
+                        "  [-2.3933809e-01 -2.7381191e-01 -1.7526165e-01]\n",
+                        "  [-6.2515378e-02 -5.2414006e-01  1.7029770e-01]]\n",
+                        "\n",
+                        " [[ 1.5014435e-01  3.4498507e-01 -1.3554120e-01]\n",
+                        "  [-3.6971044e-04  2.6463276e-01 -1.2491019e-01]\n",
+                        "  [ 3.8763803e-01  2.9023719e-01  1.6291586e-01]\n",
+                        "  [ 4.1320035e-01  4.1468274e-02  4.7670874e-01]]\n",
+                        "\n",
+                        " [[-1.9433719e-01  5.2831882e-01 -3.7554008e-01]\n",
+                        "  [ 2.2608691e-01 -4.0989807e-01  3.8292480e-01]\n",
+                        "  [-2.4945706e-01  1.6170470e-01 -2.5247774e-01]\n",
+                        "  [-7.2220474e-02  1.2077977e-01 -8.8408351e-02]]]\n"
                     ]
                 }
             ],
             "source": [
-                "@functools.partial(pjit, in_axis_resources=(state_spec, x_spec), out_axis_resources=x_spec)\n",
-                "def pjit_apply_fn(state, x):\n",
-                "  return state.apply_fn({'params': state.params}, x)\n",
-                "\n",
-                "with mesh:\n",
-                "  y = pjit_apply_fn(new_state, x)\n",
-                "print(type(y))\n",
-                "print(y.dtype)\n",
-                "print(y.shape)"
+                "class Block(nn.Module):\n",
+                "  features: int\n",
+                "  training: bool\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs):\n",
+                "    x = nn.Dense(self.features)(inputs)\n",
+                "    x = nn.Dropout(rate=0.5)(x, deterministic=not self.training)\n",
+                "    x = nn.BatchNorm(use_running_average=not self.training)(x)\n",
+                "    return x\n",
+                "\n",
+                "key1, key2, key3, key4 = random.split(random.PRNGKey(0), 4)\n",
+                "x = random.uniform(key1, (3,4,4))\n",
+                "\n",
+                "model = Block(features=3, training=True)\n",
+                "\n",
+                "init_variables = model.init({'params': key2, 'dropout': key3}, x)\n",
+                "_, init_params = init_variables.pop('params')\n",
+                "\n",
+                "# When calling `apply` with mutable kinds, returns a pair of output,\n",
+                "# mutated_variables.\n",
+                "y, mutated_variables = model.apply(\n",
+                "    init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n",
+                "\n",
+                "# Now we reassemble the full variables from the updates (in a real training\n",
+                "# loop, with the updated params from an optimizer).\n",
+                "updated_variables = freeze(dict(params=init_params,\n",
+                "                                **mutated_variables))\n",
+                "\n",
+                "print('updated variables:\\n', updated_variables)\n",
+                "print('initialized variable shapes:\\n',\n",
+                "      jax.tree_util.tree_map(jnp.shape, init_variables))\n",
+                "print('output:\\n', y)\n",
+                "\n",
+                "# Let's run these model variables during \"evaluation\":\n",
+                "eval_model = Block(features=3, training=False)\n",
+                "y = eval_model.apply(updated_variables, x)  # Nothing mutable; single return value.\n",
+                "print('eval output:\\n', y)"
+            ]
+        },
+        {
+            "attachments": {},
+            "cell_type": "markdown",
+            "metadata": {
+                "id": "Lcp28h72810L"
+            },
+            "source": [
+                "# JAX transformations inside modules"
             ]
         },
         {
+            "attachments": {},
+            "cell_type": "markdown",
+            "metadata": {
+                "id": "WEpbn8si0ATT"
+            },
+            "source": [
+                "## JIT"
+            ]
+        },
+        {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "7daa9e6e6eb4"
+                "id": "-k-5gXTJ0EpD"
             },
             "source": [
-                "## Profiling\n",
+                "It's not immediately clear what use this has, but you can compile specific submodules if there's a reason to.\n",
                 "\n",
-                "If you are running on a TPU pod or a pod slice, you can use a custom `block_all` utility function, as defined below, to measure the performance:"
+                "_Known Gotcha_: at the moment, the decorator changes the RNG stream slightly, so comparing jitted an unjitted initializations will look different."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 29,
+            "execution_count": 12,
             "metadata": {
-                "id": "a68d7cb2eb89"
+                "id": "UEUTO8bf0Kf2",
+                "outputId": "3f324d0f-259f-40f0-8273-103f7fc281c5",
+                "tags": []
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "166 ms \u00b1 5.72 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
+                        "initialized parameter shapes:\n",
+                        " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
+                        "output:\n",
+                        " [[ 0.2524199   0.11621253  0.5246693   0.19144788  0.2096542 ]\n",
+                        " [ 0.08557513 -0.04126885  0.2502836   0.03910369  0.16575359]\n",
+                        " [ 0.2804383   0.27751124  0.44969672  0.26016283  0.05875347]\n",
+                        " [ 0.2440843   0.17069656  0.45499086  0.20377949  0.13428023]]\n"
                     ]
                 }
             ],
             "source": [
-                "%%timeit\n",
+                "class MLP(nn.Module):\n",
+                "  features: Sequence[int]\n",
                 "\n",
-                "def block_all(xs):\n",
-                "  jax.tree_map(lambda x: x.block_until_ready(), xs)\n",
-                "  return xs\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs):\n",
+                "    x = inputs\n",
+                "    for i, feat in enumerate(self.features):\n",
+                "      # JIT the Module (it's __call__ fn by default.)\n",
+                "      x = nn.jit(nn.Dense)(feat, name=f'layers_{i}')(x)\n",
+                "      if i != len(self.features) - 1:\n",
+                "        x = nn.relu(x)\n",
+                "    return x\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(3), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = MLP(features=[3,4,5])\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
                 "\n",
-                "with mesh:\n",
-                "  new_state = block_all(pjit_step_fn(initialized_state, x))"
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('output:\\n', y)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "51420b514d53"
+                "id": "D1tfTdRjyJYK"
             },
             "source": [
-                "## Logical axis annotation\n",
-                "\n",
-                "JAX auto SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). \n",
-                "\n",
-                "The `LogicalDotReluDot` and `LogicalMLP` Module definition below are similar to the Modules you created earlier, except for the following:\n",
-                "\n",
-                "1. All axes are annotated with more concrete, meaningful names, such as `'embed'`, `'hidden'`, `'batch'` and `'layer'`. These names are referred to as _logical axis names_ in Flax. They make the dimensional changes inside model definitions more readable.\n",
-                "\n",
-                "2. `flax.linen.spmd.with_logical_partitioning` replaces `flax.linen.with_partitioning`; and `flax.linen.spmd.with_logical_constraint` replaces `pjit.with_sharding_constraint`, to recognize the logical axis names."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 30,
-            "metadata": {
-                "id": "a26f85a9e772"
-            },
-            "outputs": [],
-            "source": [
-                "class LogicalDotReluDot(nn.Module):\n",
-                "  depth: int\n",
-                "  @nn.compact\n",
-                "  def __call__(self, x):\n",
-                "    W1 = self.param(\n",
-                "        'W1', \n",
-                "        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('embed', 'hidden')),\n",
-                "        (x.shape[-1], self.depth)) \n",
-                "\n",
-                "    y = jax.nn.relu(jnp.dot(x, W1))\n",
-                "    # Force a local sharding annotation.\n",
-                "    y = spmd.with_logical_constraint(y, ('batch', 'hidden'))\n",
-                "\n",
-                "    W2 = self.param(\n",
-                "        'W2', \n",
-                "        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('hidden', 'embed')),\n",
-                "        (self.depth, x.shape[-1]))\n",
-                "\n",
-                "    z = jnp.dot(y, W2)\n",
-                "    # Force a local sharding annotation.\n",
-                "    z = spmd.with_logical_constraint(z, ('batch', 'embed'))\n",
-                "    return z, None\n",
-                "\n",
-                "class LogicalMLP(nn.Module):\n",
-                "  num_layers: int\n",
-                "  depth: int\n",
-                "  use_scan: bool\n",
-                "  @nn.compact\n",
-                "  def __call__(self, x):\n",
-                "    if self.use_scan:\n",
-                "      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers, \n",
-                "                    variable_axes={\"params\": 0},\n",
-                "                    split_rngs={\"params\": True},\n",
-                "                    metadata_params={nn.PARTITION_NAME: 'layer'}\n",
-                "                    )(self.depth)(x)\n",
-                "    else:\n",
-                "      for i in range(self.num_layers):\n",
-                "        x, _ = DotReluDot(self.depth)(x)\n",
-                "    return x"
+                "## Remat"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "0de93ec6cbd6"
+                "id": "goiHMi4qyLiZ"
             },
             "source": [
-                "The `LogicalMLP` model definition generates a set of `PartitionSpec` with logical axis names.\n",
+                "For memory-expensive computations, we can `remat` our method to recompute a Module's output during a backwards pass.\n",
                 "\n",
-                "Repeat the steps from earlier: instantiate a model, evaluate the `init_fn` abstractly, and use `flax.linen.get_partition_spec` to automatically generate the `PartitionSpec`:"
+                "_Known Gotcha_: at the moment, the decorator changes the RNG stream slightly, so comparing remat'd and undecorated initializations will look different."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 35,
+            "execution_count": 13,
             "metadata": {
-                "id": "14db7a1e30fd"
+                "id": "sogMxDQpyMZE",
+                "outputId": "7fe8e13b-7dd6-4e55-ee50-ce334e8ed178",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "TrainState(step=None, apply_fn=<bound method Module.apply of LogicalMLP(\n",
-                            "    # attributes\n",
-                            "    num_layers = 4\n",
-                            "    depth = 1024\n",
-                            "    use_scan = True\n",
-                            ")>, params=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
-                            "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
-                            "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
-                            "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
-                        ]
-                    },
-                    "execution_count": 35,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameter shapes:\n",
+                        " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
+                        "output:\n",
+                        " [[-0.14814317  0.06889858 -0.19695625  0.12019286  0.02068037]\n",
+                        " [-0.04439102 -0.06698258 -0.11579747 -0.19906905 -0.04342325]\n",
+                        " [-0.08875751 -0.13392815 -0.23153095 -0.39802808 -0.0868225 ]\n",
+                        " [-0.01606487 -0.02424064 -0.04190649 -0.07204203 -0.01571464]]\n"
+                    ]
                 }
             ],
             "source": [
-                "logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)\n",
-                "logical_abstract_variables = jax.eval_shape(\n",
-                "    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)\n",
-                "logical_output_spec = nn.get_partition_spec(logical_abstract_variables)\n",
-                "logical_output_spec"
+                "class RematMLP(nn.Module):\n",
+                "  features: Sequence[int]\n",
+                "  # For all transforms, we can annotate a method, or wrap an existing\n",
+                "  # Module class. Here we annotate the method.\n",
+                "  @nn.remat\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs):\n",
+                "    x = inputs\n",
+                "    for i, feat in enumerate(self.features):\n",
+                "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
+                "      if i != len(self.features) - 1:\n",
+                "        x = nn.relu(x)\n",
+                "    return x\n",
+                "\n",
+                "key1, key2 = random.split(random.PRNGKey(3), 2)\n",
+                "x = random.uniform(key1, (4,4))\n",
+                "\n",
+                "model = RematMLP(features=[3,4,5])\n",
+                "init_variables = model.init(key2, x)\n",
+                "y = model.apply(init_variables, x)\n",
+                "\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('output:\\n', y)"
+            ]
+        },
+        {
+            "attachments": {},
+            "cell_type": "markdown",
+            "metadata": {
+                "id": "l0pJtxVwyCgp"
+            },
+            "source": [
+                "## Vmap"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "d1c9b74e50b9"
+                "id": "TqVbjhOkyEaj"
             },
             "source": [
-                "To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and `jax.linen.spmd.logical_to_mesh` will convert them to the spec that `pjit` accepts.\n",
+                "You can now `vmap` Modules inside.  The transform has a lot of arguments, they have the usual jax vmap args:\n",
+                " - `in_axes` - an integer or `None` for each input argument\n",
+                " - `out_axes` - an integer or `None` for each output argument\n",
+                " - `axis_size` - the axis size if you need to give it explicitly\n",
+                "\n",
+                "In addition, we provide for each __kind__ of variable it's axis rules:\n",
+                "\n",
+                " - `variable_in_axes` - a dict from kinds to a single integer or `None` specifying the input axes to map\n",
+                " - `variable_out_axes` - a dict from kinds to a single integer or `None` specifying the output axes to map\n",
+                " - `split_rngs` - a dict from RNG-kinds to a bool, specifying whether to split the rng along the axis.\n",
                 "\n",
-                "This allows you to change the rules and try out new partition layouts without modifying the model definition."
+                "\n",
+                "Below we show an example defining a batched, multiheaded attention module from a single-headed unbatched attention implementation."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 32,
+            "execution_count": 14,
             "metadata": {
-                "id": "711cb4bde093"
+                "id": "PIGiriD0yFXo",
+                "outputId": "223d880e-c7b2-4210-ebb5-dbfcdd9aed09",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "TrainState(step=None, apply_fn=<bound method Module.apply of LogicalMLP(\n",
-                            "    # attributes\n",
-                            "    num_layers = 4\n",
-                            "    depth = 1024\n",
-                            "    use_scan = True\n",
-                            ")>, params=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: PartitionSpec(None, None, 'model'),\n",
-                            "        W2: PartitionSpec(None, 'model', None),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
-                        ]
-                    },
-                    "execution_count": 32,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameter shapes:\n",
+                        " {'params': {'attention': {'key': {'kernel': (2, 64, 32)}, 'out': {'bias': (2, 64), 'kernel': (2, 32, 64)}, 'query': {'kernel': (2, 64, 32)}, 'value': {'kernel': (2, 64, 32)}}}}\n",
+                        "output:\n",
+                        " (3, 13, 2)\n"
+                    ]
                 }
             ],
             "source": [
-                "# Unspecified rule means unsharded by default, so no need to specify `('embed', None)` and `('layer', None)`.\n",
-                "rules = (('batch', 'data'),\n",
-                "         ('hidden', 'model'))\n",
+                "class RawDotProductAttention(nn.Module):\n",
+                "  attn_dropout_rate: float = 0.1\n",
+                "  train: bool = False\n",
+                "\n",
+                "  @nn.compact\n",
+                "  def __call__(self, query, key, value, bias=None, dtype=jnp.float32):\n",
+                "    assert key.ndim == query.ndim\n",
+                "    assert key.ndim == value.ndim\n",
+                "\n",
+                "    n = query.ndim\n",
+                "    attn_weights = lax.dot_general(\n",
+                "        query, key,\n",
+                "        (((n-1,), (n - 1,)), ((), ())))\n",
+                "    if bias is not None:\n",
+                "      attn_weights += bias\n",
+                "    norm_dims = tuple(range(attn_weights.ndim // 2, attn_weights.ndim))\n",
+                "    attn_weights = jax.nn.softmax(attn_weights, axis=norm_dims)\n",
+                "    attn_weights = nn.Dropout(self.attn_dropout_rate)(attn_weights,\n",
+                "                                                      deterministic=not self.train)\n",
+                "    attn_weights = attn_weights.astype(dtype)\n",
+                "\n",
+                "    contract_dims = (\n",
+                "        tuple(range(n - 1, attn_weights.ndim)),\n",
+                "        tuple(range(0, n  - 1)))\n",
+                "    y = lax.dot_general(\n",
+                "        attn_weights, value,\n",
+                "        (contract_dims, ((), ())))\n",
+                "    return y\n",
+                "\n",
+                "class DotProductAttention(nn.Module):\n",
+                "  qkv_features: Optional[int] = None\n",
+                "  out_features: Optional[int] = None\n",
+                "  train: bool = False\n",
                 "\n",
-                "logical_state_spec = spmd.logical_to_mesh(logical_output_spec, rules)\n",
-                "logical_state_spec"
+                "  @nn.compact\n",
+                "  def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):\n",
+                "    qkv_features = self.qkv_features or inputs_q.shape[-1]\n",
+                "    out_features = self.out_features or inputs_q.shape[-1]\n",
+                "\n",
+                "    QKVDense = functools.partial(\n",
+                "      nn.Dense, features=qkv_features, use_bias=False, dtype=dtype)\n",
+                "    query = QKVDense(name='query')(inputs_q)\n",
+                "    key = QKVDense(name='key')(inputs_kv)\n",
+                "    value = QKVDense(name='value')(inputs_kv)\n",
+                "\n",
+                "    y = RawDotProductAttention(train=self.train)(\n",
+                "        query, key, value, bias=bias, dtype=dtype)\n",
+                "\n",
+                "    y = nn.Dense(features=out_features, dtype=dtype, name='out')(y)\n",
+                "    return y\n",
+                "\n",
+                "class MultiHeadDotProductAttention(nn.Module):\n",
+                "  qkv_features: Optional[int] = None\n",
+                "  out_features: Optional[int] = None\n",
+                "  batch_axes: Sequence[int] = (0,)\n",
+                "  num_heads: int = 1\n",
+                "  broadcast_dropout: bool = False\n",
+                "  train: bool = False\n",
+                "  @nn.compact\n",
+                "  def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):\n",
+                "    qkv_features = self.qkv_features or inputs_q.shape[-1]\n",
+                "    out_features = self.out_features or inputs_q.shape[-1]\n",
+                "\n",
+                "    # Make multiheaded attention from single-headed dimension.\n",
+                "    Attn = nn.vmap(DotProductAttention,\n",
+                "                   in_axes=(None, None, None),\n",
+                "                   out_axes=2,\n",
+                "                   axis_size=self.num_heads,\n",
+                "                   variable_axes={'params': 0},\n",
+                "                   split_rngs={'params': True,\n",
+                "                               'dropout': not self.broadcast_dropout})\n",
+                "\n",
+                "    # Vmap across batch dimensions.\n",
+                "    for axis in reversed(sorted(self.batch_axes)):\n",
+                "      Attn = nn.vmap(Attn,\n",
+                "                     in_axes=(axis, axis, axis),\n",
+                "                     out_axes=axis,\n",
+                "                     variable_axes={'params': None},\n",
+                "                     split_rngs={'params': False, 'dropout': False})\n",
+                "\n",
+                "    # Run the vmap'd class on inputs.\n",
+                "    y = Attn(qkv_features=qkv_features // self.num_heads,\n",
+                "             out_features=out_features,\n",
+                "             train=self.train,\n",
+                "             name='attention')(inputs_q, inputs_kv, bias)\n",
+                "\n",
+                "    return y.mean(axis=-2)\n",
+                "\n",
+                "\n",
+                "key1, key2, key3, key4 = random.split(random.PRNGKey(0), 4)\n",
+                "x = random.uniform(key1, (3, 13, 64))\n",
+                "\n",
+                "model = functools.partial(\n",
+                "  MultiHeadDotProductAttention,\n",
+                "  broadcast_dropout=False,\n",
+                "  num_heads=2,\n",
+                "  batch_axes=(0,))\n",
+                "\n",
+                "init_variables = model(train=False).init({'params': key2}, x, x)\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "\n",
+                "y = model(train=True).apply(init_variables, x, x, rngs={'dropout': key4})\n",
+                "print('output:\\n', y.shape)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {
-                "id": "58475fffb2de"
+                "id": "U-bDSQElvM09"
             },
             "source": [
-                "You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous (\"non-logical\") example. This will be the `out_axis_resources` you specify when creating `pjit`ted functions."
+                "## Scan"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 33,
+            "attachments": {},
+            "cell_type": "markdown",
             "metadata": {
-                "id": "589ff774bb4c"
+                "id": "8oiRXIC6xQ--"
             },
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 33,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
             "source": [
-                "state_spec.params['ScanDotReluDot_0'] == logical_state_spec.params['ScanLogicalDotReluDot_0']"
+                "Scan allows us to apply `lax.scan` to Modules, including their parameters and mutable variables.  To use it we have to specify how we want each \"kind\" of variable to be transformed.  For scanned variables we specify similar to vmap via in `variable_in_axes`, `variable_out_axes`:\n",
+                " - `nn.broadcast` broadcast the variable kind across the scan steps as a constant\n",
+                " - `<axis:int>` scan along `axis` for e.g. unique parameters at each step\n",
+                "\n",
+                "OR we specify that the variable kind is to be treated like a \"carry\" by passing to the `variable_carry` argument.\n",
+                "\n",
+                "Further, for `scan`'d variable kinds, we further specify whether or not to split the rng at each step."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 34,
+            "execution_count": 15,
             "metadata": {
-                "id": "77e07a0ab309"
+                "id": "oxA_lWm7tH2B",
+                "outputId": "7d9ebed3-64de-4ca8-9dce-4b09ba9e31a1",
+                "tags": []
             },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "TrainState(step=(), apply_fn=<bound method Module.apply of LogicalMLP(\n",
-                            "    # attributes\n",
-                            "    num_layers = 4\n",
-                            "    depth = 1024\n",
-                            "    use_scan = True\n",
-                            ")>, params=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
-                            "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=(), mu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
-                            "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    ScanLogicalDotReluDot_0: {\n",
-                            "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
-                            "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
-                        ]
-                    },
-                    "execution_count": 34,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "initialized parameter shapes:\n",
+                        " {'params': {'lstm_cell': {'hf': {'bias': (2,), 'kernel': (2, 2)}, 'hg': {'bias': (2,), 'kernel': (2, 2)}, 'hi': {'bias': (2,), 'kernel': (2, 2)}, 'ho': {'bias': (2,), 'kernel': (2, 2)}, 'if': {'kernel': (2, 2)}, 'ig': {'kernel': (2, 2)}, 'ii': {'kernel': (2, 2)}, 'io': {'kernel': (2, 2)}}}}\n",
+                        "output:\n",
+                        " ((DeviceArray([[-0.562219  ,  0.92847174]], dtype=float32), DeviceArray([[-0.31570646,  0.2885693 ]], dtype=float32)), DeviceArray([[[-0.08265854,  0.01302483],\n",
+                        "              [-0.10249066,  0.21991298],\n",
+                        "              [-0.26609066,  0.22519003],\n",
+                        "              [-0.27982554,  0.28393182],\n",
+                        "              [-0.31570646,  0.2885693 ]]], dtype=float32))\n"
+                    ]
                 }
             ],
             "source": [
-                "logical_pjit_init_fn = pjit(init_fn,\n",
-                "                            static_argnums=(2, 3),\n",
-                "                            in_axis_resources=(PartitionSpec(None), x_spec),  # RNG key and x\n",
-                "                            out_axis_resources=logical_state_spec\n",
-                "                            )\n",
-                "with mesh:\n",
-                "  logical_initialized_state = logical_pjit_init_fn(k, x, logical_model, optimizer)\n",
-                "jax.tree_map(jnp.shape, logical_initialized_state)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {
-                "id": "ae1754a3031d"
-            },
-            "source": [
-                "## When to use device axis / logical axis\n",
+                "class SimpleScan(nn.Module):\n",
+                "  features: int\n",
                 "\n",
-                "Choosing when to use a device or logical axis depends on how much you want to control the partitioning of your model.\n",
+                "  @nn.compact\n",
+                "  def __call__(self, xs):\n",
+                "    LSTM = nn.scan(nn.LSTMCell,\n",
+                "                   in_axes=1, out_axes=1,\n",
+                "                   variable_broadcast='params',\n",
+                "                   split_rngs={'params': False})\n",
+                "    lstm = LSTM(self.features, name=\"lstm_cell\")\n",
                 "\n",
-                "If you want a very simple model, or you are very confident of your way of partitioning, defining it with __device mesh axis__ can potentially save you a few extra lines of code of converting the logical naming back to the device naming.\n",
+                "    dummy_rng = random.PRNGKey(0)\n",
+                "    input_shape = xs[:, 0].shape\n",
+                "    init_carry = lstm.initialize_carry(dummy_rng, input_shape)\n",
                 "\n",
-                "On the other hand, the __logical naming__ helpers are useful for exploring different sharding layouts. Use this if you want to experiment around and find the most optimal partition layout for your model.\n",
+                "    return lstm(init_carry, xs)\n",
                 "\n",
-                "In really advanced use cases, you may have more complicated sharding patterns that require annotating *activation* dimension names differently from *parameter* dimension names. When people wish to have more fine-grained control on manual mesh assignments, directly using __device axis names__ could be more helpful."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {
-                "id": "576bdd5cd782"
-            },
-            "source": [
-                "## Save the data\n",
+                "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
+                "xs = random.uniform(key1, (1, 5, 2))\n",
                 "\n",
-                "You can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html) to save the cross-device array, as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).\n",
+                "model = SimpleScan(2)\n",
+                "init_variables = model.init(key2, xs)\n",
                 "\n",
-                "Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired `PartitionSpec` in place for each JAX array. The `PartitionSpec` you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array."
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "\n",
+                "y = model.apply(init_variables, xs)\n",
+                "print('output:\\n', y)"
             ]
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
         },
         "language_info": {
-            "codemirror_mode": {
-                "name": "ipython",
-                "version": 3
-            },
-            "file_extension": ".py",
-            "mimetype": "text/x-python",
             "name": "python",
-            "nbconvert_exporter": "python",
-            "pygments_lexer": "ipython3",
             "version": "3.8.10"
         }
     },
     "nbformat": 4,
-    "nbformat_minor": 1
+    "nbformat_minor": 0
 }
```

### Comparing `flax-0.6.9/docs/guides/flax_on_pjit.md` & `flax-0.7.0/docs/guides/flax_on_pjit.md`

 * *Files 11% similar despite different names*

```diff
@@ -6,153 +6,161 @@
     format_name: myst
     format_version: 0.13
     jupytext_version: 1.13.8
 ---
 
 +++ {"id": "2a9f78765c0c"}
 
-# Scale up Flax Modules on multiple devices with `pjit`
+# Scale up Flax Modules on multiple devices
 
-This guide shows how to scale up [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html) on multiple devices and hosts using JAX's [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) and [`flax.linen.spmd`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.linen.spmd).
+This guide shows how to scale up [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html) on multiple devices and hosts using [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) (formerly [`experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit)) and [`flax.linen`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/index.html).
 
-## Flax and `pjit`
++++ {"id": "b1e0e5fc8bc1"}
 
-[`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) provides a way to automatically compile and scale up JAX computations. `pjit` has the following benefits:
+## Flax and `jax.jit` scaled up
 
-* `pjit` has the similar interface of [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) and works as a decorator on a function that needs to be compiled.
-* When using `pjit`, you can write code as if it runs on a single device, and `pjit` will automatically compile and run it on multiple devices using the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm. 
-* With `pjit` you can state how the input and output of your code is partitioned across devices, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.
+[`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) follows the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm and automatically compiles your code to run it on multiple devices. You need to only specify how you want the input and output of your code to be partitioned, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.
 
-To learn more, refer to [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html) and [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html).
+Flax provides several functionalities that can help you use auto-SPMD on [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html), including:
 
-Flax provides several functionalities that can help you use `pjit` on [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html), including:
-
-1. An interface to specify partitions of your data when defining [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module).
-2. Utility functions to generate the partition information that `pjit` requires to run.
+1. An interface to specify partitions of your data when defining [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html).
+2. Utility functions to generate the sharding information that `jax.jit` requires to run.
 3. An interface to customize your axis names called "logical axis annotations" to decouple both your Module code and partition plan to experiment with different partition layouts more easily.
 
-+++ {"id": "0fa8ccbf573a"}
+You can learn more about `jax.jit` APIs for scaling up in [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html) and [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) on JAX's documentation site.
+
++++ {"id": "a9601432b448"}
 
 ## Setup
 
-Install Flax from HEAD:
+Import some necessary dependencies.
+
+**Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. You don't need this if you are already using a multi-device TPU environment.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 867203db3bef
 :tags: [skip-execution]
 
-# Once Flax v0.6.4 is released, use `pip3 install flax`.
-! pip3 install -qq "git+https://github.com/google/flax.git@main#egg=flax"
+# Once Flax v0.6.10 is released, there is no need to do this.
+# ! pip3 install -qq "git+https://github.com/google/flax.git@main#egg=flax"
 ```
 
-+++ {"id": "a9601432b448"}
-
-## Imports
-
-Import some necessary dependencies.
-
-**Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. Check out the [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html#setup) to learn more about emulating a multi-device TPU environment (in which case you should ignore running `os.environ`).
-
-```{code-cell} ipython3
+```{code-cell}
 :id: f8f42d1174e5
 
 import os
 os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'
 ```
 
-```{code-cell} ipython3
+```{code-cell}
 :id: b8da40732f0b
 
 import functools
+from typing import Optional, Callable
+
 import numpy as np
 import jax
-
 from jax import lax, random, numpy as jnp
 
 import flax
 from flax import struct, traverse_util, linen as nn
-from flax.linen import spmd # Flax Linen SPMD.
 from flax.core import freeze, unfreeze
 from flax.training import train_state, checkpoints
 
 import optax # Optax for common losses and optimizers. 
 ```
 
+```{code-cell}
+:id: bcc30de1d6eb
+
+print(f'We have 8 fake JAX devices now: {jax.devices()}')
+```
+
 +++ {"id": "c0d280def897"}
 
-Next, import all the `pjit`-related libraries.
+The code below shows how to import and set up the JAX-level device API, following JAX's [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) guide:
 
-> **Note:** [`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) is still in the experimental package of JAX, so there may be changes in the API in future.
+1. Start a 2x4 device `mesh` (8 devices) using JAX's `mesh_utils.create_device_mesh`. This layout is the same as the one of a [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).
 
-1. Start a 2x4 device mesh (8 devices)this is the same as the layout of [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).
-2. Annotate each axis with a name. A typical way to annotate axis names is `('data', 'model')`, where:
+2. Annotate each axis with a name using the `axis_names` parameter in `jax.sharding.Mesh`. A typical way to annotate axis names is `axis_name=('data', 'model')`, where:
   * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.
   * `'model'`: the mesh dimension used for sharding parameters of the model across devices.
+  
+3. Make a simple utility function `mesh_sharding` for generating a sharding object from the mesh and any layout.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 684fe9fe13a0
 
-from jax.experimental.pjit import pjit, with_sharding_constraint
-from jax.sharding import Mesh, PartitionSpec
+from jax.sharding import Mesh, PartitionSpec, NamedSharding
+from jax.lax import with_sharding_constraint
 from jax.experimental import mesh_utils
+```
 
-# Start a device mesh.
+```{code-cell}
+:id: 4589d7a6d4bb
+
+# Create a mesh and annotate each axis with a name.
 device_mesh = mesh_utils.create_device_mesh((2, 4))
 print(device_mesh)
-# Annotate each axis with a name.
+
 mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))
-mesh
+print(mesh)
+
+def mesh_sharding(pspec: PartitionSpec) -> NamedSharding:
+  return NamedSharding(mesh, pspec)
 ```
 
 +++ {"id": "307d39db6d94"}
 
 ## Define a layer
 
-Before defining a model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`), which creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.
+Before defining a simple model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`). The layer creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.
 
-To use this layer in `pjit` efficiently, apply the following APIs to annotate the parameters and intermediate variables correctly:
+To shard the parameters efficiently, apply the following APIs to annotate the parameters and intermediate variables:
 
-1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating parameters `W1` and `W2`.
+1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.
 
-2. Apply [`pjit.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern under `pjit` when the ideal constraint is known.
+2. Apply [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) (formerly, `pjit.with_sharding_constraint`) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern when the ideal constraint is known.
 
-  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because `pjit` will figure out the same sharding layout for `y` and `z` regardless.
+  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because XLA will figure out the same sharding layout for `y` and `z` regardless.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: b74c049968dc
 
 class DotReluDot(nn.Module):
   depth: int
+  dense_init: Callable = nn.initializers.xavier_normal()
   @nn.compact
   def __call__(self, x):
-    W1 = self.param(
-        'W1', 
-        nn.with_partitioning(nn.initializers.xavier_normal(), (None, 'model')),
-        (x.shape[-1], self.depth))
+    
+    y = nn.Dense(self.depth, 
+                 kernel_init=nn.with_partitioning(self.dense_init, (None, 'model')),
+                 use_bias=False,  # or overwrite with `bias_init`
+                 )(x)
 
-    y = jax.nn.relu(jnp.dot(x, W1))
+    y = jax.nn.relu(y)
     # Force a local sharding annotation.
-    y = with_sharding_constraint(y, PartitionSpec('data', 'model'))
+    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))
 
     W2 = self.param(
         'W2', 
-        nn.with_partitioning(nn.initializers.xavier_normal(), ('model', None)),
+        nn.with_partitioning(self.dense_init, ('model', None)),
         (self.depth, x.shape[-1]))
-
+    
     z = jnp.dot(y, W2)
     # Force a local sharding annotation.
-    z = with_sharding_constraint(z, PartitionSpec('data', None))
+    z = with_sharding_constraint(z, mesh_sharding(PartitionSpec('data', None)))
 
     # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.
     return z, None
 ```
 
 +++ {"id": "cbac5321c08e"}
 
-Note that device axis names like `'data'`, `'model'` or `None` are passed into both `flax.linen.with_partitioning` and `pjit_with_sharding_constraint` API calls. This refers to how each dimension of this data should be sharded  either across one of the device mesh dimensions, or not sharded at all.
+Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded  either across one of the device mesh dimensions, or not sharded at all.
 
 For example:
 
 * When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:
 
   * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.
   * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.
@@ -162,28 +170,26 @@
   * The first dimension  the batch dimension  will be sharded over the `'data'` axis. This means half of the batch will be processed on devices `0-3` (first four devices), and another half on devices `4-7` (the remaining four devices).
   * The second dimension  the data depth dimension  will be replicated across all devices.
 
 +++ {"id": "b8389c11af79"}
 
 ## Define a model with `flax.linen.scan` lifted transformation
 
-This guide uses `flax.linen.scan` to demonstrate how [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations), such as `scan`, can work together with [JAX `pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html).
-
-Having created `DotReluDot`, define the `MLP` model (by subclassing `flax.linen.Module`) as multiple layers of `DotReluDot`.
+Having created `DotReluDot`, you can now define the `MLP` model (by subclassing [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)) as multiple layers of `DotReluDot`.
 
-To replicate identical layers, you can either use `flax.linen.scan`, or a for-loop:
+To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html), or a for-loop:
 
-* `flax.linen.scan` can offer faster compilation times.
+* `flax.linen.scan` can provide faster compilation times.
 * The for-loop can be faster on runtime.
 
-The code below shows how to apply both methods.
+The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding. 
 
-**Note:** `flax.linen.scan` has another dimension for the parameters (the dimension over which `scan` is applied). You need to use the [`metadata_params`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html#flax.linen.scan) argument to annotate the partition of this dimension. Since the parameters inside your `DotReluDot` (a sub-`Module`) are already sharded along the `model` axis, you don't need to partition multiple layers across the `model` dimension here, and therefore you should denote it as `None`.
+The `flax.linen.scan` code is just to show that this API works with [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations).
 
-```{code-cell} ipython3
+```{code-cell}
 :id: a0ea0dcccbc3
 
 class MLP(nn.Module):
   num_layers: int
   depth: int
   use_scan: bool
   @nn.compact
@@ -196,239 +202,274 @@
                      )(self.depth)(x)
     else:
       for i in range(self.num_layers):
         x, _ = DotReluDot(self.depth)(x)
     return x
 ```
 
++++ {"id": "44395b62561d"}
+
+Now, create a `model` instance, and a sample input `x`.
+
+```{code-cell}
+:id: 5686299b4839
+
+# MLP hyperparameters.
+BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, False
+# Create fake inputs.
+x = jnp.ones((BATCH, DEPTH))
+# Initialize a PRNG key.
+k = random.PRNGKey(0)
+
+# Create an Optax optimizer.
+optimizer = optax.adam(learning_rate=0.001)
+# Instantiate the model.
+model = MLP(LAYERS, DEPTH, USE_SCAN)
+```
+
 +++ {"id": "5b3abfef359d"}
 
-## Specify sharding (includes initialization and `TrainState` creation)
+## Specify sharding
 
-Next, generate the [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) that `pjit` should receive as annotations of _input_ and _output_ data. `PartitionSpec` is a tuple of 2 axes (in a 2x4 mesh). To learn more, refer to [JAX-101: Introduction to `pjit`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html).
+Next, you need to tell `jax.jit` how to shard our data across devices.
 
-### Specify the input
+### The input's sharding
 
-For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `data`:
+For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `'data'`. Then, use [`jax.device_put`](https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html) to place it onto the correct `device`s.
 
-```{code-cell} ipython3
-:id: 4b8472d462f2
+```{code-cell}
+:id: 8b913a2e57d3
 
-x_spec = PartitionSpec('data', None)  # dimensions: (batch, length)
-x_spec
+x_sharding = mesh_sharding(PartitionSpec('data', None)) # dimensions: (batch, length)
+x = jax.device_put(x, x_sharding)
+jax.debug.visualize_array_sharding(x)
 ```
 
 +++ {"id": "06d134795ae1"}
 
-### Generate a `PartitionSpec` for the output
+### The output's sharding
 
-Next, generate a [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) for the _output_, you need to use some actual output as a reference.
+You need to compile `model.init()` (that is, [`flax.linen.Module.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init)), and its output as a pytree of parameters. Additionally, you may sometimes need wrap it with a [`flax.training.train_state`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) to track other variables, such as optimizer states, and that would make the output an even more complex pytree.
 
-1. Instantiate a model.
-2. Evaluate `model.init` abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).
-3. Use [`flax.linen.get_partition_spec`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_partition_spec.html) to automatically generate the `PartitionSpec`.
+To achieve this, luckily, you don't have to hardcode the output's sharding by hand. Instead, you can:
 
-The code below shows how to get the output spec if you use `flax.training.train_state` to carry out your initialization and training steps, in which case your `pjit`ted function will output a `TrainState`. 
+1. Evaluate `model.init` (in this case, a wrapper of it) abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).
 
-(In a simpler case, people might choose the variable dict as in `variables = model.init(k, x)` as their `pjit`ted function's output. That works too.)
-
-```{code-cell} ipython3
-:id: 8b913a2e57d3
-
-# MLP hyperparameters.
-BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, True
-# Create fake inputs.
-x = jnp.ones((BATCH, DEPTH))
-# Initialize a PRNG key.
-k = random.PRNGKey(0)
+1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_sharding.html) to automatically generate the `jax.sharding.NamedSharding`.
+   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) annotations in the earlier definition to generate the correct sharding for the parameters.
 
-# Create an Optax optimizer.
-optimizer = optax.adam(learning_rate=0.001)
-# Instantiate the model.
-model = MLP(LAYERS, DEPTH, USE_SCAN)
+```{code-cell}
+:id: 19094ec63385
 
-# A functional way of model initialization.
 def init_fn(k, x, model, optimizer):
   variables = model.init(k, x) # Initialize the model.
   state = train_state.TrainState.create( # Create a `TrainState`.
     apply_fn=model.apply,
     params=variables['params'],
     tx=optimizer)
   return state
+```
 
-with mesh:
-  # Create an abstract closure to wrap the function before feeding it in
-  # because `jax.eval_shape` only takes pytrees as arguments`.
-  abstract_variables = jax.eval_shape(
-      functools.partial(init_fn, model=model, optimizer=optimizer), k, x)
-# This `state_spec` has the same pytree structure as the output
+```{code-cell}
+:id: e49264a3c78e
+
+# Create an abstract closure to wrap the function before feeding it in
+# because `jax.eval_shape` only takes pytrees as arguments.
+abstract_variables = jax.eval_shape(
+    functools.partial(init_fn, model=model, optimizer=optimizer), k, x)
+
+# This `state_sharding` has the same pytree structure as `state`, the output
 # of the `init_fn`.
-state_spec = nn.get_partition_spec(abstract_variables)
-state_spec
+state_sharding = nn.get_sharding(abstract_variables, mesh)
+state_sharding
 ```
 
 +++ {"id": "2ec24614050b"}
 
-## Apply `pjit` to compile the code
+## Compile the code
 
-Now you can apply JAX [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) to your `init_fn` in a similar fashion as [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) but with two extra arguments: `in_axis_resources` and `out_axis_resources`.
+Now you can apply [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) to your `init_fn`, but with two extra arguments: `in_shardings` and `out_shardings`.
 
-You need to add a `with mesh:` context when running a `pjit`ted function, so that it can refer to `mesh` (an instance of `jax.sharding.Mesh`) to allocate data on devices correctly.
+Run it to get the `initialized_state`, in which parameters are sharded exactly as instructed:
 
-```{code-cell} ipython3
-:id: a298c5d03c0d
+```{code-cell}
+:id: 5b6e699df733
 
-pjit_init_fn = pjit(init_fn,
-                    static_argnums=(2, 3),
-                    in_axis_resources=(PartitionSpec(None), x_spec),  # PRNG key and x
-                    out_axis_resources=state_spec
-                    )
-with mesh:
-  initialized_state = pjit_init_fn(k, x, model, optimizer)
-jax.tree_map(jnp.shape, initialized_state)
+jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),
+                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x
+                      out_shardings=state_sharding)
+
+initialized_state = jit_init_fn(k, x, model, optimizer)
+
+# for weight, partitioned in initialized_state.params['DotReluDot_0'].items():
+#     print(f'Sharding of {weight}: {partitioned.names}')
+jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)
+jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['W2'].value)
 ```
 
 +++ {"id": "8f74b009f11f"}
 
 ## Inspect the Module output
 
-Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record metadata associated with it. You can access the raw `jax.Array` by adding `.value` or running `.unbox()`.
+Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it. 
 
-You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of the JAX array, which gives a hint on the way it is partitioned.
+You can access the raw `jax.Array` by adding `.value` when outside `jit`, or by `.unbox()` when inside.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 19243982c892
 
-print(type(initialized_state.params['ScanDotReluDot_0']['W1']))
-print(type(initialized_state.params['ScanDotReluDot_0']['W1'].value))
-print(initialized_state.params['ScanDotReluDot_0']['W1'].value.shape)
+print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel']))
+print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value))
+print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].names)
+print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.shape)
 ```
 
-```{code-cell} ipython3
++++ {"id": "2beee7d27bdb"}
+
+You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of each parameter, which is now more internal than `NamedSharding`. Note that numbers like `initialized_state.step` are replicated across all devices.
+
+```{code-cell}
 :id: 2067c419a826
 
-print(initialized_state.params['ScanDotReluDot_0']['W1'].value.sharding)
+initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.sharding
+```
+
+```{code-cell}
+:id: d7cf0baa334b
+
+print(initialized_state.step)
+initialized_state.step.sharding
 ```
 
 +++ {"id": "273547d3ab89"}
 
 You can use [`jax.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 29b3dae156a2
 
 diff = jax.tree_map(
     lambda a, b: a - b, 
-    initialized_state.params['ScanDotReluDot_0'], initialized_state.params['ScanDotReluDot_0'])
+    initialized_state.params['DotReluDot_0'], initialized_state.params['DotReluDot_0'])
 print(jax.tree_map(jnp.shape, diff))
-diff_array = diff['W1'].unbox()
+diff_array = diff['Dense_0']['kernel'].value
 print(type(diff_array))
 print(diff_array.shape)
 ```
 
 +++ {"id": "f7e1ccb14c6b"}
 
-## Apply `pjit` to the train step and inference 
+## Compile the train step and inference 
 
-Now, you create a `pjit`ted training step:
+Create a `jit`ted training step as follows:
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 4e3cc300cfee
 
+@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), 
+                   out_shardings=state_sharding)
 def train_step(state, x):
   # A fake loss function.
   def loss_unrolled(params):
     y = model.apply({'params': params}, x)
     return y.sum()
   grad_fn = jax.grad(loss_unrolled)
   grads = grad_fn(state.params)
   state = state.apply_gradients(grads=grads)
   return state
 
-pjit_step_fn = pjit(train_step,
-                    in_axis_resources=(state_spec, x_spec),  # input annotations
-                    out_axis_resources=state_spec,           # output annotations
-                    )
 with mesh:
-  new_state = pjit_step_fn(initialized_state, x)
+  new_state = train_step(initialized_state, x)
+```
+
+```{code-cell}
+:id: 91c6c2662c12
+
+print(f'Sharding of Weight 1:')
+jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)
+print(f'Sharding of Weight 2:')
+jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['W2'].value)
 ```
 
 +++ {"id": "2bae79e2e71b"}
 
-Apply `pjit` to inference. Note that, similar to `jax.jit`, you can use a decorator like `@functools.partial(pjit, ...)` to directly compile your function.
+Then, create a compiled inference step. Note that the output is also sharded along `(data, None)`.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: c9264a48b9ee
 
-@functools.partial(pjit, in_axis_resources=(state_spec, x_spec), out_axis_resources=x_spec)
-def pjit_apply_fn(state, x):
+@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), 
+                   out_shardings=x_sharding)
+def apply_fn(state, x):
   return state.apply_fn({'params': state.params}, x)
 
 with mesh:
-  y = pjit_apply_fn(new_state, x)
+  y = apply_fn(new_state, x)
 print(type(y))
 print(y.dtype)
 print(y.shape)
+jax.debug.visualize_array_sharding(y)
 ```
 
 +++ {"id": "7daa9e6e6eb4"}
 
 ## Profiling
 
 If you are running on a TPU pod or a pod slice, you can use a custom `block_all` utility function, as defined below, to measure the performance:
 
-```{code-cell} ipython3
+```{code-cell}
 :id: a68d7cb2eb89
 
 %%timeit
 
 def block_all(xs):
   jax.tree_map(lambda x: x.block_until_ready(), xs)
   return xs
 
 with mesh:
-  new_state = block_all(pjit_step_fn(initialized_state, x))
+  new_state = block_all(train_step(initialized_state, x))
 ```
 
 +++ {"id": "51420b514d53"}
 
 ## Logical axis annotation
 
-JAX auto SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). 
+JAX's automatic SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). 
 
 The `LogicalDotReluDot` and `LogicalMLP` Module definition below are similar to the Modules you created earlier, except for the following:
 
 1. All axes are annotated with more concrete, meaningful names, such as `'embed'`, `'hidden'`, `'batch'` and `'layer'`. These names are referred to as _logical axis names_ in Flax. They make the dimensional changes inside model definitions more readable.
 
-2. `flax.linen.spmd.with_logical_partitioning` replaces `flax.linen.with_partitioning`; and `flax.linen.spmd.with_logical_constraint` replaces `pjit.with_sharding_constraint`, to recognize the logical axis names.
+2. [`flax.linen.with_logical_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_partitioning.html) replaces `flax.linen.with_partitioning`; and [`flax.linen.with_logical_constraint`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_constraint.html#flax-linen-with-logical-constraint) replaces `jax.lax.with_sharding_constraint`, to recognize the logical axis names.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: a26f85a9e772
 
 class LogicalDotReluDot(nn.Module):
   depth: int
+  dense_init: Callable = nn.initializers.xavier_normal()
   @nn.compact
-  def __call__(self, x):
-    W1 = self.param(
-        'W1', 
-        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('embed', 'hidden')),
-        (x.shape[-1], self.depth)) 
+  def __call__(self, x):    
+    y = nn.Dense(self.depth, 
+                 kernel_init=nn.with_partitioning(self.dense_init, ('embed', 'hidden')),
+                 use_bias=False,  # or overwrite with `bias_init`
+                 )(x)
 
-    y = jax.nn.relu(jnp.dot(x, W1))
+    y = jax.nn.relu(y)
     # Force a local sharding annotation.
-    y = spmd.with_logical_constraint(y, ('batch', 'hidden'))
+    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))
 
     W2 = self.param(
         'W2', 
-        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('hidden', 'embed')),
+        nn.with_partitioning(self.dense_init, ('hidden', 'embed')),
         (self.depth, x.shape[-1]))
 
     z = jnp.dot(y, W2)
     # Force a local sharding annotation.
-    z = spmd.with_logical_constraint(z, ('batch', 'embed'))
+    z = nn.with_logical_constraint(z, ('batch', 'embed'))
     return z, None
 
 class LogicalMLP(nn.Module):
   num_layers: int
   depth: int
   use_scan: bool
   @nn.compact
@@ -437,86 +478,87 @@
       x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers, 
                     variable_axes={"params": 0},
                     split_rngs={"params": True},
                     metadata_params={nn.PARTITION_NAME: 'layer'}
                     )(self.depth)(x)
     else:
       for i in range(self.num_layers):
-        x, _ = DotReluDot(self.depth)(x)
+        x, _ = LogicalDotReluDot(self.depth)(x)
     return x
 ```
 
 +++ {"id": "0de93ec6cbd6"}
 
-The `LogicalMLP` model definition generates a set of `PartitionSpec` with logical axis names.
-
-Repeat the steps from earlier: instantiate a model, evaluate the `init_fn` abstractly, and use `flax.linen.get_partition_spec` to automatically generate the `PartitionSpec`:
-
-```{code-cell} ipython3
-:id: 14db7a1e30fd
-
-logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)
-logical_abstract_variables = jax.eval_shape(
-    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)
-logical_output_spec = nn.get_partition_spec(logical_abstract_variables)
-logical_output_spec
-```
-
-+++ {"id": "d1c9b74e50b9"}
+Now, initiate a model and try to figure out what sharding its `state` should have.
 
-To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and `jax.linen.spmd.logical_to_mesh` will convert them to the spec that `pjit` accepts.
+To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and [`flax.linen.logical_to_mesh_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.logical_to_mesh_sharding.html#flax-linen-logical-to-mesh-sharding) will convert them to the kind of sharding that the device mesh can understand.
 
 This allows you to change the rules and try out new partition layouts without modifying the model definition.
 
-```{code-cell} ipython3
-:id: 711cb4bde093
+```{code-cell}
+:id: 14db7a1e30fd
 
 # Unspecified rule means unsharded by default, so no need to specify `('embed', None)` and `('layer', None)`.
 rules = (('batch', 'data'),
          ('hidden', 'model'))
 
-logical_state_spec = spmd.logical_to_mesh(logical_output_spec, rules)
-logical_state_spec
+logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)
+
+logical_abstract_variables = jax.eval_shape(
+    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)
+logical_state_spec = nn.get_partition_spec(logical_abstract_variables)
+print('annotations are logical, not mesh-specific: ', 
+      logical_state_spec.params['LogicalDotReluDot_0']['Dense_0']['kernel'])
+
+logical_state_sharding = nn.logical_to_mesh_sharding(logical_state_spec, mesh, rules)
+print('sharding annotations are mesh-specific: ', 
+      logical_state_sharding.params['LogicalDotReluDot_0']['Dense_0']['kernel'].spec)
 ```
 
 +++ {"id": "58475fffb2de"}
 
-You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous ("non-logical") example. This will be the `out_axis_resources` you specify when creating `pjit`ted functions.
+You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous ("non-logical") example. This allows you to `jax.jit` your Module's [`flax.linen.Module.init`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init) and [`flax.linen.Module.apply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.apply) the same way in the above above.
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 589ff774bb4c
 
-state_spec.params['ScanDotReluDot_0'] == logical_state_spec.params['ScanLogicalDotReluDot_0']
+state_sharding.params['DotReluDot_0'] == logical_state_sharding.params['LogicalDotReluDot_0']
 ```
 
-```{code-cell} ipython3
+```{code-cell}
 :id: 77e07a0ab309
 
-logical_pjit_init_fn = pjit(init_fn,
-                            static_argnums=(2, 3),
-                            in_axis_resources=(PartitionSpec(None), x_spec),  # RNG key and x
-                            out_axis_resources=logical_state_spec
-                            )
-with mesh:
-  logical_initialized_state = logical_pjit_init_fn(k, x, logical_model, optimizer)
-jax.tree_map(jnp.shape, logical_initialized_state)
+logical_jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),
+                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x
+                      out_shardings=logical_state_sharding)
+
+logical_initialized_state = logical_jit_init_fn(k, x, logical_model, optimizer)
+```
+
+```{code-cell}
+:id: fb53bc20e0f9
+
+print(f'Sharding of Weight 1:')
+jax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['Dense_0']['kernel'].value)
+print(f'Sharding of Weight 2:')
+jax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['W2'].value)
 ```
 
 +++ {"id": "ae1754a3031d"}
 
 ## When to use device axis / logical axis
 
-Choosing when to use a device or logical axis depends on how much you want to control the partitioning of your model.
+Choosing when to use a device or logical axis depends on how much you want to control the partitioning of your model:
 
-If you want a very simple model, or you are very confident of your way of partitioning, defining it with __device mesh axis__ can potentially save you a few extra lines of code of converting the logical naming back to the device naming.
+* **Device mesh axis**: If you want a very simple model, or you are very confident of your way of partitioning, defining it with __device mesh axis__ can potentially save you a few extra lines of code of converting the logical naming back to the device naming.
 
-On the other hand, the __logical naming__ helpers are useful for exploring different sharding layouts. Use this if you want to experiment around and find the most optimal partition layout for your model.
+* **logical naming**: On the other hand, the __logical naming__ helpers can be useful for exploring different sharding layouts. Use this if you want to experiment around and find the most optimal partition layout for your model.
 
-In really advanced use cases, you may have more complicated sharding patterns that require annotating *activation* dimension names differently from *parameter* dimension names. When people wish to have more fine-grained control on manual mesh assignments, directly using __device axis names__ could be more helpful.
+* **Device axis names**: In really advanced use cases, you may have more complicated sharding patterns that require annotating *activation* dimension names differently from *parameter* dimension names. If you wish to have more fine-grained control on manual mesh assignments, directly using __device axis names__ could be more helpful.
 
 +++ {"id": "576bdd5cd782"}
 
 ## Save the data
 
-You can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html) to save the cross-device array, as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).
+To save the cross-device array, you can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html), as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).
 
-Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired `PartitionSpec` in place for each JAX array. The `PartitionSpec` you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array.
+Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired [`jax.sharding.Sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Sharding) in place for each JAX array. The sharding you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array.
```

### Comparing `flax-0.6.9/docs/guides/full_eval.rst` & `flax-0.7.0/docs/guides/full_eval.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/jax_for_the_impatient.ipynb` & `flax-0.7.0/docs/guides/jax_for_the_impatient.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/jax_for_the_impatient.md` & `flax-0.7.0/docs/guides/jax_for_the_impatient.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/lr_schedule.rst` & `flax-0.7.0/docs/guides/lr_schedule.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/model_surgery.ipynb` & `flax-0.7.0/docs/guides/model_surgery.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/model_surgery.md` & `flax-0.7.0/docs/guides/model_surgery.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/orbax_upgrade_guide.rst` & `flax-0.7.0/docs/guides/orbax_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/setup_or_nncompact.rst` & `flax-0.7.0/docs/guides/setup_or_nncompact.rst`

 * *Files 1% similar despite different names*

```diff
@@ -92,105 +92,105 @@
 000005b0: 656c 662c 2078 293a 0a20 2020 2020 2078  elf, x):.      x
 000005c0: 203d 2073 656c 662e 6465 6e73 6531 2878   = self.dense1(x
 000005d0: 290a 2020 2020 2020 7820 3d20 6e6e 2e72  ).      x = nn.r
 000005e0: 656c 7528 7829 0a20 2020 2020 2078 203d  elu(x).      x =
 000005f0: 2073 656c 662e 6465 6e73 6532 2878 290a   self.dense2(x).
 00000600: 2020 2020 2020 7265 7475 726e 2078 0a20        return x. 
 00000610: 202d 2d2d 0a20 2063 6c61 7373 204d 4c50   ---.  class MLP
-00000620: 286e 6e2e 4d6f 6475 6c65 293a 0a0a 0a0a  (nn.Module):....
-00000630: 0a0a 2020 2020 406e 6e2e 636f 6d70 6163  ..    @nn.compac
-00000640: 7420 2321 0a20 2020 2064 6566 205f 5f63  t #!.    def __c
-00000650: 616c 6c5f 5f28 7365 6c66 2c20 7829 3a0a  all__(self, x):.
-00000660: 2020 2020 2020 7820 3d20 6e6e 2e44 656e        x = nn.Den
-00000670: 7365 2833 322c 206e 616d 653d 2264 656e  se(32, name="den
-00000680: 7365 3122 2928 7829 2023 210a 2020 2020  se1")(x) #!.    
-00000690: 2020 7820 3d20 6e6e 2e72 656c 7528 7829    x = nn.relu(x)
-000006a0: 0a20 2020 2020 2078 203d 206e 6e2e 4465  .      x = nn.De
-000006b0: 6e73 6528 3332 2c20 6e61 6d65 3d22 6465  nse(32, name="de
-000006c0: 6e73 6532 2229 2878 2920 2321 0a20 2020  nse2")(x) #!.   
-000006d0: 2020 2072 6574 7572 6e20 780a 0a53 6f2c     return x..So,
-000006e0: 2068 6f77 2077 6f75 6c64 2079 6f75 2064   how would you d
-000006f0: 6563 6964 6520 7768 6963 6820 7374 796c  ecide which styl
-00000700: 6520 746f 2075 7365 3f20 4974 2063 616e  e to use? It can
-00000710: 2062 6520 6120 6d61 7474 6572 206f 6620   be a matter of 
-00000720: 7461 7374 652c 2062 7574 2068 6572 6520  taste, but here 
-00000730: 6172 6520 736f 6d65 2070 726f 7320 616e  are some pros an
-00000740: 6420 636f 6e73 3a0a 0a52 6561 736f 6e73  d cons:..Reasons
-00000750: 2074 6f20 7072 6566 6572 2075 7369 6e67   to prefer using
-00000760: 2060 606e 6e2e 636f 6d70 6163 7460 603a   ``nn.compact``:
-00000770: 0a5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  .^^^^^^^^^^^^^^^
+00000620: 286e 6e2e 4d6f 6475 6c65 293a 0a0a 2020  (nn.Module):..  
+00000630: 2020 406e 6e2e 636f 6d70 6163 7420 2321    @nn.compact #!
+00000640: 0a20 2020 2064 6566 205f 5f63 616c 6c5f  .    def __call_
+00000650: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
+00000660: 2020 7820 3d20 6e6e 2e44 656e 7365 2833    x = nn.Dense(3
+00000670: 322c 206e 616d 653d 2264 656e 7365 3122  2, name="dense1"
+00000680: 2928 7829 2023 210a 2020 2020 2020 7820  )(x) #!.      x 
+00000690: 3d20 6e6e 2e72 656c 7528 7829 0a20 2020  = nn.relu(x).   
+000006a0: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
+000006b0: 3332 2c20 6e61 6d65 3d22 6465 6e73 6532  32, name="dense2
+000006c0: 2229 2878 2920 2321 0a20 2020 2020 2072  ")(x) #!.      r
+000006d0: 6574 7572 6e20 780a 0a53 6f2c 2068 6f77  eturn x..So, how
+000006e0: 2077 6f75 6c64 2079 6f75 2064 6563 6964   would you decid
+000006f0: 6520 7768 6963 6820 7374 796c 6520 746f  e which style to
+00000700: 2075 7365 3f20 4974 2063 616e 2062 6520   use? It can be 
+00000710: 6120 6d61 7474 6572 206f 6620 7461 7374  a matter of tast
+00000720: 652c 2062 7574 2068 6572 6520 6172 6520  e, but here are 
+00000730: 736f 6d65 2070 726f 7320 616e 6420 636f  some pros and co
+00000740: 6e73 3a0a 0a52 6561 736f 6e73 2074 6f20  ns:..Reasons to 
+00000750: 7072 6566 6572 2075 7369 6e67 2060 606e  prefer using ``n
+00000760: 6e2e 636f 6d70 6163 7460 603a 0a5e 5e5e  n.compact``:.^^^
+00000770: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
 00000780: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
-00000790: 5e5e 5e5e 5e5e 5e5e 0a0a 312e 2041 6c6c  ^^^^^^^^..1. All
-000007a0: 6f77 7320 6465 6669 6e69 6e67 2073 7562  ows defining sub
-000007b0: 6d6f 6475 6c65 732c 2070 6172 616d 6574  modules, paramet
-000007c0: 6572 7320 616e 6420 6f74 6865 7220 7661  ers and other va
-000007d0: 7269 6162 6c65 7320 6e65 7874 2074 6f20  riables next to 
-000007e0: 7768 6572 6520 7468 6579 2061 7265 2075  where they are u
-000007f0: 7365 643a 206c 6573 730a 2020 2073 6372  sed: less.   scr
-00000800: 6f6c 6c69 6e67 2075 702f 646f 776e 2074  olling up/down t
-00000810: 6f20 7365 6520 686f 7720 6576 6572 7974  o see how everyt
-00000820: 6869 6e67 2069 7320 6465 6669 6e65 642e  hing is defined.
-00000830: 0a32 2e20 5265 6475 6365 7320 636f 6465  .2. Reduces code
-00000840: 2064 7570 6c69 6361 7469 6f6e 2077 6865   duplication whe
-00000850: 6e20 7468 6572 6520 6172 6520 636f 6e64  n there are cond
-00000860: 6974 696f 6e61 6c73 206f 7220 666f 7220  itionals or for 
-00000870: 6c6f 6f70 7320 7468 6174 2063 6f6e 6469  loops that condi
-00000880: 7469 6f6e 616c 6c79 2064 6566 696e 650a  tionally define.
-00000890: 2020 2073 7562 6d6f 6475 6c65 732c 2070     submodules, p
-000008a0: 6172 616d 6574 6572 7320 6f72 2076 6172  arameters or var
-000008b0: 6961 626c 6573 2e0a 332e 2043 6f64 6520  iables..3. Code 
-000008c0: 7479 7069 6361 6c6c 7920 6c6f 6f6b 7320  typically looks 
-000008d0: 6d6f 7265 206c 696b 6520 6d61 7468 656d  more like mathem
-000008e0: 6174 6963 616c 206e 6f74 6174 696f 6e3a  atical notation:
-000008f0: 2060 6079 203d 2073 656c 662e 7061 7261   ``y = self.para
-00000900: 6d28 2757 272c 202e 2e2e 2920 4020 7820  m('W', ...) @ x 
-00000910: 2b20 7365 6c66 2e70 6172 616d 2827 6227  + self.param('b'
-00000920: 2c20 2e2e 2e29 6060 0a20 2020 6c6f 6f6b  , ...)``.   look
-00000930: 7320 7369 6d69 6c61 7220 746f 203a 6d61  s similar to :ma
-00000940: 7468 3a60 793d 5778 2b62 6060 290a 342e  th:`y=Wx+b``).4.
-00000950: 2049 6620 796f 7520 6172 6520 7573 696e   If you are usin
-00000960: 6720 7368 6170 6520 696e 6665 7265 6e63  g shape inferenc
-00000970: 652c 2069 2e65 2e20 7573 696e 6720 7061  e, i.e. using pa
-00000980: 7261 6d65 7465 7273 2077 686f 7365 2073  rameters whose s
-00000990: 6861 7065 2f76 616c 7565 2064 6570 656e  hape/value depen
-000009a0: 6420 6f6e 2073 6861 7065 7320 6f66 0a20  d on shapes of. 
-000009b0: 2020 7468 6520 696e 7075 7473 2028 7768    the inputs (wh
-000009c0: 6963 6820 6172 6520 756e 6b6e 6f77 6e20  ich are unknown 
-000009d0: 6174 2069 6e69 7469 616c 697a 6174 696f  at initializatio
-000009e0: 6e29 2c20 7468 6973 2069 7320 6e6f 7420  n), this is not 
-000009f0: 706f 7373 6962 6c65 2075 7369 6e67 2060  possible using `
-00000a00: 6073 6574 7570 6060 2e0a 0a52 6561 736f  `setup``...Reaso
-00000a10: 6e73 2074 6f20 7072 6566 6572 2075 7369  ns to prefer usi
-00000a20: 6e67 2060 6073 6574 7570 6060 3a0a 5e5e  ng ``setup``:.^^
+00000790: 5e5e 5e5e 0a0a 312e 2041 6c6c 6f77 7320  ^^^^..1. Allows 
+000007a0: 6465 6669 6e69 6e67 2073 7562 6d6f 6475  defining submodu
+000007b0: 6c65 732c 2070 6172 616d 6574 6572 7320  les, parameters 
+000007c0: 616e 6420 6f74 6865 7220 7661 7269 6162  and other variab
+000007d0: 6c65 7320 6e65 7874 2074 6f20 7768 6572  les next to wher
+000007e0: 6520 7468 6579 2061 7265 2075 7365 643a  e they are used:
+000007f0: 206c 6573 730a 2020 2073 6372 6f6c 6c69   less.   scrolli
+00000800: 6e67 2075 702f 646f 776e 2074 6f20 7365  ng up/down to se
+00000810: 6520 686f 7720 6576 6572 7974 6869 6e67  e how everything
+00000820: 2069 7320 6465 6669 6e65 642e 0a32 2e20   is defined..2. 
+00000830: 5265 6475 6365 7320 636f 6465 2064 7570  Reduces code dup
+00000840: 6c69 6361 7469 6f6e 2077 6865 6e20 7468  lication when th
+00000850: 6572 6520 6172 6520 636f 6e64 6974 696f  ere are conditio
+00000860: 6e61 6c73 206f 7220 666f 7220 6c6f 6f70  nals or for loop
+00000870: 7320 7468 6174 2063 6f6e 6469 7469 6f6e  s that condition
+00000880: 616c 6c79 2064 6566 696e 650a 2020 2073  ally define.   s
+00000890: 7562 6d6f 6475 6c65 732c 2070 6172 616d  ubmodules, param
+000008a0: 6574 6572 7320 6f72 2076 6172 6961 626c  eters or variabl
+000008b0: 6573 2e0a 332e 2043 6f64 6520 7479 7069  es..3. Code typi
+000008c0: 6361 6c6c 7920 6c6f 6f6b 7320 6d6f 7265  cally looks more
+000008d0: 206c 696b 6520 6d61 7468 656d 6174 6963   like mathematic
+000008e0: 616c 206e 6f74 6174 696f 6e3a 2060 6079  al notation: ``y
+000008f0: 203d 2073 656c 662e 7061 7261 6d28 2757   = self.param('W
+00000900: 272c 202e 2e2e 2920 4020 7820 2b20 7365  ', ...) @ x + se
+00000910: 6c66 2e70 6172 616d 2827 6227 2c20 2e2e  lf.param('b', ..
+00000920: 2e29 6060 0a20 2020 6c6f 6f6b 7320 7369  .)``.   looks si
+00000930: 6d69 6c61 7220 746f 203a 6d61 7468 3a60  milar to :math:`
+00000940: 793d 5778 2b62 6060 290a 342e 2049 6620  y=Wx+b``).4. If 
+00000950: 796f 7520 6172 6520 7573 696e 6720 7368  you are using sh
+00000960: 6170 6520 696e 6665 7265 6e63 652c 2069  ape inference, i
+00000970: 2e65 2e20 7573 696e 6720 7061 7261 6d65  .e. using parame
+00000980: 7465 7273 2077 686f 7365 2073 6861 7065  ters whose shape
+00000990: 2f76 616c 7565 2064 6570 656e 6420 6f6e  /value depend on
+000009a0: 2073 6861 7065 7320 6f66 0a20 2020 7468   shapes of.   th
+000009b0: 6520 696e 7075 7473 2028 7768 6963 6820  e inputs (which 
+000009c0: 6172 6520 756e 6b6e 6f77 6e20 6174 2069  are unknown at i
+000009d0: 6e69 7469 616c 697a 6174 696f 6e29 2c20  nitialization), 
+000009e0: 7468 6973 2069 7320 6e6f 7420 706f 7373  this is not poss
+000009f0: 6962 6c65 2075 7369 6e67 2060 6073 6574  ible using ``set
+00000a00: 7570 6060 2e0a 0a52 6561 736f 6e73 2074  up``...Reasons t
+00000a10: 6f20 7072 6566 6572 2075 7369 6e67 2060  o prefer using `
+00000a20: 6073 6574 7570 6060 3a0a 5e5e 5e5e 5e5e  `setup``:.^^^^^^
 00000a30: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
-00000a40: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
-00000a50: 0a0a 312e 2043 6c6f 7365 7220 746f 2074  ..1. Closer to t
-00000a60: 6865 2050 7954 6f72 6368 2063 6f6e 7665  he PyTorch conve
-00000a70: 6e74 696f 6e2c 2074 6875 7320 6561 7369  ntion, thus easi
-00000a80: 6572 2077 6865 6e20 706f 7274 696e 6720  er when porting 
-00000a90: 6d6f 6465 6c73 0a20 2020 6672 6f6d 2050  models.   from P
-00000aa0: 7954 6f72 6368 0a32 2e20 536f 6d65 2070  yTorch.2. Some p
-00000ab0: 656f 706c 6520 6669 6e64 2069 7420 6d6f  eople find it mo
-00000ac0: 7265 206e 6174 7572 616c 2074 6f20 6578  re natural to ex
-00000ad0: 706c 6963 6974 6c79 2073 6570 6172 6174  plicitly separat
-00000ae0: 6520 7468 6520 6465 6669 6e69 7469 6f6e  e the definition
-00000af0: 0a20 2020 6f66 2073 7562 6d6f 6475 6c65  .   of submodule
-00000b00: 7320 616e 6420 7661 7269 6162 6c65 7320  s and variables 
-00000b10: 6672 6f6d 2077 6865 7265 2074 6865 7920  from where they 
-00000b20: 6172 6520 7573 6564 0a33 2e20 416c 6c6f  are used.3. Allo
-00000b30: 7773 2064 6566 696e 696e 6720 6d6f 7265  ws defining more
-00000b40: 2074 6861 6e20 6f6e 6520 2266 6f72 7761   than one "forwa
-00000b50: 7264 2070 6173 7322 206d 6574 686f 640a  rd pass" method.
-00000b60: 2020 2028 7365 6520 3a63 6c61 7373 3a60     (see :class:`
-00000b70: 4d75 6c74 6970 6c65 4d65 7468 6f64 7343  MultipleMethodsC
-00000b80: 6f6d 7061 6374 4572 726f 7220 3c66 6c61  ompactError <fla
-00000b90: 782e 6572 726f 7273 2e4d 756c 7469 706c  x.errors.Multipl
-00000ba0: 654d 6574 686f 6473 436f 6d70 6163 7445  eMethodsCompactE
-00000bb0: 7272 6f72 3e60 290a 0a0a 0a0a 2e2e 205f  rror>`)....... _
-00000bc0: 604c 696e 656e 603a 2068 7474 7073 3a2f  `Linen`: https:/
-00000bd0: 2f6a 6178 2e72 6561 6474 6865 646f 6373  /jax.readthedocs
-00000be0: 2e69 6f2f 656e 2f6c 6174 6573 742f 6e6f  .io/en/latest/no
-00000bf0: 7465 626f 6f6b 732f 7468 696e 6b69 6e67  tebooks/thinking
-00000c00: 5f69 6e5f 6a61 782e 6874 6d6c 234a 4954  _in_jax.html#JIT
-00000c10: 2d6d 6563 6861 6e69 6373 3a2d 7472 6163  -mechanics:-trac
-00000c20: 696e 672d 616e 642d 7374 6174 6963 2d76  ing-and-static-v
-00000c30: 6172 6961 626c 6573 0a                   ariables.
+00000a40: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 0a0a 312e  ^^^^^^^^^^^^..1.
+00000a50: 2043 6c6f 7365 7220 746f 2074 6865 2050   Closer to the P
+00000a60: 7954 6f72 6368 2063 6f6e 7665 6e74 696f  yTorch conventio
+00000a70: 6e2c 2074 6875 7320 6561 7369 6572 2077  n, thus easier w
+00000a80: 6865 6e20 706f 7274 696e 6720 6d6f 6465  hen porting mode
+00000a90: 6c73 0a20 2020 6672 6f6d 2050 7954 6f72  ls.   from PyTor
+00000aa0: 6368 0a32 2e20 536f 6d65 2070 656f 706c  ch.2. Some peopl
+00000ab0: 6520 6669 6e64 2069 7420 6d6f 7265 206e  e find it more n
+00000ac0: 6174 7572 616c 2074 6f20 6578 706c 6963  atural to explic
+00000ad0: 6974 6c79 2073 6570 6172 6174 6520 7468  itly separate th
+00000ae0: 6520 6465 6669 6e69 7469 6f6e 0a20 2020  e definition.   
+00000af0: 6f66 2073 7562 6d6f 6475 6c65 7320 616e  of submodules an
+00000b00: 6420 7661 7269 6162 6c65 7320 6672 6f6d  d variables from
+00000b10: 2077 6865 7265 2074 6865 7920 6172 6520   where they are 
+00000b20: 7573 6564 0a33 2e20 416c 6c6f 7773 2064  used.3. Allows d
+00000b30: 6566 696e 696e 6720 6d6f 7265 2074 6861  efining more tha
+00000b40: 6e20 6f6e 6520 2266 6f72 7761 7264 2070  n one "forward p
+00000b50: 6173 7322 206d 6574 686f 640a 2020 2028  ass" method.   (
+00000b60: 7365 6520 3a63 6c61 7373 3a60 4d75 6c74  see :class:`Mult
+00000b70: 6970 6c65 4d65 7468 6f64 7343 6f6d 7061  ipleMethodsCompa
+00000b80: 6374 4572 726f 7220 3c66 6c61 782e 6572  ctError <flax.er
+00000b90: 726f 7273 2e4d 756c 7469 706c 654d 6574  rors.MultipleMet
+00000ba0: 686f 6473 436f 6d70 6163 7445 7272 6f72  hodsCompactError
+00000bb0: 3e60 290a 0a0a 0a0a 2e2e 205f 604c 696e  >`)....... _`Lin
+00000bc0: 656e 603a 2068 7474 7073 3a2f 2f6a 6178  en`: https://jax
+00000bd0: 2e72 6561 6474 6865 646f 6373 2e69 6f2f  .readthedocs.io/
+00000be0: 656e 2f6c 6174 6573 742f 6e6f 7465 626f  en/latest/notebo
+00000bf0: 6f6b 732f 7468 696e 6b69 6e67 5f69 6e5f  oks/thinking_in_
+00000c00: 6a61 782e 6874 6d6c 234a 4954 2d6d 6563  jax.html#JIT-mec
+00000c10: 6861 6e69 6373 3a2d 7472 6163 696e 672d  hanics:-tracing-
+00000c20: 616e 642d 7374 6174 6963 2d76 6172 6961  and-static-varia
+00000c30: 626c 6573 0a                             bles.
```

### Comparing `flax-0.6.9/docs/guides/state_params.rst` & `flax-0.7.0/docs/guides/state_params.rst`

 * *Files 1% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 
 * manage the variables from initialization to updates.
 * split and re-assemble parameters and state.
 * use :code:`vmap` with batch-dependant state.
 
 .. testsetup::
 
+  import flax
   from flax import linen as nn
   from jax import random
   import jax.numpy as jnp
   import jax
   import optax
 
   # Create some fake data and run only for one epoch for testing.
@@ -65,15 +66,15 @@
 Then we can write the actual training code.
 
 .. testcode::
 
   model = BiasAdderWithRunningMean()
   variables = model.init(random.PRNGKey(0), dummy_input)
   # Split state and params (which are updated by optimizer).
-  state, params = variables.pop('params')
+  state, params = flax.core.pop(variables, 'params')
   del variables  # Delete variables to avoid wasting resources
   tx = optax.sgd(learning_rate=0.02)
   opt_state = tx.init(params)
 
   for _ in range(num_epochs):
     opt_state, params, state = update_step(
         model.apply, dummy_input, opt_state, params, state)
@@ -168,15 +169,15 @@
 dimension. Now we are able to train the model:
 
 .. testcode::
 
   model = MLP(hidden_size=10, out_size=1)
   variables = model.init(random.PRNGKey(0), dummy_input)
   # Split state and params (which are updated by optimizer).
-  state, params = variables.pop('params')
+  state, params = flax.core.pop(variables, 'params')
   del variables  # Delete variables to avoid wasting resources
   tx = optax.sgd(learning_rate=0.02)
   opt_state = tx.init(params)
 
   for _ in range(num_epochs):
     opt_state, params, state, loss = update_step(
         model.apply, X, Y, opt_state, params, state)
```

### Comparing `flax-0.6.9/docs/guides/transfer_learning.ipynb` & `flax-0.7.0/docs/guides/transfer_learning.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997258771929824%*

 * *Differences: {"'cells'": "{6: {'source': {insert: [(6, 'An easy way to extract the `vision_model` sub-Module "*

 * *            'defined inside `.setup()` and its variables is to use '*

 * *            '[`flax.linen.Module.bind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.bind) '*

 * *            'on the `clip` Module immediately followed by '*

 * *            '[`flax.linen.Module.unbind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.unbind) ' []*

```diff
@@ -88,15 +88,15 @@
             "source": [
                 "Note that `FlaxCLIPVisionModel` itself is not a Flax `Module` which is why we need to do this extra step.\n",
                 "\n",
                 "### Extracting a submodule\n",
                 "\n",
                 "Calling `load_model` from the snippet above returns the `FlaxCLIPModule`, which is composed of `text_model` and `vision_model` submodules.\n",
                 "\n",
-                "An easy way to extract the `vision_model` sub-Module defined inside `.setup()` and its variables is to use [`flax.linen.Module.bind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.bind) on the `clip` Module immediately followed by [`flax.linen.Module.unbind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.unbind) on the `vision_model` sub-Module."
+                "An easy way to extract the `vision_model` sub-Module defined inside `.setup()` and its variables is to use [`flax.linen.Module.bind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.bind) on the `clip` Module immediately followed by [`flax.linen.Module.unbind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.unbind) on the `vision_model` sub-Module."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 8,
             "metadata": {},
             "outputs": [],
```

### Comparing `flax-0.6.9/docs/guides/transfer_learning.md` & `flax-0.7.0/docs/guides/transfer_learning.md`

 * *Files 3% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 
 Note that `FlaxCLIPVisionModel` itself is not a Flax `Module` which is why we need to do this extra step.
 
 ### Extracting a submodule
 
 Calling `load_model` from the snippet above returns the `FlaxCLIPModule`, which is composed of `text_model` and `vision_model` submodules.
 
-An easy way to extract the `vision_model` sub-Module defined inside `.setup()` and its variables is to use [`flax.linen.Module.bind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.bind) on the `clip` Module immediately followed by [`flax.linen.Module.unbind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.unbind) on the `vision_model` sub-Module.
+An easy way to extract the `vision_model` sub-Module defined inside `.setup()` and its variables is to use [`flax.linen.Module.bind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.bind) on the `clip` Module immediately followed by [`flax.linen.Module.unbind`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.unbind) on the `vision_model` sub-Module.
 
 ```{code-cell} ipython3
 import flax.linen as nn
 
 clip, clip_variables = load_model()
 vision_model, vision_model_vars = clip.bind(clip_variables).vision_model.unbind()
 ```
```

### Comparing `flax-0.6.9/docs/guides/use_checkpointing.ipynb` & `flax-0.7.0/docs/guides/use_checkpointing.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/guides/use_checkpointing.md` & `flax-0.7.0/docs/guides/use_checkpointing.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/index.rst` & `flax-0.7.0/docs/index.rst`

 * *Files 1% similar despite different names*

```diff
@@ -46,28 +46,28 @@
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
             Flax is designed for correctness and safety. Thanks to its immutable Modules
-            and Functional API, Flax helps mitigate bugs that araise when handling state
+            and Functional API, Flax helps mitigate bugs that arise when handling state
             in JAX.
 
    .. grid-item::
       :columns: 12 12 12 6
 
       .. card:: Control
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
-            Flax grants more fine grained control and expressivity than most Neural Network
+            Flax grants more fine-grained control and expressivity than most Neural Network
             frameworks via its Variable Collections, RNG Collections and Mutability conditions.
 
    .. grid-item::
       :columns: 12 12 12 6
 
       .. card:: Functional API
          :class-card: sd-border-0
```

### Comparing `flax-0.6.9/docs/mission.md` & `flax-0.7.0/docs/mission.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/flax_sharp_bits.ipynb` & `flax-0.7.0/docs/notebooks/flax_sharp_bits.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9989035087719298%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(8, '3. When initializing the model "*

 * *            '([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html)), '*

 * *            "there\\'s no need to pass in an extra `\\'dropout\\'` PRNG keyjust the "*

 * *            '`\\\'params\\\'` key like in a "simpler" model.\\n\'), (9, "4. During the forward '*

 * *            'pass with '*

 * *            '[`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_app []*

```diff
@@ -34,38 +34,38 @@
                 "\n",
                 "### TL;DR\n",
                 "\n",
                 "When working on a model with dropout (subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)), add the `'dropout'` PRNGkey only during the forward pass.\n",
                 "\n",
                 "1. Start with [`jax.random.split()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax-random-split) to explicitly create PRNG keys for `'params'` and `'dropout'`.\n",
                 "2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)).\n",
-                "3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply)), there's no need to pass in an extra `'dropout'` PRNG key\u2014just the `'params'` key like in a \"simpler\" model.\n",
-                "4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply), pass in `rngs={'dropout': dropout_key}`.\n",
+                "3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html)), there's no need to pass in an extra `'dropout'` PRNG key\u2014just the `'params'` key like in a \"simpler\" model.\n",
+                "4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html), pass in `rngs={'dropout': dropout_key}`.\n",
                 "\n",
                 "Check out a full example below.\n",
                 "\n",
                 "### Why this works\n",
                 "\n",
-                "- Internally, `flax.linen.Dropout` makes use of [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.make_rng) to create a key for dropout (check out the [source code](https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/flax/linen/stochastic.py#L72)).\n",
+                "- Internally, `flax.linen.Dropout` makes use of [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.make_rng) to create a key for dropout (check out the [source code](https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/flax/linen/stochastic.py#L72)).\n",
                 "- Every time `make_rng` is called (in this case, it's done implicitly in `Dropout`), you get a new PRNG key split from the main/root PRNG key.\n",
                 "- `make_rng` still _guarantees full reproducibility_.\n",
                 "\n",
                 "### Background \n",
                 "\n",
                 "The [dropout](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) stochastic regularization technique randomly removes hidden and visible units in a network. Dropout is a random operation, requiring a PRNG state, and Flax (like JAX) uses [Threefry](https://github.com/google/jax/blob/main/docs/jep/263-prng.md) PRNG that is splittable. \n",
                 "\n",
                 "> Note: Recall that JAX has an explicit way of giving you PRNG keys: you can fork the main PRNG state (such as `key = jax.random.PRNGKey(seed=0)`) into multiple new PRNG keys with `key, subkey = jax.random.split(key)`. Refresh your memory in [\ud83d\udd2a JAX - The Sharp Bits \ud83d\udd2a Randomness and PRNG keys](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers).\n",
                 "\n",
-                "Flax provides an _implicit_ way of handling PRNG key streams via [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)'s [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.make_rng) helper function. It allows the code in Flax `Module`s (or its sub-`Module`s) to \"pull PRNG keys\". `make_rng` guarantees to provide a unique key each time you call it.\n",
+                "Flax provides an _implicit_ way of handling PRNG key streams via [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)'s [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.make_rng) helper function. It allows the code in Flax `Module`s (or its sub-`Module`s) to \"pull PRNG keys\". `make_rng` guarantees to provide a unique key each time you call it.\n",
                 "\n",
-                "> Note: Recall that [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module) is the base class for all neural network modules. All layers and models are subclassed from it.\n",
+                "> Note: Recall that [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) is the base class for all neural network modules. All layers and models are subclassed from it.\n",
                 "\n",
                 "### Example\n",
                 "\n",
-                "Remember that each of the Flax PRNG streams has a name. The example below uses the `'params'` stream for initializing parameters, as well as the `'dropout'` stream. The PRNG key provided to [`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply) is the one that seeds the `'params'` PRNG key stream. To draw PRNG keys during the forward pass (with dropout), provide a PRNG key to seed that stream (`'dropout'`) when you call `Module.apply()`."
+                "Remember that each of the Flax PRNG streams has a name. The example below uses the `'params'` stream for initializing parameters, as well as the `'dropout'` stream. The PRNG key provided to [`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html) is the one that seeds the `'params'` PRNG key stream. To draw PRNG keys during the forward pass (with dropout), provide a PRNG key to seed that stream (`'dropout'`) when you call `Module.apply()`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [],
```

### Comparing `flax-0.6.9/docs/notebooks/flax_sharp_bits.md` & `flax-0.7.0/docs/notebooks/flax_sharp_bits.md`

 * *Files 3% similar despite different names*

```diff
@@ -26,38 +26,38 @@
 
 ### TL;DR
 
 When working on a model with dropout (subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)), add the `'dropout'` PRNGkey only during the forward pass.
 
 1. Start with [`jax.random.split()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax-random-split) to explicitly create PRNG keys for `'params'` and `'dropout'`.
 2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)).
-3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply)), there's no need to pass in an extra `'dropout'` PRNG keyjust the `'params'` key like in a "simpler" model.
-4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply), pass in `rngs={'dropout': dropout_key}`.
+3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html)), there's no need to pass in an extra `'dropout'` PRNG keyjust the `'params'` key like in a "simpler" model.
+4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html), pass in `rngs={'dropout': dropout_key}`.
 
 Check out a full example below.
 
 ### Why this works
 
-- Internally, `flax.linen.Dropout` makes use of [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.make_rng) to create a key for dropout (check out the [source code](https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/flax/linen/stochastic.py#L72)).
+- Internally, `flax.linen.Dropout` makes use of [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.make_rng) to create a key for dropout (check out the [source code](https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/flax/linen/stochastic.py#L72)).
 - Every time `make_rng` is called (in this case, it's done implicitly in `Dropout`), you get a new PRNG key split from the main/root PRNG key.
 - `make_rng` still _guarantees full reproducibility_.
 
 ### Background 
 
 The [dropout](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) stochastic regularization technique randomly removes hidden and visible units in a network. Dropout is a random operation, requiring a PRNG state, and Flax (like JAX) uses [Threefry](https://github.com/google/jax/blob/main/docs/jep/263-prng.md) PRNG that is splittable. 
 
 > Note: Recall that JAX has an explicit way of giving you PRNG keys: you can fork the main PRNG state (such as `key = jax.random.PRNGKey(seed=0)`) into multiple new PRNG keys with `key, subkey = jax.random.split(key)`. Refresh your memory in [ JAX - The Sharp Bits  Randomness and PRNG keys](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers).
 
-Flax provides an _implicit_ way of handling PRNG key streams via [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)'s [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.make_rng) helper function. It allows the code in Flax `Module`s (or its sub-`Module`s) to "pull PRNG keys". `make_rng` guarantees to provide a unique key each time you call it.
+Flax provides an _implicit_ way of handling PRNG key streams via [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)'s [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.make_rng) helper function. It allows the code in Flax `Module`s (or its sub-`Module`s) to "pull PRNG keys". `make_rng` guarantees to provide a unique key each time you call it.
 
-> Note: Recall that [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module) is the base class for all neural network modules. All layers and models are subclassed from it.
+> Note: Recall that [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) is the base class for all neural network modules. All layers and models are subclassed from it.
 
 ### Example
 
-Remember that each of the Flax PRNG streams has a name. The example below uses the `'params'` stream for initializing parameters, as well as the `'dropout'` stream. The PRNG key provided to [`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply) is the one that seeds the `'params'` PRNG key stream. To draw PRNG keys during the forward pass (with dropout), provide a PRNG key to seed that stream (`'dropout'`) when you call `Module.apply()`.
+Remember that each of the Flax PRNG streams has a name. The example below uses the `'params'` stream for initializing parameters, as well as the `'dropout'` stream. The PRNG key provided to [`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html) is the one that seeds the `'params'` PRNG key stream. To draw PRNG keys during the forward pass (with dropout), provide a PRNG key to seed that stream (`'dropout'`) when you call `Module.apply()`.
 
 ```{code-cell} ipython3
 # Setup.
 import jax
 import jax.numpy as jnp
 import flax.linen as nn
 ```
```

### Comparing `flax-0.6.9/docs/notebooks/full_eval.ipynb` & `flax-0.7.0/docs/notebooks/full_eval.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9996428571428572%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(6, 'Please refer to above link for an explanation of the "*

 * *            "problem and the proposed solutions.')], delete: [6]}}, 10: {'source': {insert: [(17, "*

 * *            "'correct = correct.item()\\n')], delete: [17]}}, 11: {'source': {insert: [(21, "*

 * *            "'correct = correct.item()\\n')], delete: [21]}}}"}*

```diff
@@ -8,15 +8,15 @@
             "source": [
                 "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/full_eval.ipynb)\n",
                 "[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/full_eval.ipynb)\n",
                 "\n",
                 "This notebook only contains executable code cells for the examples mentioned in\n",
                 "https://flax.readthedocs.io/en/latest/guides/full_eval.html\n",
                 "\n",
-                "Please refer to above link for a an explanation of the problem and the proposed solutions."
+                "Please refer to above link for an explanation of the problem and the proposed solutions."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "Um6ZK_o1W-Vu"
             },
@@ -295,15 +295,15 @@
                 "\n",
                 "correct = total = 0\n",
                 "for batch in ds.as_numpy_iterator():\n",
                 "  preds = get_preds(variables, batch['image'])\n",
                 "  total += len(batch['label'])\n",
                 "  correct += (batch['label'] == preds.argmax(axis=1)).sum()\n",
                 "\n",
-                "correc = correct.item()\n",
+                "correct = correct.item()\n",
                 "correct, total, correct / total"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {
@@ -347,15 +347,15 @@
                 "\n",
                 "correct = total = 0\n",
                 "for batch in ds.as_numpy_iterator():\n",
                 "  preds = get_preds(variables, batch['image'])\n",
                 "  total += len(batch['label'].flatten())\n",
                 "  correct += (batch['label'] == preds.argmax(axis=-1)).sum()\n",
                 "\n",
-                "correc = correct.item()\n",
+                "correct = correct.item()\n",
                 "correct, total, correct / total"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "vfu54P0pJwEH"
```

### Comparing `flax-0.6.9/docs/notebooks/full_eval.md` & `flax-0.7.0/docs/notebooks/full_eval.md`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 
 [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/full_eval.ipynb)
 [![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/full_eval.ipynb)
 
 This notebook only contains executable code cells for the examples mentioned in
 https://flax.readthedocs.io/en/latest/guides/full_eval.html
 
-Please refer to above link for a an explanation of the problem and the proposed solutions.
+Please refer to above link for an explanation of the problem and the proposed solutions.
 
 +++ {"id": "Um6ZK_o1W-Vu"}
 
 ### setup
 
 ```{code-cell} ipython3
 :id: 62DTHYCYHWp1
@@ -136,15 +136,15 @@
 
 correct = total = 0
 for batch in ds.as_numpy_iterator():
   preds = get_preds(variables, batch['image'])
   total += len(batch['label'])
   correct += (batch['label'] == preds.argmax(axis=1)).sum()
 
-correc = correct.item()
+correct = correct.item()
 correct, total, correct / total
 ```
 
 ```{code-cell} ipython3
 :id: dlJuEBcLKY94
 :outputId: e94cf79c-a033-4bc3-a086-75ecd8bd21f0
 
@@ -165,15 +165,15 @@
 
 correct = total = 0
 for batch in ds.as_numpy_iterator():
   preds = get_preds(variables, batch['image'])
   total += len(batch['label'].flatten())
   correct += (batch['label'] == preds.argmax(axis=-1)).sum()
 
-correc = correct.item()
+correct = correct.item()
 correct, total, correct / total
 ```
 
 +++ {"id": "vfu54P0pJwEH"}
 
 ### The solution: padding
```

### Comparing `flax-0.6.9/docs/notebooks/linen_intro.md` & `flax-0.7.0/docs/notebooks/linen_intro.md`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,15 @@
 +++ {"id": "u86fYsrEfYow"}
 
 # Invoking Modules
 
 +++ {"id": "nrVbFrh1ffve"}
 
 Let's instantiate a `Dense` layer.
- - Modules are actually objects in this API, so we provide _contructor arguments_ when initializing the Module.  In this case, we only have to provide the output `features` dimension.
+ - Modules are actually objects in this API, so we provide _constructor arguments_ when initializing the Module.  In this case, we only have to provide the output `features` dimension.
 
 ```{code-cell}
 :id: EcDH20Uufc-v
 
 model = nn.Dense(features=3)
 ```
 
@@ -609,30 +609,34 @@
 
 ```{code-cell}
 :id: oxA_lWm7tH2B
 :outputId: 7d9ebed3-64de-4ca8-9dce-4b09ba9e31a1
 :tags: []
 
 class SimpleScan(nn.Module):
+  features: int
+
   @nn.compact
   def __call__(self, xs):
-    dummy_rng = random.PRNGKey(0)
-    init_carry = nn.LSTMCell.initialize_carry(dummy_rng,
-                                              xs.shape[:1],
-                                              xs.shape[-1])
     LSTM = nn.scan(nn.LSTMCell,
                    in_axes=1, out_axes=1,
                    variable_broadcast='params',
                    split_rngs={'params': False})
-    return LSTM(name="lstm_cell")(init_carry, xs)
+    lstm = LSTM(self.features, name="lstm_cell")
+
+    dummy_rng = random.PRNGKey(0)
+    input_shape = xs[:, 0].shape
+    init_carry = lstm.initialize_carry(dummy_rng, input_shape)
+
+    return lstm(init_carry, xs)
 
 key1, key2 = random.split(random.PRNGKey(0), 2)
 xs = random.uniform(key1, (1, 5, 2))
 
-model = SimpleScan()
+model = SimpleScan(2)
 init_variables = model.init(key2, xs)
 
 print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
 
 y = model.apply(init_variables, xs)
 print('output:\n', y)
 ```
```

### Comparing `flax-0.6.9/docs/notebooks/optax_update_guide.ipynb` & `flax-0.7.0/docs/notebooks/optax_update_guide.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/optax_update_guide.md` & `flax-0.7.0/docs/notebooks/optax_update_guide.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/orbax_upgrade_guide.ipynb` & `flax-0.7.0/docs/notebooks/orbax_upgrade_guide.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/orbax_upgrade_guide.md` & `flax-0.7.0/docs/notebooks/orbax_upgrade_guide.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/state_params.ipynb` & `flax-0.7.0/docs/notebooks/state_params.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/notebooks/state_params.md` & `flax-0.7.0/docs/notebooks/state_params.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/overview.md` & `flax-0.7.0/docs/overview.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/docs/requirements.txt` & `flax-0.7.0/docs/requirements.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 sphinx>=3.3.1
 sphinx-book-theme
 Pygments>=2.6.1
-jax>=0.3
+jax>=0.4
 jaxlib
 ipykernel
 myst_nb
 recommonmark
 ipython_genutils
 sphinx-design
 jupytext==1.13.8
```

### Comparing `flax-0.6.9/examples/README.md` & `flax-0.7.0/examples/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/cloud/README.md` & `flax-0.7.0/examples/cloud/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/cloud/launch_gce.py` & `flax-0.7.0/examples/cloud/launch_gce.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/cloud/startup_script.sh` & `flax-0.7.0/examples/cloud/startup_script.sh`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/README.md` & `flax-0.7.0/examples/imagenet/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -161,15 +161,15 @@
 usually a good idea to connect to a single host and execute the commands
 interactively.
 
 For convenience, the TPU creation commands are inlined below. Please note that
 we define `GCS_TFDS_BUCKET` to where your data stands in your cloud bucket.
 Also `YOUR_BUCKET` is the work directory you are experimenting in. You should
 choose ZONE based on where your TPU and work directory is. [Here](https://cloud.google.com/tpu/docs/types-zones)
-has some usefule information on which zones you can have different types of TPUs.
+has some useful information on which zones you can have different types of TPUs.
 
 ```shell
 VM_NAME=imagenet
 REPO=https://github.com/google/flax
 BRANCH=main
 WORKDIR=gs://$YOUR_BUCKET/flax/examples/imagenet/$(date +%Y%m%d_%H%M)
 
@@ -198,13 +198,13 @@
 ```
 
 #### Google Cloud GPU
 
 Can be launched with utility script described in
 [../cloud/README.md](../cloud/README.md)
 
-There are two configuratoins available:
+There are two configurations available:
 
 - `configs/v100_x8.py` : Full precision GPU training
 - `configs/v100_x8_mixed_precision.py` : Mixed precision GPU training. Note that
   mixed precision handling is implemented manually with
   [`training.dynamic_scale`](https://github.com/google/flax/blob/main/flax/training/dynamic_scale.py)
```

### Comparing `flax-0.6.9/examples/imagenet/configs/default.py` & `flax-0.7.0/examples/imagenet/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/configs/fake_data_benchmark.py` & `flax-0.7.0/examples/imagenet/configs/fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/configs/tpu.py` & `flax-0.7.0/examples/imagenet/configs/tpu.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/configs/v100_x8.py` & `flax-0.7.0/examples/imagenet/configs/v100_x8.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/configs/v100_x8_mixed_precision.py` & `flax-0.7.0/examples/imagenet/configs/v100_x8_mixed_precision.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/imagenet.ipynb` & `flax-0.7.0/examples/imagenet/imagenet.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9995975672877847%*

 * *Differences: {"'cells'": "{6: {'source': {insert: [(7, '#@markdown be restarted and any changes are "*

 * *            "lost**.\\n')], delete: [7]}}, 7: {'source': {insert: [(0, '# Note : In Colab, above "*

 * *            "cell changed the working directory.\\n')], delete: [0]}}, 23: {'source': {insert: "*

 * *            "[(3, '  #@markdown Note that everybody with the link will be able to see the "*

 * *            "data.\\n')], delete: [3]}}, 33: {'source': {insert: [(0, '# Define parallelized "*

 * *            "inference function in separ []*

```diff
@@ -201,15 +201,15 @@
                 "# (If you run this code in Jupyter[lab], then you're already in the\n",
                 "#  example directory and nothing needs to be done.)\n",
                 "\n",
                 "#@markdown **Fetch newest Flax, copy example code**\n",
                 "#@markdown\n",
                 "#@markdown **If you select no** below, then the files will be stored on the\n",
                 "#@markdown *ephemeral* Colab VM. **After some time of inactivity, this VM will\n",
-                "#@markdown be restarted an any changes are lost**.\n",
+                "#@markdown be restarted and any changes are lost**.\n",
                 "#@markdown\n",
                 "#@markdown **If you select yes** below, then you will be asked for your\n",
                 "#@markdown credentials to mount your personal Google Drive. In this case, all\n",
                 "#@markdown changes you make will be *persisted*, and even if you re-run the\n",
                 "#@markdown Colab later on, the files will still be the same (you can of course\n",
                 "#@markdown remove directories inside your Drive's `flax/` root if you want to\n",
                 "#@markdown manually revert these files).\n",
@@ -250,15 +250,15 @@
             "execution_count": null,
             "metadata": {
                 "id": "xcXZ-F3_zBuJ",
                 "outputId": "acc1f45d-5062-4ff3-e6d4-10b4ffe0f8ef"
             },
             "outputs": [],
             "source": [
-                "# Note : In Colab, above cell changed the working direcoty.\n",
+                "# Note : In Colab, above cell changed the working directory.\n",
                 "!pwd"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "Tt0rL4ycp4ZB"
@@ -764,15 +764,15 @@
                 "id": "mZOKD0Y7p4ZW"
             },
             "outputs": [],
             "source": [
                 "if 'google.colab' in str(get_ipython()):\n",
                 "  #@markdown You can upload the training results directly to https://tensorbaord.dev\n",
                 "  #@markdown\n",
-                "  #@markdown Note that everbody with the link will be able to see the data.\n",
+                "  #@markdown Note that everybody with the link will be able to see the data.\n",
                 "  upload_data = 'no' #@param ['yes', 'no']\n",
                 "  if upload_data == 'yes':\n",
                 "    !tensorboard dev upload --one_shot --logdir ./models --name 'Flax examples/mnist'"
             ]
         },
         {
             "cell_type": "markdown",
@@ -974,15 +974,15 @@
                     "text": [
                         "INFO:absl:Constructing tf.data.Dataset imagenette for split validation[0:3925], from /root/tensorflow_datasets/imagenette/full-size-v2/1.0.0\n",
                         "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
                     ]
                 }
             ],
             "source": [
-                "# Define parallelized inference function in separate cell so the the cached\n",
+                "# Define parallelized inference function in separate cell so the cached\n",
                 "# compilation can be used if below cell is executed multiple times.\n",
                 "@jax.pmap\n",
                 "def p_get_logits(images):\n",
                 "  return model.apply({'params': state.params, 'batch_stats': state.batch_stats},\n",
                 "                     images, train=False)\n",
                 "\n",
                 "eval_iter = train.create_input_iter(dataset_builder, config.batch_size,\n",
```

### Comparing `flax-0.6.9/examples/imagenet/imagenet_benchmark.py` & `flax-0.7.0/examples/imagenet/imagenet_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/imagenet_fake_data_benchmark.py` & `flax-0.7.0/examples/imagenet/imagenet_fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/input_pipeline.py` & `flax-0.7.0/examples/imagenet/input_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -44,15 +44,15 @@
         image.
     min_object_covered: An optional `float`. Defaults to `0.1`. The cropped
         area of the image must contain at least this fraction of any bounding
         box supplied.
     aspect_ratio_range: An optional list of `float`s. The cropped area of the
         image must have an aspect ratio = width / height within this range.
     area_range: An optional list of `float`s. The cropped area of the image
-        must contain a fraction of the supplied image within in this range.
+        must contain a fraction of the supplied image within this range.
     max_attempts: An optional `int`. Number of attempts at generating a cropped
         region of the image of the specified constraints. After `max_attempts`
         failures, return the entire image.
   Returns:
     cropped image `Tensor`
   """
   shape = tf.io.extract_jpeg_shape(image_bytes)
```

### Comparing `flax-0.6.9/examples/imagenet/main.py` & `flax-0.7.0/examples/imagenet/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/models.py` & `flax-0.7.0/examples/imagenet/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/models_test.py` & `flax-0.7.0/examples/imagenet/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/train.py` & `flax-0.7.0/examples/imagenet/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/imagenet/train_test.py` & `flax-0.7.0/examples/imagenet/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/attention_simple.py` & `flax-0.7.0/examples/linen_design_test/attention_simple.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/autoencoder.py` & `flax-0.7.0/examples/linen_design_test/autoencoder.py`

 * *Files 0% similar despite different names*

```diff
@@ -63,15 +63,15 @@
 # `ae` is a detached module, which has no variables.
 ae = AutoEncoder(
     encoder_widths=(32, 32, 32),
     decoder_widths=(32, 32, 32),
     input_shape=(28, 28, 1))
 
 
-# `ae.initialized` returnes a materialized copy of `ae` by
+# `ae.initialized` returns a materialized copy of `ae` by
 # running through an input to create submodules defined lazily.
 params = ae.init(
     {'params': random.PRNGKey(42)},
     jnp.ones((1, 28, 28, 1)))
 
 
 # Now you can use `ae` as a normal object, calling any methods defined on AutoEncoder
```

### Comparing `flax-0.6.9/examples/linen_design_test/dense.py` & `flax-0.7.0/examples/linen_design_test/dense.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/linear_regression.py` & `flax-0.7.0/examples/linen_design_test/linear_regression.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/mlp_explicit.py` & `flax-0.7.0/examples/linen_design_test/mlp_explicit.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/mlp_inline.py` & `flax-0.7.0/examples/linen_design_test/mlp_inline.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/mlp_lazy.py` & `flax-0.7.0/examples/linen_design_test/mlp_lazy.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/tied_autoencoder.py` & `flax-0.7.0/examples/linen_design_test/tied_autoencoder.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/linen_design_test/weight_std.py` & `flax-0.7.0/examples/linen_design_test/weight_std.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/README.md` & `flax-0.7.0/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/configs/default.py` & `flax-0.7.0/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/input_pipeline.py` & `flax-0.7.0/examples/lm1b/input_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -146,15 +146,15 @@
   """Helper-function for packing a dataset which has already been batched.
 
   Helper for pack_dataset()  Uses tf.while_loop.
 
   Args:
     dataset: a dataset containing padded batches of examples.
     keys: a list of strings
-    key2length: an dict from feature-key to integer
+    key2length: a dict from feature-key to integer
 
   Returns:
     a dataset.
   """
   empty_example = {}
   for k in keys:
     empty_example[k] = tf.zeros([0], dtype=tf.int32)
```

### Comparing `flax-0.6.9/examples/lm1b/input_pipeline_test.py` & `flax-0.7.0/examples/lm1b/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/main.py` & `flax-0.7.0/examples/lm1b/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/models.py` & `flax-0.7.0/examples/lm1b/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/temperature_sampler.py` & `flax-0.7.0/examples/lm1b/temperature_sampler.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/temperature_sampler_test.py` & `flax-0.7.0/examples/lm1b/temperature_sampler_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/tokenizer.py` & `flax-0.7.0/examples/lm1b/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/train.py` & `flax-0.7.0/examples/lm1b/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/lm1b/train_test.py` & `flax-0.7.0/examples/lm1b/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/README.md` & `flax-0.7.0/examples/mnist/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/configs/default.py` & `flax-0.7.0/examples/mnist/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/main.py` & `flax-0.7.0/examples/mnist/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/mnist.ipynb` & `flax-0.7.0/examples/mnist/mnist.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/mnist_benchmark.py` & `flax-0.7.0/examples/mnist/mnist_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/train.py` & `flax-0.7.0/examples/mnist/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/mnist/train_test.py` & `flax-0.7.0/examples/mnist/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/nlp_seq/README.md` & `flax-0.7.0/examples/nlp_seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/nlp_seq/input_pipeline.py` & `flax-0.7.0/examples/nlp_seq/input_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -100,17 +100,17 @@
   Input example: CoNLL 09 representation for a token.
     ['Ms.', 'ms.', 'ms.', 'NNP', '_', '2', 'TITLE]
   Output example: Indices as defined in self._attributes, e.g., [word form,
     part-of-speech tag, and head].
     [1025, 3, 1]
 
   Args:
-    token: CoNLL token atrributes.
+    token: CoNLL token attributes.
     attributes: selected attributes.
-    vocabs: dictonery of vocabs.
+    vocabs: dictionary of vocabs.
 
   Returns:
     List of attribute ids for a token, e.g. [1025, 3] with word id and pos id.
 
   Raises:
     ValueError: CoNLL attribute requested but not covered by mapping.
   """
@@ -130,15 +130,15 @@
 
 
 def create_sentence_with_root(attributes, vocabs):
   """Create a sentence containing a root.
 
   Args:
     attributes: attributes extracted from token.
-    vocabs: dictonery of vocabs.
+    vocabs: dictionary of vocabs.
 
   Returns:
     A list representing a sentence containing the root only,
     e.g., [[2, 1, 0]] for root word, unknown xpos, and head 0.
   """
   # Create the token properties of an artificial root node.
   token_properties = [ROOT for _ in range(12)]  # CoNLL 09 has 12 columns.
@@ -197,15 +197,15 @@
                           prefetch_size=tf.data.experimental.AUTOTUNE):
   """Combines sentences into a dataset of padded batches.
 
   Args:
     filename: file name of a corpus.
     vocabs: dictionary of dictionaries to map from strings to ids.
     attributes_input: attributes for the input.
-    attributes_target: target attributes empty targets is not inclueded.
+    attributes_target: target attributes empty targets is not included.
     batch_size: the size of a batch.
     bucket_size: the size of a bucket.
     repeat: number of times the dataset is repeated.
     prefetch_size: prefetch size of the data.
 
   Returns:
     Returns dataset as dictionary containing the data as key value pairs.
```

### Comparing `flax-0.6.9/examples/nlp_seq/input_pipeline_test.py` & `flax-0.7.0/examples/nlp_seq/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/nlp_seq/models.py` & `flax-0.7.0/examples/nlp_seq/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/nlp_seq/train.py` & `flax-0.7.0/examples/nlp_seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/README.md` & `flax-0.7.0/examples/ogbg_molpcba/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/configs/default.py` & `flax-0.7.0/examples/ogbg_molpcba/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/configs/default_graph_net.py` & `flax-0.7.0/examples/ogbg_molpcba/configs/default_graph_net.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/configs/hparam_sweep.py` & `flax-0.7.0/examples/ogbg_molpcba/configs/hparam_sweep.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/configs/test.py` & `flax-0.7.0/examples/ogbg_molpcba/configs/test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/input_pipeline.py` & `flax-0.7.0/examples/ogbg_molpcba/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/input_pipeline_test.py` & `flax-0.7.0/examples/ogbg_molpcba/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/main.py` & `flax-0.7.0/examples/ogbg_molpcba/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/models.py` & `flax-0.7.0/examples/ogbg_molpcba/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/models_test.py` & `flax-0.7.0/examples/ogbg_molpcba/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/ogbg_molpcba.ipynb` & `flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py` & `flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/train.py` & `flax-0.7.0/examples/ogbg_molpcba/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ogbg_molpcba/train_test.py` & `flax-0.7.0/examples/ogbg_molpcba/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/README.md` & `flax-0.7.0/examples/ppo/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Proximal Policy Optimization
 
 Uses the Proximal Policy Optimization algorithm ([Schulman et al., 2017](https://arxiv.org/abs/1707.06347))
 to learn playing Atari games.
 
 ## Requirements
 
-This example depends on the `gym`, `opencv-python` and `atari-py` packages
+This example depends on the `gymnasium[atari,accept-rom-license]`, `opencv-python` packages
 in addition to `jax` and `flax`.
 
 ## Supported setups
 
 The example should run with other configurations and hardware, but was explicitly
 tested on the following:
```

### Comparing `flax-0.6.9/examples/ppo/agent.py` & `flax-0.7.0/examples/ppo/agent.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/configs/default.py` & `flax-0.7.0/examples/ppo/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/env_utils.py` & `flax-0.7.0/examples/ppo/env_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,16 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Utilities for handling the Atari environment."""
 
 import collections
 
-import atari_py  # build-cleaner: keep
-import gym
+import gymnasium as gym
 import numpy as np
 
 import seed_rl_atari_preprocessing
 
 class ClipRewardEnv(gym.RewardWrapper):
   """Adapted from OpenAI baselines.
```

### Comparing `flax-0.6.9/examples/ppo/models.py` & `flax-0.7.0/examples/ppo/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/ppo_lib.py` & `flax-0.7.0/examples/ppo/ppo_lib.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/ppo_lib_test.py` & `flax-0.7.0/examples/ppo/ppo_lib_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/ppo_main.py` & `flax-0.7.0/examples/ppo/ppo_main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/ppo/seed_rl_atari_preprocessing.py` & `flax-0.7.0/examples/ppo/seed_rl_atari_preprocessing.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,15 +27,16 @@
 # limitations under the License.
 
 """A class implementing minimal Atari 2600 preprocessing.
 Adapted from SEED RL, originally adapted from Dopamine.
 """
 
 import cv2
-from gym.spaces.box import Box
+from gymnasium.spaces.box import Box
+import gymnasium as gym
 import numpy as np
 
 
 class AtariPreprocessing:
   """A class implementing image preprocessing for Atari 2600 agents.
   Specifically, this provides the following subset from the JAIR paper
   (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):
@@ -46,15 +47,15 @@
   More generally, this class follows the preprocessing guidelines set down in
   Machado et al. (2018), "Revisiting the Arcade Learning Environment:
   Evaluation Protocols and Open Problems for General Agents".
   It also provides random starting no-ops, which are used in the Rainbow, Apex
   and R2D2 papers.
   """
 
-  def __init__(self, environment, frame_skip=4, terminal_on_life_loss=False,
+  def __init__(self, environment: gym.Env, frame_skip=4, terminal_on_life_loss=False,
                screen_size=84, max_random_noops=0):
     """Constructor for an Atari 2600 preprocessor.
     Args:
       environment: Gym environment whose observations are preprocessed.
       frame_skip: int, the frequency at which the agent experiences the game.
       terminal_on_life_loss: bool, If True, the step() method returns
         is_terminal=True whenever a life is lost. See Mnih et al. 2015.
@@ -115,15 +116,15 @@
     """Steps self.environment with random no-ops."""
     if self.max_random_noops <= 0:
       return
     # Other no-ops implementations actually always do at least 1 no-op. We
     # follow them.
     no_ops = self.environment.np_random.randint(1, self.max_random_noops + 1)
     for _ in range(no_ops):
-      _, _, game_over, _ = self.environment.step(0)
+      _, _, game_over, _, _ = self.environment.step(0)
       if game_over:
         self.environment.reset()
 
   def reset(self):
     """Resets the environment.
     Returns:
       observation: numpy array, the initial observation emitted by the
@@ -169,15 +170,15 @@
       info: Gym API's info data structure.
     """
     accumulated_reward = 0.
 
     for time_step in range(self.frame_skip):
       # We bypass the Gym observation altogether and directly fetch the
       # grayscale image from the ALE. This is a little faster.
-      _, reward, game_over, info = self.environment.step(action)
+      _, reward, game_over, _, info = self.environment.step(action)
       accumulated_reward += reward
 
       if self.terminal_on_life_loss:
         new_lives = self.environment.ale.lives()
         is_terminal = game_over or new_lives < self.lives
         self.lives = new_lives
       else:
```

### Comparing `flax-0.6.9/examples/ppo/test_episodes.py` & `flax-0.7.0/examples/ppo/test_episodes.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/seq2seq/README.md` & `flax-0.7.0/examples/seq2seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/seq2seq/input_pipeline.py` & `flax-0.7.0/examples/seq2seq/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/seq2seq/models.py` & `flax-0.7.0/examples/seq2seq/models.py`

 * *Files 9% similar despite different names*

```diff
@@ -23,43 +23,52 @@
 from flax import linen as nn
 import jax
 import jax.numpy as jnp
 import numpy as np
 
 Array = jax.Array
 PRNGKey = jax.random.KeyArray
+LSTMCarry = Tuple[Array, Array]
 
 
 class DecoderLSTMCell(nn.RNNCellBase):
   """DecoderLSTM Module wrapped in a lifted scan transform.
+
   Attributes:
     teacher_force: See docstring on Seq2seq module.
     vocab_size: Size of the vocabulary.
   """
+  features: int
   teacher_force: bool
   vocab_size: int
 
   @nn.compact
-  def __call__(self, carry: Tuple[Array, Array], x: Array) -> Array:
+  def __call__(
+      self, carry: Tuple[LSTMCarry, Array], x: Array
+  ) -> Tuple[Tuple[LSTMCarry, Array], Tuple[Array, Array]]:
     """Applies the DecoderLSTM model."""
     lstm_state, last_prediction = carry
     if not self.teacher_force:
       x = last_prediction
-    lstm_state, y = nn.LSTMCell()(lstm_state, x)
+    lstm_state, y = nn.LSTMCell(self.features)(lstm_state, x)
     logits = nn.Dense(features=self.vocab_size)(y)
     # Sample the predicted token using a categorical distribution over the
     # logits.
     categorical_rng = self.make_rng('lstm')
     predicted_token = jax.random.categorical(categorical_rng, logits)
     # Convert to one-hot encoding.
     prediction = jax.nn.one_hot(
         predicted_token, self.vocab_size, dtype=jnp.float32)
 
     return (lstm_state, prediction), (logits, prediction)
 
+  @property
+  def num_feature_axes(self) -> int:
+    return 1
+
 
 class Seq2seq(nn.Module):
   """Sequence-to-sequence class using encoder/decoder architecture.
 
   Attributes:
     teacher_force: whether to use `decoder_inputs` as input to the decoder at
       every step. If False, only the first input (i.e., the "=" token) is used,
@@ -92,29 +101,27 @@
 
     Returns:
       Pair (logits, predictions), which are two arrays of length `batch_size`
       containing respectively decoded logits and predictions (in one hot
       encoding format).
     """
     # Encode inputs.
-    encoder = nn.RNN(nn.LSTMCell(), self.hidden_size, return_carry=True, name='encoder')
-    decoder = nn.RNN(DecoderLSTMCell(self.teacher_force, self.vocab_size), decoder_inputs.shape[-1],
+    encoder = nn.RNN(nn.LSTMCell(self.hidden_size), return_carry=True, name='encoder')
+    decoder = nn.RNN(DecoderLSTMCell(decoder_inputs.shape[-1], self.teacher_force, self.vocab_size),
       split_rngs={'params': False, 'lstm': True}, name='decoder')
 
-    segmentation_mask = self.get_segmentation_mask(encoder_inputs)
+    seq_lengths = self.get_seq_lengths(encoder_inputs)
 
-    encoder_state, _ = encoder(encoder_inputs, segmentation_mask=segmentation_mask)
+    encoder_state, _ = encoder(encoder_inputs, seq_lengths=seq_lengths)
     logits, predictions = decoder(decoder_inputs[:, :-1], initial_carry=(encoder_state, decoder_inputs[:, 0]))
 
     return logits, predictions
 
-  def get_segmentation_mask(self, inputs: Array) -> Array:
+  def get_seq_lengths(self, inputs: Array) -> Array:
     """Get segmentation mask for inputs."""
     # undo one-hot encoding
     inputs = jnp.argmax(inputs, axis=-1)
-    # calculate eos index
-    eos_idx = jnp.argmax(inputs == self.eos_id, axis=-1, keepdims=True)
-    # create index array
-    indexes = jnp.arange(inputs.shape[1])
-    indexes = jnp.broadcast_to(indexes, inputs.shape[:2])
-    # return mask
-    return indexes < eos_idx
+    # calculate sequence lengths
+    seq_lengths = jnp.argmax(inputs == self.eos_id, axis=-1)
+
+    return seq_lengths
+
```

### Comparing `flax-0.6.9/examples/seq2seq/seq2seq.ipynb` & `flax-0.7.0/examples/seq2seq/seq2seq.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9998471467391304%*

 * *Differences: {"'cells'": "{5: {'source': {insert: [(7, '#@markdown be restarted and any changes are "*

 * *            "lost**.\\n')], delete: [7]}}, 19: {'source': {insert: [(3, '  #@markdown Note that "*

 * *            "everybody with the link will be able to see the data.\\n')], delete: [3]}}}"}*

```diff
@@ -182,15 +182,15 @@
                 "# (If you run this code in Jupyter[lab], then you're already in the\n",
                 "#  example directory and nothing needs to be done.)\n",
                 "\n",
                 "#@markdown **Fetch newest Flax, copy example code**\n",
                 "#@markdown\n",
                 "#@markdown **If you select no** below, then the files will be stored on the\n",
                 "#@markdown *ephemeral* Colab VM. **After some time of inactivity, this VM will\n",
-                "#@markdown be restarted an any changes are lost**.\n",
+                "#@markdown be restarted and any changes are lost**.\n",
                 "#@markdown\n",
                 "#@markdown **If you select yes** below, then you will be asked for your\n",
                 "#@markdown credentials to mount your personal Google Drive. In this case, all\n",
                 "#@markdown changes you make will be *persisted*, and even if you re-run the\n",
                 "#@markdown Colab later on, the files will still be the same (you can of course\n",
                 "#@markdown remove directories inside your Drive's `flax/` root if you want to\n",
                 "#@markdown manually revert these files).\n",
@@ -643,15 +643,15 @@
                     ]
                 }
             ],
             "source": [
                 "if 'google.colab' in str(get_ipython()):\n",
                 "  #@markdown You can upload the training results directly to https://tensorboard.dev\n",
                 "  #@markdown\n",
-                "  #@markdown Note that everbody with the link will be able to see the data.\n",
+                "  #@markdown Note that everybody with the link will be able to see the data.\n",
                 "  upload_data = 'yes' #@param ['yes', 'no']\n",
                 "  if upload_data == 'yes':\n",
                 "    !tensorboard dev upload --one_shot --logdir ./workdirs --name 'Flax examples/seq2seq (Colab)'"
             ]
         },
         {
             "cell_type": "markdown",
```

### Comparing `flax-0.6.9/examples/seq2seq/train.py` & `flax-0.7.0/examples/seq2seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/seq2seq/train_test.py` & `flax-0.7.0/examples/seq2seq/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/README.md` & `flax-0.7.0/examples/sst2/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/build_vocabulary.py` & `flax-0.7.0/examples/sst2/build_vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/configs/default.py` & `flax-0.7.0/examples/sst2/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/input_pipeline.py` & `flax-0.7.0/examples/sst2/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/input_pipeline_test.py` & `flax-0.7.0/examples/sst2/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/main.py` & `flax-0.7.0/examples/sst2/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/models.py` & `flax-0.7.0/examples/sst2/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -157,49 +157,47 @@
       embedded_inputs = jax.lax.stop_gradient(embedded_inputs)
 
     return self.dropout_layer(embedded_inputs, deterministic=deterministic)
 
 
 class SimpleLSTM(nn.Module):
   """A simple unidirectional LSTM."""
+  hidden_size: int
 
   @functools.partial(
       nn.transforms.scan,
       variable_broadcast='params',
       in_axes=1, out_axes=1,
       split_rngs={'params': False})
   @nn.compact
   def __call__(self, carry, x):
-    return nn.OptimizedLSTMCell()(carry, x)
+    return nn.OptimizedLSTMCell(self.hidden_size)(carry, x)
 
-  @staticmethod
-  def initialize_carry(batch_dims, hidden_size):
+  def initialize_carry(self, input_shape):
     # Use fixed random key since default state init fn is just zeros.
-    return nn.OptimizedLSTMCell.initialize_carry(
-        jax.random.PRNGKey(0), batch_dims, hidden_size)
+    return nn.OptimizedLSTMCell(self.hidden_size, parent=None).initialize_carry(
+        jax.random.PRNGKey(0), input_shape)
 
 
 class SimpleBiLSTM(nn.Module):
   """A simple bi-directional LSTM."""
   hidden_size: int
 
   def setup(self):
-    self.forward_lstm = SimpleLSTM()
-    self.backward_lstm = SimpleLSTM()
+    self.forward_lstm = SimpleLSTM(self.hidden_size)
+    self.backward_lstm = SimpleLSTM(self.hidden_size)
 
   def __call__(self, embedded_inputs, lengths):
-    batch_size = embedded_inputs.shape[0]
-
     # Forward LSTM.
-    initial_state = SimpleLSTM.initialize_carry((batch_size,), self.hidden_size)
+    initial_state = self.forward_lstm.initialize_carry(embedded_inputs[:, 0].shape)
     _, forward_outputs = self.forward_lstm(initial_state, embedded_inputs)
 
     # Backward LSTM.
     reversed_inputs = flip_sequences(embedded_inputs, lengths)
-    initial_state = SimpleLSTM.initialize_carry((batch_size,), self.hidden_size)
+    initial_state = self.backward_lstm.initialize_carry(reversed_inputs[:, 0].shape)
     _, backward_outputs = self.backward_lstm(initial_state, reversed_inputs)
     backward_outputs = flip_sequences(backward_outputs, lengths)
 
     # Concatenate the forward and backward representations.
     outputs = jnp.concatenate([forward_outputs, backward_outputs], -1)
     return outputs
 
@@ -262,20 +260,20 @@
   Attributes:
     hidden_size: The hidden size of the MLP that computes the attention score.
   """
   hidden_size: int
 
   @nn.compact
   def __call__(self, keys: Array, mask: Array) -> Array:
-    """Applies model  to the input keys and mask.
+    """Applies model to the input keys and mask.
 
     Args:
       keys: The inputs for which to compute an attention score. Shape:
         <float32>[batch_size, seq_length, embeddings_size].
-      mask: A mask that determinines which values in `keys` are valid. Only
+      mask: A mask that determines which values in `keys` are valid. Only
         values for which the mask is True will get non-zero attention scores.
         <bool>[batch_size, seq_length].
 
     Returns:
       The normalized attention scores. <float32>[batch_size, seq_length].
     """
     hidden = nn.Dense(self.hidden_size, name='keys', use_bias=False)(keys)
@@ -393,8 +391,8 @@
   def __call__(self, token_ids: Array, lengths: Array,
                deterministic: Optional[bool] = None) -> Array:
     """Embeds the token IDs, encodes them, and classifies with attention."""
     embedded_inputs = self.embed_token_ids(
         token_ids, deterministic=deterministic)
     logits = self.logits_from_embedded_inputs(
         embedded_inputs, lengths, deterministic=deterministic)
-    return logits
+    return logits
```

### Comparing `flax-0.6.9/examples/sst2/models_test.py` & `flax-0.7.0/examples/sst2/models_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,19 +43,19 @@
 
   def test_lstm_returns_correct_output_shape(self):
     """Tests if the simple LSTM returns the correct shape."""
     batch_size = 2
     seq_len = 3
     embedding_size = 4
     hidden_size = 5
-    model = models.SimpleLSTM()
+    model = models.SimpleLSTM(5)
     rng = jax.random.PRNGKey(0)
     inputs = np.random.RandomState(0).normal(
         size=[batch_size, seq_len, embedding_size])
-    initial_state = models.SimpleLSTM.initialize_carry((batch_size,), hidden_size)
+    initial_state = model.initialize_carry(inputs[:, 0].shape)
     (_, output), _ = model.init_with_output(rng, initial_state, inputs)
     self.assertEqual((batch_size, seq_len, hidden_size), output.shape)
 
   def test_bilstm_returns_correct_output_shape(self):
     """Tests if the simple BiLSTM returns the correct shape."""
     batch_size = 2
     seq_len = 3
@@ -95,8 +95,8 @@
     lengths = np.array([2, 3], dtype=np.int32)
     output, _ = model.init_with_output(rng, token_ids, lengths)
     batch_size = token_ids.shape[0]
     self.assertEqual((batch_size, output_size), output.shape)
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.6.9/examples/sst2/sst2.ipynb` & `flax-0.7.0/examples/sst2/sst2.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/train.py` & `flax-0.7.0/examples/sst2/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/train_test.py` & `flax-0.7.0/examples/sst2/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/vocab.txt` & `flax-0.7.0/examples/sst2/vocab.txt`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/sst2/vocabulary.py` & `flax-0.7.0/examples/sst2/vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/vae/README.md` & `flax-0.7.0/examples/vae/README.md`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Basic VAE Example
 
 This is an implementation of the paper [Auto-Encoding with Variational Bayes](http://arxiv.org/abs/1312.6114) by D.P.Kingma and M.Welling.
 This code follows [pytorch/examples/vae](https://github.com/pytorch/examples/blob/master/vae/README.md).
 
 ```bash
 pip install -r requirements.txt
-python train.py
+python main.py
 ```
 
 ## Examples
 
 If you run the code by above command, you can get some generated images:
 
 ![generated_mnist](./sample.png)
```

### Comparing `flax-0.6.9/examples/vae/reconstruction.png` & `flax-0.7.0/examples/vae/reconstruction.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/vae/sample.png` & `flax-0.7.0/examples/vae/sample.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/vae/train.py` & `flax-0.7.0/examples/vae/train.py`

 * *Files 23% similar despite different names*

```diff
@@ -8,97 +8,44 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from absl import app
-from absl import flags
+# Copyright 2023 The Flax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Traininga and evaluation logic."""
+
+from absl import logging
+
 from flax import linen as nn
 from flax.training import train_state
-import jax.numpy as jnp
 import jax
 from jax import random
-import numpy as np
+import jax.numpy as jnp
 import optax
-import tensorflow as tf
 import tensorflow_datasets as tfds
 
+import input_pipeline
+import models
 import utils as vae_utils
 
 
-FLAGS = flags.FLAGS
-
-flags.DEFINE_float(
-    'learning_rate', default=1e-3,
-    help=('The learning rate for the Adam optimizer.')
-)
-
-flags.DEFINE_integer(
-    'batch_size', default=128,
-    help=('Batch size for training.')
-)
-
-flags.DEFINE_integer(
-    'num_epochs', default=30,
-    help=('Number of training epochs.')
-)
-
-flags.DEFINE_integer(
-    'latents', default=20,
-    help=('Number of latent variables.')
-)
-
-
-class Encoder(nn.Module):
-  latents: int
-
-  @nn.compact
-  def __call__(self, x):
-    x = nn.Dense(500, name='fc1')(x)
-    x = nn.relu(x)
-    mean_x = nn.Dense(self.latents, name='fc2_mean')(x)
-    logvar_x = nn.Dense(self.latents, name='fc2_logvar')(x)
-    return mean_x, logvar_x
-
-
-class Decoder(nn.Module):
-
-  @nn.compact
-  def __call__(self, z):
-    z = nn.Dense(500, name='fc1')(z)
-    z = nn.relu(z)
-    z = nn.Dense(784, name='fc2')(z)
-    return z
-
-
-class VAE(nn.Module):
-  latents: int = 20
-
-  def setup(self):
-    self.encoder = Encoder(self.latents)
-    self.decoder = Decoder()
-
-  def __call__(self, x, z_rng):
-    mean, logvar = self.encoder(x)
-    z = reparameterize(z_rng, mean, logvar)
-    recon_x = self.decoder(z)
-    return recon_x, mean, logvar
-
-  def generate(self, z):
-    return nn.sigmoid(self.decoder(z))
-
-
-def reparameterize(rng, mean, logvar):
-  std = jnp.exp(0.5 * logvar)
-  eps = random.normal(rng, logvar.shape)
-  return mean + eps * std
-
-
 @jax.vmap
 def kl_divergence(mean, logvar):
   return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))
 
 
 @jax.vmap
 def binary_cross_entropy_with_logits(logits, labels):
@@ -112,100 +59,77 @@
   return {
       'bce': bce_loss,
       'kld': kld_loss,
       'loss': bce_loss + kld_loss
   }
 
 
-def model():
-  return VAE(latents=FLAGS.latents)
-
-
-@jax.jit
-def train_step(state, batch, z_rng):
+def train_step(state, batch, z_rng, latents):
   def loss_fn(params):
-    recon_x, mean, logvar = model().apply({'params': params}, batch, z_rng)
+    recon_x, mean, logvar = models.model(latents).apply({'params': params},
+                                                      batch, z_rng)
 
     bce_loss = binary_cross_entropy_with_logits(recon_x, batch).mean()
     kld_loss = kl_divergence(mean, logvar).mean()
     loss = bce_loss + kld_loss
     return loss
   grads = jax.grad(loss_fn)(state.params)
   return state.apply_gradients(grads=grads)
 
 
-@jax.jit
-def eval(params, images, z, z_rng):
+def eval_f(params, images, z, z_rng, latents):
   def eval_model(vae):
     recon_images, mean, logvar = vae(images, z_rng)
     comparison = jnp.concatenate([images[:8].reshape(-1, 28, 28, 1),
                                   recon_images[:8].reshape(-1, 28, 28, 1)])
 
     generate_images = vae.generate(z)
     generate_images = generate_images.reshape(-1, 28, 28, 1)
     metrics = compute_metrics(recon_images, images, mean, logvar)
     return metrics, comparison, generate_images
 
-  return nn.apply(eval_model, model())({'params': params})
-
+  return nn.apply(eval_model, models.model(latents))({'params': params})
 
-def prepare_image(x):
-  x = tf.cast(x['image'], tf.float32)
-  x = tf.reshape(x, (-1,))
-  return x
-
-
-def main(argv):
-  del argv
-
-  # Make sure tf does not allocate gpu memory.
-  tf.config.experimental.set_visible_devices([], 'GPU')
 
+def train_and_evaluate(batch_size, learning_rate, num_epochs, latents):
+  """Train and evaulate pipeline."""
   rng = random.PRNGKey(0)
   rng, key = random.split(rng)
 
   ds_builder = tfds.builder('binarized_mnist')
   ds_builder.download_and_prepare()
-  train_ds = ds_builder.as_dataset(split=tfds.Split.TRAIN)
-  train_ds = train_ds.map(prepare_image)
-  train_ds = train_ds.cache()
-  train_ds = train_ds.repeat()
-  train_ds = train_ds.shuffle(50000)
-  train_ds = train_ds.batch(FLAGS.batch_size)
-  train_ds = iter(tfds.as_numpy(train_ds))
-
-  test_ds = ds_builder.as_dataset(split=tfds.Split.TEST)
-  test_ds = test_ds.map(prepare_image).batch(10000)
-  test_ds = np.array(list(test_ds)[0])
-  test_ds = jax.device_put(test_ds)
 
-  init_data = jnp.ones((FLAGS.batch_size, 784), jnp.float32)
+  logging.info('Initializing dataset.')
+  train_ds = input_pipeline.build_train_set(batch_size, ds_builder)
+  test_ds = input_pipeline.build_test_set(ds_builder)
+
+  logging.info('Initializing model.')
+  init_data = jnp.ones((batch_size, 784), jnp.float32)
+  params = models.model(latents).init(key, init_data, rng)['params']
 
   state = train_state.TrainState.create(
-      apply_fn=model().apply,
-      params=model().init(key, init_data, rng)['params'],
-      tx=optax.adam(FLAGS.learning_rate),
+      apply_fn=models.model(latents).apply,
+      params=params,
+      tx=optax.adam(learning_rate),
   )
 
   rng, z_key, eval_rng = random.split(rng, 3)
-  z = random.normal(z_key, (64, FLAGS.latents))
+  z = random.normal(z_key, (64, latents))
 
-  steps_per_epoch = 50000 // FLAGS.batch_size
+  steps_per_epoch = ds_builder.info.splits["train"].num_examples // batch_size
 
-  for epoch in range(FLAGS.num_epochs):
+  for epoch in range(num_epochs):
     for _ in range(steps_per_epoch):
       batch = next(train_ds)
       rng, key = random.split(rng)
-      state = train_step(state, batch, key)
+      state = train_step(state, batch, key, latents)
 
-    metrics, comparison, sample = eval(state.params, test_ds, z, eval_rng)
+    metrics, comparison, sample = eval_f(state.params, test_ds, z, eval_rng,
+                                         latents)
     vae_utils.save_image(
         comparison, f'results/reconstruction_{epoch}.png', nrow=8)
     vae_utils.save_image(sample, f'results/sample_{epoch}.png', nrow=8)
 
     print('eval epoch: {}, loss: {:.4f}, BCE: {:.4f}, KLD: {:.4f}'.format(
         epoch + 1, metrics['loss'], metrics['bce'], metrics['kld']
     ))
 
-
-if __name__ == '__main__':
-  app.run(main)
```

### Comparing `flax-0.6.9/examples/vae/utils.py` & `flax-0.7.0/examples/vae/utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,65 +8,81 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+# Copyright 2023 The Flax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
-    This code is created with reference to torchvision/utils.py.
-
-    Modify: torch.tensor -> jax.numpy.DeviceArray
+This code is created with reference to torchvision/utils.py.
 
-    If you want to know about this file in detail, please visit the original code:
-        https://github.com/pytorch/vision/blob/master/torchvision/utils.py
+Modify: torch.tensor -> jax.numpy.DeviceArray
+If you want to know about this file in detail, please visit the original code:
+    https://github.com/pytorch/vision/blob/master/torchvision/utils.py
 """
 import math
 
 import jax.numpy as jnp
 import numpy as np
 from PIL import Image
 
 
-def save_image(ndarray, fp, nrow=8, padding=2, pad_value=0.0, format=None):
-    """Make a grid of images and Save it into an image file.
+def save_image(ndarray, fp, nrow=8, padding=2, pad_value=0.0, format_img=None):
+  """Make a grid of images and Save it into an image file.
+
   Args:
     ndarray (array_like): 4D mini-batch images of shape (B x H x W x C)
-    fp - A filename(string) or file object
+    fp:  A filename(string) or file object
     nrow (int, optional): Number of images displayed in each row of the grid.
       The final grid size is ``(B / nrow, nrow)``. Default: ``8``.
     padding (int, optional): amount of padding. Default: ``2``.
-    scale_each (bool, optional): If ``True``, scale each image in the batch of
-      images separately rather than the (min, max) over all images. Default: ``False``.
     pad_value (float, optional): Value for the padded pixels. Default: ``0``.
-    format(Optional):  If omitted, the format to use is determined from the filename extension.
-      If a file object was used instead of a filename, this parameter should always be used.
+    format_img(Optional):  If omitted, the format to use is determined from the
+      filename extension. If a file object was used instead of a filename,
+      this parameter should always be used.
   """
-    if not (isinstance(ndarray, jnp.ndarray) or
-        (isinstance(ndarray, list) and all(isinstance(t, jnp.ndarray) for t in ndarray))):
-        raise TypeError(f'array_like of tensors expected, got {type(ndarray)}')
-
-    ndarray = jnp.asarray(ndarray)
-
-    if ndarray.ndim == 4 and ndarray.shape[-1] == 1:  # single-channel images
-        ndarray = jnp.concatenate((ndarray, ndarray, ndarray), -1)
-
-    # make the mini-batch of images into a grid
-    nmaps = ndarray.shape[0]
-    xmaps = min(nrow, nmaps)
-    ymaps = int(math.ceil(float(nmaps) / xmaps))
-    height, width = int(ndarray.shape[1] + padding), int(ndarray.shape[2] + padding)
-    num_channels = ndarray.shape[3]
-    grid = jnp.full((height * ymaps + padding, width * xmaps + padding, num_channels), pad_value).astype(jnp.float32)
-    k = 0
-    for y in range(ymaps):
-      for x in range(xmaps):
-        if k >= nmaps:
-          break
-        grid = grid.at[y * height + padding:(y + 1) * height,
-                       x * width + padding:(x + 1) * width].set(ndarray[k])
-        k = k + 1
-
-    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer
-    ndarr = np.array(jnp.clip(grid * 255.0 + 0.5, 0, 255).astype(jnp.uint8))
-    im = Image.fromarray(ndarr.copy())
-    im.save(fp, format=format)
+
+  if not (isinstance(ndarray, jnp.ndarray) or
+          (isinstance(ndarray, list) and all(isinstance(t, jnp.ndarray) for t
+                                             in ndarray))):
+    raise TypeError(f'array_like of tensors expected, got {type(ndarray)}')
+
+  ndarray = jnp.asarray(ndarray)
+
+  if ndarray.ndim == 4 and ndarray.shape[-1] == 1:  # single-channel images
+    ndarray = jnp.concatenate((ndarray, ndarray, ndarray), -1)
+
+  # make the mini-batch of images into a grid
+  nmaps = ndarray.shape[0]
+  xmaps = min(nrow, nmaps)
+  ymaps = int(math.ceil(float(nmaps) / xmaps))
+  height, width = (int(ndarray.shape[1] + padding),
+                   int(ndarray.shape[2] + padding))
+  num_channels = ndarray.shape[3]
+  grid = jnp.full((height * ymaps + padding, width * xmaps + padding,
+                   num_channels), pad_value).astype(jnp.float32)
+  k = 0
+  for y in range(ymaps):
+    for x in range(xmaps):
+      if k >= nmaps:
+        break
+      grid = grid.at[y * height + padding:(y + 1) * height,
+                     x * width + padding:(x + 1) * width].set(ndarray[k])
+      k = k + 1
+
+  # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer
+  ndarr = np.array(jnp.clip(grid * 255.0 + 0.5, 0, 255).astype(jnp.uint8))
+  im = Image.fromarray(ndarr.copy())
+  im.save(fp, format=format_img)
```

### Comparing `flax-0.6.9/examples/wmt/README.md` & `flax-0.7.0/examples/wmt/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -117,15 +117,15 @@
 usually a good idea to connect to a single host and execute the commands
 interactively.
 
 For convenience, the TPU creation commands are inlined below. Please note that
 we define `GCS_TFDS_BUCKET` to where your data stands in your cloud bucket.
 Also `YOUR_BUCKET` is the work directory you are experimenting in. You should
 choose ZONE based on where your TPU and work directory is. [Here](https://cloud.google.com/tpu/docs/types-zones)
-has some usefule information on which zones you can have different types of TPUs.
+has some useful information on which zones you can have different types of TPUs.
 
 ```shell
 VM_NAME=wmt
 REPO=https://github.com/google/flax
 BRANCH=main
 WORKDIR=gs://$YOUR_BUCKET/flax/examples/wmt/$(date +%Y%m%d_%H%M)
```

### Comparing `flax-0.6.9/examples/wmt/bleu.py` & `flax-0.7.0/examples/wmt/bleu.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/configs/default.py` & `flax-0.7.0/examples/wmt/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/decode.py` & `flax-0.7.0/examples/wmt/decode.py`

 * *Files 0% similar despite different names*

```diff
@@ -68,15 +68,15 @@
   if x.ndim == 0:  # ignore scalars (e.g. cache index)
     return x
   assert batch_size * beam_size == x.shape[0]
   return x.reshape((batch_size, beam_size) + x.shape[1:])
 
 
 def flat_batch_beam_expand(x, beam_size):
-  """Expands the each batch item by beam_size in batch_dimension."""
+  """Expands each batch item by beam_size in batch_dimension."""
   return flatten_beam_dim(add_beam_dim(x, beam_size))
 
 
 def gather_beams(nested, beam_indices, batch_size, new_beam_size):
   """Gathers the beam slices indexed by beam_indices into new beam array.
 
   Args:
```

### Comparing `flax-0.6.9/examples/wmt/input_pipeline.py` & `flax-0.7.0/examples/wmt/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/input_pipeline_test.py` & `flax-0.7.0/examples/wmt/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/main.py` & `flax-0.7.0/examples/wmt/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/models.py` & `flax-0.7.0/examples/wmt/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/tokenizer.py` & `flax-0.7.0/examples/wmt/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/train.py` & `flax-0.7.0/examples/wmt/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/examples/wmt/train_test.py` & `flax-0.7.0/examples/wmt/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/__init__.py` & `flax-0.7.0/flax/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/configurations.py` & `flax-0.7.0/flax/configurations.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,27 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Global configuration options for Flax.
+r"""Global configuration options for Flax.
 
-Now a wrapper over jax.config, in which all config vars have a 'flax_' prefix.
+Now a wrapper over jax.config, in which all config vars have a 'flax\_' prefix.
 
 To modify a config value on run time, call:
-`flax.config.update('flax_<config_name>', <value>)`
+``flax.config.update('flax_<config_name>', <value>)``
 
 """
 
 import os
 from jax import config as jax_config
 
-from contextlib import ContextDecorator
+from contextlib import contextmanager
 
 # Keep a wrapper at the flax namespace, in case we make our implementation
 # in the future.
 config = jax_config
 
 # Config parsing utils
 
@@ -64,29 +64,28 @@
   elif val in ('n', 'no', 'f', 'false', 'off', '0'):
     return False
   else:
     raise ValueError(
         'invalid truth value {!r} for environment {!r}'.format(val, varname))
 
 
-class use_regular_dict(ContextDecorator):
-  """Context decorator for test functions to temporarily use regular dicts
-  instead of FrozenDicts.
+@contextmanager
+def temp_flip_flag(var_name: str, var_value: bool):
+  """Context manager to temporarily flip feature flags for test functions.
 
-  This is a temporary feature flag to help migrate to FrozenDicts. Returning
-  FrozenDicts will be deprecated and removed in the future.
+  Args:
+    var_name: the config variable name (without the 'flax_' prefix)
+    var_value: the boolean value to set var_name to temporarily
   """
-  def __enter__(self):
-    self._old_value = config.flax_return_frozendict # save current env value
-    config.update('flax_return_frozendict', False) # return regular dicts
-    return self
-
-  def __exit__(self, *exc):
-    config.update('flax_return_frozendict', self._old_value) # switch back to old env value
-    return False
+  old_value = getattr(config, f'flax_{var_name}')
+  try:
+    config.update(f'flax_{var_name}', var_value)
+    yield
+  finally:
+    config.update(f'flax_{var_name}', old_value)
 
 
 # Flax Global Configuration Variables:
 
 # Whether to use the lazy rng implementation.
 flax_lazy_rng = static_bool_env('FLAX_LAZY_RNG', True)
 
@@ -98,25 +97,26 @@
 flax_profile = define_bool_state(
     name='profile',
     default=True,
     help=('Whether to run Module methods under jax.named_scope for profiles.'))
 
 flax_use_orbax_checkpointing = define_bool_state(
     name='use_orbax_checkpointing',
-    default=False,
-    help=('Whether to use Orbax to save checkpoints.'))
-
-flax_relaxed_naming = define_bool_state(
-    name='relaxed_naming',
     default=True,
-    help=('Whether to relax naming constraints.'))
+    help=('Whether to use Orbax to save checkpoints.'))
 
 flax_preserve_adopted_names = define_bool_state(
     name='preserve_adopted_names',
     default=False,
     help=("When adopting outside modules, don't clobber existing names."))
 
 #TODO(marcuschiam): remove this feature flag once regular dict migration is complete
 flax_return_frozendict = define_bool_state(
     name='return_frozendict',
     default=True,
     help=('Whether to return FrozenDicts when calling init or apply.'))
+
+flax_fix_rng = define_bool_state(
+    name ='fix_rng_separator',
+    default=False,
+    help=('Whether to add separator characters when folding in static data into PRNG keys.')
+)
```

### Comparing `flax-0.6.9/flax/core/__init__.py` & `flax-0.7.0/flax/core/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/axes_scan.py` & `flax-0.7.0/flax/core/axes_scan.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/flax_functional_engine.ipynb` & `flax-0.7.0/flax/core/flax_functional_engine.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/frozen_dict.py` & `flax-0.7.0/flax/core/frozen_dict.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/lift.py` & `flax-0.7.0/flax/core/lift.py`

 * *Files 0% similar despite different names*

```diff
@@ -719,15 +719,15 @@
       If split is False the PRNGs will be the same across iterations.
     in_axes: Specifies the axis to scan over for the arguments. Should be a
       prefix tree of the arguments. Use `flax.core.broadcast` to feed an entire
       input to each iteration of the scan body.
     out_axes: Specifies the axis to scan over for the return value. Should be a
       prefix tree of the return value.
     length: Specifies the number of loop iterations. This only needs
-      to be specified if it cannot be derivied from the scan arguments.
+      to be specified if it cannot be derived from the scan arguments.
     reverse: If true, scan from end to start in reverse order.
     unroll: how many scan iterations to unroll within a single
       iteration of a loop (default: 1).
     data_transform: optional function to transform raw variable and rng groups,
       intended for inline SPMD annotations.
     metadata_params: arguments dict passed to AxisMetadata instances in the
       variable tree.
@@ -987,15 +987,15 @@
   The returned values from ``branches``
   must have the same Pytree structure, shapes, and dtypes.
   The variables created or updated inside the
   branches must also have the same structure.
   Note that this constraint is violated when
   creating variables or submodules in only one branch.
   Because initializing variables in just one branch
-  causes the paramater structure to be different.
+  causes the parameter structure to be different.
 
   Example::
 
     def switch_example(scope, x, index):
       scope.variable('state', 'a_count', lambda: 0)
       scope.variable('state', 'b_count', lambda: 0)
       scope.variable('state', 'c_count', lambda: 0)
@@ -1007,15 +1007,15 @@
         return -scope.child(nn.dense)(x, 2)
       def c_fn(scope, x):
         scope.variable('state', 'c_count').value += 1
         return scope.child(nn.dense)(x, 2)
       return lift.switch(index, [a_fn, b_fn, c_fn], scope, x)
 
   If you want to have a different parameter structure for each branch
-  you should run all branche on initialization before calling switch::
+  you should run all branch on initialization before calling switch::
 
     def multihead_switch_example(scope, x, index):
       def a_fn(scope, x):
         x = scope.child(nn.dense)(x, 10)
         x = scope.child(nn.dense)(x, 7)
         return scope.child(nn.dense)(x, 5)
       def b_fn(scope, x):
@@ -1103,15 +1103,15 @@
     dense_sign_grad = lift.custom_vjp(
         f, forward_fn=fwd, backward_fn=bwd, nondiff_argnums=(2,))
 
   Args:
     fn: The function to define a custom_vjp for. The first argument
       should be a ``Module`` instance.
     forward_fn: A function with the same arguments as `fn` returning an tuple
-      with the original output and the residuals that will be passsed to
+      with the original output and the residuals that will be passed to
       `backward_fn`.
     backward_fn: arguments are passed as (*nondiff_args, residuals, tangents)
       The function should return a tuple containing the tangents for the
       variable in the collections specified by `grad_vars` and the input
       arguments (except the scope and nondiff args).
     grad_vars: The collections for which a vjp will be computed
       (default: "params").
```

### Comparing `flax-0.6.9/flax/core/meta.py` & `flax-0.7.0/flax/core/meta.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/nn/__init__.py` & `flax-0.7.0/flax/core/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/nn/attention.py` & `flax-0.7.0/flax/core/nn/attention.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/nn/linear.py` & `flax-0.7.0/flax/core/nn/linear.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/nn/normalization.py` & `flax-0.7.0/flax/core/nn/normalization.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/nn/stochastic.py` & `flax-0.7.0/flax/core/nn/stochastic.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/partial_eval.py` & `flax-0.7.0/flax/core/partial_eval.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/scope.py` & `flax-0.7.0/flax/core/scope.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Flax functional core: Scopes."""
 
+import collections
 import contextlib
 import dataclasses
 import functools
 import hashlib
 import typing
 from typing import (Any, Callable, Dict, Generic, Iterable, Mapping, Optional,
                     Sequence, Set, Tuple, TypeVar, Union)
@@ -138,14 +139,17 @@
   Returns:
    The newly generated PRNG key.
   """
   if not data:
     return rng
   m = hashlib.sha1()
   for x in data:
+    if config.flax_fix_rng_separator:
+      # encode seperate to avoid collisions like for example: ("ab", "c") and ("a", "bc")
+      m.update(b'\00')
     if isinstance(x, str):
       m.update(x.encode('utf-8'))
     elif isinstance(x, int):
       m.update(x.to_bytes((x.bit_length() + 7) // 8, byteorder='big'))
     else:
       raise ValueError(f'Expected int or string, got: {x}')
   d = m.digest()
@@ -401,15 +405,15 @@
   :class:`flax.linen.module.Module`, so users writing neural network code
   usually generally do not interact with ``Scopes`` directly.
 
   See `core design tests
   <https://github.com/google/flax/tree/main/tests/core/design>`_
   for a number of examples using ``Scopes``.
   """
-  reservations: Dict[str, Optional[str]]
+  reservations: Dict[str, Set[Optional[str]]]
 
   def __init__(self,
                variables: MutableVariableDict,
                rngs: Optional[Dict[str, Union[PRNGKey, LazyRng]]] = None,
                name: Optional[str] = None,
                mutable: CollectionFilter = False,
                parent: Optional['Scope'] = None,
@@ -435,15 +439,15 @@
     self.mutable = mutable
     self.flags = freeze({} if flags is None else flags)
 
     self._root = parent.root if parent else None
     self.trace_level = tracers.trace_level(tracers.current_trace())
 
     self.rng_counters = {key: 0 for key in self.rngs}
-    self.reservations = dict()
+    self.reservations = collections.defaultdict(set)
 
     self._invalid = False
 
   def __eq__(self, other: Any) -> bool:
     # If the root variable dict and path are the same, then two scopes behave
     # identically. Effectively, a scope is nothing more than a cursor into a
     # variable dict and an rng counter dict.
@@ -527,22 +531,19 @@
     """Checks whether a name for a child Scope or Variable is taken.
 
     Args:
       name: the name to check for collision.
       col: if a variable, the collection used.
     """
     if name in self.reservations:
-      # with relaxed naming, allow the same name for two variables in
+      # allow the same name for two variables in
       # different collections, otherwise raise error.
-      if config.flax_relaxed_naming:
-        if (self.reservations[name] is None or col is None
-            or self.reservations[name] == col):
-            return True
-      else:
-        return True
+      if (None in self.reservations[name] or col is None
+          or col in self.reservations[name]):
+          return True
     return False
 
   def reserve(self, name: str, col: Optional[str] = None):
     """Reserves a name for a child Scope or Variable.
 
     Throws an error if the name exists already.
 
@@ -551,15 +552,15 @@
       col: if a variable, the collection used.
     """
     if not isinstance(name, str):
       raise TypeError('The type of scope "{name}" should be string but '
                       f'it is {type(name)}')
     if self.name_reserved(name, col):
       raise ValueError(f'Duplicate use of scope name: "{name}"')
-    self.reservations[name] = col
+    self.reservations[name].add(col)
 
   def default_name(self, prefix: str) -> str:
     """Generates an unreserved name with the given prefix.
 
     Args:
       prefix: prefix to use for generating an unreserved name.
 
@@ -872,27 +873,27 @@
 def bind(variables: VariableDict,
          rngs: Optional[RNGSequences] = None,
          mutable: CollectionFilter = False,
          flags: Optional[Mapping] = None):
   """Binds variables and rngs to a new ``Scope``.
 
   bind provides a ``Scope`` instance without transforming a function with
-  ``apply``. This is particalary useful for debugging and interactive use cases
+  ``apply``. This is particularly useful for debugging and interactive use cases
   like notebooks where a function would limit the ability split up code into
   different cells.
 
   a ``Scope`` instance is a stateful object. Note that idiomatic JAX is
   functional and therefore a ``Scope` does not mix well well with vanilla JAX
   APIs. Therefore, we recommend using ``apply`` when code should be reusable and
   compatible across the JAX software ecosystem.
 
   Args:
     variables: Variable dictionary to bind.
     rngs: RNGs to bind.
-    mutable: Which variable colections to treat as mutable.
+    mutable: Which variable collections to treat as mutable.
     flags: internal flags.
 
   Returns:
     A new scope with the variables and rngs bound to it.
   """
   if not _is_valid_variables(variables):
     raise errors.ApplyScopeInvalidVariablesTypeError()
```

### Comparing `flax-0.6.9/flax/core/tracers.py` & `flax-0.7.0/flax/core/tracers.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/core/variables.py` & `flax-0.7.0/flax/core/variables.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/errors.py` & `flax-0.7.0/flax/errors.py`

 * *Files 0% similar despite different names*

```diff
@@ -82,15 +82,15 @@
         k = self.param("kernel", lambda _: x)
         return x * k
     Foo().lazy_init(random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32))
   """
 
   def __init__(self, partial_val):
     super().__init__(
-        f'Lazy init encoutered a value that could with '
+        f'Lazy init encountered a value that could with '
         f'the given inputs (shape: {partial_val}).')
 
 
 #################################################
 # scope.py errors                               #
 #################################################
```

### Comparing `flax-0.6.9/flax/ids.py` & `flax-0.7.0/flax/ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/io.py` & `flax-0.7.0/flax/io.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/jax_utils.py` & `flax-0.7.0/flax/jax_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -122,15 +122,15 @@
       prefetch step on CPU.
 
     devices: the list of devices to which the arrays should be prefetched.
 
       Defaults to the order of devices expected by `jax.pmap`.
 
   Yields:
-    The original items from the iterator where each ndarray is now a sharded to
+    The original items from the iterator where each ndarray is now sharded to
     the specified devices.
   """
   queue = collections.deque()
   devices = devices or _pmap_device_order()
 
   def _prefetch(xs):
     return jax.device_put_sharded(list(xs), devices)
@@ -185,15 +185,15 @@
   Args:
     body_fn: the body of the loop of type (c, x) -> (c, y).
     init: initial value for the carry.
     xs: a pytree of tensors to scan over.
     axis: the axis to scan over.
     keepdims: keep the dimensions that are scanned over.
     unroll: an optional positive integer, or tuple of positive integers
-      showing how many iterations of the loop to be unroll into a single
+      showing how many iterations of the loop to be unrolled into a single
       iteration for each axis.
   Returns:
     A tuple of the final carry and the values returned by the body.
   """
   if not isinstance(axis, Iterable):
     axis = (axis,)
```

### Comparing `flax-0.6.9/flax/linen/README.md` & `flax-0.7.0/flax/linen/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 
 In Linen, Modules behave much closer to vanilla Python objects, while still letting you opt-in to the concise single-method pattern many of our users love.
 
 The Linen Module API is stable and currently recommended for new projects. We are already supporting users in the OSS community and within Google. Minor changes may come to the top-level `apply` and `init` patterns, which we will communicate clearly. We plan a few improvements, including writing up short design notes, adding more design tests (see last link below), and an API for interactive module instances.
 
 Please open a [discussion](https://github.com/google/flax/discussions) if you have any questions or thoughts.
 
-**See the [Linen API reference docs](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html)**, or take a look at our additional material:
+**See the [Linen API reference docs](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/index.html)**, or take a look at our additional material:
 
 * 2-page intro to the [Linen Design Principles](https://docs.google.com/document/d/1ZlL_4bXCw5Xl0WstQw1GpnZqfb9JFOeUGAPcBVk-kn8)
 * [Slides from a talk to the JAX core team](https://docs.google.com/presentation/d/1ngKWUwsSqAwPRvATG8sAxMzu9ujv4N__cKsUofdNno0)
 * [Brief Intro to Linen](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/linen_intro.ipynb) in Colab
 * An [upgrade guide](https://docs.google.com/document/d/1hYavTVPaKVVe9Be8pCB7yW7r6dDv3RALVNit8NZca4c) + some additional questions we're considering
 * Ported [examples](https://github.com/google/flax/tree/main/examples)
 * "Design tests" used to ensure that our "functional core" supports [various advanced use-cases](https://github.com/google/flax/tree/main/tests/core/design), and that the mostly-syntactic-sugar Module abstraction
```

### Comparing `flax-0.6.9/flax/linen/__init__.py` & `flax-0.7.0/flax/linen/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/activation.py` & `flax-0.7.0/flax/linen/activation.py`

 * *Files 10% similar despite different names*

```diff
@@ -55,14 +55,20 @@
 Array = Any
 Dtype = Any
 
 
 class PReLU(Module):
   """Parametric Rectified Linear Unit (PReLU) activation function.
 
+  Note that PReLU is a Flax layer and not a simple activation function, so
+  it needs to be initialized before being called.
+
+  Example usage::
+    x = nn.PReLU()(x)
+
   Attributes:
     param_dtype: the dtype passed to parameter initializers (default: float32).
     negative_slope_init: the value to initialize the negative slope
       (default 0.01).
   """
   param_dtype: Dtype = jnp.float32
   negative_slope_init: float = 0.01
```

### Comparing `flax-0.6.9/flax/linen/attention.py` & `flax-0.7.0/flax/linen/attention.py`

 * *Files 2% similar despite different names*

```diff
@@ -257,15 +257,17 @@
 
     Returns:
       output of shape `[batch_sizes..., length, features]`.
     """
     features = self.out_features or inputs_q.shape[-1]
     qkv_features = self.qkv_features or inputs_q.shape[-1]
     assert qkv_features % self.num_heads == 0, (
-        'Memory dimension must be divisible by number of heads.')
+        f'Memory dimension ({qkv_features}) must be divisible by number of'
+        f' heads ({self.num_heads}).'
+    )
     head_dim = qkv_features // self.num_heads
 
     dense = functools.partial(
         DenseGeneral,
         axis=-1,
         dtype=self.dtype,
         param_dtype=self.param_dtype,
```

### Comparing `flax-0.6.9/flax/linen/combinators.py` & `flax-0.7.0/flax/linen/combinators.py`

 * *Files 24% similar despite different names*

```diff
@@ -23,32 +23,33 @@
   """Applies a linear chain of Modules.
 
   Meant to be used only for the simple case of fusing together callables where
   the input of a particular module/op is the output of the previous one.
 
   Modules will be applied in the order that they are passed in the constructor.
 
-  The apply() method of Sequential accepts any input and forwards it to the
+  The ``__call__`` method of Sequential accepts any input and forwards it to the
   first module it contains. It chains the output sequentially to the input of
   the next module and returns the output of the final module.
 
   Example usage::
 
     class Foo(nn.Module):
-      feature_sizes: Sequence[int]
 
       @nn.compact
       def __call__(self, x):
         return nn.Sequential([nn.Dense(4),
                               nn.relu,
                               nn.Dense(2),
                               nn.log_softmax])(x)
 
   This combinator supports also layers that return multiple outputs if returned
-  as a tuple or a dictionary.
+  as a tuple or a dictionary. If the output of a layer is a ``tuple`` it will be
+  expanded as ``*args`` in the next layer, if its a ``dict`` it
+  will be expanded as ``**kwargs``.
 
   Example usage::
 
     class CrossAttentionBlock(nn.Module):
       num_heads: int = 2
       qkv_features: int = 16
 
@@ -66,14 +67,20 @@
       @nn.compact
       def __call__(self, x):
         return nn.Sequential([CrossAttentionBlock() for _ in
                               range(self.num_layers)])(query, key_value)
   """
   layers: Sequence[Callable[..., Any]]
 
+  def __post_init__(self):
+    if not isinstance(self.layers, Sequence):
+      raise ValueError('\'layers\' must be a sequence, '
+                       f'got \'{type(self.layers).__name__}\'.')
+    super().__post_init__()
+
   def __call__(self, *args, **kwargs):
     if not self.layers:
       raise ValueError(f'Empty Sequential module {self.name}.')
 
     outputs = self.layers[0](*args, **kwargs)
     for layer in self.layers[1:]:
       if isinstance(outputs, tuple):
```

### Comparing `flax-0.6.9/flax/linen/dtypes.py` & `flax-0.7.0/flax/linen/dtypes.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/experimental/layers_with_named_axes.py` & `flax-0.7.0/flax/linen/experimental/layers_with_named_axes.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/initializers.py` & `flax-0.7.0/flax/linen/initializers.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/kw_only_dataclasses.py` & `flax-0.7.0/flax/linen/kw_only_dataclasses.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/linear.py` & `flax-0.7.0/flax/linen/linear.py`

 * *Files 2% similar despite different names*

```diff
@@ -259,16 +259,16 @@
       the kernel size can be passed as an integer. For all other cases, it must
       be a sequence of integers.
     strides: an integer or a sequence of `n` integers, representing the
       inter-window strides (default: 1).
     padding: either the string `'SAME'`, the string `'VALID'`, the string
       `'CIRCULAR'` (periodic boundary conditions), or a sequence of `n` `(low,
       high)` integer pairs that give the padding to apply before and after each
-      spatial dimension. A single int is interpeted as applying the same padding
-      in all dims and passign a single int in a sequence causes the same padding
+      spatial dimension. A single int is interpreted as applying the same padding
+      in all dims and assign a single int in a sequence causes the same padding
       to be used on both sides. `'CAUSAL'` padding for a 1D convolution will
       left-pad the convolution axis, resulting in same-sized output.
     input_dilation: an integer or a sequence of `n` integers, giving the
       dilation factor to apply in each spatial dimension of `inputs`
       (default: 1). Convolution with input dilation `d` is equivalent to
       transposed convolution with stride `d`.
     kernel_dilation: an integer or a sequence of `n` integers, giving the
@@ -489,16 +489,16 @@
       the kernel size can be passed as an integer. For all other cases, it must
       be a sequence of integers.
     strides: an integer or a sequence of `n` integers, representing the
       inter-window strides (default: 1).
     padding: either the string `'SAME'`, the string `'VALID'`, the string
       `'CIRCULAR'` (periodic boundary conditions), or a sequence of `n` `(low,
       high)` integer pairs that give the padding to apply before and after each
-      spatial dimension. A single int is interpeted as applying the same padding
-      in all dims and passign a single int in a sequence causes the same padding
+      spatial dimension. A single int is interpreted as applying the same padding
+      in all dims and assign a single int in a sequence causes the same padding
       to be used on both sides. `'CAUSAL'` padding for a 1D convolution will
       left-pad the convolution axis, resulting in same-sized output.
     input_dilation: an integer or a sequence of `n` integers, giving the
       dilation factor to apply in each spatial dimension of `inputs`
       (default: 1). Convolution with input dilation `d` is equivalent to
       transposed convolution with stride `d`.
     kernel_dilation: an integer or a sequence of `n` integers, giving the
@@ -532,16 +532,16 @@
       the kernel size can be passed as an integer. For all other cases, it must
       be a sequence of integers.
     strides: an integer or a sequence of `n` integers, representing the
       inter-window strides (default: 1).
     padding: either the string `'SAME'`, the string `'VALID'`, the string
       `'CIRCULAR'` (periodic boundary conditions), or a sequence of `n` `(low,
       high)` integer pairs that give the padding to apply before and after each
-      spatial dimension. A single int is interpeted as applying the same padding
-      in all dims and passign a single int in a sequence causes the same padding
+      spatial dimension. A single int is interpreted as applying the same padding
+      in all dims and assign a single int in a sequence causes the same padding
       to be used on both sides. `'CAUSAL'` padding for a 1D convolution will
       left-pad the convolution axis, resulting in same-sized output.
     input_dilation: an integer or a sequence of `n` integers, giving the
       dilation factor to apply in each spatial dimension of `inputs`
       (default: 1). Convolution with input dilation `d` is equivalent to
       transposed convolution with stride `d`.
     kernel_dilation: an integer or a sequence of `n` integers, giving the
@@ -574,16 +574,16 @@
     kernel_size: shape of the convolutional kernel. For 1D convolution,
       the kernel size can be passed as an integer. For all other cases, it must
       be a sequence of integers.
     strides: a sequence of `n` integers, representing the inter-window strides.
     padding: either the string `'SAME'`, the string `'VALID'`, the string
       `'CIRCULAR'` (periodic boundary conditions), or a sequence of `n` `(low,
       high)` integer pairs that give the padding to apply before and after each
-      spatial dimension. A single int is interpeted as applying the same padding
-      in all dims and passign a single int in a sequence causes the same padding
+      spatial dimension. A single int is interpreted as applying the same padding
+      in all dims and assign a single int in a sequence causes the same padding
       to be used on both sides.
     kernel_dilation: `None`, or a sequence of `n` integers, giving the
       dilation factor to apply in each spatial dimension of the convolution
       kernel. Convolution with kernel dilation is also known as 'atrous
       convolution'.
     use_bias: whether to add a bias to the output (default: True).
     mask: Optional mask for the weights during masked convolution. The mask must
```

### Comparing `flax-0.6.9/flax/linen/module.py` & `flax-0.7.0/flax/linen/module.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,5324 +61,5422 @@
 000003c0: 7320 6a6e 700a 6672 6f6d 2074 7970 696e  s jnp.from typin
 000003d0: 675f 6578 7465 6e73 696f 6e73 2069 6d70  g_extensions imp
 000003e0: 6f72 7420 5072 6f74 6f63 6f6c 2c20 5c0a  ort Protocol, \.
 000003f0: 2020 6461 7461 636c 6173 735f 7472 616e    dataclass_tran
 00000400: 7366 6f72 6d20 2023 2070 7974 7970 653a  sform  # pytype:
 00000410: 2064 6973 6162 6c65 3d6e 6f74 2d73 7570   disable=not-sup
 00000420: 706f 7274 6564 2d79 6574 0a0a 696d 706f  ported-yet..impo
-00000430: 7274 2066 6c61 780a 6672 6f6d 2066 6c61  rt flax.from fla
-00000440: 7820 696d 706f 7274 2028 636f 6e66 6967  x import (config
-00000450: 2c20 636f 7265 2c20 6572 726f 7273 2c20  , core, errors, 
-00000460: 7365 7269 616c 697a 6174 696f 6e2c 2074  serialization, t
-00000470: 7261 6365 6261 636b 5f75 7469 6c2c 0a20  raceback_util,. 
-00000480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000490: 2074 7261 7665 7273 655f 7574 696c 290a   traverse_util).
-000004a0: 6672 6f6d 2066 6c61 782e 636f 7265 2069  from flax.core i
-000004b0: 6d70 6f72 7420 5363 6f70 650a 6672 6f6d  mport Scope.from
-000004c0: 2066 6c61 782e 636f 7265 2069 6d70 6f72   flax.core impor
-000004d0: 7420 7061 7274 6961 6c5f 6576 616c 0a66  t partial_eval.f
-000004e0: 726f 6d20 666c 6178 2e63 6f72 652e 6672  rom flax.core.fr
-000004f0: 6f7a 656e 5f64 6963 7420 696d 706f 7274  ozen_dict import
-00000500: 2046 726f 7a65 6e44 6963 740a 6672 6f6d   FrozenDict.from
-00000510: 2066 6c61 782e 636f 7265 2e73 636f 7065   flax.core.scope
-00000520: 2069 6d70 6f72 7420 2820 2023 2070 796c   import (  # pyl
-00000530: 696e 743a 2064 6973 6162 6c65 3d67 2d6d  int: disable=g-m
-00000540: 756c 7469 706c 652d 696d 706f 7274 0a20  ultiple-import. 
-00000550: 2020 2043 6f6c 6c65 6374 696f 6e46 696c     CollectionFil
-00000560: 7465 722c 2044 656e 794c 6973 742c 2046  ter, DenyList, F
-00000570: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
-00000580: 742c 2056 6172 6961 626c 652c 2056 6172  t, Variable, Var
-00000590: 6961 626c 6544 6963 742c 0a20 2020 2075  iableDict,.    u
-000005a0: 6e69 6f6e 5f66 696c 7465 7273 290a 6672  nion_filters).fr
-000005b0: 6f6d 2066 6c61 782e 6964 7320 696d 706f  om flax.ids impo
-000005c0: 7274 2046 6c61 7849 640a 6672 6f6d 2066  rt FlaxId.from f
-000005d0: 6c61 782e 6964 7320 696d 706f 7274 2075  lax.ids import u
-000005e0: 7569 640a 6672 6f6d 2066 6c61 782e 6c69  uid.from flax.li
-000005f0: 6e65 6e20 696d 706f 7274 206b 775f 6f6e  nen import kw_on
-00000600: 6c79 5f64 6174 6163 6c61 7373 6573 0a0a  ly_dataclasses..
-00000610: 0a74 7261 6365 6261 636b 5f75 7469 6c2e  .traceback_util.
-00000620: 7265 6769 7374 6572 5f65 7863 6c75 7369  register_exclusi
-00000630: 6f6e 285f 5f66 696c 655f 5f29 0a0a 5052  on(__file__)..PR
-00000640: 4e47 4b65 7920 3d20 416e 7920 2023 2070  NGKey = Any  # p
-00000650: 796c 696e 743a 2064 6973 6162 6c65 3d69  ylint: disable=i
-00000660: 6e76 616c 6964 2d6e 616d 650a 524e 4753  nvalid-name.RNGS
-00000670: 6571 7565 6e63 6573 203d 2044 6963 745b  equences = Dict[
-00000680: 7374 722c 2050 524e 474b 6579 5d0a 4172  str, PRNGKey].Ar
-00000690: 7261 7920 3d20 416e 7920 2020 2023 2070  ray = Any    # p
-000006a0: 796c 696e 743a 2064 6973 6162 6c65 3d69  ylint: disable=i
-000006b0: 6e76 616c 6964 2d6e 616d 650a 0a0a 5420  nvalid-name...T 
-000006c0: 3d20 5479 7065 5661 7228 2754 2729 0a4b  = TypeVar('T').K
-000006d0: 203d 2054 7970 6556 6172 2827 4b27 290a   = TypeVar('K').
-000006e0: 4d20 3d20 5479 7065 5661 7228 274d 272c  M = TypeVar('M',
-000006f0: 2062 6f75 6e64 3d27 4d6f 6475 6c65 2729   bound='Module')
-00000700: 0a5f 4361 6c6c 6162 6c65 5420 3d20 5479  ._CallableT = Ty
-00000710: 7065 5661 7228 275f 4361 6c6c 6162 6c65  peVar('_Callable
-00000720: 5427 2c20 626f 756e 643d 4361 6c6c 6162  T', bound=Callab
-00000730: 6c65 290a 0a0a 2320 5573 6564 2066 6f72  le)...# Used for
-00000740: 2061 6273 7472 6163 746c 7920 7465 7374   abstractly test
-00000750: 696e 6720 6d6f 6475 6c65 2062 6568 6176  ing module behav
-00000760: 696f 722e 0a54 6573 7453 636f 7065 203d  ior..TestScope =
-00000770: 2074 7970 6528 2754 6573 7453 636f 7065   type('TestScope
-00000780: 272c 0a20 2020 2020 2020 2020 2020 2020  ',.             
-00000790: 2020 2020 2853 636f 7065 2c29 2c0a 2020      (Scope,),.  
-000007a0: 2020 2020 2020 2020 2020 2020 2020 207b                 {
-000007b0: 276d 616b 655f 726e 6727 3a20 6c61 6d62  'make_rng': lamb
-000007c0: 6461 2073 656c 662c 206e 616d 653a 206a  da self, name: j
-000007d0: 6178 2e72 616e 646f 6d2e 5052 4e47 4b65  ax.random.PRNGKe
-000007e0: 7928 3029 7d29 0a0a 0a23 2070 796c 696e  y(0)})...# pylin
-000007f0: 743a 2064 6973 6162 6c65 3d70 726f 7465  t: disable=prote
-00000800: 6374 6564 2d61 6363 6573 732c 6174 7472  cted-access,attr
-00000810: 6962 7574 652d 6465 6669 6e65 642d 6f75  ibute-defined-ou
-00000820: 7473 6964 652d 696e 6974 0a0a 6465 6620  tside-init..def 
-00000830: 5f69 6e64 656e 7428 783a 2073 7472 2c20  _indent(x: str, 
-00000840: 6e75 6d5f 7370 6163 6573 3a20 696e 7429  num_spaces: int)
-00000850: 3a0a 2020 696e 6465 6e74 5f73 7472 203d  :.  indent_str =
-00000860: 2027 2027 202a 206e 756d 5f73 7061 6365   ' ' * num_space
-00000870: 730a 2020 6c69 6e65 7320 3d20 782e 7370  s.  lines = x.sp
-00000880: 6c69 7428 275c 6e27 290a 2020 2320 736b  lit('\n').  # sk
-00000890: 6970 206c 6173 7420 6c69 6e65 2062 6563  ip last line bec
-000008a0: 6175 7365 2069 7420 6973 2061 6c77 6179  ause it is alway
-000008b0: 7320 656d 7074 7920 616e 6420 7368 6f75  s empty and shou
-000008c0: 6c64 206e 6f74 2062 6520 696e 6465 6e74  ld not be indent
-000008d0: 6564 2e0a 2020 6173 7365 7274 206e 6f74  ed..  assert not
-000008e0: 206c 696e 6573 5b2d 315d 0a20 2072 6574   lines[-1].  ret
-000008f0: 7572 6e20 275c 6e27 2e6a 6f69 6e28 696e  urn '\n'.join(in
-00000900: 6465 6e74 5f73 7472 202b 206c 696e 6520  dent_str + line 
-00000910: 666f 7220 6c69 6e65 2069 6e20 6c69 6e65  for line in line
-00000920: 735b 3a2d 315d 2920 2b20 275c 6e27 0a0a  s[:-1]) + '\n'..
-00000930: 0a64 6566 205f 6174 7472 5f72 6570 7228  .def _attr_repr(
-00000940: 7661 6c75 653a 2041 6e79 293a 0a20 2069  value: Any):.  i
-00000950: 6620 6361 6c6c 6162 6c65 2876 616c 7565  f callable(value
-00000960: 2920 616e 6420 6765 7461 7474 7228 7661  ) and getattr(va
-00000970: 6c75 652c 2027 5f5f 6e61 6d65 5f5f 272c  lue, '__name__',
-00000980: 204e 6f6e 6529 3a0a 2020 2020 7661 6c75   None):.    valu
-00000990: 655f 7265 7020 3d20 7661 6c75 652e 5f5f  e_rep = value.__
-000009a0: 6e61 6d65 5f5f 0a20 2065 6c73 653a 0a20  name__.  else:. 
-000009b0: 2020 2076 616c 7565 5f72 6570 203d 2072     value_rep = r
-000009c0: 6570 7228 7661 6c75 6529 0a20 2072 6574  epr(value).  ret
-000009d0: 7572 6e20 7661 6c75 655f 7265 700a 0a0a  urn value_rep...
-000009e0: 6465 6620 5f6d 6f64 756c 655f 7265 7072  def _module_repr
-000009f0: 286d 6f64 756c 653a 2027 4d6f 6475 6c65  (module: 'Module
-00000a00: 272c 206e 756d 5f73 7061 6365 733a 2069  ', num_spaces: i
-00000a10: 6e74 203d 2034 293a 0a20 2022 2222 5265  nt = 4):.  """Re
-00000a20: 7475 726e 7320 6120 7072 6574 7479 2070  turns a pretty p
-00000a30: 7269 6e74 6564 2072 6570 7265 7365 6e74  rinted represent
-00000a40: 6174 696f 6e20 6f66 2074 6865 206d 6f64  ation of the mod
-00000a50: 756c 652e 2222 220a 2020 636c 7320 3d20  ule.""".  cls = 
-00000a60: 7479 7065 286d 6f64 756c 6529 0a20 2063  type(module).  c
-00000a70: 6c73 5f6e 616d 6520 3d20 636c 732e 5f5f  ls_name = cls.__
-00000a80: 6e61 6d65 5f5f 0a20 2072 6570 203d 2027  name__.  rep = '
-00000a90: 270a 0a20 2061 7474 7269 6275 7465 7320  '..  attributes 
-00000aa0: 3d20 7b0a 2020 2020 2020 662e 6e61 6d65  = {.      f.name
-00000ab0: 3a20 662e 7479 7065 0a20 2020 2020 2066  : f.type.      f
-00000ac0: 6f72 2066 2069 6e20 6461 7461 636c 6173  or f in dataclas
-00000ad0: 7365 732e 6669 656c 6473 2863 6c73 290a  ses.fields(cls).
-00000ae0: 2020 2020 2020 6966 2066 2e6e 616d 6520        if f.name 
-00000af0: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
-00000b00: 2c20 276e 616d 6527 2920 616e 6420 662e  , 'name') and f.
-00000b10: 7265 7072 0a20 207d 0a20 2063 6869 6c64  repr.  }.  child
-00000b20: 5f6d 6f64 756c 6573 203d 207b 6b3a 2076  _modules = {k: v
-00000b30: 2066 6f72 206b 2c20 7620 696e 206d 6f64   for k, v in mod
-00000b40: 756c 652e 5f73 7461 7465 2e63 6869 6c64  ule._state.child
-00000b50: 7265 6e2e 6974 656d 7328 2920 2023 2070  ren.items()  # p
-00000b60: 7974 7970 653a 2064 6973 6162 6c65 3d61  ytype: disable=a
-00000b70: 7474 7269 6275 7465 2d65 7272 6f72 0a20  ttribute-error. 
-00000b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b90: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-00000ba0: 762c 204d 6f64 756c 6529 7d0a 2020 6966  v, Module)}.  if
-00000bb0: 2061 7474 7269 6275 7465 733a 0a20 2020   attributes:.   
-00000bc0: 2072 6570 202b 3d20 2723 2061 7474 7269   rep += '# attri
-00000bd0: 6275 7465 735c 6e27 0a20 2020 2066 6f72  butes\n'.    for
-00000be0: 2061 7474 7220 696e 2061 7474 7269 6275   attr in attribu
-00000bf0: 7465 732e 6b65 7973 2829 3a0a 2020 2020  tes.keys():.    
-00000c00: 2020 2320 544f 444f 286a 6865 656b 293a    # TODO(jheek):
-00000c10: 2063 616e 2077 6520 6765 7420 6120 6e69   can we get a ni
-00000c20: 6365 2073 7472 696e 6720 7265 7072 6573  ce string repres
-00000c30: 656e 7461 7469 6f6e 206f 6620 6174 7472  entation of attr
-00000c40: 6962 7574 6520 7479 7065 733f 0a20 2020  ibute types?.   
-00000c50: 2020 2076 616c 7565 203d 2067 6574 6174     value = getat
-00000c60: 7472 286d 6f64 756c 652c 2061 7474 722c  tr(module, attr,
-00000c70: 204e 6f6e 6529 0a20 2020 2020 2076 616c   None).      val
-00000c80: 7565 5f72 6570 203d 205f 6174 7472 5f72  ue_rep = _attr_r
-00000c90: 6570 7228 7661 6c75 6529 0a20 2020 2020  epr(value).     
-00000ca0: 2072 6570 202b 3d20 6627 7b61 7474 727d   rep += f'{attr}
-00000cb0: 203d 207b 7661 6c75 655f 7265 707d 5c6e   = {value_rep}\n
-00000cc0: 270a 2020 6966 2063 6869 6c64 5f6d 6f64  '.  if child_mod
-00000cd0: 756c 6573 3a0a 2020 2020 7265 7020 2b3d  ules:.    rep +=
-00000ce0: 2027 2320 6368 696c 6472 656e 5c6e 270a   '# children\n'.
-00000cf0: 2020 2020 666f 7220 6e61 6d65 2c20 6368      for name, ch
-00000d00: 696c 6420 696e 2063 6869 6c64 5f6d 6f64  ild in child_mod
-00000d10: 756c 6573 2e69 7465 6d73 2829 3a0a 2020  ules.items():.  
-00000d20: 2020 2020 6368 696c 645f 7265 7020 3d20      child_rep = 
-00000d30: 5f6d 6f64 756c 655f 7265 7072 2863 6869  _module_repr(chi
-00000d40: 6c64 2c20 6e75 6d5f 7370 6163 6573 290a  ld, num_spaces).
-00000d50: 2020 2020 2020 7265 7020 2b3d 2066 277b        rep += f'{
-00000d60: 6e61 6d65 7d20 3d20 7b63 6869 6c64 5f72  name} = {child_r
-00000d70: 6570 7d5c 6e27 0a20 2069 6620 7265 703a  ep}\n'.  if rep:
-00000d80: 0a20 2020 2072 6574 7572 6e20 6627 7b63  .    return f'{c
-00000d90: 6c73 5f6e 616d 657d 285c 6e7b 5f69 6e64  ls_name}(\n{_ind
-00000da0: 656e 7428 7265 702c 206e 756d 5f73 7061  ent(rep, num_spa
-00000db0: 6365 7329 7d29 270a 2020 656c 7365 3a0a  ces)})'.  else:.
-00000dc0: 2020 2020 7265 7475 726e 2066 277b 636c      return f'{cl
-00000dd0: 735f 6e61 6d65 7d28 2927 0a0a 2320 5461  s_name}()'..# Ta
-00000de0: 6275 6c61 7469 6f6e 2075 7469 6c69 7469  bulation utiliti
-00000df0: 6573 2e0a 2320 2d2d 2d2d 2d2d 2d2d 2d2d  es..# ----------
-00000e00: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e10: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e20: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e40: 2d2d 2d0a 0a5f 6669 6e64 5f6e 6f6e 5f6c  ---.._find_non_l
-00000e50: 6966 7465 645f 6d6f 6475 6c65 203d 2072  ifted_module = r
-00000e60: 652e 636f 6d70 696c 6528 7227 2e2a 5c28  e.compile(r'.*\(
-00000e70: 282e 2a29 5c29 2729 0a0a 6465 6620 5f66  (.*)\)')..def _f
-00000e80: 6978 5f70 6174 685f 7061 7274 2870 6172  ix_path_part(par
-00000e90: 743a 2073 7472 293a 0a20 2022 2222 4669  t: str):.  """Fi
-00000ea0: 7865 7320 6120 7061 7468 2070 6172 7420  xes a path part 
-00000eb0: 6279 2072 656d 6f76 696e 6720 7472 616e  by removing tran
-00000ec0: 7366 6f72 6d61 7469 6f6e 206e 616d 6520  sformation name 
-00000ed0: 616e 6420 7061 7265 6e74 6865 7369 7320  and parenthesis 
-00000ee0: 736f 6d65 7469 6d65 730a 2020 696e 7365  sometimes.  inse
-00000ef0: 7274 6564 2062 7920 6c69 6674 6564 2074  rted by lifted t
-00000f00: 7261 6e73 666f 726d 6174 696f 6e73 2222  ransformations""
-00000f10: 220a 2020 6d61 7463 6820 3d20 5f66 696e  ".  match = _fin
-00000f20: 645f 6e6f 6e5f 6c69 6674 6564 5f6d 6f64  d_non_lifted_mod
-00000f30: 756c 652e 6d61 7463 6828 7061 7274 290a  ule.match(part).
-00000f40: 2020 6966 206d 6174 6368 3a0a 2020 2020    if match:.    
-00000f50: 7265 7475 726e 206d 6174 6368 2e67 726f  return match.gro
-00000f60: 7570 2831 290a 2020 7265 7475 726e 2070  up(1).  return p
-00000f70: 6172 740a 0a40 6461 7461 636c 6173 7365  art..@dataclasse
-00000f80: 732e 6461 7461 636c 6173 730a 636c 6173  s.dataclass.clas
-00000f90: 7320 5f43 616c 6c49 6e66 6f3a 0a20 2069  s _CallInfo:.  i
-00000fa0: 6e64 6578 3a20 696e 740a 2020 7061 7468  ndex: int.  path
-00000fb0: 3a20 5475 706c 655b 7374 722c 202e 2e2e  : Tuple[str, ...
-00000fc0: 5d0a 2020 6d6f 6475 6c65 5f74 7970 653a  ].  module_type:
-00000fd0: 2054 7970 655b 274d 6f64 756c 6527 5d0a   Type['Module'].
-00000fe0: 2020 6d65 7468 6f64 3a20 7374 720a 2020    method: str.  
-00000ff0: 6172 6773 3a20 5475 706c 655b 416e 792c  args: Tuple[Any,
-00001000: 202e 2e2e 5d0a 2020 6b77 6172 6773 3a20   ...].  kwargs: 
-00001010: 4469 6374 5b73 7472 2c20 416e 795d 0a20  Dict[str, Any]. 
-00001020: 206f 7574 7075 7473 3a20 416e 790a 0a40   outputs: Any..@
-00001030: 6461 7461 636c 6173 7365 732e 6461 7461  dataclasses.data
-00001040: 636c 6173 730a 636c 6173 7320 5f43 616c  class.class _Cal
-00001050: 6c49 6e66 6f43 6f6e 7465 7874 2874 6872  lInfoContext(thr
-00001060: 6561 6469 6e67 2e6c 6f63 616c 293a 0a20  eading.local):. 
-00001070: 2069 6e64 6578 3a20 696e 740a 2020 6361   index: int.  ca
-00001080: 6c6c 733a 204c 6973 745b 5f43 616c 6c49  lls: List[_CallI
-00001090: 6e66 6f5d 0a0a 2020 6465 6620 6765 745f  nfo]..  def get_
-000010a0: 6361 6c6c 5f69 6e64 6578 2873 656c 662c  call_index(self,
-000010b0: 206d 6f64 756c 653a 2027 4d6f 6475 6c65   module: 'Module
-000010c0: 2729 202d 3e20 696e 743a 0a20 2020 2069  ') -> int:.    i
-000010d0: 6e64 6578 203d 2073 656c 662e 696e 6465  ndex = self.inde
-000010e0: 780a 2020 2020 7365 6c66 2e69 6e64 6578  x.    self.index
-000010f0: 202b 3d20 310a 2020 2020 7265 7475 726e   += 1.    return
-00001100: 2069 6e64 6578 0a0a 4063 6f6e 7465 7874   index..@context
-00001110: 6c69 622e 636f 6e74 6578 746d 616e 6167  lib.contextmanag
-00001120: 6572 0a64 6566 205f 7461 6275 6c61 7465  er.def _tabulate
-00001130: 5f63 6f6e 7465 7874 2829 3a0a 2020 5f63  _context():.  _c
-00001140: 6f6e 7465 7874 2e63 616c 6c5f 696e 666f  ontext.call_info
-00001150: 5f73 7461 636b 2e61 7070 656e 6428 5f43  _stack.append(_C
-00001160: 616c 6c49 6e66 6f43 6f6e 7465 7874 2830  allInfoContext(0
-00001170: 2c20 5b5d 2929 0a20 2074 7279 3a0a 2020  , [])).  try:.  
-00001180: 2020 7969 656c 640a 2020 6669 6e61 6c6c    yield.  finall
-00001190: 793a 0a20 2020 205f 636f 6e74 6578 742e  y:.    _context.
-000011a0: 6361 6c6c 5f69 6e66 6f5f 7374 6163 6b2e  call_info_stack.
-000011b0: 706f 7028 290a 0a23 2054 7261 636b 2070  pop()..# Track p
-000011c0: 6172 656e 7420 7265 6c61 7469 6f6e 7368  arent relationsh
-000011d0: 6970 2061 6372 6f73 7320 4d6f 6475 6c65  ip across Module
-000011e0: 732e 0a23 202d 2d2d 2d2d 2d2d 2d2d 2d2d  s..# -----------
-000011f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001200: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001210: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001220: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001230: 2d2d 0a63 6c61 7373 205f 4479 6e61 6d69  --.class _Dynami
-00001240: 6343 6f6e 7465 7874 2874 6872 6561 6469  cContext(threadi
-00001250: 6e67 2e6c 6f63 616c 293a 0a20 2022 2222  ng.local):.  """
-00001260: 4479 6e61 6d69 6320 636f 6e74 6578 742e  Dynamic context.
-00001270: 2222 220a 2020 2320 544f 444f 286d 6172  """.  # TODO(mar
-00001280: 6376 616e 7a65 6529 3a20 7377 6974 6368  cvanzee): switch
-00001290: 2074 6f20 7573 696e 6720 636f 6e74 6578   to using contex
-000012a0: 7476 6172 7320 6f6e 6365 206d 696e 696d  tvars once minim
-000012b0: 756d 2070 7974 686f 6e20 7665 7273 696f  um python versio
-000012c0: 6e20 6973 0a20 2023 2033 2e37 0a0a 2020  n is.  # 3.7..  
-000012d0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
-000012e0: 6629 3a0a 2020 2020 7365 6c66 2e6d 6f64  f):.    self.mod
-000012f0: 756c 655f 7374 6163 6b20 3d20 5b4e 6f6e  ule_stack = [Non
-00001300: 652c 5d0a 2020 2020 7365 6c66 2e63 6170  e,].    self.cap
-00001310: 7475 7265 5f73 7461 636b 203d 205b 5d0a  ture_stack = [].
-00001320: 2020 2020 7365 6c66 2e63 616c 6c5f 696e      self.call_in
-00001330: 666f 5f73 7461 636b 203d 205b 5d0a 0a23  fo_stack = []..#
-00001340: 2054 6865 2067 6c6f 6261 6c20 636f 6e74   The global cont
-00001350: 6578 740a 5f63 6f6e 7465 7874 203d 205f  ext._context = _
-00001360: 4479 6e61 6d69 6343 6f6e 7465 7874 2829  DynamicContext()
-00001370: 0a0a 0a63 6c61 7373 205f 5365 6e74 696e  ...class _Sentin
-00001380: 656c 3a0a 0a20 2064 6566 205f 5f63 6f70  el:..  def __cop
-00001390: 795f 5f28 7365 6c66 293a 0a20 2020 2072  y__(self):.    r
-000013a0: 6574 7572 6e20 7365 6c66 2020 2320 446f  eturn self  # Do
-000013b0: 206e 6f74 2063 6f70 7920 7369 6e67 6c65   not copy single
-000013c0: 746f 6e20 7365 6e74 696e 656c 2e0a 0a20  ton sentinel... 
-000013d0: 2064 6566 205f 5f64 6565 7063 6f70 795f   def __deepcopy_
-000013e0: 5f28 7365 6c66 2c20 6d65 6d6f 293a 0a20  _(self, memo):. 
-000013f0: 2020 2064 656c 206d 656d 6f0a 2020 2020     del memo.    
-00001400: 7265 7475 726e 2073 656c 6620 2023 2044  return self  # D
-00001410: 6f20 6e6f 7420 636f 7079 2073 696e 676c  o not copy singl
-00001420: 6574 6f6e 2073 656e 7469 6e65 6c2e 0a0a  eton sentinel...
-00001430: 0a5f 756e 7370 6563 6966 6965 645f 7061  ._unspecified_pa
-00001440: 7265 6e74 203d 205f 5365 6e74 696e 656c  rent = _Sentinel
-00001450: 2829 0a0a 0a23 2045 6e61 626c 6520 6175  ()...# Enable au
-00001460: 746f 6d61 7469 6320 6e61 6d65 645f 6361  tomatic named_ca
-00001470: 6c6c 2077 7261 7070 696e 6720 666f 7220  ll wrapping for 
-00001480: 6c61 6265 6c6c 696e 6720 7072 6f66 696c  labelling profil
-00001490: 6520 7472 6163 6573 2e0a 2320 2d2d 2d2d  e traces..# ----
-000014a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000014b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000014c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000014d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000014e0: 2d2d 2d2d 2d2d 2d2d 2d0a 5f75 7365 5f6e  ---------._use_n
-000014f0: 616d 6564 5f63 616c 6c20 3d20 636f 6e66  amed_call = conf
-00001500: 6967 2e66 6c61 785f 7072 6f66 696c 650a  ig.flax_profile.
-00001510: 0a0a 6465 6620 5f64 6572 6976 655f 7072  ..def _derive_pr
-00001520: 6f66 696c 696e 675f 6e61 6d65 286d 6f64  ofiling_name(mod
-00001530: 756c 652c 2066 6e29 3a0a 2020 6465 6620  ule, fn):.  def 
-00001540: 5f67 6574 5f66 6e5f 6e61 6d65 2866 6e29  _get_fn_name(fn)
-00001550: 3a0a 2020 2020 6966 2069 7369 6e73 7461  :.    if isinsta
-00001560: 6e63 6528 666e 2c20 6675 6e63 746f 6f6c  nce(fn, functool
-00001570: 732e 7061 7274 6961 6c29 3a0a 2020 2020  s.partial):.    
-00001580: 2020 7265 7475 726e 205f 6765 745f 666e    return _get_fn
-00001590: 5f6e 616d 6528 666e 2e66 756e 6329 0a20  _name(fn.func). 
-000015a0: 2020 2072 6574 7572 6e20 666e 2e5f 5f6e     return fn.__n
-000015b0: 616d 655f 5f0a 2020 666e 5f6e 616d 6520  ame__.  fn_name 
-000015c0: 3d20 5f67 6574 5f66 6e5f 6e61 6d65 2866  = _get_fn_name(f
-000015d0: 6e29 0a20 206d 6574 686f 645f 7375 6666  n).  method_suff
-000015e0: 6978 203d 2066 272e 7b66 6e5f 6e61 6d65  ix = f'.{fn_name
-000015f0: 7d27 2069 6620 666e 5f6e 616d 6520 213d  }' if fn_name !=
-00001600: 2027 5f5f 6361 6c6c 5f5f 2720 656c 7365   '__call__' else
-00001610: 2027 270a 2020 6d6f 6475 6c65 5f6e 616d   ''.  module_nam
-00001620: 6520 3d20 6d6f 6475 6c65 2e6e 616d 6520  e = module.name 
-00001630: 6f72 206d 6f64 756c 652e 5f5f 636c 6173  or module.__clas
-00001640: 735f 5f2e 5f5f 6e61 6d65 5f5f 0a20 2072  s__.__name__.  r
-00001650: 6574 7572 6e20 6627 7b6d 6f64 756c 655f  eturn f'{module_
-00001660: 6e61 6d65 7d7b 6d65 7468 6f64 5f73 7566  name}{method_suf
-00001670: 6669 787d 270a 0a0a 6465 6620 656e 6162  fix}'...def enab
-00001680: 6c65 5f6e 616d 6564 5f63 616c 6c28 293a  le_named_call():
-00001690: 0a20 2022 2222 456e 6162 6c65 7320 6e61  .  """Enables na
-000016a0: 6d65 6420 6361 6c6c 2077 7261 7070 696e  med call wrappin
-000016b0: 6720 666f 7220 6c61 6265 6c6c 696e 6720  g for labelling 
-000016c0: 7072 6f66 696c 6520 7472 6163 6573 2e0a  profile traces..
-000016d0: 0a20 2057 6865 6e20 6e61 6d65 6420 6361  .  When named ca
-000016e0: 6c6c 2077 7261 7070 696e 6720 6973 2065  ll wrapping is e
-000016f0: 6e61 626c 6564 2061 6c6c 204a 4158 206f  nabled all JAX o
-00001700: 7073 2065 7865 6375 7465 6420 696e 2061  ps executed in a
-00001710: 204d 6f64 756c 650a 2020 7769 6c6c 2062   Module.  will b
-00001720: 6520 7275 6e20 756e 6465 7220 6060 6a61  e run under ``ja
-00001730: 782e 6e61 6d65 645f 7363 6f70 6560 602e  x.named_scope``.
-00001740: 2054 6865 2060 604d 6f64 756c 6560 6020   The ``Module`` 
-00001750: 636c 6173 7320 6e61 6d65 2077 696c 6c0a  class name will.
-00001760: 2020 7368 6f77 2075 7020 6172 6f75 6e64    show up around
-00001770: 2074 6865 206f 7065 7261 7469 6f6e 7320   the operations 
-00001780: 6265 6c6f 6e67 696e 6720 746f 2074 6861  belonging to tha
-00001790: 7420 4d6f 6475 6c65 2069 6e20 7468 650a  t Module in the.
-000017a0: 2020 5465 6e73 6f72 626f 6172 6420 7072    Tensorboard pr
-000017b0: 6f66 696c 696e 6720 5549 2c20 7369 6d70  ofiling UI, simp
-000017c0: 6c69 6679 696e 6720 7468 6520 7072 6f66  lifying the prof
-000017d0: 696c 696e 6720 7072 6f63 6573 732e 0a0a  iling process...
-000017e0: 2020 4e6f 7465 2074 6861 7420 6060 6a61    Note that ``ja
-000017f0: 782e 6e61 6d65 645f 7363 6f70 6560 6020  x.named_scope`` 
-00001800: 6f6e 6c79 2077 6f72 6b73 2066 6f72 0a20  only works for. 
-00001810: 2063 6f6d 7069 6c65 6420 6675 6e63 7469   compiled functi
-00001820: 6f6e 7320 2865 2e67 2e3a 2075 7369 6e67  ons (e.g.: using
-00001830: 206a 6178 2e6a 6974 206f 7220 6a61 782e   jax.jit or jax.
-00001840: 706d 6170 292e 0a20 2022 2222 0a20 2067  pmap)..  """.  g
-00001850: 6c6f 6261 6c20 5f75 7365 5f6e 616d 6564  lobal _use_named
-00001860: 5f63 616c 6c0a 2020 5f75 7365 5f6e 616d  _call.  _use_nam
-00001870: 6564 5f63 616c 6c20 3d20 5472 7565 0a0a  ed_call = True..
-00001880: 0a64 6566 2064 6973 6162 6c65 5f6e 616d  .def disable_nam
-00001890: 6564 5f63 616c 6c28 293a 0a20 2022 2222  ed_call():.  """
-000018a0: 4469 7361 626c 6573 206e 616d 6564 2063  Disables named c
-000018b0: 616c 6c20 7772 6170 7069 6e67 2e0a 0a20  all wrapping... 
-000018c0: 2053 6565 2060 6065 6e61 626c 655f 6e61   See ``enable_na
-000018d0: 6d65 645f 6361 6c6c 6060 0a20 2022 2222  med_call``.  """
-000018e0: 0a20 2067 6c6f 6261 6c20 5f75 7365 5f6e  .  global _use_n
-000018f0: 616d 6564 5f63 616c 6c0a 2020 5f75 7365  amed_call.  _use
-00001900: 5f6e 616d 6564 5f63 616c 6c20 3d20 4661  _named_call = Fa
-00001910: 6c73 650a 0a0a 4063 6f6e 7465 7874 6c69  lse...@contextli
-00001920: 622e 636f 6e74 6578 746d 616e 6167 6572  b.contextmanager
-00001930: 0a64 6566 206f 7665 7272 6964 655f 6e61  .def override_na
-00001940: 6d65 645f 6361 6c6c 2865 6e61 626c 653a  med_call(enable:
-00001950: 2062 6f6f 6c20 3d20 5472 7565 293a 0a20   bool = True):. 
-00001960: 2023 2070 796c 696e 743a 2064 6973 6162   # pylint: disab
-00001970: 6c65 3d67 2d64 6f63 2d72 6574 7572 6e2d  le=g-doc-return-
-00001980: 6f72 2d79 6965 6c64 0a20 2022 2222 5265  or-yield.  """Re
-00001990: 7475 726e 7320 6120 636f 6e74 6578 7420  turns a context 
-000019a0: 6d61 6e61 6765 7220 7468 6174 2065 6e61  manager that ena
-000019b0: 626c 6573 2f64 6973 6162 6c65 7320 6e61  bles/disables na
-000019c0: 6d65 6420 6361 6c6c 2077 7261 7070 696e  med call wrappin
-000019d0: 672e 0a0a 2020 4172 6773 3a0a 2020 2020  g...  Args:.    
-000019e0: 656e 6162 6c65 3a20 4966 2074 7275 652c  enable: If true,
-000019f0: 2065 6e61 626c 6573 206e 616d 6564 2063   enables named c
-00001a00: 616c 6c20 7772 6170 7069 6e67 2066 6f72  all wrapping for
-00001a10: 206c 6162 656c 6c69 6e67 2070 726f 6669   labelling profi
-00001a20: 6c65 2074 7261 6365 732e 0a20 2020 2020  le traces..     
-00001a30: 2028 7365 6520 6060 656e 6162 6c65 645f   (see ``enabled_
-00001a40: 6e61 6d65 645f 6361 6c6c 6060 292e 0a20  named_call``).. 
-00001a50: 2022 2222 0a20 2023 2070 796c 696e 743a   """.  # pylint:
-00001a60: 2065 6e61 626c 653d 672d 646f 632d 7265   enable=g-doc-re
-00001a70: 7475 726e 2d6f 722d 7969 656c 640a 2020  turn-or-yield.  
-00001a80: 676c 6f62 616c 205f 7573 655f 6e61 6d65  global _use_name
-00001a90: 645f 6361 6c6c 0a20 2075 7365 5f6e 616d  d_call.  use_nam
-00001aa0: 6564 5f63 616c 6c5f 7072 6576 203d 205f  ed_call_prev = _
-00001ab0: 7573 655f 6e61 6d65 645f 6361 6c6c 0a20  use_named_call. 
-00001ac0: 205f 7573 655f 6e61 6d65 645f 6361 6c6c   _use_named_call
-00001ad0: 203d 2065 6e61 626c 650a 2020 7472 793a   = enable.  try:
-00001ae0: 0a20 2020 2079 6965 6c64 0a20 2066 696e  .    yield.  fin
-00001af0: 616c 6c79 3a0a 2020 2020 5f75 7365 5f6e  ally:.    _use_n
-00001b00: 616d 6564 5f63 616c 6c20 3d20 7573 655f  amed_call = use_
-00001b10: 6e61 6d65 645f 6361 6c6c 5f70 7265 760a  named_call_prev.
-00001b20: 0a0a 2320 5574 696c 6974 6965 7320 666f  ..# Utilities fo
-00001b30: 7220 7079 7472 6565 7320 6f66 204d 6f64  r pytrees of Mod
-00001b40: 756c 6573 2064 6566 696e 6564 2069 6e73  ules defined ins
-00001b50: 6964 6520 7365 7475 7028 290a 2320 2d2d  ide setup().# --
-00001b60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001b70: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001b80: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001b90: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001ba0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 0a0a 6465  -----------...de
-00001bb0: 6620 5f73 6f72 7465 645f 6974 656d 7328  f _sorted_items(
-00001bc0: 7829 3a0a 2020 2222 2252 6574 7572 6e73  x):.  """Returns
-00001bd0: 2069 7465 6d73 206f 6620 6120 6469 6374   items of a dict
-00001be0: 206f 7264 6572 6564 2062 7920 6b65 7973   ordered by keys
-00001bf0: 2e22 2222 0a20 2072 6574 7572 6e20 736f  .""".  return so
-00001c00: 7274 6564 2878 2e69 7465 6d73 2829 2c20  rted(x.items(), 
-00001c10: 6b65 793d 6c61 6d62 6461 2078 3a20 785b  key=lambda x: x[
-00001c20: 305d 290a 0a0a 6465 6620 5f67 6574 5f73  0])...def _get_s
-00001c30: 7566 6669 785f 7661 6c75 655f 7061 6972  uffix_value_pair
-00001c40: 7328 0a20 2020 2074 7265 655f 6f72 5f6c  s(.    tree_or_l
-00001c50: 6561 663a 2041 6e79 2920 2d3e 204c 6973  eaf: Any) -> Lis
-00001c60: 745b 5475 706c 655b 7374 722c 2054 7970  t[Tuple[str, Typ
-00001c70: 655b 274d 6f64 756c 6527 5d5d 5d3a 0a20  e['Module']]]:. 
-00001c80: 2022 2222 4865 6c70 6572 2066 6f72 206e   """Helper for n
-00001c90: 616d 696e 6720 7079 7472 6565 7320 6f66  aming pytrees of
-00001ca0: 2073 7562 6d6f 6475 6c65 732e 2222 220a   submodules.""".
-00001cb0: 2020 6469 6374 5f6f 725f 6c65 6166 203d    dict_or_leaf =
-00001cc0: 2073 6572 6961 6c69 7a61 7469 6f6e 2e74   serialization.t
-00001cd0: 6f5f 7374 6174 655f 6469 6374 2874 7265  o_state_dict(tre
-00001ce0: 655f 6f72 5f6c 6561 6629 0a20 2069 6620  e_or_leaf).  if 
-00001cf0: 6e6f 7420 6973 696e 7374 616e 6365 2864  not isinstance(d
-00001d00: 6963 745f 6f72 5f6c 6561 662c 2064 6963  ict_or_leaf, dic
-00001d10: 7429 206f 7220 6e6f 7420 6469 6374 5f6f  t) or not dict_o
-00001d20: 725f 6c65 6166 3a0a 2020 2020 7265 7475  r_leaf:.    retu
-00001d30: 726e 205b 2827 272c 2074 7265 655f 6f72  rn [('', tree_or
-00001d40: 5f6c 6561 6629 5d0a 2020 656c 7365 3a0a  _leaf)].  else:.
-00001d50: 2020 2020 666c 6174 5f64 6963 7420 3d20      flat_dict = 
-00001d60: 7472 6176 6572 7365 5f75 7469 6c2e 666c  traverse_util.fl
-00001d70: 6174 7465 6e5f 6469 6374 2864 6963 745f  atten_dict(dict_
-00001d80: 6f72 5f6c 6561 6629 0a20 2020 2072 6574  or_leaf).    ret
-00001d90: 7572 6e20 5b28 275f 2720 2b20 275f 272e  urn [('_' + '_'.
-00001da0: 6a6f 696e 286b 292c 2076 2920 666f 7220  join(k), v) for 
-00001db0: 6b2c 2076 2069 6e20 5f73 6f72 7465 645f  k, v in _sorted_
-00001dc0: 6974 656d 7328 666c 6174 5f64 6963 7429  items(flat_dict)
-00001dd0: 5d0a 0a0a 6465 6620 5f6d 6170 5f6f 7665  ]...def _map_ove
-00001de0: 725f 6d6f 6475 6c65 735f 696e 5f74 7265  r_modules_in_tre
-00001df0: 6528 666e 2c20 7472 6565 5f6f 725f 6c65  e(fn, tree_or_le
-00001e00: 6166 293a 0a20 2022 2222 4865 6c70 6572  af):.  """Helper
-00001e10: 2066 6f72 206d 6170 7069 6e67 2066 756e   for mapping fun
-00001e20: 6374 696f 6e20 6f76 6572 2073 7562 6d6f  ction over submo
-00001e30: 6475 6c65 732e 2222 220a 2020 6469 6374  dules.""".  dict
-00001e40: 5f6f 725f 6c65 6166 203d 2073 6572 6961  _or_leaf = seria
-00001e50: 6c69 7a61 7469 6f6e 2e74 6f5f 7374 6174  lization.to_stat
-00001e60: 655f 6469 6374 2874 7265 655f 6f72 5f6c  e_dict(tree_or_l
-00001e70: 6561 6629 0a20 2069 6620 6e6f 7420 6973  eaf).  if not is
-00001e80: 696e 7374 616e 6365 2864 6963 745f 6f72  instance(dict_or
-00001e90: 5f6c 6561 662c 2064 6963 7429 206f 7220  _leaf, dict) or 
-00001ea0: 6e6f 7420 6469 6374 5f6f 725f 6c65 6166  not dict_or_leaf
-00001eb0: 3a0a 2020 2020 7265 7475 726e 2066 6e28  :.    return fn(
-00001ec0: 2727 2c20 7472 6565 5f6f 725f 6c65 6166  '', tree_or_leaf
-00001ed0: 290a 2020 656c 7365 3a0a 2020 2020 666c  ).  else:.    fl
-00001ee0: 6174 5f64 6963 7420 3d20 7472 6176 6572  at_dict = traver
-00001ef0: 7365 5f75 7469 6c2e 666c 6174 7465 6e5f  se_util.flatten_
-00001f00: 6469 6374 2864 6963 745f 6f72 5f6c 6561  dict(dict_or_lea
-00001f10: 662c 206b 6565 705f 656d 7074 795f 6e6f  f, keep_empty_no
-00001f20: 6465 733d 5472 7565 290a 2020 2020 6d61  des=True).    ma
-00001f30: 7070 6564 5f66 6c61 745f 6469 6374 203d  pped_flat_dict =
-00001f40: 207b 6b3a 2066 6e28 275f 2720 2b20 275f   {k: fn('_' + '_
-00001f50: 272e 6a6f 696e 286b 292c 2076 290a 2020  '.join(k), v).  
-00001f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f70: 2020 2020 2020 666f 7220 6b2c 2076 2069        for k, v i
-00001f80: 6e20 5f73 6f72 7465 645f 6974 656d 7328  n _sorted_items(
-00001f90: 666c 6174 5f64 6963 7429 7d0a 2020 2020  flat_dict)}.    
-00001fa0: 7265 7475 726e 2073 6572 6961 6c69 7a61  return serializa
-00001fb0: 7469 6f6e 2e66 726f 6d5f 7374 6174 655f  tion.from_state_
-00001fc0: 6469 6374 280a 2020 2020 2020 2020 7472  dict(.        tr
-00001fd0: 6565 5f6f 725f 6c65 6166 2c20 7472 6176  ee_or_leaf, trav
-00001fe0: 6572 7365 5f75 7469 6c2e 756e 666c 6174  erse_util.unflat
-00001ff0: 7465 6e5f 6469 6374 286d 6170 7065 645f  ten_dict(mapped_
-00002000: 666c 6174 5f64 6963 7429 290a 0a0a 6465  flat_dict))...de
-00002010: 6620 5f61 6c6c 5f6e 616d 6573 5f6f 6e5f  f _all_names_on_
-00002020: 6f62 6a65 6374 286f 626a 3a20 416e 7929  object(obj: Any)
-00002030: 202d 3e20 5365 745b 7374 725d 3a0a 2020   -> Set[str]:.  
-00002040: 2222 2247 6574 7320 616c 6c20 6e61 6d65  """Gets all name
-00002050: 7320 6f66 2061 7474 7269 6275 7465 7320  s of attributes 
-00002060: 6f6e 2060 6f62 6a60 2061 6e64 2069 7473  on `obj` and its
-00002070: 2063 6c61 7373 6573 2074 6872 6f75 6768   classes through
-00002080: 6f75 7420 4d52 4f2e 0a0a 2020 4172 6773  out MRO...  Args
-00002090: 3a0a 2020 2020 6f62 6a3a 2054 6865 206f  :.    obj: The o
-000020a0: 626a 6563 7420 746f 2067 6574 206e 616d  bject to get nam
-000020b0: 6573 2066 6f72 2e0a 2020 5265 7475 726e  es for..  Return
-000020c0: 733a 0a20 2020 2041 2073 6574 206f 6620  s:.    A set of 
-000020d0: 6e61 6d65 7320 6f66 2061 7474 7269 6275  names of attribu
-000020e0: 7465 7320 6f66 2060 6f62 6a60 2061 6e64  tes of `obj` and
-000020f0: 2069 7473 2063 6c61 7373 6573 2e0a 2020   its classes..  
-00002100: 2222 220a 2020 6e61 6d65 7365 7420 3d20  """.  nameset = 
-00002110: 7365 7428 6f62 6a2e 5f5f 6469 6374 5f5f  set(obj.__dict__
-00002120: 2e6b 6579 7328 2929 0a20 2066 6f72 2063  .keys()).  for c
-00002130: 6c73 2069 6e20 6f62 6a2e 5f5f 636c 6173  ls in obj.__clas
-00002140: 735f 5f2e 5f5f 6d72 6f5f 5f3a 0a20 2020  s__.__mro__:.   
-00002150: 206e 616d 6573 6574 203d 206e 616d 6573   nameset = names
-00002160: 6574 2e75 6e69 6f6e 2873 6574 2863 6c73  et.union(set(cls
-00002170: 2e5f 5f64 6963 745f 5f2e 6b65 7973 2829  .__dict__.keys()
-00002180: 2929 0a20 2072 6574 7572 6e20 6e61 6d65  )).  return name
-00002190: 7365 740a 0a0a 6465 6620 5f66 7265 657a  set...def _freez
-000021a0: 655f 6174 7472 2876 616c 3a20 416e 7929  e_attr(val: Any)
-000021b0: 202d 3e20 416e 793a 0a20 2022 2222 5265   -> Any:.  """Re
-000021c0: 6375 7273 6976 656c 7920 7772 6170 2074  cursively wrap t
-000021d0: 6865 2067 6976 656e 2061 7474 7269 6275  he given attribu
-000021e0: 7465 2060 7661 7260 2069 6e20 6060 4672  te `var` in ``Fr
-000021f0: 6f7a 656e 4469 6374 6060 2e22 2222 0a20  ozenDict``.""". 
-00002200: 2069 6620 6973 696e 7374 616e 6365 2876   if isinstance(v
-00002210: 616c 2c20 2864 6963 742c 2046 726f 7a65  al, (dict, Froze
-00002220: 6e44 6963 7429 293a 0a20 2020 2072 6574  nDict)):.    ret
-00002230: 7572 6e20 4672 6f7a 656e 4469 6374 287b  urn FrozenDict({
-00002240: 6b3a 205f 6672 6565 7a65 5f61 7474 7228  k: _freeze_attr(
-00002250: 7629 2066 6f72 206b 2c20 7620 696e 2076  v) for k, v in v
-00002260: 616c 2e69 7465 6d73 2829 7d29 0a20 2065  al.items()}).  e
-00002270: 6c69 6620 6973 696e 7374 616e 6365 2876  lif isinstance(v
-00002280: 616c 2c20 7475 706c 6529 3a0a 2020 2020  al, tuple):.    
-00002290: 2320 5370 6563 6961 6c20 6361 7365 206e  # Special case n
-000022a0: 616d 6564 7475 706c 6573 2061 6e64 2073  amedtuples and s
-000022b0: 7065 6369 616c 204a 4158 2074 7570 6c65  pecial JAX tuple
-000022c0: 2073 7472 7563 7475 7265 7320 6f74 6865   structures othe
-000022d0: 7277 6973 6520 7468 6579 0a20 2020 2023  rwise they.    #
-000022e0: 2077 6f75 6c64 2062 6520 646f 776e 6772   would be downgr
-000022f0: 6164 6564 2074 6f20 6e6f 726d 616c 2074  aded to normal t
-00002300: 7570 6c65 732e 0a20 2020 2069 6620 6861  uples..    if ha
-00002310: 7361 7474 7228 7661 6c2c 2027 5f66 6965  sattr(val, '_fie
-00002320: 6c64 7327 2920 6f72 2074 7970 6528 7661  lds') or type(va
-00002330: 6c29 2e5f 5f6e 616d 655f 5f20 3d3d 2027  l).__name__ == '
-00002340: 5061 7274 6974 696f 6e53 7065 6327 3a0a  PartitionSpec':.
-00002350: 2020 2020 2020 7265 7475 726e 2074 7970        return typ
-00002360: 6528 7661 6c29 282a 5b5f 6672 6565 7a65  e(val)(*[_freeze
-00002370: 5f61 7474 7228 7629 2066 6f72 2076 2069  _attr(v) for v i
-00002380: 6e20 7661 6c5d 290a 2020 2020 656c 7365  n val]).    else
-00002390: 3a0a 2020 2020 2020 7265 7475 726e 2074  :.      return t
-000023a0: 7570 6c65 285f 6672 6565 7a65 5f61 7474  uple(_freeze_att
-000023b0: 7228 7629 2066 6f72 2076 2069 6e20 7661  r(v) for v in va
-000023c0: 6c29 0a20 2065 6c69 6620 6973 696e 7374  l).  elif isinst
-000023d0: 616e 6365 2876 616c 2c20 6c69 7374 293a  ance(val, list):
-000023e0: 0a20 2020 2072 6574 7572 6e20 7475 706c  .    return tupl
-000023f0: 6528 5f66 7265 657a 655f 6174 7472 2876  e(_freeze_attr(v
-00002400: 2920 666f 7220 7620 696e 2076 616c 290a  ) for v in val).
-00002410: 2020 656c 7365 3a0a 2020 2020 7265 7475    else:.    retu
-00002420: 726e 2076 616c 0a0a 0a23 204d 6574 686f  rn val...# Metho
-00002430: 6420 7772 6170 7069 6e67 206f 6620 2263  d wrapping of "c
-00002440: 6f6d 7061 6374 206d 6574 686f 6473 2220  ompact methods" 
-00002450: 616e 6420 7365 7475 7028 290a 2320 2d2d  and setup().# --
-00002460: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002470: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002480: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002490: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000024a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 6465 6620  -----------.def 
-000024b0: 636f 6d70 6163 7428 6675 6e3a 205f 4361  compact(fun: _Ca
-000024c0: 6c6c 6162 6c65 5429 202d 3e20 5f43 616c  llableT) -> _Cal
-000024d0: 6c61 626c 6554 3a0a 2020 2222 224d 6172  lableT:.  """Mar
-000024e0: 6b73 2074 6865 2067 6976 656e 206d 6f64  ks the given mod
-000024f0: 756c 6520 6d65 7468 6f64 2061 6c6c 6f77  ule method allow
-00002500: 696e 6720 696e 6c69 6e65 6420 7375 626d  ing inlined subm
-00002510: 6f64 756c 6573 2e0a 0a20 204d 6574 686f  odules...  Metho
-00002520: 6473 2077 7261 7070 6564 2069 6e20 4063  ds wrapped in @c
-00002530: 6f6d 7061 6374 2063 616e 2064 6566 696e  ompact can defin
-00002540: 6520 7375 626d 6f64 756c 6573 2064 6972  e submodules dir
-00002550: 6563 746c 7920 7769 7468 696e 2074 6865  ectly within the
-00002560: 206d 6574 686f 642e 0a0a 2020 466f 7220   method...  For 
-00002570: 696e 7374 616e 6365 3a3a 0a0a 2020 2020  instance::..    
-00002580: 4063 6f6d 7061 6374 0a20 2020 205f 5f63  @compact.    __c
-00002590: 616c 6c5f 5f28 7365 6c66 2c20 782c 2066  all__(self, x, f
-000025a0: 6561 7475 7265 7329 3a0a 2020 2020 2020  eatures):.      
-000025b0: 7820 3d20 6e6e 2e44 656e 7365 2866 6561  x = nn.Dense(fea
-000025c0: 7475 7265 7329 2878 290a 2020 2020 2020  tures)(x).      
-000025d0: 2e2e 2e0a 0a20 2041 7420 6d6f 7374 206f  .....  At most o
-000025e0: 6e65 206d 6574 686f 6420 696e 2065 6163  ne method in eac
-000025f0: 6820 4d6f 6475 6c65 206d 6179 2062 6520  h Module may be 
-00002600: 7772 6170 7065 6420 7769 7468 2040 636f  wrapped with @co
-00002610: 6d70 6163 742e 0a0a 2020 4172 6773 3a0a  mpact...  Args:.
-00002620: 2020 2020 6675 6e3a 2054 6865 204d 6f64      fun: The Mod
-00002630: 756c 6520 6d65 7468 6f64 2074 6f20 6d61  ule method to ma
-00002640: 726b 2061 7320 636f 6d70 6163 742e 0a20  rk as compact.. 
-00002650: 2052 6574 7572 6e73 3a0a 2020 2020 5468   Returns:.    Th
-00002660: 6520 6769 7665 6e20 6675 6e63 7469 6f6e  e given function
-00002670: 2060 6675 6e60 206d 6172 6b65 6420 6173   `fun` marked as
-00002680: 2063 6f6d 7061 6374 2e0a 2020 2222 220a   compact..  """.
-00002690: 2020 6675 6e2e 636f 6d70 6163 7420 3d20    fun.compact = 
-000026a0: 5472 7565 2020 2320 7479 7065 3a20 6967  True  # type: ig
-000026b0: 6e6f 7265 5b61 7474 722d 6465 6669 6e65  nore[attr-define
-000026c0: 645d 0a20 2072 6574 7572 6e20 6675 6e0a  d].  return fun.
-000026d0: 0a0a 6465 6620 6e6f 7772 6170 2866 756e  ..def nowrap(fun
-000026e0: 3a20 5f43 616c 6c61 626c 6554 2920 2d3e  : _CallableT) ->
-000026f0: 205f 4361 6c6c 6162 6c65 543a 0a20 2022   _CallableT:.  "
-00002700: 2222 4d61 726b 7320 7468 6520 6769 7665  ""Marks the give
-00002710: 6e20 6d6f 6475 6c65 206d 6574 686f 6420  n module method 
-00002720: 6173 2061 2068 656c 7065 7220 6d65 7468  as a helper meth
-00002730: 6f64 2074 6861 7420 6e65 6564 6e27 7420  od that needn't 
-00002740: 6265 2077 7261 7070 6564 2e0a 0a20 204d  be wrapped...  M
-00002750: 6574 686f 6473 2077 7261 7070 6564 2069  ethods wrapped i
-00002760: 6e20 406e 6f77 7261 7020 6172 6520 7072  n @nowrap are pr
-00002770: 6976 6174 6520 6865 6c70 6572 206d 6574  ivate helper met
-00002780: 686f 6473 2074 6861 7420 6e65 6564 6e27  hods that needn'
-00002790: 7420 6265 2077 7261 7070 6564 0a20 2077  t be wrapped.  w
-000027a0: 6974 6820 7468 6520 7374 6174 6520 6861  ith the state ha
-000027b0: 6e64 6c65 7220 6f72 2061 2073 6570 6172  ndler or a separ
-000027c0: 6174 6520 6e61 6d65 645f 6361 6c6c 2074  ate named_call t
-000027d0: 7261 6e73 666f 726d 2e0a 0a20 2054 6869  ransform...  Thi
-000027e0: 7320 6973 206e 6565 6465 6420 696e 2073  s is needed in s
-000027f0: 6576 6572 616c 2063 6f6e 6372 6574 6520  everal concrete 
-00002800: 696e 7374 616e 6365 733a 0a20 2020 2d20  instances:.   - 
-00002810: 6966 2079 6f75 2772 6520 7375 6263 6c61  if you're subcla
-00002820: 7373 696e 6720 6120 6d65 7468 6f64 206c  ssing a method l
-00002830: 696b 6520 4d6f 6475 6c65 2e70 6172 616d  ike Module.param
-00002840: 2061 6e64 2064 6f6e 2774 2077 616e 7420   and don't want 
-00002850: 7468 6973 0a20 2020 2020 6f76 6572 7269  this.     overri
-00002860: 6465 6e20 636f 7265 2066 756e 6374 696f  den core functio
-00002870: 6e20 6465 636f 7261 7465 6420 7769 7468  n decorated with
-00002880: 2074 6865 2073 7461 7465 206d 616e 6167   the state manag
-00002890: 656d 656e 7420 7772 6170 7065 722e 0a20  ement wrapper.. 
-000028a0: 2020 2d20 4966 2079 6f75 2077 616e 7420    - If you want 
-000028b0: 6120 6d65 7468 6f64 2074 6f20 6265 2063  a method to be c
-000028c0: 616c 6c61 626c 6520 6672 6f6d 2061 6e20  allable from an 
-000028d0: 756e 626f 756e 6420 4d6f 6475 6c65 2028  unbound Module (
-000028e0: 652e 672e 3a20 610a 2020 2020 2066 756e  e.g.: a.     fun
-000028f0: 6374 696f 6e20 6f66 2063 6f6e 7374 7275  ction of constru
-00002900: 6374 696f 6e20 6f66 2061 7267 756d 656e  ction of argumen
-00002910: 7473 2074 6861 7420 646f 6573 6e27 7420  ts that doesn't 
-00002920: 6465 7065 6e64 206f 6e20 7061 7261 6d73  depend on params
-00002930: 2f52 4e47 7329 0a0a 2020 466f 7220 696e  /RNGs)..  For in
-00002940: 7374 616e 6365 3a3a 0a0a 2020 2020 406e  stance::..    @n
-00002950: 6f77 7261 700a 2020 2020 6465 6620 5f6d  owrap.    def _m
-00002960: 616b 655f 6465 6e73 6528 7365 6c66 2c20  ake_dense(self, 
-00002970: 6e75 6d5f 6665 6174 7572 6573 293a 0a20  num_features):. 
-00002980: 2020 2020 2072 6574 7572 6e20 6e6e 2e44       return nn.D
-00002990: 656e 7365 286e 756d 5f66 6561 7475 7265  ense(num_feature
-000029a0: 7329 0a0a 2020 2020 4063 6f6d 7061 6374  s)..    @compact
-000029b0: 0a20 2020 2064 6566 205f 5f63 616c 6c5f  .    def __call_
-000029c0: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
-000029d0: 2020 2320 6e6f 7720 7361 6665 2074 6f20    # now safe to 
-000029e0: 7573 6520 636f 6e73 7472 7563 746f 7220  use constructor 
-000029f0: 6865 6c70 6572 2065 7665 6e20 6966 2075  helper even if u
-00002a00: 7369 6e67 206e 616d 6564 5f63 616c 6c0a  sing named_call.
-00002a10: 2020 2020 2020 6465 6e73 6520 3d20 7365        dense = se
-00002a20: 6c66 2e5f 6d61 6b65 5f64 656e 7365 2873  lf._make_dense(s
-00002a30: 656c 662e 6e75 6d5f 6665 6174 7572 6573  elf.num_features
-00002a40: 290a 2020 2020 2020 7265 7475 726e 2064  ).      return d
-00002a50: 656e 7365 2878 290a 0a20 2041 7267 733a  ense(x)..  Args:
-00002a60: 0a20 2020 2066 756e 3a20 5468 6520 4d6f  .    fun: The Mo
-00002a70: 6475 6c65 206d 6574 686f 6420 746f 206d  dule method to m
-00002a80: 6172 6b20 6173 206e 6f77 7261 702e 0a20  ark as nowrap.. 
-00002a90: 2052 6574 7572 6e73 3a0a 2020 2020 5468   Returns:.    Th
-00002aa0: 6520 6769 7665 6e20 6675 6e63 7469 6f6e  e given function
-00002ab0: 2060 6675 6e60 206d 6172 6b65 6420 6173   `fun` marked as
-00002ac0: 206e 6f77 7261 702e 0a20 2022 2222 0a20   nowrap..  """. 
-00002ad0: 2066 756e 2e6e 6f77 7261 7020 3d20 5472   fun.nowrap = Tr
-00002ae0: 7565 2020 2320 7479 7065 3a20 6967 6e6f  ue  # type: igno
-00002af0: 7265 5b61 7474 722d 6465 6669 6e65 645d  re[attr-defined]
-00002b00: 0a20 2072 6574 7572 6e20 6675 6e0a 0a0a  .  return fun...
-00002b10: 6465 6620 5f67 6574 5f6c 6f63 616c 5f6d  def _get_local_m
-00002b20: 6574 686f 645f 6e61 6d65 7328 636c 733a  ethod_names(cls:
-00002b30: 2041 6e79 2c0a 2020 2020 2020 2020 2020   Any,.          
-00002b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002b50: 2020 6578 636c 7564 653a 2049 7465 7261    exclude: Itera
-00002b60: 626c 655b 7374 725d 203d 2028 2929 202d  ble[str] = ()) -
-00002b70: 3e20 5475 706c 655b 7374 722c 202e 2e2e  > Tuple[str, ...
-00002b80: 5d3a 0a20 2022 2222 4765 7473 206d 6574  ]:.  """Gets met
-00002b90: 686f 6420 6e61 6d65 7320 6f66 2061 2063  hod names of a c
-00002ba0: 6c61 7373 2c20 6578 636c 7564 696e 6720  lass, excluding 
-00002bb0: 636c 6173 7320 616e 6420 7374 6174 6963  class and static
-00002bc0: 206d 6574 686f 6473 2e0a 0a20 2041 7267   methods...  Arg
-00002bd0: 733a 0a20 2020 2063 6c73 3a20 5468 6520  s:.    cls: The 
-00002be0: 636c 6173 7320 746f 2067 6574 206d 6574  class to get met
-00002bf0: 686f 6420 6e61 6d65 7320 666f 722e 0a20  hod names for.. 
-00002c00: 2020 2065 7863 6c75 6465 3a20 4e61 6d65     exclude: Name
-00002c10: 7320 746f 2065 7863 6c75 6465 2066 726f  s to exclude fro
-00002c20: 6d20 6f75 7470 7574 2e0a 2020 5265 7475  m output..  Retu
-00002c30: 726e 733a 0a20 2020 2041 206c 6973 7420  rns:.    A list 
-00002c40: 6f66 206d 6574 686f 6420 6e61 6d65 732e  of method names.
-00002c50: 0a20 2022 2222 0a20 2074 7275 655f 6d65  .  """.  true_me
-00002c60: 7468 6f64 7320 3d20 7365 7428 290a 2020  thods = set().  
-00002c70: 666f 7220 6d20 696e 2063 6c73 2e5f 5f64  for m in cls.__d
-00002c80: 6963 745f 5f3a 0a20 2020 2069 6620 6361  ict__:.    if ca
-00002c90: 6c6c 6162 6c65 2863 6c73 2e5f 5f64 6963  llable(cls.__dic
-00002ca0: 745f 5f5b 6d5d 2920 616e 6420 6e6f 7420  t__[m]) and not 
-00002cb0: 696e 7370 6563 742e 6973 636c 6173 7328  inspect.isclass(
-00002cc0: 636c 732e 5f5f 6469 6374 5f5f 5b6d 5d29  cls.__dict__[m])
-00002cd0: 3a20 2023 2070 7974 7970 653a 2064 6973  :  # pytype: dis
-00002ce0: 6162 6c65 3d6e 6f74 2d73 7570 706f 7274  able=not-support
-00002cf0: 6564 2d79 6574 0a20 2020 2020 206d 7479  ed-yet.      mty
-00002d00: 7065 203d 2074 7970 6528 636c 732e 5f5f  pe = type(cls.__
-00002d10: 6469 6374 5f5f 5b6d 5d29 0a20 2020 2020  dict__[m]).     
-00002d20: 2069 6620 6d74 7970 6520 213d 2073 7461   if mtype != sta
-00002d30: 7469 636d 6574 686f 6420 616e 6420 6d74  ticmethod and mt
-00002d40: 7970 6520 213d 2063 6c61 7373 6d65 7468  ype != classmeth
-00002d50: 6f64 3a0a 2020 2020 2020 2020 7472 7565  od:.        true
-00002d60: 5f6d 6574 686f 6473 2e61 6464 286d 290a  _methods.add(m).
-00002d70: 2020 7265 7475 726e 2074 7570 6c65 2874    return tuple(t
-00002d80: 7275 655f 6d65 7468 6f64 732e 6469 6666  rue_methods.diff
-00002d90: 6572 656e 6365 2873 6574 2865 7863 6c75  erence(set(exclu
-00002da0: 6465 2929 290a 0a64 6566 205f 6765 745f  de)))..def _get_
-00002db0: 6c6f 6361 6c5f 6465 7363 7269 7074 6f72  local_descriptor
-00002dc0: 5f6e 616d 6573 2863 6c73 3a20 416e 792c  _names(cls: Any,
-00002dd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00002de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002df0: 2065 7863 6c75 6465 3a20 4974 6572 6162   exclude: Iterab
-00002e00: 6c65 5b73 7472 5d20 3d20 2829 2920 2d3e  le[str] = ()) ->
-00002e10: 2054 7570 6c65 5b73 7472 2c20 2e2e 2e5d   Tuple[str, ...]
-00002e20: 3a0a 2020 2222 2247 6574 7320 6465 7363  :.  """Gets desc
-00002e30: 7269 7074 6f72 206e 616d 6573 206f 6620  riptor names of 
-00002e40: 6120 636c 6173 732e 0a0a 2020 4172 6773  a class...  Args
-00002e50: 3a0a 2020 2020 636c 733a 2054 6865 2063  :.    cls: The c
-00002e60: 6c61 7373 2074 6f20 6765 7420 7072 6f70  lass to get prop
-00002e70: 6572 7479 206e 616d 6573 2066 6f72 2e0a  erty names for..
-00002e80: 2020 2020 6578 636c 7564 653a 204e 616d      exclude: Nam
-00002e90: 6573 2074 6f20 6578 636c 7564 6520 6672  es to exclude fr
-00002ea0: 6f6d 206f 7574 7075 742e 0a20 2052 6574  om output..  Ret
-00002eb0: 7572 6e73 3a0a 2020 2020 4120 6c69 7374  urns:.    A list
-00002ec0: 206f 6620 7072 6f70 6572 7479 206e 616d   of property nam
-00002ed0: 6573 2e0a 2020 2222 220a 2020 7472 7565  es..  """.  true
-00002ee0: 5f70 726f 7065 7274 6965 7320 3d20 7365  _properties = se
-00002ef0: 7428 290a 2020 666f 7220 6d2c 2061 7474  t().  for m, att
-00002f00: 7220 696e 2063 6c73 2e5f 5f64 6963 745f  r in cls.__dict_
-00002f10: 5f2e 6974 656d 7328 293a 0a20 2020 2069  _.items():.    i
-00002f20: 6620 6e6f 7420 6361 6c6c 6162 6c65 2861  f not callable(a
-00002f30: 7474 7229 2061 6e64 2028 0a20 2020 2020  ttr) and (.     
-00002f40: 2068 6173 6174 7472 2861 7474 722c 2027   hasattr(attr, '
-00002f50: 5f5f 6765 745f 5f27 2920 6f72 2068 6173  __get__') or has
-00002f60: 6174 7472 2861 7474 722c 2027 5f5f 7365  attr(attr, '__se
-00002f70: 745f 5f27 2920 6f72 0a20 2020 2020 2068  t__') or.      h
-00002f80: 6173 6174 7472 2861 7474 722c 2027 5f5f  asattr(attr, '__
-00002f90: 6465 6c65 7465 5f5f 2729 0a20 2020 2029  delete__').    )
-00002fa0: 3a0a 2020 2020 2020 6d74 7970 6520 3d20  :.      mtype = 
-00002fb0: 7479 7065 2861 7474 7229 0a20 2020 2020  type(attr).     
-00002fc0: 2069 6620 6d74 7970 6520 213d 2073 7461   if mtype != sta
-00002fd0: 7469 636d 6574 686f 6420 616e 6420 6d74  ticmethod and mt
-00002fe0: 7970 6520 213d 2063 6c61 7373 6d65 7468  ype != classmeth
-00002ff0: 6f64 3a0a 2020 2020 2020 2020 7472 7565  od:.        true
-00003000: 5f70 726f 7065 7274 6965 732e 6164 6428  _properties.add(
-00003010: 6d29 0a20 2072 6574 7572 6e20 7475 706c  m).  return tupl
-00003020: 6528 7472 7565 5f70 726f 7065 7274 6965  e(true_propertie
-00003030: 732e 6469 6666 6572 656e 6365 2873 6574  s.difference(set
-00003040: 2865 7863 6c75 6465 2929 290a 0a0a 6465  (exclude)))...de
-00003050: 6620 7772 6170 5f6d 6574 686f 645f 6f6e  f wrap_method_on
-00003060: 6365 2866 756e 3a20 4361 6c6c 6162 6c65  ce(fun: Callable
-00003070: 5b2e 2e2e 2c20 416e 795d 2920 2d3e 2043  [..., Any]) -> C
-00003080: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
-00003090: 5d3a 0a20 2022 2222 4d61 6e61 6765 7320  ]:.  """Manages 
-000030a0: 4d6f 6475 6c65 2073 7461 7465 2066 6f72  Module state for
-000030b0: 2061 2067 6976 656e 2075 7365 722d 6465   a given user-de
-000030c0: 6669 6e65 6420 6d65 7468 6f64 2e0a 0a20  fined method... 
-000030d0: 2041 7267 733a 0a20 2020 2066 756e 3a20   Args:.    fun: 
-000030e0: 5573 6572 2d64 6566 696e 6564 204d 6f64  User-defined Mod
-000030f0: 756c 6520 6d65 7468 6f64 2074 6f20 6d61  ule method to ma
-00003100: 6e61 6765 2073 7461 7465 2066 6f72 2e0a  nage state for..
-00003110: 2020 5265 7475 726e 733a 0a20 2020 2057    Returns:.    W
-00003120: 7261 7070 6564 206d 6574 686f 642e 0a20  rapped method.. 
-00003130: 2022 2222 0a20 2023 2044 6f6e 2774 2072   """.  # Don't r
-00003140: 6577 7261 7020 6d65 7468 6f64 7320 7468  ewrap methods th
-00003150: 6174 2068 6176 6520 616c 7265 6164 7920  at have already 
-00003160: 6861 6420 7468 6520 7374 6174 6520 6d61  had the state ma
-00003170: 6e61 6765 6d65 6e74 2077 7261 7070 6572  nagement wrapper
-00003180: 0a20 2023 2061 7070 6c69 6564 2069 6e20  .  # applied in 
-00003190: 7468 6520 6465 636f 7261 746f 7220 7374  the decorator st
-000031a0: 6163 6b2e 2020 5468 6973 2077 7261 7070  ack.  This wrapp
-000031b0: 6572 2073 686f 756c 6420 616c 7761 7973  er should always
-000031c0: 2062 6520 6170 706c 6965 640a 2020 2320   be applied.  # 
-000031d0: 6265 666f 7265 2074 7261 6e73 666f 726d  before transform
-000031e0: 6174 696f 6e20 7772 6170 7065 7273 2e0a  ation wrappers..
-000031f0: 2020 6966 2068 6173 6174 7472 2866 756e    if hasattr(fun
-00003200: 2c20 276d 6574 686f 645f 6861 6e64 6c65  , 'method_handle
-00003210: 725f 7772 6170 7065 6427 293a 0a20 2020  r_wrapped'):.   
-00003220: 2072 6574 7572 6e20 6675 6e0a 0a20 2040   return fun..  @
-00003230: 6675 6e63 746f 6f6c 732e 7772 6170 7328  functools.wraps(
-00003240: 6675 6e29 0a20 2064 6566 2077 7261 7070  fun).  def wrapp
-00003250: 6564 5f6d 6f64 756c 655f 6d65 7468 6f64  ed_module_method
-00003260: 282a 6172 6773 2c20 2a2a 6b77 6172 6773  (*args, **kwargs
-00003270: 293a 0a20 2020 2023 2057 6520 6d69 6768  ):.    # We migh
-00003280: 7420 6861 7665 2069 6e63 6f72 7265 6374  t have incorrect
-00003290: 6c79 2077 7261 7070 7065 6420 6120 6361  ly wrappped a ca
-000032a0: 6c6c 6162 6c65 0a20 2020 2023 2074 6861  llable.    # tha
-000032b0: 7420 6973 206e 6f74 2061 206d 6574 686f  t is not a metho
-000032c0: 642e 2043 6865 636b 2077 6865 7468 6572  d. Check whether
-000032d0: 2074 6865 2066 6972 7374 2061 7267 2069   the first arg i
-000032e0: 7320 7365 6c66 2c0a 2020 2020 2320 6f74  s self,.    # ot
-000032f0: 6865 7277 6973 6520 6361 6c6c 2074 6865  herwise call the
-00003300: 2077 7261 7070 6564 2066 756e 6374 696f   wrapped functio
-00003310: 6e20 6173 2069 732e 0a20 2020 2069 6620  n as is..    if 
-00003320: 6172 6773 2061 6e64 2069 7369 6e73 7461  args and isinsta
-00003330: 6e63 6528 6172 6773 5b30 5d2c 204d 6f64  nce(args[0], Mod
-00003340: 756c 6529 3a0a 2020 2020 2020 7365 6c66  ule):.      self
-00003350: 2c20 6172 6773 203d 2061 7267 735b 305d  , args = args[0]
-00003360: 2c20 6172 6773 5b31 3a5d 0a20 2020 2020  , args[1:].     
-00003370: 2072 6574 7572 6e20 7365 6c66 2e5f 6361   return self._ca
-00003380: 6c6c 5f77 7261 7070 6564 5f6d 6574 686f  ll_wrapped_metho
-00003390: 6428 6675 6e2c 2061 7267 732c 206b 7761  d(fun, args, kwa
-000033a0: 7267 7329 0a20 2020 2065 6c73 653a 0a20  rgs).    else:. 
-000033b0: 2020 2020 2072 6574 7572 6e20 6675 6e28       return fun(
-000033c0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-000033d0: 0a20 2077 7261 7070 6564 5f6d 6f64 756c  .  wrapped_modul
-000033e0: 655f 6d65 7468 6f64 2e6d 6574 686f 645f  e_method.method_
-000033f0: 6861 6e64 6c65 725f 7772 6170 7065 6420  handler_wrapped 
-00003400: 3d20 5472 7565 2020 2320 7479 7065 3a20  = True  # type: 
-00003410: 6967 6e6f 7265 5b61 7474 722d 6465 6669  ignore[attr-defi
-00003420: 6e65 645d 0a20 2072 6574 7572 6e20 7772  ned].  return wr
-00003430: 6170 7065 645f 6d6f 6475 6c65 5f6d 6574  apped_module_met
-00003440: 686f 640a 0a64 6566 2077 7261 705f 6465  hod..def wrap_de
-00003450: 7363 7269 7074 6f72 5f6f 6e63 6528 6465  scriptor_once(de
-00003460: 7363 7269 7074 6f72 2920 2d3e 2022 4465  scriptor) -> "De
-00003470: 7363 7269 7074 6f72 5772 6170 7065 7222  scriptorWrapper"
-00003480: 3a0a 2020 2222 2257 7261 7073 2061 2064  :.  """Wraps a d
-00003490: 6573 6372 6970 746f 7220 746f 2067 6976  escriptor to giv
-000034a0: 6520 6265 7474 6572 2065 7272 6f72 206d  e better error m
-000034b0: 6573 7361 6765 732e 0a0a 2020 4172 6773  essages...  Args
-000034c0: 3a0a 2020 2020 7072 6f70 3a20 5573 6572  :.    prop: User
-000034d0: 2d64 6566 696e 6564 204d 6f64 756c 6520  -defined Module 
-000034e0: 6174 7472 6962 7574 6520 6465 7363 7269  attribute descri
-000034f0: 7074 6f72 2e0a 2020 5265 7475 726e 733a  ptor..  Returns:
-00003500: 0a20 2020 2057 7261 7070 6564 2064 6573  .    Wrapped des
-00003510: 6372 6970 746f 722e 0a20 2022 2222 0a20  criptor..  """. 
-00003520: 2023 2044 6f6e 2774 2072 6577 7261 7020   # Don't rewrap 
-00003530: 6465 7363 7269 7074 6f72 732e 0a20 2069  descriptors..  i
-00003540: 6620 6973 696e 7374 616e 6365 2864 6573  f isinstance(des
-00003550: 6372 6970 746f 722c 2044 6573 6372 6970  criptor, Descrip
-00003560: 746f 7257 7261 7070 6572 293a 0a20 2020  torWrapper):.   
-00003570: 2072 6574 7572 6e20 6465 7363 7269 7074   return descript
-00003580: 6f72 0a0a 2020 7265 7475 726e 2063 7265  or..  return cre
-00003590: 6174 655f 6465 7363 7269 7074 6f72 5f77  ate_descriptor_w
-000035a0: 7261 7070 6572 2864 6573 6372 6970 746f  rapper(descripto
-000035b0: 7229 0a0a 0a64 6566 205f 7772 6170 5f68  r)...def _wrap_h
-000035c0: 6173 6828 6861 7368 5f66 6e3a 2043 616c  ash(hash_fn: Cal
-000035d0: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d29  lable[..., Any])
-000035e0: 202d 3e20 4361 6c6c 6162 6c65 5b2e 2e2e   -> Callable[...
-000035f0: 2c20 416e 795d 3a0a 2020 2222 2257 7261  , Any]:.  """Wra
-00003600: 7073 2061 2068 6173 6820 6675 6e63 7469  ps a hash functi
-00003610: 6f6e 2077 6974 6820 736f 6d65 2063 6865  on with some che
-00003620: 636b 2066 6f72 2046 6c61 7820 4d6f 6475  ck for Flax Modu
-00003630: 6c65 732e 2222 220a 2020 4066 756e 6374  les.""".  @funct
-00003640: 6f6f 6c73 2e77 7261 7073 2868 6173 685f  ools.wraps(hash_
-00003650: 666e 290a 2020 6465 6620 7772 6170 7065  fn).  def wrappe
-00003660: 6428 7365 6c66 293a 0a20 2020 2069 6620  d(self):.    if 
-00003670: 7365 6c66 2e73 636f 7065 2069 7320 6e6f  self.scope is no
-00003680: 7420 4e6f 6e65 3a0a 2020 2020 2020 7261  t None:.      ra
-00003690: 6973 6520 5479 7065 4572 726f 7228 2743  ise TypeError('C
-000036a0: 616e 5c27 7420 6361 6c6c 205f 5f68 6173  an\'t call __has
-000036b0: 685f 5f20 6f6e 206d 6f64 756c 6573 2074  h__ on modules t
-000036c0: 6861 7420 686f 6c64 2076 6172 6961 626c  hat hold variabl
-000036d0: 6573 2e27 290a 2020 2020 7472 793a 0a20  es.').    try:. 
-000036e0: 2020 2020 2068 6173 685f 7661 6c75 6520       hash_value 
-000036f0: 3d20 6861 7368 5f66 6e28 7365 6c66 290a  = hash_fn(self).
-00003700: 2020 2020 6578 6365 7074 2054 7970 6545      except TypeE
-00003710: 7272 6f72 2061 7320 6578 633a 0a20 2020  rror as exc:.   
-00003720: 2020 2072 6169 7365 2054 7970 6545 7272     raise TypeErr
-00003730: 6f72 2827 4661 696c 6564 2074 6f20 6861  or('Failed to ha
-00003740: 7368 2046 6c61 7820 4d6f 6475 6c65 2e20  sh Flax Module. 
-00003750: 2027 0a20 2020 2020 2020 2020 2020 2020   '.             
-00003760: 2020 2020 2020 2020 2027 5468 6520 6d6f           'The mo
-00003770: 6475 6c65 2070 726f 6261 626c 7920 636f  dule probably co
-00003780: 6e74 6169 6e73 2075 6e68 6173 6861 626c  ntains unhashabl
-00003790: 6520 6174 7472 6962 7574 6573 2e20 2027  e attributes.  '
-000037a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000037b0: 2020 2020 2020 2066 274d 6f64 756c 653d         f'Module=
-000037c0: 7b73 656c 667d 2729 2066 726f 6d20 6578  {self}') from ex
-000037d0: 630a 2020 2020 7265 7475 726e 2068 6173  c.    return has
-000037e0: 685f 7661 6c75 650a 2020 7265 7475 726e  h_value.  return
-000037f0: 2077 7261 7070 6564 0a0a 0a64 6566 205f   wrapped...def _
-00003800: 6765 745f 756e 626f 756e 645f 666e 286d  get_unbound_fn(m
-00003810: 6574 686f 645f 6f72 5f66 6e3a 2043 616c  ethod_or_fn: Cal
-00003820: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d29  lable[..., Any])
-00003830: 202d 3e20 4361 6c6c 6162 6c65 5b2e 2e2e   -> Callable[...
-00003840: 2c20 416e 795d 3a0a 2020 2222 2252 6574  , Any]:.  """Ret
-00003850: 7572 6e73 2061 6e20 756e 626f 756e 6420  urns an unbound 
-00003860: 6675 6e63 7469 6f6e 2066 726f 6d20 6120  function from a 
-00003870: 6d65 7468 6f64 2074 6861 7420 6973 2070  method that is p
-00003880: 6f73 7369 626c 7920 626f 756e 642e 0a0a  ossibly bound...
-00003890: 2020 5468 6973 206d 6561 6e73 2074 6861    This means tha
-000038a0: 7420 6966 2074 6865 2070 6173 7365 6420  t if the passed 
-000038b0: 6675 6e63 7469 6f6e 2062 656c 6f6e 6773  function belongs
-000038c0: 206f 6620 616e 2069 6e73 7461 6e63 6520   of an instance 
-000038d0: 6f66 2061 2063 6c61 7373 2c20 7468 656e  of a class, then
-000038e0: 0a20 2074 6865 2072 6574 7572 6e65 6420  .  the returned 
-000038f0: 6675 6e63 7469 6f6e 2064 6f65 7320 6e6f  function does no
-00003900: 206c 6f6e 6765 7220 6465 7065 6e64 206f   longer depend o
-00003910: 6e20 7468 6520 696e 7374 616e 6365 2c20  n the instance, 
-00003920: 7768 6963 6820 6973 2070 6173 7365 640a  which is passed.
-00003930: 2020 6173 2074 6865 2066 6972 7374 2061    as the first a
-00003940: 7267 756d 656e 7420 746f 2074 6865 2066  rgument to the f
-00003950: 756e 6374 696f 6e2e 0a0a 2020 4172 6773  unction...  Args
-00003960: 3a0a 2020 2020 6d65 7468 6f64 5f6f 725f  :.    method_or_
-00003970: 666e 3a20 4120 636c 6173 7320 6d65 7468  fn: A class meth
-00003980: 6f64 206f 7220 6675 6e63 7469 6f6e 2e0a  od or function..
-00003990: 2020 5265 7475 726e 733a 0a20 2020 2041    Returns:.    A
-000039a0: 6e20 756e 626f 756e 6420 7665 7273 696f  n unbound versio
-000039b0: 6e20 6f66 2069 6e70 7574 2066 756e 6374  n of input funct
-000039c0: 696f 6e2e 0a20 2022 2222 0a20 2069 6620  ion..  """.  if 
-000039d0: 2869 6e73 7065 6374 2e69 736d 6574 686f  (inspect.ismetho
-000039e0: 6428 6d65 7468 6f64 5f6f 725f 666e 2920  d(method_or_fn) 
-000039f0: 616e 640a 2020 2020 2020 6973 696e 7374  and.      isinst
-00003a00: 616e 6365 286d 6574 686f 645f 6f72 5f66  ance(method_or_f
-00003a10: 6e2e 5f5f 7365 6c66 5f5f 2c20 4d6f 6475  n.__self__, Modu
-00003a20: 6c65 2929 3a20 2023 2070 7974 7970 653a  le)):  # pytype:
-00003a30: 2064 6973 6162 6c65 3d61 7474 7269 6275   disable=attribu
-00003a40: 7465 2d65 7272 6f72 0a20 2020 206d 6574  te-error.    met
-00003a50: 686f 645f 6f72 5f66 6e20 3d20 6d65 7468  hod_or_fn = meth
-00003a60: 6f64 5f6f 725f 666e 2e5f 5f66 756e 635f  od_or_fn.__func_
-00003a70: 5f20 2023 2070 7974 7970 653a 2064 6973  _  # pytype: dis
-00003a80: 6162 6c65 3d61 7474 7269 6275 7465 2d65  able=attribute-e
-00003a90: 7272 6f72 0a0a 2020 2320 5468 6520 6d65  rror..  # The me
-00003aa0: 7468 6f64 2073 686f 756c 6420 6265 2063  thod should be c
-00003ab0: 616c 6c61 626c 652c 2061 6e64 2069 7420  allable, and it 
-00003ac0: 7368 6f75 6c64 2068 6176 6520 6174 206c  should have at l
-00003ad0: 6561 7374 206f 6e65 2061 7267 756d 656e  east one argumen
-00003ae0: 740a 2020 2320 7265 7072 6573 656e 7469  t.  # representi
-00003af0: 6e67 2074 6865 2063 6c61 7373 2074 6861  ng the class tha
-00003b00: 7420 6973 2070 6173 7365 6420 696e 2e0a  t is passed in..
-00003b10: 2020 6966 2028 6e6f 7420 6361 6c6c 6162    if (not callab
-00003b20: 6c65 286d 6574 686f 645f 6f72 5f66 6e29  le(method_or_fn)
-00003b30: 206f 720a 2020 2020 2020 6c65 6e28 696e   or.      len(in
-00003b40: 7370 6563 742e 7369 676e 6174 7572 6528  spect.signature(
-00003b50: 6d65 7468 6f64 5f6f 725f 666e 292e 7061  method_or_fn).pa
-00003b60: 7261 6d65 7465 7273 2920 3c20 3129 3a0a  rameters) < 1):.
-00003b70: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
-00003b80: 2e41 7070 6c79 4d6f 6475 6c65 496e 7661  .ApplyModuleInva
-00003b90: 6c69 644d 6574 686f 6445 7272 6f72 286d  lidMethodError(m
-00003ba0: 6574 686f 645f 6f72 5f66 6e29 0a0a 2020  ethod_or_fn)..  
-00003bb0: 7265 7475 726e 206d 6574 686f 645f 6f72  return method_or
-00003bc0: 5f66 6e0a 0a0a 636c 6173 7320 5365 7475  _fn...class Setu
-00003bd0: 7053 7461 7465 2865 6e75 6d2e 496e 7445  pState(enum.IntE
-00003be0: 6e75 6d29 3a0a 2020 2320 7365 7475 7028  num):.  # setup(
-00003bf0: 2920 6861 7320 6e6f 7420 6265 656e 2063  ) has not been c
-00003c00: 616c 6c65 642e 0a20 204e 4557 203d 2030  alled..  NEW = 0
-00003c10: 0a20 2023 2073 6574 7570 2829 2068 6173  .  # setup() has
-00003c20: 2062 6565 6e20 6361 6c6c 6564 206f 7574   been called out
-00003c30: 7369 6465 2061 2074 7261 6e73 666f 726d  side a transform
-00003c40: 2062 6f75 6e64 6172 792e 0a20 2054 5241   boundary..  TRA
-00003c50: 4e53 464f 524d 4544 203d 2031 0a20 2023  NSFORMED = 1.  #
-00003c60: 2073 6574 7570 2829 2068 6173 2062 6565   setup() has bee
-00003c70: 6e20 6361 6c6c 6564 2e0a 2020 444f 4e45  n called..  DONE
-00003c80: 203d 2032 0a0a 0a40 6461 7461 636c 6173   = 2...@dataclas
-00003c90: 7365 732e 6461 7461 636c 6173 730a 636c  ses.dataclass.cl
-00003ca0: 6173 7320 5f4d 6f64 756c 6549 6e74 6572  ass _ModuleInter
-00003cb0: 6e61 6c53 7461 7465 3a0a 2020 2222 2245  nalState:.  """E
-00003cc0: 7068 656d 6572 616c 204d 6f64 756c 6520  phemeral Module 
-00003cd0: 4576 616c 7561 7469 6f6e 2053 7461 7465  Evaluation State
-00003ce0: 2e0a 0a20 2046 6f72 2063 6c61 7269 7479  ...  For clarity
-00003cf0: 2c20 7765 2063 6f6c 6c65 6374 2061 6c6c  , we collect all
-00003d00: 206f 6620 7468 6520 7465 6d70 6f72 6172   of the temporar
-00003d10: 7920 666c 6167 7320 616e 6420 6570 6865  y flags and ephe
-00003d20: 6d65 7261 6c20 7374 6174 6520 7573 6564  meral state used
-00003d30: 2062 790a 2020 4d6f 6475 6c65 7320 666f   by.  Modules fo
-00003d40: 7220 6175 746f 6e61 6d69 6e67 2061 6e64  r autonaming and
-00003d50: 2065 7272 6f72 206d 6573 7361 6765 7320   error messages 
-00003d60: 6865 7265 2c20 616c 6f6e 6773 6964 6520  here, alongside 
-00003d70: 7468 6520 7275 6c65 7320 7573 6564 0a20  the rules used. 
-00003d80: 2074 6f20 7061 7373 2074 6869 7320 6570   to pass this ep
-00003d90: 6865 6d65 7261 6c20 7374 6174 6520 6163  hemeral state ac
-00003da0: 726f 7373 2074 7261 6e73 666f 726d 2062  ross transform b
-00003db0: 6f75 6e64 6172 6965 732e 0a20 2022 2222  oundaries..  """
-00003dc0: 0a20 2069 6e5f 636f 6d70 6163 745f 6d65  .  in_compact_me
-00003dd0: 7468 6f64 3a20 626f 6f6c 203d 2046 616c  thod: bool = Fal
-00003de0: 7365 0a20 2069 6e5f 7365 7475 703a 2062  se.  in_setup: b
-00003df0: 6f6f 6c20 3d20 4661 6c73 650a 2020 7365  ool = False.  se
-00003e00: 7475 705f 6361 6c6c 6564 3a20 5365 7475  tup_called: Setu
-00003e10: 7053 7461 7465 203d 2053 6574 7570 5374  pState = SetupSt
-00003e20: 6174 652e 4e45 570a 2020 6973 5f69 6e69  ate.NEW.  is_ini
-00003e30: 7469 616c 697a 6564 3a20 626f 6f6c 203d  tialized: bool =
-00003e40: 2046 616c 7365 0a20 2061 7574 6f6e 616d   False.  autonam
-00003e50: 655f 6375 7273 6f72 3a20 4469 6374 5b73  e_cursor: Dict[s
-00003e60: 7472 2c20 696e 745d 203d 2064 6174 6163  tr, int] = datac
-00003e70: 6c61 7373 6573 2e66 6965 6c64 2864 6566  lasses.field(def
-00003e80: 6175 6c74 5f66 6163 746f 7279 3d64 6963  ault_factory=dic
-00003e90: 7429 0a20 2063 6869 6c64 7265 6e3a 2044  t).  children: D
-00003ea0: 6963 745b 7374 722c 2055 6e69 6f6e 5b73  ict[str, Union[s
-00003eb0: 7472 2c20 274d 6f64 756c 6527 5d5d 203d  tr, 'Module']] =
-00003ec0: 2064 6174 6163 6c61 7373 6573 2e66 6965   dataclasses.fie
-00003ed0: 6c64 280a 2020 2020 2020 6465 6661 756c  ld(.      defaul
-00003ee0: 745f 6661 6374 6f72 793d 6469 6374 290a  t_factory=dict).
-00003ef0: 0a20 2064 6566 2072 6573 6574 2873 656c  .  def reset(sel
-00003f00: 6629 202d 3e20 4e6f 6e65 3a0a 2020 2020  f) -> None:.    
-00003f10: 2222 2252 6573 6574 7320 7472 616e 7369  """Resets transi
-00003f20: 656e 7420 7374 6174 652e 0a0a 2020 2020  ent state...    
-00003f30: 5468 6973 2066 756e 6374 696f 6e20 6973  This function is
-00003f40: 2063 616c 6c65 6420 6166 7465 7220 6561   called after ea
-00003f50: 6368 206d 6f64 756c 6520 6d65 7468 6f64  ch module method
-00003f60: 2c20 736f 206f 6e6c 7920 6174 7472 6962  , so only attrib
-00003f70: 7574 6573 2074 6861 740a 2020 2020 6172  utes that.    ar
-00003f80: 6520 6d65 7468 6f64 2d64 6570 656e 6465  e method-depende
-00003f90: 6e74 2061 7265 2072 6573 6574 2e0a 2020  nt are reset..  
-00003fa0: 2020 2222 220a 2020 2020 7365 6c66 2e69    """.    self.i
-00003fb0: 6e5f 636f 6d70 6163 745f 6d65 7468 6f64  n_compact_method
-00003fc0: 203d 2046 616c 7365 0a20 2020 2073 656c   = False.    sel
-00003fd0: 662e 696e 5f73 6574 7570 203d 2046 616c  f.in_setup = Fal
-00003fe0: 7365 0a20 2020 2073 656c 662e 6175 746f  se.    self.auto
-00003ff0: 6e61 6d65 5f63 7572 736f 7220 3d20 6469  name_cursor = di
-00004000: 6374 2829 0a0a 2020 6465 6620 6578 706f  ct()..  def expo
-00004010: 7274 2873 656c 6629 202d 3e20 275f 4d6f  rt(self) -> '_Mo
-00004020: 6475 6c65 496e 7465 726e 616c 5374 6174  duleInternalStat
-00004030: 6527 3a0a 2020 2020 2222 2245 7870 6f72  e':.    """Expor
-00004040: 7473 2074 7261 6e73 666f 726d 2d70 7265  ts transform-pre
-00004050: 7365 7276 6564 2073 7461 7465 2061 6372  served state acr
-00004060: 6f73 7320 7472 616e 7366 6f72 6d20 626f  oss transform bo
-00004070: 756e 6461 7279 2e22 2222 0a20 2020 2073  undary.""".    s
-00004080: 6574 7570 5f73 7461 7465 203d 2053 6574  etup_state = Set
-00004090: 7570 5374 6174 652e 5452 414e 5346 4f52  upState.TRANSFOR
-000040a0: 4d45 4420 6966 2073 656c 662e 7365 7475  MED if self.setu
-000040b0: 705f 6361 6c6c 6564 2065 6c73 6520 5365  p_called else Se
-000040c0: 7475 7053 7461 7465 2e4e 4557 0a20 2020  tupState.NEW.   
-000040d0: 2063 6c6f 6e65 6420 3d20 5f4d 6f64 756c   cloned = _Modul
-000040e0: 6549 6e74 6572 6e61 6c53 7461 7465 280a  eInternalState(.
-000040f0: 2020 2020 2020 2020 696e 5f63 6f6d 7061          in_compa
-00004100: 6374 5f6d 6574 686f 643d 7365 6c66 2e69  ct_method=self.i
-00004110: 6e5f 636f 6d70 6163 745f 6d65 7468 6f64  n_compact_method
-00004120: 2c0a 2020 2020 2020 2020 696e 5f73 6574  ,.        in_set
-00004130: 7570 3d73 656c 662e 696e 5f73 6574 7570  up=self.in_setup
-00004140: 2c0a 2020 2020 2020 2020 7365 7475 705f  ,.        setup_
-00004150: 6361 6c6c 6564 3d73 6574 7570 5f73 7461  called=setup_sta
-00004160: 7465 2c0a 2020 2020 2020 2020 6973 5f69  te,.        is_i
-00004170: 6e69 7469 616c 697a 6564 3d73 656c 662e  nitialized=self.
-00004180: 6973 5f69 6e69 7469 616c 697a 6564 2c0a  is_initialized,.
-00004190: 2020 2020 2020 2020 6175 746f 6e61 6d65          autoname
-000041a0: 5f63 7572 736f 723d 6469 6374 2873 656c  _cursor=dict(sel
-000041b0: 662e 6175 746f 6e61 6d65 5f63 7572 736f  f.autoname_curso
-000041c0: 7229 290a 2020 2020 7265 7475 726e 2063  r)).    return c
-000041d0: 6c6f 6e65 640a 0a20 2064 6566 2072 6569  loned..  def rei
-000041e0: 6d70 6f72 7428 7365 6c66 2c20 6f74 6865  mport(self, othe
-000041f0: 723a 2027 5f4d 6f64 756c 6549 6e74 6572  r: '_ModuleInter
-00004200: 6e61 6c53 7461 7465 2729 202d 3e20 4e6f  nalState') -> No
-00004210: 6e65 3a0a 2020 2020 2222 2252 652d 696d  ne:.    """Re-im
-00004220: 706f 7274 7320 7472 616e 7366 6f72 6d2d  ports transform-
-00004230: 7072 6573 6572 7665 6420 7374 6174 6520  preserved state 
-00004240: 6672 6f6d 2061 6372 6f73 7320 7472 616e  from across tran
-00004250: 7366 6f72 6d20 626f 756e 6461 7279 2e22  sform boundary."
-00004260: 2222 0a20 2020 2073 656c 662e 696e 5f63  "".    self.in_c
-00004270: 6f6d 7061 6374 5f6d 6574 686f 6420 3d20  ompact_method = 
-00004280: 6f74 6865 722e 696e 5f63 6f6d 7061 6374  other.in_compact
-00004290: 5f6d 6574 686f 640a 2020 2020 7365 6c66  _method.    self
-000042a0: 2e69 6e5f 7365 7475 7020 3d20 6f74 6865  .in_setup = othe
-000042b0: 722e 696e 5f73 6574 7570 0a20 2020 2073  r.in_setup.    s
-000042c0: 656c 662e 6973 5f69 6e69 7469 616c 697a  elf.is_initializ
-000042d0: 6564 203d 206f 7468 6572 2e69 735f 696e  ed = other.is_in
-000042e0: 6974 6961 6c69 7a65 640a 2020 2020 7365  itialized.    se
-000042f0: 6c66 2e61 7574 6f6e 616d 655f 6375 7273  lf.autoname_curs
-00004300: 6f72 203d 2064 6963 7428 6f74 6865 722e  or = dict(other.
-00004310: 6175 746f 6e61 6d65 5f63 7572 736f 7229  autoname_cursor)
-00004320: 0a0a 5f75 6e69 6e69 7469 616c 697a 6564  .._uninitialized
-00004330: 5f6d 6f64 756c 655f 696e 7465 726e 616c  _module_internal
-00004340: 5f73 7461 7465 203d 205f 4d6f 6475 6c65  _state = _Module
-00004350: 496e 7465 726e 616c 5374 6174 6528 290a  InternalState().
-00004360: 0a0a 5f55 4e44 4546 494e 4544 5f43 4f50  .._UNDEFINED_COP
-00004370: 595f 5049 434b 4c45 5f4d 4554 484f 4453  Y_PICKLE_METHODS
-00004380: 203d 2028 0a20 2020 2027 5f5f 6765 7473   = (.    '__gets
-00004390: 7461 7465 5f5f 272c 2027 5f5f 7365 7473  tate__', '__sets
-000043a0: 7461 7465 5f5f 272c 2027 5f5f 6765 746e  tate__', '__getn
-000043b0: 6577 6172 6773 5f65 785f 5f27 2c0a 2020  ewargs_ex__',.  
-000043c0: 2020 275f 5f72 6564 7563 655f 5f27 2c20    '__reduce__', 
-000043d0: 275f 5f72 6564 7563 655f 6578 5f5f 272c  '__reduce_ex__',
-000043e0: 2027 5f5f 636f 7079 5f5f 272c 2027 5f5f   '__copy__', '__
-000043f0: 6465 6570 636f 7079 5f5f 2729 0a0a 0a5f  deepcopy__')..._
-00004400: 6361 6368 6573 3a20 2777 6561 6b72 6566  caches: 'weakref
-00004410: 2e57 6561 6b4b 6579 4469 6374 696f 6e61  .WeakKeyDictiona
-00004420: 7279 5b53 636f 7065 2c20 7765 616b 7265  ry[Scope, weakre
-00004430: 662e 5765 616b 5661 6c75 6544 6963 7469  f.WeakValueDicti
-00004440: 6f6e 6172 795b 466c 6178 4964 2c20 4d6f  onary[FlaxId, Mo
-00004450: 6475 6c65 5d5d 2720 3d20 280a 2020 2020  dule]]' = (.    
-00004460: 7765 616b 7265 662e 5765 616b 4b65 7944  weakref.WeakKeyD
-00004470: 6963 7469 6f6e 6172 7928 2929 0a0a 0a74  ictionary())...t
-00004480: 7570 6c65 5f72 6564 7563 6520 3d20 6c61  uple_reduce = la
-00004490: 6d62 6461 2078 732c 2078 3a20 7873 202b  mbda xs, x: xs +
-000044a0: 2028 782c 290a 7475 706c 655f 696e 6974   (x,).tuple_init
-000044b0: 203d 206c 616d 6264 613a 2028 290a 0a0a   = lambda: ()...
-000044c0: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
-000044d0: 6572 6d65 6469 6174 6573 203d 206c 616d  ermediates = lam
-000044e0: 6264 6120 5f2c 206d 6574 686f 645f 6e61  bda _, method_na
-000044f0: 6d65 3a20 6d65 7468 6f64 5f6e 616d 6520  me: method_name 
-00004500: 3d3d 2027 5f5f 6361 6c6c 5f5f 270a 0a0a  == '__call__'...
-00004510: 636c 6173 7320 5061 7265 6e74 4465 7363  class ParentDesc
-00004520: 7269 7074 6f72 3a0a 2020 2222 2257 7261  riptor:.  """Wra
-00004530: 7073 2070 6172 656e 7420 6d6f 6475 6c65  ps parent module
-00004540: 2072 6566 6572 656e 6365 7320 696e 2077   references in w
-00004550: 6561 6b20 7265 6673 2e0a 0a20 2054 6869  eak refs...  Thi
-00004560: 7320 7072 6576 656e 7473 2072 6566 6572  s prevents refer
-00004570: 656e 6365 2063 7963 6c65 7320 6672 6f6d  ence cycles from
-00004580: 2066 6f72 6d69 6e67 2076 6961 2070 6172   forming via par
-00004590: 656e 7420 6c69 6e6b 7320 7768 6963 6820  ent links which 
-000045a0: 6361 6e20 6c65 6164 0a20 2074 6f20 6163  can lead.  to ac
-000045b0: 6369 6465 6e74 616c 204f 4f4d 7320 696e  cidental OOMs in
-000045c0: 2065 6167 6572 206d 6f64 6520 6475 6520   eager mode due 
-000045d0: 746f 2073 6c6f 7720 6761 7262 6167 6520  to slow garbage 
-000045e0: 636f 6c6c 6563 7469 6f6e 2061 7320 7765  collection as we
-000045f0: 6c6c 2061 730a 2020 7370 7572 696f 7573  ll as.  spurious
-00004600: 2074 7261 6365 7220 6c65 616b 7320 6475   tracer leaks du
-00004610: 7269 6e67 206a 6974 2063 6f6d 7069 6c61  ring jit compila
-00004620: 7469 6f6e 2e0a 0a20 204e 6f74 653a 2022  tion...  Note: "
-00004630: 6465 7363 7269 7074 6f72 7322 2061 7265  descriptors" are
-00004640: 2074 6865 2075 6e64 6572 6c79 696e 6720   the underlying 
-00004650: 7079 7468 6f6e 206d 6563 6861 6e69 736d  python mechanism
-00004660: 2066 6f72 2069 6d70 6c65 6d65 6e74 696e   for implementin
-00004670: 670a 2020 6479 6e61 6d69 6320 4070 726f  g.  dynamic @pro
-00004680: 7065 7274 7920 6465 636f 7261 746f 7273  perty decorators
-00004690: 2e20 2057 6520 6e65 6564 2074 6f20 7573  .  We need to us
-000046a0: 6520 6120 7261 7720 6465 7363 7269 7074  e a raw descript
-000046b0: 6f72 2069 6e73 7465 6164 206f 6620 7468  or instead of th
-000046c0: 650a 2020 6d6f 7265 2063 6f6d 6d6f 6e20  e.  more common 
-000046d0: 6465 636f 7261 746f 7220 696e 206f 7264  decorator in ord
-000046e0: 6572 2074 6f20 666f 7263 6520 7468 6174  er to force that
-000046f0: 2074 6865 2061 7070 726f 7072 6961 7465   the appropriate
-00004700: 2067 6574 7465 722f 7365 7474 6572 0a20   getter/setter. 
-00004710: 206c 6f67 6963 2061 7070 6c69 6573 2069   logic applies i
-00004720: 6e20 7375 6263 6c61 7373 6573 2065 7665  n subclasses eve
-00004730: 6e20 6166 7465 7220 7661 7269 6f75 7320  n after various 
-00004740: 6461 7461 636c 6173 7320 7472 616e 7366  dataclass transf
-00004750: 6f72 6d73 2e0a 2020 2222 220a 2020 6465  orms..  """.  de
-00004760: 6620 5f5f 6765 745f 5f28 7365 6c66 2c20  f __get__(self, 
-00004770: 6f62 6a2c 206f 626a 7479 7065 3d4e 6f6e  obj, objtype=Non
-00004780: 6529 3a0a 2020 2020 2320 6368 6563 6b20  e):.    # check 
-00004790: 6966 206f 626a 2069 7320 4e6f 6e65 2c20  if obj is None, 
-000047a0: 6861 7070 656e 7320 6475 7269 6e67 2025  happens during %
-000047b0: 6175 746f 7265 6c6f 6164 0a20 2020 2069  autoreload.    i
-000047c0: 6620 6f62 6a20 6973 204e 6f6e 653a 0a20  f obj is None:. 
-000047d0: 2020 2020 2072 6574 7572 6e20 4e6f 6e65       return None
-000047e0: 0a20 2020 2070 6172 656e 7420 3d20 6f62  .    parent = ob
-000047f0: 6a65 6374 2e5f 5f67 6574 6174 7472 6962  ject.__getattrib
-00004800: 7574 655f 5f28 6f62 6a2c 2022 5f70 6172  ute__(obj, "_par
-00004810: 656e 745f 7265 6622 290a 2020 2020 7265  ent_ref").    re
-00004820: 7475 726e 2070 6172 656e 7428 2920 6966  turn parent() if
-00004830: 2069 7369 6e73 7461 6e63 6528 7061 7265   isinstance(pare
-00004840: 6e74 2c20 7765 616b 7265 662e 5265 6665  nt, weakref.Refe
-00004850: 7265 6e63 6554 7970 6529 2065 6c73 6520  renceType) else 
-00004860: 7061 7265 6e74 0a0a 2020 6465 6620 5f5f  parent..  def __
-00004870: 7365 745f 5f28 7365 6c66 2c20 6f62 6a2c  set__(self, obj,
-00004880: 2076 616c 7565 293a 0a20 2020 206d 6179   value):.    may
-00004890: 6265 5f77 6561 6b20 3d20 7765 616b 7265  be_weak = weakre
-000048a0: 662e 7265 6628 7661 6c75 6529 2069 6620  f.ref(value) if 
-000048b0: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
-000048c0: 2c20 4d6f 6475 6c65 2920 656c 7365 2076  , Module) else v
-000048d0: 616c 7565 0a20 2020 206f 626a 6563 742e  alue.    object.
-000048e0: 5f5f 7365 7461 7474 725f 5f28 6f62 6a2c  __setattr__(obj,
-000048f0: 2022 5f70 6172 656e 745f 7265 6622 2c20   "_parent_ref", 
-00004900: 6d61 7962 655f 7765 616b 290a 0a0a 636c  maybe_weak)...cl
-00004910: 6173 7320 4465 7363 7269 7074 6f72 2850  ass Descriptor(P
-00004920: 726f 746f 636f 6c29 3a0a 2020 5f5f 6973  rotocol):.  __is
-00004930: 6162 7374 7261 6374 6d65 7468 6f64 5f5f  abstractmethod__
-00004940: 3a20 626f 6f6c 0a20 2064 6566 205f 5f67  : bool.  def __g
-00004950: 6574 5f5f 2873 656c 662c 206f 626a 2c20  et__(self, obj, 
-00004960: 6f62 6a74 7970 653d 4e6f 6e65 2920 2d3e  objtype=None) ->
-00004970: 2041 6e79 3a20 2e2e 2e0a 2020 6465 6620   Any: ....  def 
-00004980: 5f5f 7365 745f 5f28 7365 6c66 2c20 6f62  __set__(self, ob
-00004990: 6a2c 2076 616c 7565 2920 2d3e 204e 6f6e  j, value) -> Non
-000049a0: 653a 202e 2e2e 0a20 2064 6566 205f 5f64  e: ....  def __d
-000049b0: 656c 6574 655f 5f28 7365 6c66 2c20 6f62  elete__(self, ob
-000049c0: 6a29 202d 3e20 4e6f 6e65 3a20 2e2e 2e0a  j) -> None: ....
-000049d0: 2020 6465 6620 5f5f 7365 745f 6e61 6d65    def __set_name
-000049e0: 5f5f 2873 656c 662c 206f 776e 6572 2c20  __(self, owner, 
-000049f0: 6e61 6d65 2920 2d3e 204e 6f6e 653a 202e  name) -> None: .
-00004a00: 2e2e 0a0a 636c 6173 7320 4465 7363 7269  ....class Descri
-00004a10: 7074 6f72 5772 6170 7065 723a 0a20 2070  ptorWrapper:.  p
-00004a20: 6173 730a 0a64 6566 2063 7265 6174 655f  ass..def create_
-00004a30: 6465 7363 7269 7074 6f72 5f77 7261 7070  descriptor_wrapp
-00004a40: 6572 2864 6573 6372 6970 746f 723a 2044  er(descriptor: D
-00004a50: 6573 6372 6970 746f 7229 3a0a 2020 2222  escriptor):.  ""
-00004a60: 2243 7265 6174 6573 2061 2064 6573 6372  "Creates a descr
-00004a70: 6970 746f 7220 7772 6170 7065 7220 7468  iptor wrapper th
-00004a80: 6174 2063 616c 6c73 2061 2067 6574 5f66  at calls a get_f
-00004a90: 6e20 6f6e 2074 6865 2064 6573 6372 6970  n on the descrip
-00004aa0: 746f 722e 2222 220a 0a20 2063 6c61 7373  tor."""..  class
-00004ab0: 205f 4465 7363 7269 7074 6f72 5772 6170   _DescriptorWrap
-00004ac0: 7065 7228 4465 7363 7269 7074 6f72 5772  per(DescriptorWr
-00004ad0: 6170 7065 7229 3a0a 2020 2020 2222 2241  apper):.    """A
-00004ae0: 2064 6573 6372 6970 746f 7220 7468 6174   descriptor that
-00004af0: 2063 616e 2077 7261 7020 616e 7920 6465   can wrap any de
-00004b00: 7363 7269 7074 6f72 2222 220a 0a20 2020  scriptor"""..   
-00004b10: 2069 6620 6861 7361 7474 7228 6465 7363   if hasattr(desc
-00004b20: 7269 7074 6f72 2c20 275f 5f69 7361 6273  riptor, '__isabs
-00004b30: 7472 6163 746d 6574 686f 645f 5f27 293a  tractmethod__'):
-00004b40: 0a20 2020 2020 205f 5f69 7361 6273 7472  .      __isabstr
-00004b50: 6163 746d 6574 686f 645f 5f20 3d20 6465  actmethod__ = de
-00004b60: 7363 7269 7074 6f72 2e5f 5f69 7361 6273  scriptor.__isabs
-00004b70: 7472 6163 746d 6574 686f 645f 5f0a 0a20  tractmethod__.. 
-00004b80: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
-00004b90: 7365 6c66 2c20 7772 6170 7065 643a 2044  self, wrapped: D
-00004ba0: 6573 6372 6970 746f 7229 3a0a 2020 2020  escriptor):.    
-00004bb0: 2020 7365 6c66 2e77 7261 7070 6564 203d    self.wrapped =
-00004bc0: 2077 7261 7070 6564 0a0a 2020 2020 2320   wrapped..    # 
-00004bd0: 636f 6e64 6974 696f 6e61 6c6c 7920 6465  conditionally de
-00004be0: 6669 6e65 2064 6573 6372 6970 746f 7220  fine descriptor 
-00004bf0: 6d65 7468 6f64 730a 2020 2020 6966 2068  methods.    if h
-00004c00: 6173 6174 7472 2864 6573 6372 6970 746f  asattr(descripto
-00004c10: 722c 2027 5f5f 6765 745f 5f27 293a 0a20  r, '__get__'):. 
-00004c20: 2020 2020 2064 6566 205f 5f67 6574 5f5f       def __get__
-00004c30: 2873 656c 662c 202a 6172 6773 2c20 2a2a  (self, *args, **
-00004c40: 6b77 6172 6773 293a 0a20 2020 2020 2020  kwargs):.       
-00004c50: 2023 2068 6572 6520 7765 2077 696c 6c20   # here we will 
-00004c60: 6361 7463 6820 696e 7465 726e 616c 2041  catch internal A
-00004c70: 7474 7269 6275 7465 4572 726f 7220 616e  ttributeError an
-00004c80: 6420 7265 2d72 6169 7365 2069 7420 6173  d re-raise it as
-00004c90: 2061 0a20 2020 2020 2020 2023 206d 6f72   a.        # mor
-00004ca0: 6520 696e 666f 726d 6174 6976 6520 616e  e informative an
-00004cb0: 6420 636f 7272 6563 7420 6572 726f 7220  d correct error 
-00004cc0: 6d65 7373 6167 652e 0a20 2020 2020 2020  message..       
-00004cd0: 2074 7279 3a0a 2020 2020 2020 2020 2020   try:.          
-00004ce0: 7265 7475 726e 2073 656c 662e 7772 6170  return self.wrap
-00004cf0: 7065 642e 5f5f 6765 745f 5f28 2a61 7267  ped.__get__(*arg
-00004d00: 732c 202a 2a6b 7761 7267 7329 0a20 2020  s, **kwargs).   
-00004d10: 2020 2020 2065 7863 6570 7420 4174 7472       except Attr
-00004d20: 6962 7574 6545 7272 6f72 2061 7320 653a  ibuteError as e:
-00004d30: 0a20 2020 2020 2020 2020 2072 6169 7365  .          raise
-00004d40: 2065 7272 6f72 732e 4465 7363 7269 7074   errors.Descript
-00004d50: 6f72 4174 7472 6962 7574 6545 7272 6f72  orAttributeError
-00004d60: 2829 2066 726f 6d20 650a 0a20 2020 2069  () from e..    i
-00004d70: 6620 6861 7361 7474 7228 6465 7363 7269  f hasattr(descri
-00004d80: 7074 6f72 2c20 275f 5f73 6574 5f5f 2729  ptor, '__set__')
-00004d90: 3a0a 2020 2020 2020 6465 6620 5f5f 7365  :.      def __se
-00004da0: 745f 5f28 7365 6c66 2c20 2a61 7267 732c  t__(self, *args,
-00004db0: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
-00004dc0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00004dd0: 7772 6170 7065 642e 5f5f 7365 745f 5f28  wrapped.__set__(
-00004de0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-00004df0: 0a0a 2020 2020 6966 2068 6173 6174 7472  ..    if hasattr
-00004e00: 2864 6573 6372 6970 746f 722c 2027 5f5f  (descriptor, '__
-00004e10: 6465 6c65 7465 5f5f 2729 3a0a 2020 2020  delete__'):.    
-00004e20: 2020 6465 6620 5f5f 6465 6c65 7465 5f5f    def __delete__
-00004e30: 2873 656c 662c 202a 6172 6773 2c20 2a2a  (self, *args, **
-00004e40: 6b77 6172 6773 293a 0a20 2020 2020 2020  kwargs):.       
-00004e50: 2072 6574 7572 6e20 7365 6c66 2e77 7261   return self.wra
-00004e60: 7070 6564 2e5f 5f64 656c 6574 655f 5f28  pped.__delete__(
-00004e70: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-00004e80: 0a0a 2020 2020 6966 2068 6173 6174 7472  ..    if hasattr
-00004e90: 2864 6573 6372 6970 746f 722c 2027 5f5f  (descriptor, '__
-00004ea0: 7365 745f 6e61 6d65 5f5f 2729 3a0a 2020  set_name__'):.  
-00004eb0: 2020 2020 6465 6620 5f5f 7365 745f 6e61      def __set_na
-00004ec0: 6d65 5f5f 2873 656c 662c 202a 6172 6773  me__(self, *args
-00004ed0: 2c20 2a2a 6b77 6172 6773 293a 0a20 2020  , **kwargs):.   
-00004ee0: 2020 2020 2073 656c 662e 7772 6170 7065       self.wrappe
-00004ef0: 642e 5f5f 7365 745f 6e61 6d65 5f5f 282a  d.__set_name__(*
-00004f00: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
-00004f10: 0a20 2020 2064 6566 205f 5f67 6574 6174  .    def __getat
-00004f20: 7472 5f5f 2873 656c 662c 206e 616d 6529  tr__(self, name)
-00004f30: 3a0a 2020 2020 2020 7265 7475 726e 2067  :.      return g
-00004f40: 6574 6174 7472 2873 656c 662e 7772 6170  etattr(self.wrap
-00004f50: 7065 642c 206e 616d 6529 0a0a 2020 7265  ped, name)..  re
-00004f60: 7475 726e 205f 4465 7363 7269 7074 6f72  turn _Descriptor
-00004f70: 5772 6170 7065 7228 6465 7363 7269 7074  Wrapper(descript
-00004f80: 6f72 290a 0a23 2042 6173 6520 4d6f 6475  or)..# Base Modu
-00004f90: 6c65 2064 6566 696e 6974 696f 6e2e 0a23  le definition..#
-00004fa0: 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d   ---------------
-00004fb0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000430: 7274 2066 6c61 780a 696d 706f 7274 2066  rt flax.import f
+00000440: 6c61 782e 6c69 6e65 6e20 6173 206e 6e0a  lax.linen as nn.
+00000450: 6672 6f6d 2066 6c61 7820 696d 706f 7274  from flax import
+00000460: 2028 636f 6e66 6967 2c20 636f 7265 2c20   (config, core, 
+00000470: 6572 726f 7273 2c20 7365 7269 616c 697a  errors, serializ
+00000480: 6174 696f 6e2c 2074 7261 6365 6261 636b  ation, traceback
+00000490: 5f75 7469 6c2c 0a20 2020 2020 2020 2020  _util,.         
+000004a0: 2020 2020 2020 2020 2074 7261 7665 7273           travers
+000004b0: 655f 7574 696c 290a 6672 6f6d 2066 6c61  e_util).from fla
+000004c0: 782e 636f 7265 2069 6d70 6f72 7420 5363  x.core import Sc
+000004d0: 6f70 650a 6672 6f6d 2066 6c61 782e 636f  ope.from flax.co
+000004e0: 7265 2069 6d70 6f72 7420 7061 7274 6961  re import partia
+000004f0: 6c5f 6576 616c 0a66 726f 6d20 666c 6178  l_eval.from flax
+00000500: 2e63 6f72 652e 6672 6f7a 656e 5f64 6963  .core.frozen_dic
+00000510: 7420 696d 706f 7274 2046 726f 7a65 6e44  t import FrozenD
+00000520: 6963 740a 6672 6f6d 2066 6c61 782e 636f  ict.from flax.co
+00000530: 7265 2e73 636f 7065 2069 6d70 6f72 7420  re.scope import 
+00000540: 2820 2023 2070 796c 696e 743a 2064 6973  (  # pylint: dis
+00000550: 6162 6c65 3d67 2d6d 756c 7469 706c 652d  able=g-multiple-
+00000560: 696d 706f 7274 0a20 2020 2043 6f6c 6c65  import.    Colle
+00000570: 6374 696f 6e46 696c 7465 722c 2044 656e  ctionFilter, Den
+00000580: 794c 6973 742c 2046 726f 7a65 6e56 6172  yList, FrozenVar
+00000590: 6961 626c 6544 6963 742c 2056 6172 6961  iableDict, Varia
+000005a0: 626c 652c 2056 6172 6961 626c 6544 6963  ble, VariableDic
+000005b0: 742c 0a20 2020 2075 6e69 6f6e 5f66 696c  t,.    union_fil
+000005c0: 7465 7273 290a 6672 6f6d 2066 6c61 782e  ters).from flax.
+000005d0: 6964 7320 696d 706f 7274 2046 6c61 7849  ids import FlaxI
+000005e0: 640a 6672 6f6d 2066 6c61 782e 6964 7320  d.from flax.ids 
+000005f0: 696d 706f 7274 2075 7569 640a 6672 6f6d  import uuid.from
+00000600: 2066 6c61 782e 6c69 6e65 6e20 696d 706f   flax.linen impo
+00000610: 7274 206b 775f 6f6e 6c79 5f64 6174 6163  rt kw_only_datac
+00000620: 6c61 7373 6573 0a0a 0a74 7261 6365 6261  lasses...traceba
+00000630: 636b 5f75 7469 6c2e 7265 6769 7374 6572  ck_util.register
+00000640: 5f65 7863 6c75 7369 6f6e 285f 5f66 696c  _exclusion(__fil
+00000650: 655f 5f29 0a0a 4b65 7941 7272 6179 203d  e__)..KeyArray =
+00000660: 2055 6e69 6f6e 5b6a 6178 2e41 7272 6179   Union[jax.Array
+00000670: 2c20 6a61 782e 7261 6e64 6f6d 2e4b 6579  , jax.random.Key
+00000680: 4172 7261 795d 2020 2320 7079 6c69 6e74  Array]  # pylint
+00000690: 3a20 6469 7361 626c 653d 696e 7661 6c69  : disable=invali
+000006a0: 642d 6e61 6d65 0a52 4e47 5365 7175 656e  d-name.RNGSequen
+000006b0: 6365 7320 3d20 4469 6374 5b73 7472 2c20  ces = Dict[str, 
+000006c0: 4b65 7941 7272 6179 5d0a 4172 7261 7920  KeyArray].Array 
+000006d0: 3d20 416e 7920 2020 2023 2070 796c 696e  = Any    # pylin
+000006e0: 743a 2064 6973 6162 6c65 3d69 6e76 616c  t: disable=inval
+000006f0: 6964 2d6e 616d 650a 0a0a 5420 3d20 5479  id-name...T = Ty
+00000700: 7065 5661 7228 2754 2729 0a4b 203d 2054  peVar('T').K = T
+00000710: 7970 6556 6172 2827 4b27 290a 4d20 3d20  ypeVar('K').M = 
+00000720: 5479 7065 5661 7228 274d 272c 2062 6f75  TypeVar('M', bou
+00000730: 6e64 3d27 4d6f 6475 6c65 2729 0a5f 4361  nd='Module')._Ca
+00000740: 6c6c 6162 6c65 5420 3d20 5479 7065 5661  llableT = TypeVa
+00000750: 7228 275f 4361 6c6c 6162 6c65 5427 2c20  r('_CallableT', 
+00000760: 626f 756e 643d 4361 6c6c 6162 6c65 290a  bound=Callable).
+00000770: 0a0a 2320 5573 6564 2066 6f72 2061 6273  ..# Used for abs
+00000780: 7472 6163 746c 7920 7465 7374 696e 6720  tractly testing 
+00000790: 6d6f 6475 6c65 2062 6568 6176 696f 722e  module behavior.
+000007a0: 0a54 6573 7453 636f 7065 203d 2074 7970  .TestScope = typ
+000007b0: 6528 2754 6573 7453 636f 7065 272c 0a20  e('TestScope',. 
+000007c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000007d0: 2853 636f 7065 2c29 2c0a 2020 2020 2020  (Scope,),.      
+000007e0: 2020 2020 2020 2020 2020 207b 276d 616b             {'mak
+000007f0: 655f 726e 6727 3a20 6c61 6d62 6461 2073  e_rng': lambda s
+00000800: 656c 662c 206e 616d 653a 206a 6178 2e72  elf, name: jax.r
+00000810: 616e 646f 6d2e 5052 4e47 4b65 7928 3029  andom.PRNGKey(0)
+00000820: 7d29 0a0a 0a23 2070 796c 696e 743a 2064  })...# pylint: d
+00000830: 6973 6162 6c65 3d70 726f 7465 6374 6564  isable=protected
+00000840: 2d61 6363 6573 732c 6174 7472 6962 7574  -access,attribut
+00000850: 652d 6465 6669 6e65 642d 6f75 7473 6964  e-defined-outsid
+00000860: 652d 696e 6974 0a0a 6465 6620 5f69 6e64  e-init..def _ind
+00000870: 656e 7428 783a 2073 7472 2c20 6e75 6d5f  ent(x: str, num_
+00000880: 7370 6163 6573 3a20 696e 7429 3a0a 2020  spaces: int):.  
+00000890: 696e 6465 6e74 5f73 7472 203d 2027 2027  indent_str = ' '
+000008a0: 202a 206e 756d 5f73 7061 6365 730a 2020   * num_spaces.  
+000008b0: 6c69 6e65 7320 3d20 782e 7370 6c69 7428  lines = x.split(
+000008c0: 275c 6e27 290a 2020 2320 736b 6970 206c  '\n').  # skip l
+000008d0: 6173 7420 6c69 6e65 2062 6563 6175 7365  ast line because
+000008e0: 2069 7420 6973 2061 6c77 6179 7320 656d   it is always em
+000008f0: 7074 7920 616e 6420 7368 6f75 6c64 206e  pty and should n
+00000900: 6f74 2062 6520 696e 6465 6e74 6564 2e0a  ot be indented..
+00000910: 2020 6173 7365 7274 206e 6f74 206c 696e    assert not lin
+00000920: 6573 5b2d 315d 0a20 2072 6574 7572 6e20  es[-1].  return 
+00000930: 275c 6e27 2e6a 6f69 6e28 696e 6465 6e74  '\n'.join(indent
+00000940: 5f73 7472 202b 206c 696e 6520 666f 7220  _str + line for 
+00000950: 6c69 6e65 2069 6e20 6c69 6e65 735b 3a2d  line in lines[:-
+00000960: 315d 2920 2b20 275c 6e27 0a0a 0a64 6566  1]) + '\n'...def
+00000970: 205f 6174 7472 5f72 6570 7228 7661 6c75   _attr_repr(valu
+00000980: 653a 2041 6e79 293a 0a20 2069 6620 6361  e: Any):.  if ca
+00000990: 6c6c 6162 6c65 2876 616c 7565 2920 616e  llable(value) an
+000009a0: 6420 280a 2020 2020 2020 2869 7369 6e73  d (.      (isins
+000009b0: 7461 6e63 6528 7661 6c75 652c 206e 6e2e  tance(value, nn.
+000009c0: 4d6f 6475 6c65 2920 616e 6420 7661 6c75  Module) and valu
+000009d0: 652e 5f5f 6469 6374 5f5f 2e67 6574 2827  e.__dict__.get('
+000009e0: 5f5f 6e61 6d65 5f5f 272c 204e 6f6e 6529  __name__', None)
+000009f0: 290a 2020 2020 2020 6f72 2028 6e6f 7420  ).      or (not 
+00000a00: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
+00000a10: 2c20 6e6e 2e4d 6f64 756c 6529 2061 6e64  , nn.Module) and
+00000a20: 2067 6574 6174 7472 2876 616c 7565 2c20   getattr(value, 
+00000a30: 275f 5f6e 616d 655f 5f27 2c20 4e6f 6e65  '__name__', None
+00000a40: 2929 0a20 2029 3a0a 2020 2020 7661 6c75  )).  ):.    valu
+00000a50: 655f 7265 7020 3d20 7661 6c75 652e 5f5f  e_rep = value.__
+00000a60: 6e61 6d65 5f5f 0a20 2065 6c73 653a 0a20  name__.  else:. 
+00000a70: 2020 2076 616c 7565 5f72 6570 203d 2072     value_rep = r
+00000a80: 6570 7228 7661 6c75 6529 0a20 2072 6574  epr(value).  ret
+00000a90: 7572 6e20 7661 6c75 655f 7265 700a 0a0a  urn value_rep...
+00000aa0: 6465 6620 5f6d 6f64 756c 655f 7265 7072  def _module_repr
+00000ab0: 286d 6f64 756c 653a 2027 4d6f 6475 6c65  (module: 'Module
+00000ac0: 272c 206e 756d 5f73 7061 6365 733a 2069  ', num_spaces: i
+00000ad0: 6e74 203d 2034 293a 0a20 2022 2222 5265  nt = 4):.  """Re
+00000ae0: 7475 726e 7320 6120 7072 6574 7479 2070  turns a pretty p
+00000af0: 7269 6e74 6564 2072 6570 7265 7365 6e74  rinted represent
+00000b00: 6174 696f 6e20 6f66 2074 6865 206d 6f64  ation of the mod
+00000b10: 756c 652e 2222 220a 2020 636c 7320 3d20  ule.""".  cls = 
+00000b20: 7479 7065 286d 6f64 756c 6529 0a20 2063  type(module).  c
+00000b30: 6c73 5f6e 616d 6520 3d20 636c 732e 5f5f  ls_name = cls.__
+00000b40: 6e61 6d65 5f5f 0a20 2072 6570 203d 2027  name__.  rep = '
+00000b50: 270a 0a20 2061 7474 7269 6275 7465 7320  '..  attributes 
+00000b60: 3d20 7b0a 2020 2020 2020 662e 6e61 6d65  = {.      f.name
+00000b70: 3a20 662e 7479 7065 0a20 2020 2020 2066  : f.type.      f
+00000b80: 6f72 2066 2069 6e20 6461 7461 636c 6173  or f in dataclas
+00000b90: 7365 732e 6669 656c 6473 2863 6c73 290a  ses.fields(cls).
+00000ba0: 2020 2020 2020 6966 2066 2e6e 616d 6520        if f.name 
+00000bb0: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
+00000bc0: 2c20 276e 616d 6527 2920 616e 6420 662e  , 'name') and f.
+00000bd0: 7265 7072 0a20 207d 0a20 2063 6869 6c64  repr.  }.  child
+00000be0: 5f6d 6f64 756c 6573 203d 207b 6b3a 2076  _modules = {k: v
+00000bf0: 2066 6f72 206b 2c20 7620 696e 206d 6f64   for k, v in mod
+00000c00: 756c 652e 5f73 7461 7465 2e63 6869 6c64  ule._state.child
+00000c10: 7265 6e2e 6974 656d 7328 2920 2023 2070  ren.items()  # p
+00000c20: 7974 7970 653a 2064 6973 6162 6c65 3d61  ytype: disable=a
+00000c30: 7474 7269 6275 7465 2d65 7272 6f72 0a20  ttribute-error. 
+00000c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000c50: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+00000c60: 762c 204d 6f64 756c 6529 7d0a 2020 6966  v, Module)}.  if
+00000c70: 2061 7474 7269 6275 7465 733a 0a20 2020   attributes:.   
+00000c80: 2072 6570 202b 3d20 2723 2061 7474 7269   rep += '# attri
+00000c90: 6275 7465 735c 6e27 0a20 2020 2066 6f72  butes\n'.    for
+00000ca0: 2061 7474 7220 696e 2061 7474 7269 6275   attr in attribu
+00000cb0: 7465 732e 6b65 7973 2829 3a0a 2020 2020  tes.keys():.    
+00000cc0: 2020 2320 544f 444f 286a 6865 656b 293a    # TODO(jheek):
+00000cd0: 2063 616e 2077 6520 6765 7420 6120 6e69   can we get a ni
+00000ce0: 6365 2073 7472 696e 6720 7265 7072 6573  ce string repres
+00000cf0: 656e 7461 7469 6f6e 206f 6620 6174 7472  entation of attr
+00000d00: 6962 7574 6520 7479 7065 733f 0a20 2020  ibute types?.   
+00000d10: 2020 2076 616c 7565 203d 206d 6f64 756c     value = modul
+00000d20: 652e 5f5f 6469 6374 5f5f 2e67 6574 2861  e.__dict__.get(a
+00000d30: 7474 722c 204e 6f6e 6529 0a20 2020 2020  ttr, None).     
+00000d40: 2076 616c 7565 5f72 6570 203d 205f 6174   value_rep = _at
+00000d50: 7472 5f72 6570 7228 7661 6c75 6529 0a20  tr_repr(value). 
+00000d60: 2020 2020 2072 6570 202b 3d20 6627 7b61       rep += f'{a
+00000d70: 7474 727d 203d 207b 7661 6c75 655f 7265  ttr} = {value_re
+00000d80: 707d 5c6e 270a 2020 6966 2063 6869 6c64  p}\n'.  if child
+00000d90: 5f6d 6f64 756c 6573 3a0a 2020 2020 7265  _modules:.    re
+00000da0: 7020 2b3d 2027 2320 6368 696c 6472 656e  p += '# children
+00000db0: 5c6e 270a 2020 2020 666f 7220 6e61 6d65  \n'.    for name
+00000dc0: 2c20 6368 696c 6420 696e 2063 6869 6c64  , child in child
+00000dd0: 5f6d 6f64 756c 6573 2e69 7465 6d73 2829  _modules.items()
+00000de0: 3a0a 2020 2020 2020 6368 696c 645f 7265  :.      child_re
+00000df0: 7020 3d20 5f6d 6f64 756c 655f 7265 7072  p = _module_repr
+00000e00: 2863 6869 6c64 2c20 6e75 6d5f 7370 6163  (child, num_spac
+00000e10: 6573 290a 2020 2020 2020 7265 7020 2b3d  es).      rep +=
+00000e20: 2066 277b 6e61 6d65 7d20 3d20 7b63 6869   f'{name} = {chi
+00000e30: 6c64 5f72 6570 7d5c 6e27 0a20 2069 6620  ld_rep}\n'.  if 
+00000e40: 7265 703a 0a20 2020 2072 6574 7572 6e20  rep:.    return 
+00000e50: 6627 7b63 6c73 5f6e 616d 657d 285c 6e7b  f'{cls_name}(\n{
+00000e60: 5f69 6e64 656e 7428 7265 702c 206e 756d  _indent(rep, num
+00000e70: 5f73 7061 6365 7329 7d29 270a 2020 656c  _spaces)})'.  el
+00000e80: 7365 3a0a 2020 2020 7265 7475 726e 2066  se:.    return f
+00000e90: 277b 636c 735f 6e61 6d65 7d28 2927 0a0a  '{cls_name}()'..
+00000ea0: 2320 5461 6275 6c61 7469 6f6e 2075 7469  # Tabulation uti
+00000eb0: 6c69 7469 6573 2e0a 2320 2d2d 2d2d 2d2d  lities..# ------
+00000ec0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000ed0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000ee0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000ef0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000f00: 2d2d 2d2d 2d2d 2d0a 0a5f 6669 6e64 5f6e  -------.._find_n
+00000f10: 6f6e 5f6c 6966 7465 645f 6d6f 6475 6c65  on_lifted_module
+00000f20: 203d 2072 652e 636f 6d70 696c 6528 7227   = re.compile(r'
+00000f30: 2e2a 5c28 282e 2a29 5c29 2729 0a0a 6465  .*\((.*)\)')..de
+00000f40: 6620 5f66 6978 5f70 6174 685f 7061 7274  f _fix_path_part
+00000f50: 2870 6172 743a 2073 7472 293a 0a20 2022  (part: str):.  "
+00000f60: 2222 4669 7865 7320 6120 7061 7468 2070  ""Fixes a path p
+00000f70: 6172 7420 6279 2072 656d 6f76 696e 6720  art by removing 
+00000f80: 7472 616e 7366 6f72 6d61 7469 6f6e 206e  transformation n
+00000f90: 616d 6520 616e 6420 7061 7265 6e74 6865  ame and parenthe
+00000fa0: 7369 7320 736f 6d65 7469 6d65 730a 2020  sis sometimes.  
+00000fb0: 696e 7365 7274 6564 2062 7920 6c69 6674  inserted by lift
+00000fc0: 6564 2074 7261 6e73 666f 726d 6174 696f  ed transformatio
+00000fd0: 6e73 2222 220a 2020 6d61 7463 6820 3d20  ns""".  match = 
+00000fe0: 5f66 696e 645f 6e6f 6e5f 6c69 6674 6564  _find_non_lifted
+00000ff0: 5f6d 6f64 756c 652e 6d61 7463 6828 7061  _module.match(pa
+00001000: 7274 290a 2020 6966 206d 6174 6368 3a0a  rt).  if match:.
+00001010: 2020 2020 7265 7475 726e 206d 6174 6368      return match
+00001020: 2e67 726f 7570 2831 290a 2020 7265 7475  .group(1).  retu
+00001030: 726e 2070 6172 740a 0a40 6461 7461 636c  rn part..@datacl
+00001040: 6173 7365 732e 6461 7461 636c 6173 730a  asses.dataclass.
+00001050: 636c 6173 7320 5f43 616c 6c49 6e66 6f3a  class _CallInfo:
+00001060: 0a20 2069 6e64 6578 3a20 696e 740a 2020  .  index: int.  
+00001070: 7061 7468 3a20 5475 706c 655b 7374 722c  path: Tuple[str,
+00001080: 202e 2e2e 5d0a 2020 6d6f 6475 6c65 5f74   ...].  module_t
+00001090: 7970 653a 2054 7970 655b 274d 6f64 756c  ype: Type['Modul
+000010a0: 6527 5d0a 2020 6d65 7468 6f64 3a20 7374  e'].  method: st
+000010b0: 720a 2020 6172 6773 3a20 5475 706c 655b  r.  args: Tuple[
+000010c0: 416e 792c 202e 2e2e 5d0a 2020 6b77 6172  Any, ...].  kwar
+000010d0: 6773 3a20 4469 6374 5b73 7472 2c20 416e  gs: Dict[str, An
+000010e0: 795d 0a20 206f 7574 7075 7473 3a20 416e  y].  outputs: An
+000010f0: 790a 0a40 6461 7461 636c 6173 7365 732e  y..@dataclasses.
+00001100: 6461 7461 636c 6173 730a 636c 6173 7320  dataclass.class 
+00001110: 5f43 616c 6c49 6e66 6f43 6f6e 7465 7874  _CallInfoContext
+00001120: 2874 6872 6561 6469 6e67 2e6c 6f63 616c  (threading.local
+00001130: 293a 0a20 2069 6e64 6578 3a20 696e 740a  ):.  index: int.
+00001140: 2020 6361 6c6c 733a 204c 6973 745b 5f43    calls: List[_C
+00001150: 616c 6c49 6e66 6f5d 0a0a 2020 6465 6620  allInfo]..  def 
+00001160: 6765 745f 6361 6c6c 5f69 6e64 6578 2873  get_call_index(s
+00001170: 656c 662c 206d 6f64 756c 653a 2027 4d6f  elf, module: 'Mo
+00001180: 6475 6c65 2729 202d 3e20 696e 743a 0a20  dule') -> int:. 
+00001190: 2020 2069 6e64 6578 203d 2073 656c 662e     index = self.
+000011a0: 696e 6465 780a 2020 2020 7365 6c66 2e69  index.    self.i
+000011b0: 6e64 6578 202b 3d20 310a 2020 2020 7265  ndex += 1.    re
+000011c0: 7475 726e 2069 6e64 6578 0a0a 4063 6f6e  turn index..@con
+000011d0: 7465 7874 6c69 622e 636f 6e74 6578 746d  textlib.contextm
+000011e0: 616e 6167 6572 0a64 6566 205f 7461 6275  anager.def _tabu
+000011f0: 6c61 7465 5f63 6f6e 7465 7874 2829 3a0a  late_context():.
+00001200: 2020 5f63 6f6e 7465 7874 2e63 616c 6c5f    _context.call_
+00001210: 696e 666f 5f73 7461 636b 2e61 7070 656e  info_stack.appen
+00001220: 6428 5f43 616c 6c49 6e66 6f43 6f6e 7465  d(_CallInfoConte
+00001230: 7874 2830 2c20 5b5d 2929 0a20 2074 7279  xt(0, [])).  try
+00001240: 3a0a 2020 2020 7969 656c 640a 2020 6669  :.    yield.  fi
+00001250: 6e61 6c6c 793a 0a20 2020 205f 636f 6e74  nally:.    _cont
+00001260: 6578 742e 6361 6c6c 5f69 6e66 6f5f 7374  ext.call_info_st
+00001270: 6163 6b2e 706f 7028 290a 0a23 2054 7261  ack.pop()..# Tra
+00001280: 636b 2070 6172 656e 7420 7265 6c61 7469  ck parent relati
+00001290: 6f6e 7368 6970 2061 6372 6f73 7320 4d6f  onship across Mo
+000012a0: 6475 6c65 732e 0a23 202d 2d2d 2d2d 2d2d  dules..# -------
+000012b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012f0: 2d2d 2d2d 2d2d 0a63 6c61 7373 205f 4479  ------.class _Dy
+00001300: 6e61 6d69 6343 6f6e 7465 7874 2874 6872  namicContext(thr
+00001310: 6561 6469 6e67 2e6c 6f63 616c 293a 0a20  eading.local):. 
+00001320: 2022 2222 4479 6e61 6d69 6320 636f 6e74   """Dynamic cont
+00001330: 6578 742e 2222 220a 2020 2320 544f 444f  ext.""".  # TODO
+00001340: 286d 6172 6376 616e 7a65 6529 3a20 7377  (marcvanzee): sw
+00001350: 6974 6368 2074 6f20 7573 696e 6720 636f  itch to using co
+00001360: 6e74 6578 7476 6172 7320 6f6e 6365 206d  ntextvars once m
+00001370: 696e 696d 756d 2070 7974 686f 6e20 7665  inimum python ve
+00001380: 7273 696f 6e20 6973 0a20 2023 2033 2e37  rsion is.  # 3.7
+00001390: 0a0a 2020 6465 6620 5f5f 696e 6974 5f5f  ..  def __init__
+000013a0: 2873 656c 6629 3a0a 2020 2020 7365 6c66  (self):.    self
+000013b0: 2e6d 6f64 756c 655f 7374 6163 6b20 3d20  .module_stack = 
+000013c0: 5b4e 6f6e 652c 5d0a 2020 2020 7365 6c66  [None,].    self
+000013d0: 2e63 6170 7475 7265 5f73 7461 636b 203d  .capture_stack =
+000013e0: 205b 5d0a 2020 2020 7365 6c66 2e63 616c   [].    self.cal
+000013f0: 6c5f 696e 666f 5f73 7461 636b 203d 205b  l_info_stack = [
+00001400: 5d0a 0a23 2054 6865 2067 6c6f 6261 6c20  ]..# The global 
+00001410: 636f 6e74 6578 740a 5f63 6f6e 7465 7874  context._context
+00001420: 203d 205f 4479 6e61 6d69 6343 6f6e 7465   = _DynamicConte
+00001430: 7874 2829 0a0a 0a63 6c61 7373 205f 5365  xt()...class _Se
+00001440: 6e74 696e 656c 3a0a 0a20 2064 6566 205f  ntinel:..  def _
+00001450: 5f63 6f70 795f 5f28 7365 6c66 293a 0a20  _copy__(self):. 
+00001460: 2020 2072 6574 7572 6e20 7365 6c66 2020     return self  
+00001470: 2320 446f 206e 6f74 2063 6f70 7920 7369  # Do not copy si
+00001480: 6e67 6c65 746f 6e20 7365 6e74 696e 656c  ngleton sentinel
+00001490: 2e0a 0a20 2064 6566 205f 5f64 6565 7063  ...  def __deepc
+000014a0: 6f70 795f 5f28 7365 6c66 2c20 6d65 6d6f  opy__(self, memo
+000014b0: 293a 0a20 2020 2064 656c 206d 656d 6f0a  ):.    del memo.
+000014c0: 2020 2020 7265 7475 726e 2073 656c 6620      return self 
+000014d0: 2023 2044 6f20 6e6f 7420 636f 7079 2073   # Do not copy s
+000014e0: 696e 676c 6574 6f6e 2073 656e 7469 6e65  ingleton sentine
+000014f0: 6c2e 0a0a 0a5f 756e 7370 6563 6966 6965  l...._unspecifie
+00001500: 645f 7061 7265 6e74 203d 205f 5365 6e74  d_parent = _Sent
+00001510: 696e 656c 2829 0a0a 0a23 2045 6e61 626c  inel()...# Enabl
+00001520: 6520 6175 746f 6d61 7469 6320 6e61 6d65  e automatic name
+00001530: 645f 6361 6c6c 2077 7261 7070 696e 6720  d_call wrapping 
+00001540: 666f 7220 6c61 6265 6c6c 696e 6720 7072  for labelling pr
+00001550: 6f66 696c 6520 7472 6163 6573 2e0a 2320  ofile traces..# 
+00001560: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001570: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001580: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001590: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 5f75  -------------._u
+000015b0: 7365 5f6e 616d 6564 5f63 616c 6c20 3d20  se_named_call = 
+000015c0: 636f 6e66 6967 2e66 6c61 785f 7072 6f66  config.flax_prof
+000015d0: 696c 650a 0a0a 6465 6620 5f64 6572 6976  ile...def _deriv
+000015e0: 655f 7072 6f66 696c 696e 675f 6e61 6d65  e_profiling_name
+000015f0: 286d 6f64 756c 652c 2066 6e29 3a0a 2020  (module, fn):.  
+00001600: 6465 6620 5f67 6574 5f66 6e5f 6e61 6d65  def _get_fn_name
+00001610: 2866 6e29 3a0a 2020 2020 6966 2069 7369  (fn):.    if isi
+00001620: 6e73 7461 6e63 6528 666e 2c20 6675 6e63  nstance(fn, func
+00001630: 746f 6f6c 732e 7061 7274 6961 6c29 3a0a  tools.partial):.
+00001640: 2020 2020 2020 7265 7475 726e 205f 6765        return _ge
+00001650: 745f 666e 5f6e 616d 6528 666e 2e66 756e  t_fn_name(fn.fun
+00001660: 6329 0a20 2020 2072 6574 7572 6e20 666e  c).    return fn
+00001670: 2e5f 5f6e 616d 655f 5f0a 2020 666e 5f6e  .__name__.  fn_n
+00001680: 616d 6520 3d20 5f67 6574 5f66 6e5f 6e61  ame = _get_fn_na
+00001690: 6d65 2866 6e29 0a20 206d 6574 686f 645f  me(fn).  method_
+000016a0: 7375 6666 6978 203d 2066 272e 7b66 6e5f  suffix = f'.{fn_
+000016b0: 6e61 6d65 7d27 2069 6620 666e 5f6e 616d  name}' if fn_nam
+000016c0: 6520 213d 2027 5f5f 6361 6c6c 5f5f 2720  e != '__call__' 
+000016d0: 656c 7365 2027 270a 2020 6d6f 6475 6c65  else ''.  module
+000016e0: 5f6e 616d 6520 3d20 6d6f 6475 6c65 2e6e  _name = module.n
+000016f0: 616d 6520 6f72 206d 6f64 756c 652e 5f5f  ame or module.__
+00001700: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
+00001710: 0a20 2072 6574 7572 6e20 6627 7b6d 6f64  .  return f'{mod
+00001720: 756c 655f 6e61 6d65 7d7b 6d65 7468 6f64  ule_name}{method
+00001730: 5f73 7566 6669 787d 270a 0a0a 6465 6620  _suffix}'...def 
+00001740: 656e 6162 6c65 5f6e 616d 6564 5f63 616c  enable_named_cal
+00001750: 6c28 293a 0a20 2022 2222 456e 6162 6c65  l():.  """Enable
+00001760: 7320 6e61 6d65 6420 6361 6c6c 2077 7261  s named call wra
+00001770: 7070 696e 6720 666f 7220 6c61 6265 6c6c  pping for labell
+00001780: 696e 6720 7072 6f66 696c 6520 7472 6163  ing profile trac
+00001790: 6573 2e0a 0a20 2057 6865 6e20 6e61 6d65  es...  When name
+000017a0: 6420 6361 6c6c 2077 7261 7070 696e 6720  d call wrapping 
+000017b0: 6973 2065 6e61 626c 6564 2061 6c6c 204a  is enabled all J
+000017c0: 4158 206f 7073 2065 7865 6375 7465 6420  AX ops executed 
+000017d0: 696e 2061 204d 6f64 756c 650a 2020 7769  in a Module.  wi
+000017e0: 6c6c 2062 6520 7275 6e20 756e 6465 7220  ll be run under 
+000017f0: 6060 6a61 782e 6e61 6d65 645f 7363 6f70  ``jax.named_scop
+00001800: 6560 602e 2054 6865 2060 604d 6f64 756c  e``. The ``Modul
+00001810: 6560 6020 636c 6173 7320 6e61 6d65 2077  e`` class name w
+00001820: 696c 6c0a 2020 7368 6f77 2075 7020 6172  ill.  show up ar
+00001830: 6f75 6e64 2074 6865 206f 7065 7261 7469  ound the operati
+00001840: 6f6e 7320 6265 6c6f 6e67 696e 6720 746f  ons belonging to
+00001850: 2074 6861 7420 4d6f 6475 6c65 2069 6e20   that Module in 
+00001860: 7468 650a 2020 5465 6e73 6f72 626f 6172  the.  Tensorboar
+00001870: 6420 7072 6f66 696c 696e 6720 5549 2c20  d profiling UI, 
+00001880: 7369 6d70 6c69 6679 696e 6720 7468 6520  simplifying the 
+00001890: 7072 6f66 696c 696e 6720 7072 6f63 6573  profiling proces
+000018a0: 732e 0a0a 2020 4e6f 7465 2074 6861 7420  s...  Note that 
+000018b0: 6060 6a61 782e 6e61 6d65 645f 7363 6f70  ``jax.named_scop
+000018c0: 6560 6020 6f6e 6c79 2077 6f72 6b73 2066  e`` only works f
+000018d0: 6f72 0a20 2063 6f6d 7069 6c65 6420 6675  or.  compiled fu
+000018e0: 6e63 7469 6f6e 7320 2865 2e67 2e3a 2075  nctions (e.g.: u
+000018f0: 7369 6e67 206a 6178 2e6a 6974 206f 7220  sing jax.jit or 
+00001900: 6a61 782e 706d 6170 292e 0a20 2022 2222  jax.pmap)..  """
+00001910: 0a20 2067 6c6f 6261 6c20 5f75 7365 5f6e  .  global _use_n
+00001920: 616d 6564 5f63 616c 6c0a 2020 5f75 7365  amed_call.  _use
+00001930: 5f6e 616d 6564 5f63 616c 6c20 3d20 5472  _named_call = Tr
+00001940: 7565 0a0a 0a64 6566 2064 6973 6162 6c65  ue...def disable
+00001950: 5f6e 616d 6564 5f63 616c 6c28 293a 0a20  _named_call():. 
+00001960: 2022 2222 4469 7361 626c 6573 206e 616d   """Disables nam
+00001970: 6564 2063 616c 6c20 7772 6170 7069 6e67  ed call wrapping
+00001980: 2e0a 0a20 2053 6565 2060 6065 6e61 626c  ...  See ``enabl
+00001990: 655f 6e61 6d65 645f 6361 6c6c 6060 0a20  e_named_call``. 
+000019a0: 2022 2222 0a20 2067 6c6f 6261 6c20 5f75   """.  global _u
+000019b0: 7365 5f6e 616d 6564 5f63 616c 6c0a 2020  se_named_call.  
+000019c0: 5f75 7365 5f6e 616d 6564 5f63 616c 6c20  _use_named_call 
+000019d0: 3d20 4661 6c73 650a 0a0a 4063 6f6e 7465  = False...@conte
+000019e0: 7874 6c69 622e 636f 6e74 6578 746d 616e  xtlib.contextman
+000019f0: 6167 6572 0a64 6566 206f 7665 7272 6964  ager.def overrid
+00001a00: 655f 6e61 6d65 645f 6361 6c6c 2865 6e61  e_named_call(ena
+00001a10: 626c 653a 2062 6f6f 6c20 3d20 5472 7565  ble: bool = True
+00001a20: 293a 0a20 2023 2070 796c 696e 743a 2064  ):.  # pylint: d
+00001a30: 6973 6162 6c65 3d67 2d64 6f63 2d72 6574  isable=g-doc-ret
+00001a40: 7572 6e2d 6f72 2d79 6965 6c64 0a20 2022  urn-or-yield.  "
+00001a50: 2222 5265 7475 726e 7320 6120 636f 6e74  ""Returns a cont
+00001a60: 6578 7420 6d61 6e61 6765 7220 7468 6174  ext manager that
+00001a70: 2065 6e61 626c 6573 2f64 6973 6162 6c65   enables/disable
+00001a80: 7320 6e61 6d65 6420 6361 6c6c 2077 7261  s named call wra
+00001a90: 7070 696e 672e 0a0a 2020 4172 6773 3a0a  pping...  Args:.
+00001aa0: 2020 2020 656e 6162 6c65 3a20 4966 2074      enable: If t
+00001ab0: 7275 652c 2065 6e61 626c 6573 206e 616d  rue, enables nam
+00001ac0: 6564 2063 616c 6c20 7772 6170 7069 6e67  ed call wrapping
+00001ad0: 2066 6f72 206c 6162 656c 6c69 6e67 2070   for labelling p
+00001ae0: 726f 6669 6c65 2074 7261 6365 732e 0a20  rofile traces.. 
+00001af0: 2020 2020 2028 7365 6520 6060 656e 6162       (see ``enab
+00001b00: 6c65 645f 6e61 6d65 645f 6361 6c6c 6060  led_named_call``
+00001b10: 292e 0a20 2022 2222 0a20 2023 2070 796c  )..  """.  # pyl
+00001b20: 696e 743a 2065 6e61 626c 653d 672d 646f  int: enable=g-do
+00001b30: 632d 7265 7475 726e 2d6f 722d 7969 656c  c-return-or-yiel
+00001b40: 640a 2020 676c 6f62 616c 205f 7573 655f  d.  global _use_
+00001b50: 6e61 6d65 645f 6361 6c6c 0a20 2075 7365  named_call.  use
+00001b60: 5f6e 616d 6564 5f63 616c 6c5f 7072 6576  _named_call_prev
+00001b70: 203d 205f 7573 655f 6e61 6d65 645f 6361   = _use_named_ca
+00001b80: 6c6c 0a20 205f 7573 655f 6e61 6d65 645f  ll.  _use_named_
+00001b90: 6361 6c6c 203d 2065 6e61 626c 650a 2020  call = enable.  
+00001ba0: 7472 793a 0a20 2020 2079 6965 6c64 0a20  try:.    yield. 
+00001bb0: 2066 696e 616c 6c79 3a0a 2020 2020 5f75   finally:.    _u
+00001bc0: 7365 5f6e 616d 6564 5f63 616c 6c20 3d20  se_named_call = 
+00001bd0: 7573 655f 6e61 6d65 645f 6361 6c6c 5f70  use_named_call_p
+00001be0: 7265 760a 0a0a 2320 5574 696c 6974 6965  rev...# Utilitie
+00001bf0: 7320 666f 7220 7079 7472 6565 7320 6f66  s for pytrees of
+00001c00: 204d 6f64 756c 6573 2064 6566 696e 6564   Modules defined
+00001c10: 2069 6e73 6964 6520 7365 7475 7028 290a   inside setup().
+00001c20: 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  # --------------
+00001c30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001c40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001c50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001c60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
+00001c70: 0a0a 6465 6620 5f73 6f72 7465 645f 6974  ..def _sorted_it
+00001c80: 656d 7328 7829 3a0a 2020 2222 2252 6574  ems(x):.  """Ret
+00001c90: 7572 6e73 2069 7465 6d73 206f 6620 6120  urns items of a 
+00001ca0: 6469 6374 206f 7264 6572 6564 2062 7920  dict ordered by 
+00001cb0: 6b65 7973 2e22 2222 0a20 2072 6574 7572  keys.""".  retur
+00001cc0: 6e20 736f 7274 6564 2878 2e69 7465 6d73  n sorted(x.items
+00001cd0: 2829 2c20 6b65 793d 6c61 6d62 6461 2078  (), key=lambda x
+00001ce0: 3a20 785b 305d 290a 0a0a 6465 6620 5f67  : x[0])...def _g
+00001cf0: 6574 5f73 7566 6669 785f 7661 6c75 655f  et_suffix_value_
+00001d00: 7061 6972 7328 0a20 2020 2074 7265 655f  pairs(.    tree_
+00001d10: 6f72 5f6c 6561 663a 2041 6e79 2920 2d3e  or_leaf: Any) ->
+00001d20: 204c 6973 745b 5475 706c 655b 7374 722c   List[Tuple[str,
+00001d30: 2054 7970 655b 274d 6f64 756c 6527 5d5d   Type['Module']]
+00001d40: 5d3a 0a20 2022 2222 4865 6c70 6572 2066  ]:.  """Helper f
+00001d50: 6f72 206e 616d 696e 6720 7079 7472 6565  or naming pytree
+00001d60: 7320 6f66 2073 7562 6d6f 6475 6c65 732e  s of submodules.
+00001d70: 2222 220a 2020 6469 6374 5f6f 725f 6c65  """.  dict_or_le
+00001d80: 6166 203d 2073 6572 6961 6c69 7a61 7469  af = serializati
+00001d90: 6f6e 2e74 6f5f 7374 6174 655f 6469 6374  on.to_state_dict
+00001da0: 2874 7265 655f 6f72 5f6c 6561 6629 0a20  (tree_or_leaf). 
+00001db0: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan
+00001dc0: 6365 2864 6963 745f 6f72 5f6c 6561 662c  ce(dict_or_leaf,
+00001dd0: 2064 6963 7429 206f 7220 6e6f 7420 6469   dict) or not di
+00001de0: 6374 5f6f 725f 6c65 6166 3a0a 2020 2020  ct_or_leaf:.    
+00001df0: 7265 7475 726e 205b 2827 272c 2074 7265  return [('', tre
+00001e00: 655f 6f72 5f6c 6561 6629 5d0a 2020 656c  e_or_leaf)].  el
+00001e10: 7365 3a0a 2020 2020 666c 6174 5f64 6963  se:.    flat_dic
+00001e20: 7420 3d20 7472 6176 6572 7365 5f75 7469  t = traverse_uti
+00001e30: 6c2e 666c 6174 7465 6e5f 6469 6374 2864  l.flatten_dict(d
+00001e40: 6963 745f 6f72 5f6c 6561 6629 0a20 2020  ict_or_leaf).   
+00001e50: 2072 6574 7572 6e20 5b28 275f 2720 2b20   return [('_' + 
+00001e60: 275f 272e 6a6f 696e 286b 292c 2076 2920  '_'.join(k), v) 
+00001e70: 666f 7220 6b2c 2076 2069 6e20 5f73 6f72  for k, v in _sor
+00001e80: 7465 645f 6974 656d 7328 666c 6174 5f64  ted_items(flat_d
+00001e90: 6963 7429 5d0a 0a0a 6465 6620 5f6d 6170  ict)]...def _map
+00001ea0: 5f6f 7665 725f 6d6f 6475 6c65 735f 696e  _over_modules_in
+00001eb0: 5f74 7265 6528 666e 2c20 7472 6565 5f6f  _tree(fn, tree_o
+00001ec0: 725f 6c65 6166 293a 0a20 2022 2222 4865  r_leaf):.  """He
+00001ed0: 6c70 6572 2066 6f72 206d 6170 7069 6e67  lper for mapping
+00001ee0: 2066 756e 6374 696f 6e20 6f76 6572 2073   function over s
+00001ef0: 7562 6d6f 6475 6c65 732e 2222 220a 2020  ubmodules.""".  
+00001f00: 6469 6374 5f6f 725f 6c65 6166 203d 2073  dict_or_leaf = s
+00001f10: 6572 6961 6c69 7a61 7469 6f6e 2e74 6f5f  erialization.to_
+00001f20: 7374 6174 655f 6469 6374 2874 7265 655f  state_dict(tree_
+00001f30: 6f72 5f6c 6561 6629 0a20 2069 6620 6e6f  or_leaf).  if no
+00001f40: 7420 6973 696e 7374 616e 6365 2864 6963  t isinstance(dic
+00001f50: 745f 6f72 5f6c 6561 662c 2064 6963 7429  t_or_leaf, dict)
+00001f60: 206f 7220 6e6f 7420 6469 6374 5f6f 725f   or not dict_or_
+00001f70: 6c65 6166 3a0a 2020 2020 7265 7475 726e  leaf:.    return
+00001f80: 2066 6e28 2727 2c20 7472 6565 5f6f 725f   fn('', tree_or_
+00001f90: 6c65 6166 290a 2020 656c 7365 3a0a 2020  leaf).  else:.  
+00001fa0: 2020 666c 6174 5f64 6963 7420 3d20 7472    flat_dict = tr
+00001fb0: 6176 6572 7365 5f75 7469 6c2e 666c 6174  averse_util.flat
+00001fc0: 7465 6e5f 6469 6374 2864 6963 745f 6f72  ten_dict(dict_or
+00001fd0: 5f6c 6561 662c 206b 6565 705f 656d 7074  _leaf, keep_empt
+00001fe0: 795f 6e6f 6465 733d 5472 7565 290a 2020  y_nodes=True).  
+00001ff0: 2020 6d61 7070 6564 5f66 6c61 745f 6469    mapped_flat_di
+00002000: 6374 203d 207b 6b3a 2066 6e28 275f 2720  ct = {k: fn('_' 
+00002010: 2b20 275f 272e 6a6f 696e 286b 292c 2076  + '_'.join(k), v
+00002020: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00002030: 2020 2020 2020 2020 2020 666f 7220 6b2c            for k,
+00002040: 2076 2069 6e20 5f73 6f72 7465 645f 6974   v in _sorted_it
+00002050: 656d 7328 666c 6174 5f64 6963 7429 7d0a  ems(flat_dict)}.
+00002060: 2020 2020 7265 7475 726e 2073 6572 6961      return seria
+00002070: 6c69 7a61 7469 6f6e 2e66 726f 6d5f 7374  lization.from_st
+00002080: 6174 655f 6469 6374 280a 2020 2020 2020  ate_dict(.      
+00002090: 2020 7472 6565 5f6f 725f 6c65 6166 2c20    tree_or_leaf, 
+000020a0: 7472 6176 6572 7365 5f75 7469 6c2e 756e  traverse_util.un
+000020b0: 666c 6174 7465 6e5f 6469 6374 286d 6170  flatten_dict(map
+000020c0: 7065 645f 666c 6174 5f64 6963 7429 290a  ped_flat_dict)).
+000020d0: 0a0a 6465 6620 5f66 7265 657a 655f 6174  ..def _freeze_at
+000020e0: 7472 2876 616c 3a20 416e 7929 202d 3e20  tr(val: Any) -> 
+000020f0: 416e 793a 0a20 2022 2222 5265 6375 7273  Any:.  """Recurs
+00002100: 6976 656c 7920 7772 6170 2074 6865 2067  ively wrap the g
+00002110: 6976 656e 2061 7474 7269 6275 7465 2060  iven attribute `
+00002120: 7661 7260 2069 6e20 6060 4672 6f7a 656e  var` in ``Frozen
+00002130: 4469 6374 6060 2e22 2222 0a20 2069 6620  Dict``.""".  if 
+00002140: 6973 696e 7374 616e 6365 2876 616c 2c20  isinstance(val, 
+00002150: 2864 6963 742c 2046 726f 7a65 6e44 6963  (dict, FrozenDic
+00002160: 7429 293a 0a20 2020 2072 6574 7572 6e20  t)):.    return 
+00002170: 4672 6f7a 656e 4469 6374 287b 6b3a 205f  FrozenDict({k: _
+00002180: 6672 6565 7a65 5f61 7474 7228 7629 2066  freeze_attr(v) f
+00002190: 6f72 206b 2c20 7620 696e 2076 616c 2e69  or k, v in val.i
+000021a0: 7465 6d73 2829 7d29 0a20 2065 6c69 6620  tems()}).  elif 
+000021b0: 6973 696e 7374 616e 6365 2876 616c 2c20  isinstance(val, 
+000021c0: 7475 706c 6529 3a0a 2020 2020 2320 5370  tuple):.    # Sp
+000021d0: 6563 6961 6c20 6361 7365 206e 616d 6564  ecial case named
+000021e0: 7475 706c 6573 2061 6e64 2073 7065 6369  tuples and speci
+000021f0: 616c 204a 4158 2074 7570 6c65 2073 7472  al JAX tuple str
+00002200: 7563 7475 7265 7320 6f74 6865 7277 6973  uctures otherwis
+00002210: 6520 7468 6579 0a20 2020 2023 2077 6f75  e they.    # wou
+00002220: 6c64 2062 6520 646f 776e 6772 6164 6564  ld be downgraded
+00002230: 2074 6f20 6e6f 726d 616c 2074 7570 6c65   to normal tuple
+00002240: 732e 0a20 2020 2069 6620 6861 7361 7474  s..    if hasatt
+00002250: 7228 7661 6c2c 2027 5f66 6965 6c64 7327  r(val, '_fields'
+00002260: 2920 6f72 2074 7970 6528 7661 6c29 2e5f  ) or type(val)._
+00002270: 5f6e 616d 655f 5f20 3d3d 2027 5061 7274  _name__ == 'Part
+00002280: 6974 696f 6e53 7065 6327 3a0a 2020 2020  itionSpec':.    
+00002290: 2020 7265 7475 726e 2074 7970 6528 7661    return type(va
+000022a0: 6c29 282a 5b5f 6672 6565 7a65 5f61 7474  l)(*[_freeze_att
+000022b0: 7228 7629 2066 6f72 2076 2069 6e20 7661  r(v) for v in va
+000022c0: 6c5d 290a 2020 2020 656c 7365 3a0a 2020  l]).    else:.  
+000022d0: 2020 2020 7265 7475 726e 2074 7570 6c65      return tuple
+000022e0: 285f 6672 6565 7a65 5f61 7474 7228 7629  (_freeze_attr(v)
+000022f0: 2066 6f72 2076 2069 6e20 7661 6c29 0a20   for v in val). 
+00002300: 2065 6c69 6620 6973 696e 7374 616e 6365   elif isinstance
+00002310: 2876 616c 2c20 6c69 7374 293a 0a20 2020  (val, list):.   
+00002320: 2072 6574 7572 6e20 7475 706c 6528 5f66   return tuple(_f
+00002330: 7265 657a 655f 6174 7472 2876 2920 666f  reeze_attr(v) fo
+00002340: 7220 7620 696e 2076 616c 290a 2020 656c  r v in val).  el
+00002350: 7365 3a0a 2020 2020 7265 7475 726e 2076  se:.    return v
+00002360: 616c 0a0a 0a23 204d 6574 686f 6420 7772  al...# Method wr
+00002370: 6170 7069 6e67 206f 6620 2263 6f6d 7061  apping of "compa
+00002380: 6374 206d 6574 686f 6473 2220 616e 6420  ct methods" and 
+00002390: 7365 7475 7028 290a 2320 2d2d 2d2d 2d2d  setup().# ------
+000023a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000023b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000023c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000023d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000023e0: 2d2d 2d2d 2d2d 2d0a 6465 6620 636f 6d70  -------.def comp
+000023f0: 6163 7428 6675 6e3a 205f 4361 6c6c 6162  act(fun: _Callab
+00002400: 6c65 5429 202d 3e20 5f43 616c 6c61 626c  leT) -> _Callabl
+00002410: 6554 3a0a 2020 2222 224d 6172 6b73 2074  eT:.  """Marks t
+00002420: 6865 2067 6976 656e 206d 6f64 756c 6520  he given module 
+00002430: 6d65 7468 6f64 2061 6c6c 6f77 696e 6720  method allowing 
+00002440: 696e 6c69 6e65 6420 7375 626d 6f64 756c  inlined submodul
+00002450: 6573 2e0a 0a20 204d 6574 686f 6473 2077  es...  Methods w
+00002460: 7261 7070 6564 2069 6e20 4063 6f6d 7061  rapped in @compa
+00002470: 6374 2063 616e 2064 6566 696e 6520 7375  ct can define su
+00002480: 626d 6f64 756c 6573 2064 6972 6563 746c  bmodules directl
+00002490: 7920 7769 7468 696e 2074 6865 206d 6574  y within the met
+000024a0: 686f 642e 0a0a 2020 466f 7220 696e 7374  hod...  For inst
+000024b0: 616e 6365 3a3a 0a0a 2020 2020 4063 6f6d  ance::..    @com
+000024c0: 7061 6374 0a20 2020 205f 5f63 616c 6c5f  pact.    __call_
+000024d0: 5f28 7365 6c66 2c20 782c 2066 6561 7475  _(self, x, featu
+000024e0: 7265 7329 3a0a 2020 2020 2020 7820 3d20  res):.      x = 
+000024f0: 6e6e 2e44 656e 7365 2866 6561 7475 7265  nn.Dense(feature
+00002500: 7329 2878 290a 2020 2020 2020 2e2e 2e0a  s)(x).      ....
+00002510: 0a20 2041 7420 6d6f 7374 206f 6e65 206d  .  At most one m
+00002520: 6574 686f 6420 696e 2065 6163 6820 4d6f  ethod in each Mo
+00002530: 6475 6c65 206d 6179 2062 6520 7772 6170  dule may be wrap
+00002540: 7065 6420 7769 7468 2040 636f 6d70 6163  ped with @compac
+00002550: 742e 0a0a 2020 4172 6773 3a0a 2020 2020  t...  Args:.    
+00002560: 6675 6e3a 2054 6865 204d 6f64 756c 6520  fun: The Module 
+00002570: 6d65 7468 6f64 2074 6f20 6d61 726b 2061  method to mark a
+00002580: 7320 636f 6d70 6163 742e 0a20 2052 6574  s compact..  Ret
+00002590: 7572 6e73 3a0a 2020 2020 5468 6520 6769  urns:.    The gi
+000025a0: 7665 6e20 6675 6e63 7469 6f6e 2060 6675  ven function `fu
+000025b0: 6e60 206d 6172 6b65 6420 6173 2063 6f6d  n` marked as com
+000025c0: 7061 6374 2e0a 2020 2222 220a 2020 6675  pact..  """.  fu
+000025d0: 6e2e 636f 6d70 6163 7420 3d20 5472 7565  n.compact = True
+000025e0: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
+000025f0: 5b61 7474 722d 6465 6669 6e65 645d 0a20  [attr-defined]. 
+00002600: 2072 6574 7572 6e20 6675 6e0a 0a0a 6465   return fun...de
+00002610: 6620 6e6f 7772 6170 2866 756e 3a20 5f43  f nowrap(fun: _C
+00002620: 616c 6c61 626c 6554 2920 2d3e 205f 4361  allableT) -> _Ca
+00002630: 6c6c 6162 6c65 543a 0a20 2022 2222 4d61  llableT:.  """Ma
+00002640: 726b 7320 7468 6520 6769 7665 6e20 6d6f  rks the given mo
+00002650: 6475 6c65 206d 6574 686f 6420 6173 2061  dule method as a
+00002660: 2068 656c 7065 7220 6d65 7468 6f64 2074   helper method t
+00002670: 6861 7420 6e65 6564 6e27 7420 6265 2077  hat needn't be w
+00002680: 7261 7070 6564 2e0a 0a20 204d 6574 686f  rapped...  Metho
+00002690: 6473 2077 7261 7070 6564 2069 6e20 406e  ds wrapped in @n
+000026a0: 6f77 7261 7020 6172 6520 7072 6976 6174  owrap are privat
+000026b0: 6520 6865 6c70 6572 206d 6574 686f 6473  e helper methods
+000026c0: 2074 6861 7420 6e65 6564 6e27 7420 6265   that needn't be
+000026d0: 2077 7261 7070 6564 0a20 2077 6974 6820   wrapped.  with 
+000026e0: 7468 6520 7374 6174 6520 6861 6e64 6c65  the state handle
+000026f0: 7220 6f72 2061 2073 6570 6172 6174 6520  r or a separate 
+00002700: 6e61 6d65 645f 6361 6c6c 2074 7261 6e73  named_call trans
+00002710: 666f 726d 2e0a 0a20 2054 6869 7320 6973  form...  This is
+00002720: 206e 6565 6465 6420 696e 2073 6576 6572   needed in sever
+00002730: 616c 2063 6f6e 6372 6574 6520 696e 7374  al concrete inst
+00002740: 616e 6365 733a 0a20 2020 2d20 6966 2079  ances:.   - if y
+00002750: 6f75 2772 6520 7375 6263 6c61 7373 696e  ou're subclassin
+00002760: 6720 6120 6d65 7468 6f64 206c 696b 6520  g a method like 
+00002770: 4d6f 6475 6c65 2e70 6172 616d 2061 6e64  Module.param and
+00002780: 2064 6f6e 2774 2077 616e 7420 7468 6973   don't want this
+00002790: 0a20 2020 2020 6f76 6572 7269 6465 6e20  .     overriden 
+000027a0: 636f 7265 2066 756e 6374 696f 6e20 6465  core function de
+000027b0: 636f 7261 7465 6420 7769 7468 2074 6865  corated with the
+000027c0: 2073 7461 7465 206d 616e 6167 656d 656e   state managemen
+000027d0: 7420 7772 6170 7065 722e 0a20 2020 2d20  t wrapper..   - 
+000027e0: 4966 2079 6f75 2077 616e 7420 6120 6d65  If you want a me
+000027f0: 7468 6f64 2074 6f20 6265 2063 616c 6c61  thod to be calla
+00002800: 626c 6520 6672 6f6d 2061 6e20 756e 626f  ble from an unbo
+00002810: 756e 6420 4d6f 6475 6c65 2028 652e 672e  und Module (e.g.
+00002820: 3a20 610a 2020 2020 2066 756e 6374 696f  : a.     functio
+00002830: 6e20 6f66 2063 6f6e 7374 7275 6374 696f  n of constructio
+00002840: 6e20 6f66 2061 7267 756d 656e 7473 2074  n of arguments t
+00002850: 6861 7420 646f 6573 6e27 7420 6465 7065  hat doesn't depe
+00002860: 6e64 206f 6e20 7061 7261 6d73 2f52 4e47  nd on params/RNG
+00002870: 7329 0a0a 2020 466f 7220 696e 7374 616e  s)..  For instan
+00002880: 6365 3a3a 0a0a 2020 2020 406e 6f77 7261  ce::..    @nowra
+00002890: 700a 2020 2020 6465 6620 5f6d 616b 655f  p.    def _make_
+000028a0: 6465 6e73 6528 7365 6c66 2c20 6e75 6d5f  dense(self, num_
+000028b0: 6665 6174 7572 6573 293a 0a20 2020 2020  features):.     
+000028c0: 2072 6574 7572 6e20 6e6e 2e44 656e 7365   return nn.Dense
+000028d0: 286e 756d 5f66 6561 7475 7265 7329 0a0a  (num_features)..
+000028e0: 2020 2020 4063 6f6d 7061 6374 0a20 2020      @compact.   
+000028f0: 2064 6566 205f 5f63 616c 6c5f 5f28 7365   def __call__(se
+00002900: 6c66 2c20 7829 3a0a 2020 2020 2020 2320  lf, x):.      # 
+00002910: 6e6f 7720 7361 6665 2074 6f20 7573 6520  now safe to use 
+00002920: 636f 6e73 7472 7563 746f 7220 6865 6c70  constructor help
+00002930: 6572 2065 7665 6e20 6966 2075 7369 6e67  er even if using
+00002940: 206e 616d 6564 5f63 616c 6c0a 2020 2020   named_call.    
+00002950: 2020 6465 6e73 6520 3d20 7365 6c66 2e5f    dense = self._
+00002960: 6d61 6b65 5f64 656e 7365 2873 656c 662e  make_dense(self.
+00002970: 6e75 6d5f 6665 6174 7572 6573 290a 2020  num_features).  
+00002980: 2020 2020 7265 7475 726e 2064 656e 7365      return dense
+00002990: 2878 290a 0a20 2041 7267 733a 0a20 2020  (x)..  Args:.   
+000029a0: 2066 756e 3a20 5468 6520 4d6f 6475 6c65   fun: The Module
+000029b0: 206d 6574 686f 6420 746f 206d 6172 6b20   method to mark 
+000029c0: 6173 206e 6f77 7261 702e 0a20 2052 6574  as nowrap..  Ret
+000029d0: 7572 6e73 3a0a 2020 2020 5468 6520 6769  urns:.    The gi
+000029e0: 7665 6e20 6675 6e63 7469 6f6e 2060 6675  ven function `fu
+000029f0: 6e60 206d 6172 6b65 6420 6173 206e 6f77  n` marked as now
+00002a00: 7261 702e 0a20 2022 2222 0a20 2066 756e  rap..  """.  fun
+00002a10: 2e6e 6f77 7261 7020 3d20 5472 7565 2020  .nowrap = True  
+00002a20: 2320 7479 7065 3a20 6967 6e6f 7265 5b61  # type: ignore[a
+00002a30: 7474 722d 6465 6669 6e65 645d 0a20 2072  ttr-defined].  r
+00002a40: 6574 7572 6e20 6675 6e0a 0a0a 6465 6620  eturn fun...def 
+00002a50: 5f67 6574 5f6c 6f63 616c 5f6d 6574 686f  _get_local_metho
+00002a60: 645f 6e61 6d65 7328 636c 733a 2041 6e79  d_names(cls: Any
+00002a70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00002a80: 2020 2020 2020 2020 2020 2020 2020 6578                ex
+00002a90: 636c 7564 653a 2049 7465 7261 626c 655b  clude: Iterable[
+00002aa0: 7374 725d 203d 2028 2929 202d 3e20 5475  str] = ()) -> Tu
+00002ab0: 706c 655b 7374 722c 202e 2e2e 5d3a 0a20  ple[str, ...]:. 
+00002ac0: 2022 2222 4765 7473 206d 6574 686f 6420   """Gets method 
+00002ad0: 6e61 6d65 7320 6f66 2061 2063 6c61 7373  names of a class
+00002ae0: 2c20 6578 636c 7564 696e 6720 636c 6173  , excluding clas
+00002af0: 7320 616e 6420 7374 6174 6963 206d 6574  s and static met
+00002b00: 686f 6473 2e0a 0a20 2041 7267 733a 0a20  hods...  Args:. 
+00002b10: 2020 2063 6c73 3a20 5468 6520 636c 6173     cls: The clas
+00002b20: 7320 746f 2067 6574 206d 6574 686f 6420  s to get method 
+00002b30: 6e61 6d65 7320 666f 722e 0a20 2020 2065  names for..    e
+00002b40: 7863 6c75 6465 3a20 4e61 6d65 7320 746f  xclude: Names to
+00002b50: 2065 7863 6c75 6465 2066 726f 6d20 6f75   exclude from ou
+00002b60: 7470 7574 2e0a 2020 5265 7475 726e 733a  tput..  Returns:
+00002b70: 0a20 2020 2041 206c 6973 7420 6f66 206d  .    A list of m
+00002b80: 6574 686f 6420 6e61 6d65 732e 0a20 2022  ethod names..  "
+00002b90: 2222 0a20 2074 7275 655f 6d65 7468 6f64  "".  true_method
+00002ba0: 7320 3d20 7365 7428 290a 2020 666f 7220  s = set().  for 
+00002bb0: 6d20 696e 2063 6c73 2e5f 5f64 6963 745f  m in cls.__dict_
+00002bc0: 5f3a 0a20 2020 2069 6620 6361 6c6c 6162  _:.    if callab
+00002bd0: 6c65 2863 6c73 2e5f 5f64 6963 745f 5f5b  le(cls.__dict__[
+00002be0: 6d5d 2920 616e 6420 6e6f 7420 696e 7370  m]) and not insp
+00002bf0: 6563 742e 6973 636c 6173 7328 636c 732e  ect.isclass(cls.
+00002c00: 5f5f 6469 6374 5f5f 5b6d 5d29 3a20 2023  __dict__[m]):  #
+00002c10: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
+00002c20: 3d6e 6f74 2d73 7570 706f 7274 6564 2d79  =not-supported-y
+00002c30: 6574 0a20 2020 2020 206d 7479 7065 203d  et.      mtype =
+00002c40: 2074 7970 6528 636c 732e 5f5f 6469 6374   type(cls.__dict
+00002c50: 5f5f 5b6d 5d29 0a20 2020 2020 2069 6620  __[m]).      if 
+00002c60: 6d74 7970 6520 213d 2073 7461 7469 636d  mtype != staticm
+00002c70: 6574 686f 6420 616e 6420 6d74 7970 6520  ethod and mtype 
+00002c80: 213d 2063 6c61 7373 6d65 7468 6f64 3a0a  != classmethod:.
+00002c90: 2020 2020 2020 2020 7472 7565 5f6d 6574          true_met
+00002ca0: 686f 6473 2e61 6464 286d 290a 2020 7265  hods.add(m).  re
+00002cb0: 7475 726e 2074 7570 6c65 2874 7275 655f  turn tuple(true_
+00002cc0: 6d65 7468 6f64 732e 6469 6666 6572 656e  methods.differen
+00002cd0: 6365 2873 6574 2865 7863 6c75 6465 2929  ce(set(exclude))
+00002ce0: 290a 0a64 6566 205f 6765 745f 6c6f 6361  )..def _get_loca
+00002cf0: 6c5f 6465 7363 7269 7074 6f72 5f6e 616d  l_descriptor_nam
+00002d00: 6573 2863 6c73 3a20 416e 792c 0a20 2020  es(cls: Any,.   
+00002d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002d20: 2020 2020 2020 2020 2020 2020 2065 7863               exc
+00002d30: 6c75 6465 3a20 4974 6572 6162 6c65 5b73  lude: Iterable[s
+00002d40: 7472 5d20 3d20 2829 2920 2d3e 2054 7570  tr] = ()) -> Tup
+00002d50: 6c65 5b73 7472 2c20 2e2e 2e5d 3a0a 2020  le[str, ...]:.  
+00002d60: 2222 2247 6574 7320 6465 7363 7269 7074  """Gets descript
+00002d70: 6f72 206e 616d 6573 206f 6620 6120 636c  or names of a cl
+00002d80: 6173 732e 0a0a 2020 4172 6773 3a0a 2020  ass...  Args:.  
+00002d90: 2020 636c 733a 2054 6865 2063 6c61 7373    cls: The class
+00002da0: 2074 6f20 6765 7420 7072 6f70 6572 7479   to get property
+00002db0: 206e 616d 6573 2066 6f72 2e0a 2020 2020   names for..    
+00002dc0: 6578 636c 7564 653a 204e 616d 6573 2074  exclude: Names t
+00002dd0: 6f20 6578 636c 7564 6520 6672 6f6d 206f  o exclude from o
+00002de0: 7574 7075 742e 0a20 2052 6574 7572 6e73  utput..  Returns
+00002df0: 3a0a 2020 2020 4120 6c69 7374 206f 6620  :.    A list of 
+00002e00: 7072 6f70 6572 7479 206e 616d 6573 2e0a  property names..
+00002e10: 2020 2222 220a 2020 7472 7565 5f70 726f    """.  true_pro
+00002e20: 7065 7274 6965 7320 3d20 7365 7428 290a  perties = set().
+00002e30: 2020 666f 7220 6d2c 2061 7474 7220 696e    for m, attr in
+00002e40: 2063 6c73 2e5f 5f64 6963 745f 5f2e 6974   cls.__dict__.it
+00002e50: 656d 7328 293a 0a20 2020 2069 6620 6e6f  ems():.    if no
+00002e60: 7420 6361 6c6c 6162 6c65 2861 7474 7229  t callable(attr)
+00002e70: 2061 6e64 2028 0a20 2020 2020 2068 6173   and (.      has
+00002e80: 6174 7472 2861 7474 722c 2027 5f5f 6765  attr(attr, '__ge
+00002e90: 745f 5f27 2920 6f72 2068 6173 6174 7472  t__') or hasattr
+00002ea0: 2861 7474 722c 2027 5f5f 7365 745f 5f27  (attr, '__set__'
+00002eb0: 2920 6f72 0a20 2020 2020 2068 6173 6174  ) or.      hasat
+00002ec0: 7472 2861 7474 722c 2027 5f5f 6465 6c65  tr(attr, '__dele
+00002ed0: 7465 5f5f 2729 0a20 2020 2029 3a0a 2020  te__').    ):.  
+00002ee0: 2020 2020 6d74 7970 6520 3d20 7479 7065      mtype = type
+00002ef0: 2861 7474 7229 0a20 2020 2020 2069 6620  (attr).      if 
+00002f00: 6d74 7970 6520 213d 2073 7461 7469 636d  mtype != staticm
+00002f10: 6574 686f 6420 616e 6420 6d74 7970 6520  ethod and mtype 
+00002f20: 213d 2063 6c61 7373 6d65 7468 6f64 3a0a  != classmethod:.
+00002f30: 2020 2020 2020 2020 7472 7565 5f70 726f          true_pro
+00002f40: 7065 7274 6965 732e 6164 6428 6d29 0a20  perties.add(m). 
+00002f50: 2072 6574 7572 6e20 7475 706c 6528 7472   return tuple(tr
+00002f60: 7565 5f70 726f 7065 7274 6965 732e 6469  ue_properties.di
+00002f70: 6666 6572 656e 6365 2873 6574 2865 7863  fference(set(exc
+00002f80: 6c75 6465 2929 290a 0a0a 6465 6620 7772  lude)))...def wr
+00002f90: 6170 5f6d 6574 686f 645f 6f6e 6365 2866  ap_method_once(f
+00002fa0: 756e 3a20 4361 6c6c 6162 6c65 5b2e 2e2e  un: Callable[...
+00002fb0: 2c20 416e 795d 2920 2d3e 2043 616c 6c61  , Any]) -> Calla
+00002fc0: 626c 655b 2e2e 2e2c 2041 6e79 5d3a 0a20  ble[..., Any]:. 
+00002fd0: 2022 2222 4d61 6e61 6765 7320 4d6f 6475   """Manages Modu
+00002fe0: 6c65 2073 7461 7465 2066 6f72 2061 2067  le state for a g
+00002ff0: 6976 656e 2075 7365 722d 6465 6669 6e65  iven user-define
+00003000: 6420 6d65 7468 6f64 2e0a 0a20 2041 7267  d method...  Arg
+00003010: 733a 0a20 2020 2066 756e 3a20 5573 6572  s:.    fun: User
+00003020: 2d64 6566 696e 6564 204d 6f64 756c 6520  -defined Module 
+00003030: 6d65 7468 6f64 2074 6f20 6d61 6e61 6765  method to manage
+00003040: 2073 7461 7465 2066 6f72 2e0a 2020 5265   state for..  Re
+00003050: 7475 726e 733a 0a20 2020 2057 7261 7070  turns:.    Wrapp
+00003060: 6564 206d 6574 686f 642e 0a20 2022 2222  ed method..  """
+00003070: 0a20 2023 2044 6f6e 2774 2072 6577 7261  .  # Don't rewra
+00003080: 7020 6d65 7468 6f64 7320 7468 6174 2068  p methods that h
+00003090: 6176 6520 616c 7265 6164 7920 6861 6420  ave already had 
+000030a0: 7468 6520 7374 6174 6520 6d61 6e61 6765  the state manage
+000030b0: 6d65 6e74 2077 7261 7070 6572 0a20 2023  ment wrapper.  #
+000030c0: 2061 7070 6c69 6564 2069 6e20 7468 6520   applied in the 
+000030d0: 6465 636f 7261 746f 7220 7374 6163 6b2e  decorator stack.
+000030e0: 2020 5468 6973 2077 7261 7070 6572 2073    This wrapper s
+000030f0: 686f 756c 6420 616c 7761 7973 2062 6520  hould always be 
+00003100: 6170 706c 6965 640a 2020 2320 6265 666f  applied.  # befo
+00003110: 7265 2074 7261 6e73 666f 726d 6174 696f  re transformatio
+00003120: 6e20 7772 6170 7065 7273 2e0a 2020 6966  n wrappers..  if
+00003130: 2068 6173 6174 7472 2866 756e 2c20 276d   hasattr(fun, 'm
+00003140: 6574 686f 645f 6861 6e64 6c65 725f 7772  ethod_handler_wr
+00003150: 6170 7065 6427 293a 0a20 2020 2072 6574  apped'):.    ret
+00003160: 7572 6e20 6675 6e0a 0a20 2040 6675 6e63  urn fun..  @func
+00003170: 746f 6f6c 732e 7772 6170 7328 6675 6e29  tools.wraps(fun)
+00003180: 0a20 2064 6566 2077 7261 7070 6564 5f6d  .  def wrapped_m
+00003190: 6f64 756c 655f 6d65 7468 6f64 282a 6172  odule_method(*ar
+000031a0: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
+000031b0: 2020 2023 2057 6520 6d69 6768 7420 6861     # We might ha
+000031c0: 7665 2069 6e63 6f72 7265 6374 6c79 2077  ve incorrectly w
+000031d0: 7261 7070 7065 6420 6120 6361 6c6c 6162  rappped a callab
+000031e0: 6c65 0a20 2020 2023 2074 6861 7420 6973  le.    # that is
+000031f0: 206e 6f74 2061 206d 6574 686f 642e 2043   not a method. C
+00003200: 6865 636b 2077 6865 7468 6572 2074 6865  heck whether the
+00003210: 2066 6972 7374 2061 7267 2069 7320 7365   first arg is se
+00003220: 6c66 2c0a 2020 2020 2320 6f74 6865 7277  lf,.    # otherw
+00003230: 6973 6520 6361 6c6c 2074 6865 2077 7261  ise call the wra
+00003240: 7070 6564 2066 756e 6374 696f 6e20 6173  pped function as
+00003250: 2069 732e 0a20 2020 2069 6620 6172 6773   is..    if args
+00003260: 2061 6e64 2069 7369 6e73 7461 6e63 6528   and isinstance(
+00003270: 6172 6773 5b30 5d2c 204d 6f64 756c 6529  args[0], Module)
+00003280: 3a0a 2020 2020 2020 7365 6c66 2c20 6172  :.      self, ar
+00003290: 6773 203d 2061 7267 735b 305d 2c20 6172  gs = args[0], ar
+000032a0: 6773 5b31 3a5d 0a20 2020 2020 2072 6574  gs[1:].      ret
+000032b0: 7572 6e20 7365 6c66 2e5f 6361 6c6c 5f77  urn self._call_w
+000032c0: 7261 7070 6564 5f6d 6574 686f 6428 6675  rapped_method(fu
+000032d0: 6e2c 2061 7267 732c 206b 7761 7267 7329  n, args, kwargs)
+000032e0: 0a20 2020 2065 6c73 653a 0a20 2020 2020  .    else:.     
+000032f0: 2072 6574 7572 6e20 6675 6e28 2a61 7267   return fun(*arg
+00003300: 732c 202a 2a6b 7761 7267 7329 0a20 2077  s, **kwargs).  w
+00003310: 7261 7070 6564 5f6d 6f64 756c 655f 6d65  rapped_module_me
+00003320: 7468 6f64 2e6d 6574 686f 645f 6861 6e64  thod.method_hand
+00003330: 6c65 725f 7772 6170 7065 6420 3d20 5472  ler_wrapped = Tr
+00003340: 7565 2020 2320 7479 7065 3a20 6967 6e6f  ue  # type: igno
+00003350: 7265 5b61 7474 722d 6465 6669 6e65 645d  re[attr-defined]
+00003360: 0a20 2072 6574 7572 6e20 7772 6170 7065  .  return wrappe
+00003370: 645f 6d6f 6475 6c65 5f6d 6574 686f 640a  d_module_method.
+00003380: 0a64 6566 2077 7261 705f 6465 7363 7269  .def wrap_descri
+00003390: 7074 6f72 5f6f 6e63 6528 6465 7363 7269  ptor_once(descri
+000033a0: 7074 6f72 2920 2d3e 2022 4465 7363 7269  ptor) -> "Descri
+000033b0: 7074 6f72 5772 6170 7065 7222 3a0a 2020  ptorWrapper":.  
+000033c0: 2222 2257 7261 7073 2061 2064 6573 6372  """Wraps a descr
+000033d0: 6970 746f 7220 746f 2067 6976 6520 6265  iptor to give be
+000033e0: 7474 6572 2065 7272 6f72 206d 6573 7361  tter error messa
+000033f0: 6765 732e 0a0a 2020 4172 6773 3a0a 2020  ges...  Args:.  
+00003400: 2020 7072 6f70 3a20 5573 6572 2d64 6566    prop: User-def
+00003410: 696e 6564 204d 6f64 756c 6520 6174 7472  ined Module attr
+00003420: 6962 7574 6520 6465 7363 7269 7074 6f72  ibute descriptor
+00003430: 2e0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
+00003440: 2057 7261 7070 6564 2064 6573 6372 6970   Wrapped descrip
+00003450: 746f 722e 0a20 2022 2222 0a20 2023 2044  tor..  """.  # D
+00003460: 6f6e 2774 2072 6577 7261 7020 6465 7363  on't rewrap desc
+00003470: 7269 7074 6f72 732e 0a20 2069 6620 6973  riptors..  if is
+00003480: 696e 7374 616e 6365 2864 6573 6372 6970  instance(descrip
+00003490: 746f 722c 2044 6573 6372 6970 746f 7257  tor, DescriptorW
+000034a0: 7261 7070 6572 293a 0a20 2020 2072 6574  rapper):.    ret
+000034b0: 7572 6e20 6465 7363 7269 7074 6f72 0a0a  urn descriptor..
+000034c0: 2020 7265 7475 726e 2063 7265 6174 655f    return create_
+000034d0: 6465 7363 7269 7074 6f72 5f77 7261 7070  descriptor_wrapp
+000034e0: 6572 2864 6573 6372 6970 746f 7229 0a0a  er(descriptor)..
+000034f0: 0a64 6566 205f 7772 6170 5f68 6173 6828  .def _wrap_hash(
+00003500: 6861 7368 5f66 6e3a 2043 616c 6c61 626c  hash_fn: Callabl
+00003510: 655b 2e2e 2e2c 2041 6e79 5d29 202d 3e20  e[..., Any]) -> 
+00003520: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
+00003530: 795d 3a0a 2020 2222 2257 7261 7073 2061  y]:.  """Wraps a
+00003540: 2068 6173 6820 6675 6e63 7469 6f6e 2077   hash function w
+00003550: 6974 6820 736f 6d65 2063 6865 636b 2066  ith some check f
+00003560: 6f72 2046 6c61 7820 4d6f 6475 6c65 732e  or Flax Modules.
+00003570: 2222 220a 2020 4066 756e 6374 6f6f 6c73  """.  @functools
+00003580: 2e77 7261 7073 2868 6173 685f 666e 290a  .wraps(hash_fn).
+00003590: 2020 6465 6620 7772 6170 7065 6428 7365    def wrapped(se
+000035a0: 6c66 293a 0a20 2020 2069 6620 7365 6c66  lf):.    if self
+000035b0: 2e73 636f 7065 2069 7320 6e6f 7420 4e6f  .scope is not No
+000035c0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+000035d0: 5479 7065 4572 726f 7228 2743 616e 5c27  TypeError('Can\'
+000035e0: 7420 6361 6c6c 205f 5f68 6173 685f 5f20  t call __hash__ 
+000035f0: 6f6e 206d 6f64 756c 6573 2074 6861 7420  on modules that 
+00003600: 686f 6c64 2076 6172 6961 626c 6573 2e27  hold variables.'
+00003610: 290a 2020 2020 7472 793a 0a20 2020 2020  ).    try:.     
+00003620: 2068 6173 685f 7661 6c75 6520 3d20 6861   hash_value = ha
+00003630: 7368 5f66 6e28 7365 6c66 290a 2020 2020  sh_fn(self).    
+00003640: 6578 6365 7074 2054 7970 6545 7272 6f72  except TypeError
+00003650: 2061 7320 6578 633a 0a20 2020 2020 2072   as exc:.      r
+00003660: 6169 7365 2054 7970 6545 7272 6f72 2827  aise TypeError('
+00003670: 4661 696c 6564 2074 6f20 6861 7368 2046  Failed to hash F
+00003680: 6c61 7820 4d6f 6475 6c65 2e20 2027 0a20  lax Module.  '. 
+00003690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000036a0: 2020 2020 2027 5468 6520 6d6f 6475 6c65       'The module
+000036b0: 2070 726f 6261 626c 7920 636f 6e74 6169   probably contai
+000036c0: 6e73 2075 6e68 6173 6861 626c 6520 6174  ns unhashable at
+000036d0: 7472 6962 7574 6573 2e20 2027 0a20 2020  tributes.  '.   
+000036e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000036f0: 2020 2066 274d 6f64 756c 653d 7b73 656c     f'Module={sel
+00003700: 667d 2729 2066 726f 6d20 6578 630a 2020  f}') from exc.  
+00003710: 2020 7265 7475 726e 2068 6173 685f 7661    return hash_va
+00003720: 6c75 650a 2020 7265 7475 726e 2077 7261  lue.  return wra
+00003730: 7070 6564 0a0a 0a64 6566 205f 6765 745f  pped...def _get_
+00003740: 756e 626f 756e 645f 666e 286d 6574 686f  unbound_fn(metho
+00003750: 645f 6f72 5f66 6e3a 2043 616c 6c61 626c  d_or_fn: Callabl
+00003760: 655b 2e2e 2e2c 2041 6e79 5d29 202d 3e20  e[..., Any]) -> 
+00003770: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
+00003780: 795d 3a0a 2020 2222 2252 6574 7572 6e73  y]:.  """Returns
+00003790: 2061 6e20 756e 626f 756e 6420 6675 6e63   an unbound func
+000037a0: 7469 6f6e 2066 726f 6d20 6120 6d65 7468  tion from a meth
+000037b0: 6f64 2074 6861 7420 6973 2070 6f73 7369  od that is possi
+000037c0: 626c 7920 626f 756e 642e 0a0a 2020 5468  bly bound...  Th
+000037d0: 6973 206d 6561 6e73 2074 6861 7420 6966  is means that if
+000037e0: 2074 6865 2070 6173 7365 6420 6675 6e63   the passed func
+000037f0: 7469 6f6e 2062 656c 6f6e 6773 206f 6620  tion belongs of 
+00003800: 616e 2069 6e73 7461 6e63 6520 6f66 2061  an instance of a
+00003810: 2063 6c61 7373 2c20 7468 656e 0a20 2074   class, then.  t
+00003820: 6865 2072 6574 7572 6e65 6420 6675 6e63  he returned func
+00003830: 7469 6f6e 2064 6f65 7320 6e6f 206c 6f6e  tion does no lon
+00003840: 6765 7220 6465 7065 6e64 206f 6e20 7468  ger depend on th
+00003850: 6520 696e 7374 616e 6365 2c20 7768 6963  e instance, whic
+00003860: 6820 6973 2070 6173 7365 640a 2020 6173  h is passed.  as
+00003870: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
+00003880: 656e 7420 746f 2074 6865 2066 756e 6374  ent to the funct
+00003890: 696f 6e2e 0a0a 2020 4172 6773 3a0a 2020  ion...  Args:.  
+000038a0: 2020 6d65 7468 6f64 5f6f 725f 666e 3a20    method_or_fn: 
+000038b0: 4120 636c 6173 7320 6d65 7468 6f64 206f  A class method o
+000038c0: 7220 6675 6e63 7469 6f6e 2e0a 2020 5265  r function..  Re
+000038d0: 7475 726e 733a 0a20 2020 2041 6e20 756e  turns:.    An un
+000038e0: 626f 756e 6420 7665 7273 696f 6e20 6f66  bound version of
+000038f0: 2069 6e70 7574 2066 756e 6374 696f 6e2e   input function.
+00003900: 0a20 2022 2222 0a20 2069 6620 2869 6e73  .  """.  if (ins
+00003910: 7065 6374 2e69 736d 6574 686f 6428 6d65  pect.ismethod(me
+00003920: 7468 6f64 5f6f 725f 666e 2920 616e 640a  thod_or_fn) and.
+00003930: 2020 2020 2020 6973 696e 7374 616e 6365        isinstance
+00003940: 286d 6574 686f 645f 6f72 5f66 6e2e 5f5f  (method_or_fn.__
+00003950: 7365 6c66 5f5f 2c20 4d6f 6475 6c65 2929  self__, Module))
+00003960: 3a20 2023 2070 7974 7970 653a 2064 6973  :  # pytype: dis
+00003970: 6162 6c65 3d61 7474 7269 6275 7465 2d65  able=attribute-e
+00003980: 7272 6f72 0a20 2020 206d 6574 686f 645f  rror.    method_
+00003990: 6f72 5f66 6e20 3d20 6d65 7468 6f64 5f6f  or_fn = method_o
+000039a0: 725f 666e 2e5f 5f66 756e 635f 5f20 2023  r_fn.__func__  #
+000039b0: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
+000039c0: 3d61 7474 7269 6275 7465 2d65 7272 6f72  =attribute-error
+000039d0: 0a0a 2020 2320 5468 6520 6d65 7468 6f64  ..  # The method
+000039e0: 2073 686f 756c 6420 6265 2063 616c 6c61   should be calla
+000039f0: 626c 652c 2061 6e64 2069 7420 7368 6f75  ble, and it shou
+00003a00: 6c64 2068 6176 6520 6174 206c 6561 7374  ld have at least
+00003a10: 206f 6e65 2061 7267 756d 656e 740a 2020   one argument.  
+00003a20: 2320 7265 7072 6573 656e 7469 6e67 2074  # representing t
+00003a30: 6865 2063 6c61 7373 2074 6861 7420 6973  he class that is
+00003a40: 2070 6173 7365 6420 696e 2e0a 2020 6966   passed in..  if
+00003a50: 2028 6e6f 7420 6361 6c6c 6162 6c65 286d   (not callable(m
+00003a60: 6574 686f 645f 6f72 5f66 6e29 206f 720a  ethod_or_fn) or.
+00003a70: 2020 2020 2020 6c65 6e28 696e 7370 6563        len(inspec
+00003a80: 742e 7369 676e 6174 7572 6528 6d65 7468  t.signature(meth
+00003a90: 6f64 5f6f 725f 666e 292e 7061 7261 6d65  od_or_fn).parame
+00003aa0: 7465 7273 2920 3c20 3129 3a0a 2020 2020  ters) < 1):.    
+00003ab0: 7261 6973 6520 6572 726f 7273 2e41 7070  raise errors.App
+00003ac0: 6c79 4d6f 6475 6c65 496e 7661 6c69 644d  lyModuleInvalidM
+00003ad0: 6574 686f 6445 7272 6f72 286d 6574 686f  ethodError(metho
+00003ae0: 645f 6f72 5f66 6e29 0a0a 2020 7265 7475  d_or_fn)..  retu
+00003af0: 726e 206d 6574 686f 645f 6f72 5f66 6e0a  rn method_or_fn.
+00003b00: 0a64 6566 205f 6d61 705f 7375 626d 6f64  .def _map_submod
+00003b10: 756c 6573 2866 6e3a 2043 616c 6c61 626c  ules(fn: Callabl
+00003b20: 655b 5b27 4d6f 6475 6c65 275d 2c20 416e  e[['Module'], An
+00003b30: 795d 2c20 7472 6565 293a 0a20 2022 2222  y], tree):.  """
+00003b40: 4d61 7020 6120 6675 6e63 7469 6f6e 206f  Map a function o
+00003b50: 7665 7220 616c 6c20 7375 626d 6f64 756c  ver all submodul
+00003b60: 6573 2069 6e20 6120 7472 6565 2e22 2222  es in a tree."""
+00003b70: 0a20 2067 203d 206c 616d 6264 6120 5f2c  .  g = lambda _,
+00003b80: 2078 3a20 666e 2878 2920 6966 2069 7369   x: fn(x) if isi
+00003b90: 6e73 7461 6e63 6528 782c 204d 6f64 756c  nstance(x, Modul
+00003ba0: 6529 2065 6c73 6520 780a 2020 7265 7475  e) else x.  retu
+00003bb0: 726e 205f 6672 6565 7a65 5f61 7474 7228  rn _freeze_attr(
+00003bc0: 5f6d 6170 5f6f 7665 725f 6d6f 6475 6c65  _map_over_module
+00003bd0: 735f 696e 5f74 7265 6528 672c 2074 7265  s_in_tree(g, tre
+00003be0: 6529 290a 0a63 6c61 7373 2053 6574 7570  e))..class Setup
+00003bf0: 5374 6174 6528 656e 756d 2e49 6e74 456e  State(enum.IntEn
+00003c00: 756d 293a 0a20 2023 2073 6574 7570 2829  um):.  # setup()
+00003c10: 2068 6173 206e 6f74 2062 6565 6e20 6361   has not been ca
+00003c20: 6c6c 6564 2e0a 2020 4e45 5720 3d20 300a  lled..  NEW = 0.
+00003c30: 2020 2320 7365 7475 7028 2920 6861 7320    # setup() has 
+00003c40: 6265 656e 2063 616c 6c65 6420 6f75 7473  been called outs
+00003c50: 6964 6520 6120 7472 616e 7366 6f72 6d20  ide a transform 
+00003c60: 626f 756e 6461 7279 2e0a 2020 5452 414e  boundary..  TRAN
+00003c70: 5346 4f52 4d45 4420 3d20 310a 2020 2320  SFORMED = 1.  # 
+00003c80: 7365 7475 7028 2920 6861 7320 6265 656e  setup() has been
+00003c90: 2063 616c 6c65 642e 0a20 2044 4f4e 4520   called..  DONE 
+00003ca0: 3d20 320a 0a0a 4064 6174 6163 6c61 7373  = 2...@dataclass
+00003cb0: 6573 2e64 6174 6163 6c61 7373 0a63 6c61  es.dataclass.cla
+00003cc0: 7373 205f 4d6f 6475 6c65 496e 7465 726e  ss _ModuleIntern
+00003cd0: 616c 5374 6174 653a 0a20 2022 2222 4570  alState:.  """Ep
+00003ce0: 6865 6d65 7261 6c20 4d6f 6475 6c65 2045  hemeral Module E
+00003cf0: 7661 6c75 6174 696f 6e20 5374 6174 652e  valuation State.
+00003d00: 0a0a 2020 466f 7220 636c 6172 6974 792c  ..  For clarity,
+00003d10: 2077 6520 636f 6c6c 6563 7420 616c 6c20   we collect all 
+00003d20: 6f66 2074 6865 2074 656d 706f 7261 7279  of the temporary
+00003d30: 2066 6c61 6773 2061 6e64 2065 7068 656d   flags and ephem
+00003d40: 6572 616c 2073 7461 7465 2075 7365 6420  eral state used 
+00003d50: 6279 0a20 204d 6f64 756c 6573 2066 6f72  by.  Modules for
+00003d60: 2061 7574 6f6e 616d 696e 6720 616e 6420   autonaming and 
+00003d70: 6572 726f 7220 6d65 7373 6167 6573 2068  error messages h
+00003d80: 6572 652c 2061 6c6f 6e67 7369 6465 2074  ere, alongside t
+00003d90: 6865 2072 756c 6573 2075 7365 640a 2020  he rules used.  
+00003da0: 746f 2070 6173 7320 7468 6973 2065 7068  to pass this eph
+00003db0: 656d 6572 616c 2073 7461 7465 2061 6372  emeral state acr
+00003dc0: 6f73 7320 7472 616e 7366 6f72 6d20 626f  oss transform bo
+00003dd0: 756e 6461 7269 6573 2e0a 2020 2222 220a  undaries..  """.
+00003de0: 2020 696e 5f63 6f6d 7061 6374 5f6d 6574    in_compact_met
+00003df0: 686f 643a 2062 6f6f 6c20 3d20 4661 6c73  hod: bool = Fals
+00003e00: 650a 2020 696e 5f73 6574 7570 3a20 626f  e.  in_setup: bo
+00003e10: 6f6c 203d 2046 616c 7365 0a20 2073 6574  ol = False.  set
+00003e20: 7570 5f63 616c 6c65 643a 2053 6574 7570  up_called: Setup
+00003e30: 5374 6174 6520 3d20 5365 7475 7053 7461  State = SetupSta
+00003e40: 7465 2e4e 4557 0a20 2069 735f 696e 6974  te.NEW.  is_init
+00003e50: 6961 6c69 7a65 643a 2062 6f6f 6c20 3d20  ialized: bool = 
+00003e60: 4661 6c73 650a 2020 6175 746f 6e61 6d65  False.  autoname
+00003e70: 5f63 7572 736f 723a 2044 6963 745b 7374  _cursor: Dict[st
+00003e80: 722c 2069 6e74 5d20 3d20 6461 7461 636c  r, int] = datacl
+00003e90: 6173 7365 732e 6669 656c 6428 6465 6661  asses.field(defa
+00003ea0: 756c 745f 6661 6374 6f72 793d 6469 6374  ult_factory=dict
+00003eb0: 290a 2020 6368 696c 6472 656e 3a20 4469  ).  children: Di
+00003ec0: 6374 5b73 7472 2c20 556e 696f 6e5b 7374  ct[str, Union[st
+00003ed0: 722c 2027 4d6f 6475 6c65 275d 5d20 3d20  r, 'Module']] = 
+00003ee0: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
+00003ef0: 6428 0a20 2020 2020 2064 6566 6175 6c74  d(.      default
+00003f00: 5f66 6163 746f 7279 3d64 6963 7429 0a0a  _factory=dict)..
+00003f10: 2020 6465 6620 7265 7365 7428 7365 6c66    def reset(self
+00003f20: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    "
+00003f30: 2222 5265 7365 7473 2074 7261 6e73 6965  ""Resets transie
+00003f40: 6e74 2073 7461 7465 2e0a 0a20 2020 2054  nt state...    T
+00003f50: 6869 7320 6675 6e63 7469 6f6e 2069 7320  his function is 
+00003f60: 6361 6c6c 6564 2061 6674 6572 2065 6163  called after eac
+00003f70: 6820 6d6f 6475 6c65 206d 6574 686f 642c  h module method,
+00003f80: 2073 6f20 6f6e 6c79 2061 7474 7269 6275   so only attribu
+00003f90: 7465 7320 7468 6174 0a20 2020 2061 7265  tes that.    are
+00003fa0: 206d 6574 686f 642d 6465 7065 6e64 656e   method-dependen
+00003fb0: 7420 6172 6520 7265 7365 742e 0a20 2020  t are reset..   
+00003fc0: 2022 2222 0a20 2020 2073 656c 662e 696e   """.    self.in
+00003fd0: 5f63 6f6d 7061 6374 5f6d 6574 686f 6420  _compact_method 
+00003fe0: 3d20 4661 6c73 650a 2020 2020 7365 6c66  = False.    self
+00003ff0: 2e69 6e5f 7365 7475 7020 3d20 4661 6c73  .in_setup = Fals
+00004000: 650a 2020 2020 7365 6c66 2e61 7574 6f6e  e.    self.auton
+00004010: 616d 655f 6375 7273 6f72 203d 2064 6963  ame_cursor = dic
+00004020: 7428 290a 0a20 2064 6566 2065 7870 6f72  t()..  def expor
+00004030: 7428 7365 6c66 2920 2d3e 2027 5f4d 6f64  t(self) -> '_Mod
+00004040: 756c 6549 6e74 6572 6e61 6c53 7461 7465  uleInternalState
+00004050: 273a 0a20 2020 2022 2222 4578 706f 7274  ':.    """Export
+00004060: 7320 7472 616e 7366 6f72 6d2d 7072 6573  s transform-pres
+00004070: 6572 7665 6420 7374 6174 6520 6163 726f  erved state acro
+00004080: 7373 2074 7261 6e73 666f 726d 2062 6f75  ss transform bou
+00004090: 6e64 6172 792e 2222 220a 2020 2020 7365  ndary.""".    se
+000040a0: 7475 705f 7374 6174 6520 3d20 5365 7475  tup_state = Setu
+000040b0: 7053 7461 7465 2e54 5241 4e53 464f 524d  pState.TRANSFORM
+000040c0: 4544 2069 6620 7365 6c66 2e73 6574 7570  ED if self.setup
+000040d0: 5f63 616c 6c65 6420 656c 7365 2053 6574  _called else Set
+000040e0: 7570 5374 6174 652e 4e45 570a 2020 2020  upState.NEW.    
+000040f0: 636c 6f6e 6564 203d 205f 4d6f 6475 6c65  cloned = _Module
+00004100: 496e 7465 726e 616c 5374 6174 6528 0a20  InternalState(. 
+00004110: 2020 2020 2020 2069 6e5f 636f 6d70 6163         in_compac
+00004120: 745f 6d65 7468 6f64 3d73 656c 662e 696e  t_method=self.in
+00004130: 5f63 6f6d 7061 6374 5f6d 6574 686f 642c  _compact_method,
+00004140: 0a20 2020 2020 2020 2069 6e5f 7365 7475  .        in_setu
+00004150: 703d 7365 6c66 2e69 6e5f 7365 7475 702c  p=self.in_setup,
+00004160: 0a20 2020 2020 2020 2073 6574 7570 5f63  .        setup_c
+00004170: 616c 6c65 643d 7365 7475 705f 7374 6174  alled=setup_stat
+00004180: 652c 0a20 2020 2020 2020 2069 735f 696e  e,.        is_in
+00004190: 6974 6961 6c69 7a65 643d 7365 6c66 2e69  itialized=self.i
+000041a0: 735f 696e 6974 6961 6c69 7a65 642c 0a20  s_initialized,. 
+000041b0: 2020 2020 2020 2061 7574 6f6e 616d 655f         autoname_
+000041c0: 6375 7273 6f72 3d64 6963 7428 7365 6c66  cursor=dict(self
+000041d0: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
+000041e0: 2929 0a20 2020 2072 6574 7572 6e20 636c  )).    return cl
+000041f0: 6f6e 6564 0a0a 2020 6465 6620 7265 696d  oned..  def reim
+00004200: 706f 7274 2873 656c 662c 206f 7468 6572  port(self, other
+00004210: 3a20 275f 4d6f 6475 6c65 496e 7465 726e  : '_ModuleIntern
+00004220: 616c 5374 6174 6527 2920 2d3e 204e 6f6e  alState') -> Non
+00004230: 653a 0a20 2020 2022 2222 5265 2d69 6d70  e:.    """Re-imp
+00004240: 6f72 7473 2074 7261 6e73 666f 726d 2d70  orts transform-p
+00004250: 7265 7365 7276 6564 2073 7461 7465 2066  reserved state f
+00004260: 726f 6d20 6163 726f 7373 2074 7261 6e73  rom across trans
+00004270: 666f 726d 2062 6f75 6e64 6172 792e 2222  form boundary.""
+00004280: 220a 2020 2020 7365 6c66 2e69 6e5f 636f  ".    self.in_co
+00004290: 6d70 6163 745f 6d65 7468 6f64 203d 206f  mpact_method = o
+000042a0: 7468 6572 2e69 6e5f 636f 6d70 6163 745f  ther.in_compact_
+000042b0: 6d65 7468 6f64 0a20 2020 2073 656c 662e  method.    self.
+000042c0: 696e 5f73 6574 7570 203d 206f 7468 6572  in_setup = other
+000042d0: 2e69 6e5f 7365 7475 700a 2020 2020 7365  .in_setup.    se
+000042e0: 6c66 2e69 735f 696e 6974 6961 6c69 7a65  lf.is_initialize
+000042f0: 6420 3d20 6f74 6865 722e 6973 5f69 6e69  d = other.is_ini
+00004300: 7469 616c 697a 6564 0a20 2020 2073 656c  tialized.    sel
+00004310: 662e 6175 746f 6e61 6d65 5f63 7572 736f  f.autoname_curso
+00004320: 7220 3d20 6469 6374 286f 7468 6572 2e61  r = dict(other.a
+00004330: 7574 6f6e 616d 655f 6375 7273 6f72 290a  utoname_cursor).
+00004340: 0a5f 756e 696e 6974 6961 6c69 7a65 645f  ._uninitialized_
+00004350: 6d6f 6475 6c65 5f69 6e74 6572 6e61 6c5f  module_internal_
+00004360: 7374 6174 6520 3d20 5f4d 6f64 756c 6549  state = _ModuleI
+00004370: 6e74 6572 6e61 6c53 7461 7465 2829 0a0a  nternalState()..
+00004380: 0a5f 554e 4445 4649 4e45 445f 434f 5059  ._UNDEFINED_COPY
+00004390: 5f50 4943 4b4c 455f 4d45 5448 4f44 5320  _PICKLE_METHODS 
+000043a0: 3d20 280a 2020 2020 275f 5f67 6574 7374  = (.    '__getst
+000043b0: 6174 655f 5f27 2c20 275f 5f73 6574 7374  ate__', '__setst
+000043c0: 6174 655f 5f27 2c20 275f 5f67 6574 6e65  ate__', '__getne
+000043d0: 7761 7267 735f 6578 5f5f 272c 0a20 2020  wargs_ex__',.   
+000043e0: 2027 5f5f 7265 6475 6365 5f5f 272c 2027   '__reduce__', '
+000043f0: 5f5f 7265 6475 6365 5f65 785f 5f27 2c20  __reduce_ex__', 
+00004400: 275f 5f63 6f70 795f 5f27 2c20 275f 5f64  '__copy__', '__d
+00004410: 6565 7063 6f70 795f 5f27 290a 0a0a 5f63  eepcopy__')..._c
+00004420: 6163 6865 733a 2027 7765 616b 7265 662e  aches: 'weakref.
+00004430: 5765 616b 4b65 7944 6963 7469 6f6e 6172  WeakKeyDictionar
+00004440: 795b 5363 6f70 652c 2077 6561 6b72 6566  y[Scope, weakref
+00004450: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
+00004460: 6e61 7279 5b46 6c61 7849 642c 204d 6f64  nary[FlaxId, Mod
+00004470: 756c 655d 5d27 203d 2028 0a20 2020 2077  ule]]' = (.    w
+00004480: 6561 6b72 6566 2e57 6561 6b4b 6579 4469  eakref.WeakKeyDi
+00004490: 6374 696f 6e61 7279 2829 290a 0a0a 7475  ctionary())...tu
+000044a0: 706c 655f 7265 6475 6365 203d 206c 616d  ple_reduce = lam
+000044b0: 6264 6120 7873 2c20 783a 2078 7320 2b20  bda xs, x: xs + 
+000044c0: 2878 2c29 0a74 7570 6c65 5f69 6e69 7420  (x,).tuple_init 
+000044d0: 3d20 6c61 6d62 6461 3a20 2829 0a0a 0a63  = lambda: ()...c
+000044e0: 6170 7475 7265 5f63 616c 6c5f 696e 7465  apture_call_inte
+000044f0: 726d 6564 6961 7465 7320 3d20 6c61 6d62  rmediates = lamb
+00004500: 6461 205f 2c20 6d65 7468 6f64 5f6e 616d  da _, method_nam
+00004510: 653a 206d 6574 686f 645f 6e61 6d65 203d  e: method_name =
+00004520: 3d20 275f 5f63 616c 6c5f 5f27 0a0a 0a63  = '__call__'...c
+00004530: 6c61 7373 2050 6172 656e 7444 6573 6372  lass ParentDescr
+00004540: 6970 746f 723a 0a20 2022 2222 5772 6170  iptor:.  """Wrap
+00004550: 7320 7061 7265 6e74 206d 6f64 756c 6520  s parent module 
+00004560: 7265 6665 7265 6e63 6573 2069 6e20 7765  references in we
+00004570: 616b 2072 6566 732e 0a0a 2020 5468 6973  ak refs...  This
+00004580: 2070 7265 7665 6e74 7320 7265 6665 7265   prevents refere
+00004590: 6e63 6520 6379 636c 6573 2066 726f 6d20  nce cycles from 
+000045a0: 666f 726d 696e 6720 7669 6120 7061 7265  forming via pare
+000045b0: 6e74 206c 696e 6b73 2077 6869 6368 2063  nt links which c
+000045c0: 616e 206c 6561 640a 2020 746f 2061 6363  an lead.  to acc
+000045d0: 6964 656e 7461 6c20 4f4f 4d73 2069 6e20  idental OOMs in 
+000045e0: 6561 6765 7220 6d6f 6465 2064 7565 2074  eager mode due t
+000045f0: 6f20 736c 6f77 2067 6172 6261 6765 2063  o slow garbage c
+00004600: 6f6c 6c65 6374 696f 6e20 6173 2077 656c  ollection as wel
+00004610: 6c20 6173 0a20 2073 7075 7269 6f75 7320  l as.  spurious 
+00004620: 7472 6163 6572 206c 6561 6b73 2064 7572  tracer leaks dur
+00004630: 696e 6720 6a69 7420 636f 6d70 696c 6174  ing jit compilat
+00004640: 696f 6e2e 0a0a 2020 4e6f 7465 3a20 2264  ion...  Note: "d
+00004650: 6573 6372 6970 746f 7273 2220 6172 6520  escriptors" are 
+00004660: 7468 6520 756e 6465 726c 7969 6e67 2070  the underlying p
+00004670: 7974 686f 6e20 6d65 6368 616e 6973 6d20  ython mechanism 
+00004680: 666f 7220 696d 706c 656d 656e 7469 6e67  for implementing
+00004690: 0a20 2064 796e 616d 6963 2040 7072 6f70  .  dynamic @prop
+000046a0: 6572 7479 2064 6563 6f72 6174 6f72 732e  erty decorators.
+000046b0: 2020 5765 206e 6565 6420 746f 2075 7365    We need to use
+000046c0: 2061 2072 6177 2064 6573 6372 6970 746f   a raw descripto
+000046d0: 7220 696e 7374 6561 6420 6f66 2074 6865  r instead of the
+000046e0: 0a20 206d 6f72 6520 636f 6d6d 6f6e 2064  .  more common d
+000046f0: 6563 6f72 6174 6f72 2069 6e20 6f72 6465  ecorator in orde
+00004700: 7220 746f 2066 6f72 6365 2074 6861 7420  r to force that 
+00004710: 7468 6520 6170 7072 6f70 7269 6174 6520  the appropriate 
+00004720: 6765 7474 6572 2f73 6574 7465 720a 2020  getter/setter.  
+00004730: 6c6f 6769 6320 6170 706c 6965 7320 696e  logic applies in
+00004740: 2073 7562 636c 6173 7365 7320 6576 656e   subclasses even
+00004750: 2061 6674 6572 2076 6172 696f 7573 2064   after various d
+00004760: 6174 6163 6c61 7373 2074 7261 6e73 666f  ataclass transfo
+00004770: 726d 732e 0a20 2022 2222 0a20 2064 6566  rms..  """.  def
+00004780: 205f 5f67 6574 5f5f 2873 656c 662c 206f   __get__(self, o
+00004790: 626a 2c20 6f62 6a74 7970 653d 4e6f 6e65  bj, objtype=None
+000047a0: 293a 0a20 2020 2023 2063 6865 636b 2069  ):.    # check i
+000047b0: 6620 6f62 6a20 6973 204e 6f6e 652c 2068  f obj is None, h
+000047c0: 6170 7065 6e73 2064 7572 696e 6720 2561  appens during %a
+000047d0: 7574 6f72 656c 6f61 640a 2020 2020 6966  utoreload.    if
+000047e0: 206f 626a 2069 7320 4e6f 6e65 3a0a 2020   obj is None:.  
+000047f0: 2020 2020 7265 7475 726e 204e 6f6e 650a      return None.
+00004800: 2020 2020 7061 7265 6e74 203d 206f 626a      parent = obj
+00004810: 6563 742e 5f5f 6765 7461 7474 7269 6275  ect.__getattribu
+00004820: 7465 5f5f 286f 626a 2c20 225f 7061 7265  te__(obj, "_pare
+00004830: 6e74 5f72 6566 2229 0a20 2020 2072 6574  nt_ref").    ret
+00004840: 7572 6e20 7061 7265 6e74 2829 2069 6620  urn parent() if 
+00004850: 6973 696e 7374 616e 6365 2870 6172 656e  isinstance(paren
+00004860: 742c 2077 6561 6b72 6566 2e52 6566 6572  t, weakref.Refer
+00004870: 656e 6365 5479 7065 2920 656c 7365 2070  enceType) else p
+00004880: 6172 656e 740a 0a20 2064 6566 205f 5f73  arent..  def __s
+00004890: 6574 5f5f 2873 656c 662c 206f 626a 2c20  et__(self, obj, 
+000048a0: 7661 6c75 6529 3a0a 2020 2020 6d61 7962  value):.    mayb
+000048b0: 655f 7765 616b 203d 2077 6561 6b72 6566  e_weak = weakref
+000048c0: 2e72 6566 2876 616c 7565 2920 6966 2069  .ref(value) if i
+000048d0: 7369 6e73 7461 6e63 6528 7661 6c75 652c  sinstance(value,
+000048e0: 204d 6f64 756c 6529 2065 6c73 6520 7661   Module) else va
+000048f0: 6c75 650a 2020 2020 6f62 6a65 6374 2e5f  lue.    object._
+00004900: 5f73 6574 6174 7472 5f5f 286f 626a 2c20  _setattr__(obj, 
+00004910: 225f 7061 7265 6e74 5f72 6566 222c 206d  "_parent_ref", m
+00004920: 6179 6265 5f77 6561 6b29 0a0a 0a63 6c61  aybe_weak)...cla
+00004930: 7373 2044 6573 6372 6970 746f 7228 5072  ss Descriptor(Pr
+00004940: 6f74 6f63 6f6c 293a 0a20 205f 5f69 7361  otocol):.  __isa
+00004950: 6273 7472 6163 746d 6574 686f 645f 5f3a  bstractmethod__:
+00004960: 2062 6f6f 6c0a 2020 6465 6620 5f5f 6765   bool.  def __ge
+00004970: 745f 5f28 7365 6c66 2c20 6f62 6a2c 206f  t__(self, obj, o
+00004980: 626a 7479 7065 3d4e 6f6e 6529 202d 3e20  bjtype=None) -> 
+00004990: 416e 793a 202e 2e2e 0a20 2064 6566 205f  Any: ....  def _
+000049a0: 5f73 6574 5f5f 2873 656c 662c 206f 626a  _set__(self, obj
+000049b0: 2c20 7661 6c75 6529 202d 3e20 4e6f 6e65  , value) -> None
+000049c0: 3a20 2e2e 2e0a 2020 6465 6620 5f5f 6465  : ....  def __de
+000049d0: 6c65 7465 5f5f 2873 656c 662c 206f 626a  lete__(self, obj
+000049e0: 2920 2d3e 204e 6f6e 653a 202e 2e2e 0a20  ) -> None: .... 
+000049f0: 2064 6566 205f 5f73 6574 5f6e 616d 655f   def __set_name_
+00004a00: 5f28 7365 6c66 2c20 6f77 6e65 722c 206e  _(self, owner, n
+00004a10: 616d 6529 202d 3e20 4e6f 6e65 3a20 2e2e  ame) -> None: ..
+00004a20: 2e0a 0a63 6c61 7373 2044 6573 6372 6970  ...class Descrip
+00004a30: 746f 7257 7261 7070 6572 3a0a 2020 7061  torWrapper:.  pa
+00004a40: 7373 0a0a 6465 6620 6372 6561 7465 5f64  ss..def create_d
+00004a50: 6573 6372 6970 746f 725f 7772 6170 7065  escriptor_wrappe
+00004a60: 7228 6465 7363 7269 7074 6f72 3a20 4465  r(descriptor: De
+00004a70: 7363 7269 7074 6f72 293a 0a20 2022 2222  scriptor):.  """
+00004a80: 4372 6561 7465 7320 6120 6465 7363 7269  Creates a descri
+00004a90: 7074 6f72 2077 7261 7070 6572 2074 6861  ptor wrapper tha
+00004aa0: 7420 6361 6c6c 7320 6120 6765 745f 666e  t calls a get_fn
+00004ab0: 206f 6e20 7468 6520 6465 7363 7269 7074   on the descript
+00004ac0: 6f72 2e22 2222 0a0a 2020 636c 6173 7320  or."""..  class 
+00004ad0: 5f44 6573 6372 6970 746f 7257 7261 7070  _DescriptorWrapp
+00004ae0: 6572 2844 6573 6372 6970 746f 7257 7261  er(DescriptorWra
+00004af0: 7070 6572 293a 0a20 2020 2022 2222 4120  pper):.    """A 
+00004b00: 6465 7363 7269 7074 6f72 2074 6861 7420  descriptor that 
+00004b10: 6361 6e20 7772 6170 2061 6e79 2064 6573  can wrap any des
+00004b20: 6372 6970 746f 7222 2222 0a0a 2020 2020  criptor"""..    
+00004b30: 6966 2068 6173 6174 7472 2864 6573 6372  if hasattr(descr
+00004b40: 6970 746f 722c 2027 5f5f 6973 6162 7374  iptor, '__isabst
+00004b50: 7261 6374 6d65 7468 6f64 5f5f 2729 3a0a  ractmethod__'):.
+00004b60: 2020 2020 2020 5f5f 6973 6162 7374 7261        __isabstra
+00004b70: 6374 6d65 7468 6f64 5f5f 203d 2064 6573  ctmethod__ = des
+00004b80: 6372 6970 746f 722e 5f5f 6973 6162 7374  criptor.__isabst
+00004b90: 7261 6374 6d65 7468 6f64 5f5f 0a0a 2020  ractmethod__..  
+00004ba0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+00004bb0: 656c 662c 2077 7261 7070 6564 3a20 4465  elf, wrapped: De
+00004bc0: 7363 7269 7074 6f72 293a 0a20 2020 2020  scriptor):.     
+00004bd0: 2073 656c 662e 7772 6170 7065 6420 3d20   self.wrapped = 
+00004be0: 7772 6170 7065 640a 0a20 2020 2023 2063  wrapped..    # c
+00004bf0: 6f6e 6469 7469 6f6e 616c 6c79 2064 6566  onditionally def
+00004c00: 696e 6520 6465 7363 7269 7074 6f72 206d  ine descriptor m
+00004c10: 6574 686f 6473 0a20 2020 2069 6620 6861  ethods.    if ha
+00004c20: 7361 7474 7228 6465 7363 7269 7074 6f72  sattr(descriptor
+00004c30: 2c20 275f 5f67 6574 5f5f 2729 3a0a 2020  , '__get__'):.  
+00004c40: 2020 2020 6465 6620 5f5f 6765 745f 5f28      def __get__(
+00004c50: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+00004c60: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
+00004c70: 2320 6865 7265 2077 6520 7769 6c6c 2063  # here we will c
+00004c80: 6174 6368 2069 6e74 6572 6e61 6c20 4174  atch internal At
+00004c90: 7472 6962 7574 6545 7272 6f72 2061 6e64  tributeError and
+00004ca0: 2072 652d 7261 6973 6520 6974 2061 7320   re-raise it as 
+00004cb0: 610a 2020 2020 2020 2020 2320 6d6f 7265  a.        # more
+00004cc0: 2069 6e66 6f72 6d61 7469 7665 2061 6e64   informative and
+00004cd0: 2063 6f72 7265 6374 2065 7272 6f72 206d   correct error m
+00004ce0: 6573 7361 6765 2e0a 2020 2020 2020 2020  essage..        
+00004cf0: 7472 793a 0a20 2020 2020 2020 2020 2072  try:.          r
+00004d00: 6574 7572 6e20 7365 6c66 2e77 7261 7070  eturn self.wrapp
+00004d10: 6564 2e5f 5f67 6574 5f5f 282a 6172 6773  ed.__get__(*args
+00004d20: 2c20 2a2a 6b77 6172 6773 290a 2020 2020  , **kwargs).    
+00004d30: 2020 2020 6578 6365 7074 2041 7474 7269      except Attri
+00004d40: 6275 7465 4572 726f 7220 6173 2065 3a0a  buteError as e:.
+00004d50: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+00004d60: 6572 726f 7273 2e44 6573 6372 6970 746f  errors.Descripto
+00004d70: 7241 7474 7269 6275 7465 4572 726f 7228  rAttributeError(
+00004d80: 2920 6672 6f6d 2065 0a0a 2020 2020 6966  ) from e..    if
+00004d90: 2068 6173 6174 7472 2864 6573 6372 6970   hasattr(descrip
+00004da0: 746f 722c 2027 5f5f 7365 745f 5f27 293a  tor, '__set__'):
+00004db0: 0a20 2020 2020 2064 6566 205f 5f73 6574  .      def __set
+00004dc0: 5f5f 2873 656c 662c 202a 6172 6773 2c20  __(self, *args, 
+00004dd0: 2a2a 6b77 6172 6773 293a 0a20 2020 2020  **kwargs):.     
+00004de0: 2020 2072 6574 7572 6e20 7365 6c66 2e77     return self.w
+00004df0: 7261 7070 6564 2e5f 5f73 6574 5f5f 282a  rapped.__set__(*
+00004e00: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
+00004e10: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
+00004e20: 6465 7363 7269 7074 6f72 2c20 275f 5f64  descriptor, '__d
+00004e30: 656c 6574 655f 5f27 293a 0a20 2020 2020  elete__'):.     
+00004e40: 2064 6566 205f 5f64 656c 6574 655f 5f28   def __delete__(
+00004e50: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+00004e60: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
+00004e70: 7265 7475 726e 2073 656c 662e 7772 6170  return self.wrap
+00004e80: 7065 642e 5f5f 6465 6c65 7465 5f5f 282a  ped.__delete__(*
+00004e90: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
+00004ea0: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
+00004eb0: 6465 7363 7269 7074 6f72 2c20 275f 5f73  descriptor, '__s
+00004ec0: 6574 5f6e 616d 655f 5f27 293a 0a20 2020  et_name__'):.   
+00004ed0: 2020 2064 6566 205f 5f73 6574 5f6e 616d     def __set_nam
+00004ee0: 655f 5f28 7365 6c66 2c20 2a61 7267 732c  e__(self, *args,
+00004ef0: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
+00004f00: 2020 2020 7365 6c66 2e77 7261 7070 6564      self.wrapped
+00004f10: 2e5f 5f73 6574 5f6e 616d 655f 5f28 2a61  .__set_name__(*a
+00004f20: 7267 732c 202a 2a6b 7761 7267 7329 0a0a  rgs, **kwargs)..
+00004f30: 2020 2020 6465 6620 5f5f 6765 7461 7474      def __getatt
+00004f40: 725f 5f28 7365 6c66 2c20 6e61 6d65 293a  r__(self, name):
+00004f50: 0a20 2020 2020 2072 6574 7572 6e20 6765  .      return ge
+00004f60: 7461 7474 7228 7365 6c66 2e77 7261 7070  tattr(self.wrapp
+00004f70: 6564 2c20 6e61 6d65 290a 0a20 2072 6574  ed, name)..  ret
+00004f80: 7572 6e20 5f44 6573 6372 6970 746f 7257  urn _DescriptorW
+00004f90: 7261 7070 6572 2864 6573 6372 6970 746f  rapper(descripto
+00004fa0: 7229 0a0a 2320 4261 7365 204d 6f64 756c  r)..# Base Modul
+00004fb0: 6520 6465 6669 6e69 7469 6f6e 2e0a 2320  e definition..# 
 00004fc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00004fd0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00004fe0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a  --------------..
-00004ff0: 2320 5468 6520 4d6f 6475 6c65 4261 7365  # The ModuleBase
-00005000: 2063 6c61 7373 2069 7320 6372 6561 7465   class is create
-00005010: 6420 6f6e 6c79 2074 6f20 6d61 6b65 2073  d only to make s
-00005020: 7461 7469 6320 616e 616c 797a 6572 7320  tatic analyzers 
-00005030: 6861 7070 790a 2320 6d61 696e 6c79 2070  happy.# mainly p
-00005040: 7974 7970 6520 616e 6420 7079 7269 6768  ytype and pyrigh
-00005050: 742e 2053 6f6d 6520 6e6f 7465 733a 0a23  t. Some notes:.#
-00005060: 202a 2070 7972 6967 6874 2028 636f 7272   * pyright (corr
-00005070: 6563 746c 7929 2063 6f6d 706c 6169 6e73  ectly) complains
-00005080: 2074 6861 7420 4d6f 6475 6c65 2069 7473   that Module its
-00005090: 656c 6620 6973 206e 6f74 2061 2064 6174  elf is not a dat
-000050a0: 6163 6c61 7373 2c20 6576 656e 0a23 2020  aclass, even.#  
-000050b0: 2074 686f 7567 6820 616c 6c20 6974 7320   though all its 
-000050c0: 7375 6263 6c61 7373 6573 2061 6e64 2069  subclasses and i
-000050d0: 6e74 616e 6365 7320 4152 4520 6461 7461  ntances ARE data
-000050e0: 636c 6173 7365 732e 2042 6563 6175 7365  classes. Because
-000050f0: 2074 6865 7265 2069 7320 6e6f 0a23 2020   there is no.#  
-00005100: 2077 6179 2074 6f20 616e 6e6f 7461 7465   way to annotate
-00005110: 2074 6869 7320 696e 2061 2077 6179 2074   this in a way t
-00005120: 6861 7420 7079 7269 6768 7420 756e 6465  hat pyright unde
-00005130: 7273 7461 6e64 732c 2077 6520 6372 6561  rstands, we crea
-00005140: 7465 2061 0a23 2020 204d 6f64 756c 6542  te a.#   ModuleB
-00005150: 6173 6520 636c 6173 7320 6465 636f 7261  ase class decora
-00005160: 7465 6420 7769 7468 2060 6461 7461 636c  ted with `datacl
-00005170: 6173 735f 7472 616e 7366 6f72 6d60 2073  ass_transform` s
-00005180: 7563 6820 7468 6174 2070 7972 6967 6874  uch that pyright
-00005190: 0a23 2020 2074 6869 6e6b 7320 4d6f 6475  .#   thinks Modu
-000051a0: 6c65 2069 7320 6120 6461 7461 636c 6173  le is a dataclas
-000051b0: 7320 2869 6e20 7265 616c 6974 7920 6f6e  s (in reality on
-000051c0: 6c79 2073 7562 636c 6173 7365 7320 6172  ly subclasses ar
-000051d0: 6520 696e 7374 616e 7469 6174 6564 0a23  e instantiated.#
-000051e0: 2020 2073 6f20 7468 6973 2069 7320 6669     so this is fi
-000051f0: 6e65 292e 0a23 202a 2054 6865 2060 5f5f  ne)..# * The `__
-00005200: 6461 7461 636c 6173 735f 6669 656c 6473  dataclass_fields
-00005210: 5f5f 6020 6174 7472 6962 7574 6520 6973  __` attribute is
-00005220: 206e 6565 6465 6420 6265 6361 7573 6520   needed because 
-00005230: 7079 7479 7065 2073 6565 6d73 2074 6f0a  pytype seems to.
-00005240: 2320 2020 6e6f 7420 756e 6465 7273 7461  #   not understa
-00005250: 6e64 2074 6865 2060 6461 7461 636c 6173  nd the `dataclas
-00005260: 735f 7472 616e 7366 6f72 6d60 2064 6563  s_transform` dec
-00005270: 6f72 6174 6f72 2c20 7468 6572 6566 6f72  orator, therefor
-00005280: 6520 7765 206e 6565 640a 2320 2020 746f  e we need.#   to
-00005290: 2061 6464 2074 6865 2061 7474 7269 6275   add the attribu
-000052a0: 7465 206d 616e 7561 6c6c 792e 0a23 202a  te manually..# *
-000052b0: 204f 7468 6572 2061 7474 7269 6275 7465   Other attribute
-000052c0: 7320 6172 6520 616e 6e6f 7461 7465 6420  s are annotated 
-000052d0: 666f 7220 636f 6d70 6c65 7465 6e65 7373  for completeness
-000052e0: 2e20 4265 6361 7573 6520 7765 2061 7265  . Because we are
-000052f0: 2075 7369 6e67 0a23 2020 2074 6865 2060   using.#   the `
-00005300: 6966 2074 7970 696e 672e 5459 5045 5f43  if typing.TYPE_C
-00005310: 4845 434b 494e 4760 2070 6174 7465 726e  HECKING` pattern
-00005320: 2c20 7468 6573 6520 616e 6e6f 7461 7469  , these annotati
-00005330: 6f6e 7320 6172 6520 6e6f 7420 7072 6573  ons are not pres
-00005340: 656e 740a 2320 2020 6174 2072 756e 7469  ent.#   at runti
-00005350: 6d65 2073 6f20 7468 6579 2064 6f6e 2774  me so they don't
-00005360: 2061 6666 6563 7420 7468 6520 6461 7461   affect the data
-00005370: 636c 6173 7320 6265 6861 7669 6f72 2e0a  class behavior..
-00005380: 4064 6174 6163 6c61 7373 5f74 7261 6e73  @dataclass_trans
-00005390: 666f 726d 2829 0a63 6c61 7373 204d 6f64  form().class Mod
-000053a0: 756c 6542 6173 653a 0a20 2069 6620 7479  uleBase:.  if ty
-000053b0: 7069 6e67 2e54 5950 455f 4348 4543 4b49  ping.TYPE_CHECKI
-000053c0: 4e47 3a0a 2020 2020 7363 6f70 653a 204f  NG:.    scope: O
-000053d0: 7074 696f 6e61 6c5b 5363 6f70 655d 0a20  ptional[Scope]. 
-000053e0: 2020 205f 7374 6174 653a 205f 4d6f 6475     _state: _Modu
-000053f0: 6c65 496e 7465 726e 616c 5374 6174 650a  leInternalState.
-00005400: 2020 2020 5f70 6172 656e 745f 7265 663a      _parent_ref:
-00005410: 2055 6e69 6f6e 5b27 4d6f 6475 6c65 272c   Union['Module',
-00005420: 2077 6561 6b72 6566 2e52 6566 6572 656e   weakref.Referen
-00005430: 6365 5479 7065 5b27 4d6f 6475 6c65 275d  ceType['Module']
-00005440: 2c20 4e6f 6e65 5d0a 2020 2020 7061 7265  , None].    pare
-00005450: 6e74 3a20 556e 696f 6e5b 274d 6f64 756c  nt: Union['Modul
-00005460: 6527 2c20 5f53 656e 7469 6e65 6c2c 204e  e', _Sentinel, N
-00005470: 6f6e 655d 0a20 2020 205f 5f64 6174 6163  one].    __datac
-00005480: 6c61 7373 5f66 6965 6c64 735f 5f3a 2044  lass_fields__: D
-00005490: 6963 745b 7374 722c 2064 6174 6163 6c61  ict[str, datacla
-000054a0: 7373 6573 2e46 6965 6c64 5d0a 0a63 6c61  sses.Field]..cla
-000054b0: 7373 204d 6f64 756c 6528 4d6f 6475 6c65  ss Module(Module
-000054c0: 4261 7365 293a 0a20 2022 2222 4261 7365  Base):.  """Base
-000054d0: 2063 6c61 7373 2066 6f72 2061 6c6c 206e   class for all n
-000054e0: 6575 7261 6c20 6e65 7477 6f72 6b20 6d6f  eural network mo
-000054f0: 6475 6c65 732e 204c 6179 6572 7320 616e  dules. Layers an
-00005500: 6420 6d6f 6465 6c73 2073 686f 756c 6420  d models should 
-00005510: 7375 6263 6c61 7373 2074 6869 7320 636c  subclass this cl
-00005520: 6173 732e 0a0a 2020 416c 6c20 466c 6178  ass...  All Flax
-00005530: 204d 6f64 756c 6573 2061 7265 2050 7974   Modules are Pyt
-00005540: 686f 6e20 332e 370a 2020 6064 6174 6163  hon 3.7.  `datac
-00005550: 6c61 7373 6573 203c 6874 7470 733a 2f2f  lasses <https://
-00005560: 646f 6373 2e70 7974 686f 6e2e 6f72 672f  docs.python.org/
-00005570: 332f 6c69 6272 6172 792f 6461 7461 636c  3/library/datacl
-00005580: 6173 7365 732e 6874 6d6c 3e60 5f2e 2053  asses.html>`_. S
-00005590: 696e 6365 0a20 2064 6174 6163 6c61 7373  ince.  dataclass
-000055a0: 6573 2074 616b 6520 6f76 6572 2060 605f  es take over ``_
-000055b0: 5f69 6e69 745f 5f60 602c 2079 6f75 2073  _init__``, you s
-000055c0: 686f 756c 6420 696e 7374 6561 6420 6f76  hould instead ov
-000055d0: 6572 7269 6465 203a 6d65 7468 3a60 7365  erride :meth:`se
-000055e0: 7475 7060 2c0a 2020 7768 6963 6820 6973  tup`,.  which is
-000055f0: 2061 7574 6f6d 6174 6963 616c 6c79 2063   automatically c
-00005600: 616c 6c65 6420 746f 2069 6e69 7469 616c  alled to initial
-00005610: 697a 6520 7468 6520 6d6f 6475 6c65 2e0a  ize the module..
-00005620: 0a20 204d 6f64 756c 6573 2063 616e 2063  .  Modules can c
-00005630: 6f6e 7461 696e 2073 7562 6d6f 6475 6c65  ontain submodule
-00005640: 732c 2061 6e64 2069 6e20 7468 6973 2077  s, and in this w
-00005650: 6179 2063 616e 2062 6520 6e65 7374 6564  ay can be nested
-00005660: 2069 6e20 6120 7472 6565 0a20 2073 7472   in a tree.  str
-00005670: 7563 7475 7265 2e20 5375 626d 6f64 656c  ucture. Submodel
-00005680: 7320 6361 6e20 6265 2061 7373 6967 6e65  s can be assigne
-00005690: 6420 6173 2072 6567 756c 6172 2061 7474  d as regular att
-000056a0: 7269 6275 7465 7320 696e 7369 6465 2074  ributes inside t
-000056b0: 6865 0a20 203a 6d65 7468 3a60 7365 7475  he.  :meth:`setu
-000056c0: 7060 206d 6574 686f 642e 0a0a 2020 596f  p` method...  Yo
-000056d0: 7520 6361 6e20 6465 6669 6e65 2061 7262  u can define arb
-000056e0: 6974 7261 7279 2022 666f 7277 6172 6420  itrary "forward 
-000056f0: 7061 7373 2220 6d65 7468 6f64 7320 6f6e  pass" methods on
-00005700: 2079 6f75 7220 4d6f 6475 6c65 2073 7562   your Module sub
-00005710: 636c 6173 732e 0a20 2057 6869 6c65 206e  class..  While n
-00005720: 6f20 6d65 7468 6f64 7320 6172 6520 7370  o methods are sp
-00005730: 6563 6961 6c2d 6361 7365 642c 2060 605f  ecial-cased, ``_
-00005740: 5f63 616c 6c5f 5f60 6020 6973 2061 2070  _call__`` is a p
-00005750: 6f70 756c 6172 2063 686f 6963 6520 6265  opular choice be
-00005760: 6361 7573 650a 2020 6974 2061 6c6c 6f77  cause.  it allow
-00005770: 7320 796f 7520 746f 2075 7365 206d 6f64  s you to use mod
-00005780: 756c 6520 696e 7374 616e 6365 7320 6173  ule instances as
-00005790: 2069 6620 7468 6579 2061 7265 2066 756e   if they are fun
-000057a0: 6374 696f 6e73 3a3a 0a0a 2020 2020 6672  ctions::..    fr
-000057b0: 6f6d 2066 6c61 7820 696d 706f 7274 206c  om flax import l
-000057c0: 696e 656e 2061 7320 6e6e 0a0a 2020 2020  inen as nn..    
-000057d0: 636c 6173 7320 4d6f 6475 6c65 286e 6e2e  class Module(nn.
-000057e0: 4d6f 6475 6c65 293a 0a20 2020 2020 2066  Module):.      f
-000057f0: 6561 7475 7265 733a 2054 7570 6c65 5b69  eatures: Tuple[i
-00005800: 6e74 2c20 2e2e 2e5d 203d 2028 3136 2c20  nt, ...] = (16, 
-00005810: 3429 0a0a 2020 2020 2020 6465 6620 7365  4)..      def se
-00005820: 7475 7028 7365 6c66 293a 0a20 2020 2020  tup(self):.     
-00005830: 2020 2073 656c 662e 6465 6e73 6531 203d     self.dense1 =
-00005840: 2044 656e 7365 2873 656c 662e 6665 6174   Dense(self.feat
-00005850: 7572 6573 5b30 5d29 0a20 2020 2020 2020  ures[0]).       
-00005860: 2073 656c 662e 6465 6e73 6532 203d 2044   self.dense2 = D
-00005870: 656e 7365 2873 656c 662e 6665 6174 7572  ense(self.featur
-00005880: 6573 5b31 5d29 0a0a 2020 2020 2020 6465  es[1])..      de
-00005890: 6620 5f5f 6361 6c6c 5f5f 2873 656c 662c  f __call__(self,
-000058a0: 2078 293a 0a20 2020 2020 2020 2072 6574   x):.        ret
-000058b0: 7572 6e20 7365 6c66 2e64 656e 7365 3228  urn self.dense2(
-000058c0: 6e6e 2e72 656c 7528 7365 6c66 2e64 656e  nn.relu(self.den
-000058d0: 7365 3128 7829 2929 0a0a 2020 4f70 7469  se1(x)))..  Opti
-000058e0: 6f6e 616c 6c79 2c20 666f 7220 6d6f 7265  onally, for more
-000058f0: 2063 6f6e 6369 7365 206d 6f64 756c 6520   concise module 
-00005900: 696d 706c 656d 656e 7461 7469 6f6e 7320  implementations 
-00005910: 7768 6572 6520 7375 626d 6f64 756c 6573  where submodules
-00005920: 0a20 2064 6566 696e 6974 696f 6e73 2061  .  definitions a
-00005930: 7265 2063 6f2d 6c6f 6361 7465 6420 7769  re co-located wi
-00005940: 7468 2074 6865 6972 2075 7361 6765 2c20  th their usage, 
-00005950: 796f 7520 6361 6e20 7573 6520 7468 650a  you can use the.
-00005960: 2020 3a6d 6574 683a 6063 6f6d 7061 6374    :meth:`compact
-00005970: 6020 7772 6170 7065 722e 0a20 2022 2222  ` wrapper..  """
-00005980: 0a0a 2020 6966 2074 7970 696e 672e 5459  ..  if typing.TY
-00005990: 5045 5f43 4845 434b 494e 473a 0a20 2020  PE_CHECKING:.   
-000059a0: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
-000059b0: 6c66 2c20 2a61 7267 732c 202a 2a6b 7761  lf, *args, **kwa
-000059c0: 7267 7329 3a0a 2020 2020 2020 2320 7468  rgs):.      # th
-000059d0: 6973 2073 7475 6220 6d61 6b65 7320 7375  is stub makes su
-000059e0: 7265 2070 7974 7970 6520 6163 6365 7074  re pytype accept
-000059f0: 7320 636f 6e73 7472 7563 746f 7220 6172  s constructor ar
-00005a00: 6775 6d65 6e74 732e 0a20 2020 2020 2070  guments..      p
-00005a10: 6173 730a 0a20 2020 2064 6566 205f 5f63  ass..    def __c
-00005a20: 616c 6c5f 5f28 7365 6c66 2c20 2a61 7267  all__(self, *arg
-00005a30: 732c 202a 2a6b 7761 7267 7329 202d 3e20  s, **kwargs) -> 
-00005a40: 416e 793a 0a20 2020 2020 2023 2074 6869  Any:.      # thi
-00005a50: 7320 7374 7562 2061 6c6c 6f77 7320 7079  s stub allows py
-00005a60: 7479 7065 2074 6f20 6163 6365 7074 204d  type to accept M
-00005a70: 6f64 756c 6573 2061 7320 4361 6c6c 6162  odules as Callab
-00005a80: 6c65 732e 0a20 2020 2020 2070 6173 730a  les..      pass.
-00005a90: 0a20 2040 636c 6173 736d 6574 686f 640a  .  @classmethod.
-00005aa0: 2020 6465 6620 5f5f 696e 6974 5f73 7562    def __init_sub
-00005ab0: 636c 6173 735f 5f28 636c 732c 206b 775f  class__(cls, kw_
-00005ac0: 6f6e 6c79 3a20 626f 6f6c 203d 2046 616c  only: bool = Fal
-00005ad0: 7365 2c20 2a2a 6b77 6172 6773 3a20 416e  se, **kwargs: An
-00005ae0: 7929 202d 3e20 4e6f 6e65 3a0a 2020 2020  y) -> None:.    
-00005af0: 2222 2241 7574 6f6d 6174 6963 616c 6c79  """Automatically
-00005b00: 2069 6e69 7469 616c 697a 6573 2061 6c6c   initializes all
-00005b10: 2073 7562 636c 6173 7365 7320 6173 2063   subclasses as c
-00005b20: 7573 746f 6d20 6461 7461 636c 6173 7365  ustom dataclasse
-00005b30: 732e 2222 220a 2020 2020 7375 7065 7228  s.""".    super(
-00005b40: 292e 5f5f 696e 6974 5f73 7562 636c 6173  ).__init_subclas
-00005b50: 735f 5f28 2a2a 6b77 6172 6773 290a 2020  s__(**kwargs).  
-00005b60: 2020 2320 416c 6c20 466c 6178 204d 6f64    # All Flax Mod
-00005b70: 756c 6573 2061 7265 2064 6174 6163 6c61  ules are datacla
-00005b80: 7373 6573 2e20 2057 6520 666f 7263 6520  sses.  We force 
-00005b90: 7468 6973 2063 6f6e 7665 6e74 696f 6e20  this convention 
-00005ba0: 7369 6e63 650a 2020 2020 2320 6974 2065  since.    # it e
-00005bb0: 6e63 6f75 7261 6765 7320 7468 6520 7374  ncourages the st
-00005bc0: 6174 656c 6573 7320 6265 6861 7669 6f72  ateless behavior
-00005bd0: 206e 6565 6465 6420 746f 2063 6c6f 6e65   needed to clone
-00005be0: 206d 6f64 756c 6520 696e 7374 616e 6365   module instance
-00005bf0: 7320 666f 720a 2020 2020 2320 6675 6e63  s for.    # func
-00005c00: 7469 6f6e 616c 2074 7261 6e73 666f 726d  tional transform
-00005c10: 6174 696f 6e2e 2020 496e 7374 6561 6420  ation.  Instead 
-00005c20: 6f66 2075 7369 6e67 2061 2070 7974 686f  of using a pytho
-00005c30: 6e20 6d65 7461 636c 6173 732c 2077 650a  n metaclass, we.
-00005c40: 2020 2020 2320 6175 746f 6d61 7469 6361      # automatica
-00005c50: 6c6c 7920 7472 616e 7366 6f72 6d20 4d6f  lly transform Mo
-00005c60: 6475 6c65 7320 696e 746f 2064 6174 6163  dules into datac
-00005c70: 6c61 7373 6573 2061 7420 7375 6263 6c61  lasses at subcla
-00005c80: 7373 2063 7265 6174 696f 6e0a 2020 2020  ss creation.    
-00005c90: 2320 7469 6d65 2c20 616e 6420 7765 2073  # time, and we s
-00005ca0: 6574 2074 6865 206c 6173 7420 6461 7461  et the last data
-00005cb0: 636c 6173 7320 6172 6775 6d65 6e74 7320  class arguments 
-00005cc0: 746f 2060 7061 7265 6e74 6020 616e 6420  to `parent` and 
-00005cd0: 606e 616d 6560 2e0a 2020 2020 636c 732e  `name`..    cls.
-00005ce0: 5f63 7573 746f 6d69 7a65 645f 6461 7461  _customized_data
-00005cf0: 636c 6173 735f 7472 616e 7366 6f72 6d28  class_transform(
-00005d00: 6b77 5f6f 6e6c 7929 0a20 2020 2023 2057  kw_only).    # W
-00005d10: 6520 7772 6170 2075 7365 722d 6465 6669  e wrap user-defi
-00005d20: 6e65 6420 6d65 7468 6f64 7320 696e 636c  ned methods incl
-00005d30: 7564 696e 6720 7365 7475 7020 616e 6420  uding setup and 
-00005d40: 5f5f 6361 6c6c 5f5f 2074 6f20 656e 666f  __call__ to enfo
-00005d50: 7263 650a 2020 2020 2320 6120 6e75 6d62  rce.    # a numb
-00005d60: 6572 206f 6620 6469 6666 6572 656e 7420  er of different 
-00005d70: 6368 6563 6b73 2061 6e64 2074 6f20 7072  checks and to pr
-00005d80: 6f76 6964 6520 636c 6561 7220 6572 726f  ovide clear erro
-00005d90: 7220 6d65 7373 6167 6573 2e0a 2020 2020  r messages..    
-00005da0: 636c 732e 5f76 6572 6966 795f 7369 6e67  cls._verify_sing
-00005db0: 6c65 5f6f 725f 6e6f 5f63 6f6d 7061 6374  le_or_no_compact
-00005dc0: 2829 0a20 2020 2063 6c73 2e5f 7772 6170  ().    cls._wrap
-00005dd0: 5f6d 6f64 756c 655f 6174 7472 6962 7574  _module_attribut
-00005de0: 6573 2829 0a20 2020 2023 2053 6574 2065  es().    # Set e
-00005df0: 6d70 7479 2063 6c61 7373 2064 6566 6175  mpty class defau
-00005e00: 6c74 732e 0a20 2020 2063 6c73 2e5f 7374  lts..    cls._st
-00005e10: 6174 6520 3d20 5f75 6e69 6e69 7469 616c  ate = _uninitial
-00005e20: 697a 6564 5f6d 6f64 756c 655f 696e 7465  ized_module_inte
-00005e30: 726e 616c 5f73 7461 7465 2023 2074 7970  rnal_state # typ
-00005e40: 653a 2069 676e 6f72 655b 6174 7472 2d64  e: ignore[attr-d
-00005e50: 6566 696e 6564 5d0a 2020 2020 636c 732e  efined].    cls.
-00005e60: 7363 6f70 653a 204f 7074 696f 6e61 6c5b  scope: Optional[
-00005e70: 5363 6f70 655d 203d 204e 6f6e 6520 2320  Scope] = None # 
-00005e80: 7479 7065 3a20 6967 6e6f 7265 0a20 2020  type: ignore.   
-00005e90: 2023 2048 616e 646c 6573 2077 6561 6b20   # Handles weak 
-00005ea0: 7265 6665 7265 6e63 696e 6720 6f66 2070  referencing of p
-00005eb0: 6172 656e 7420 4d6f 6475 6c65 7320 746f  arent Modules to
-00005ec0: 2070 7265 7665 6e74 2072 6566 6572 656e   prevent referen
-00005ed0: 6365 2063 7963 6c65 732e 0a20 2020 2063  ce cycles..    c
-00005ee0: 6c73 2e5f 7061 7265 6e74 5f72 6566 203d  ls._parent_ref =
-00005ef0: 204e 6f6e 6520 2320 7479 7065 3a20 6967   None # type: ig
-00005f00: 6e6f 7265 5b61 7474 722d 6465 6669 6e65  nore[attr-define
-00005f10: 645d 0a20 2020 2063 6c73 2e70 6172 656e  d].    cls.paren
-00005f20: 7420 3d20 5061 7265 6e74 4465 7363 7269  t = ParentDescri
-00005f30: 7074 6f72 2829 2023 2074 7970 653a 2069  ptor() # type: i
-00005f40: 676e 6f72 655b 6173 7369 676e 6d65 6e74  gnore[assignment
-00005f50: 5d0a 0a20 2040 636c 6173 736d 6574 686f  ]..  @classmetho
-00005f60: 640a 2020 6465 6620 5f63 7573 746f 6d69  d.  def _customi
-00005f70: 7a65 645f 6461 7461 636c 6173 735f 7472  zed_dataclass_tr
-00005f80: 616e 7366 6f72 6d28 636c 732c 206b 775f  ansform(cls, kw_
-00005f90: 6f6e 6c79 3a20 626f 6f6c 293a 0a20 2020  only: bool):.   
-00005fa0: 2022 2222 5472 616e 7366 6f72 6d73 2060   """Transforms `
-00005fb0: 636c 7360 2069 6e74 6f20 6120 6461 7461  cls` into a data
-00005fc0: 636c 6173 732c 2077 6974 6820 6375 7374  class, with cust
-00005fd0: 6f6d 2061 6464 6974 696f 6e61 6c20 6265  om additional be
-00005fe0: 6861 7669 6f72 2e0a 0a20 2020 2031 2e20  havior...    1. 
-00005ff0: 496e 6a65 6374 2060 7061 7265 6e74 6020  Inject `parent` 
-00006000: 616e 6420 606e 616d 6560 2066 6965 6c64  and `name` field
-00006010: 732e 2020 2849 6620 7468 6579 2061 7265  s.  (If they are
-00006020: 2061 6c72 6561 6479 2070 7265 7365 6e74   already present
-00006030: 2c0a 2020 2020 2020 2074 6865 6e20 6368  ,.       then ch
-00006040: 6563 6b20 7468 6174 2074 6865 7920 6861  eck that they ha
-00006050: 7665 2074 6865 2065 7870 6563 7465 6420  ve the expected 
-00006060: 7479 7065 732e 290a 2020 2020 322e 2053  types.).    2. S
-00006070: 6574 2063 6f6d 7061 7265 2c20 6861 7368  et compare, hash
-00006080: 2c20 616e 6420 7265 7072 2074 6f20 4661  , and repr to Fa
-00006090: 6c73 6520 666f 7220 6e6f 6e2d 696e 6974  lse for non-init
-000060a0: 2066 6965 6c64 732e 0a20 2020 2033 2e20   fields..    3. 
-000060b0: 4765 6e65 7261 7465 2061 2068 6173 6820  Generate a hash 
-000060c0: 6675 6e63 7469 6f6e 2028 6966 206e 6f74  function (if not
-000060d0: 2070 726f 7669 6465 6420 6279 2063 6c73   provided by cls
-000060e0: 292e 0a20 2020 2022 2222 0a20 2020 2023  )..    """.    #
-000060f0: 2043 6865 636b 2072 6573 6572 7665 6420   Check reserved 
-00006100: 6174 7472 6962 7574 6573 2068 6176 6520  attributes have 
-00006110: 6578 7065 6374 6564 2074 7970 6520 616e  expected type an
-00006120: 6e6f 7461 7469 6f6e 732e 0a20 2020 2061  notations..    a
-00006130: 6e6e 6f74 6174 696f 6e73 203d 2064 6963  nnotations = dic
-00006140: 7428 636c 732e 5f5f 6469 6374 5f5f 2e67  t(cls.__dict__.g
-00006150: 6574 2827 5f5f 616e 6e6f 7461 7469 6f6e  et('__annotation
-00006160: 735f 5f27 2c20 7b7d 2929 0a20 2020 2069  s__', {})).    i
-00006170: 6620 616e 6e6f 7461 7469 6f6e 732e 6765  f annotations.ge
-00006180: 7428 2770 6172 656e 7427 2c20 5f50 6172  t('parent', _Par
-00006190: 656e 7454 7970 6529 2021 3d20 5f50 6172  entType) != _Par
-000061a0: 656e 7454 7970 653a 0a20 2020 2020 2072  entType:.      r
-000061b0: 6169 7365 2065 7272 6f72 732e 5265 7365  aise errors.Rese
-000061c0: 7276 6564 4d6f 6475 6c65 4174 7472 6962  rvedModuleAttrib
-000061d0: 7574 6545 7272 6f72 2861 6e6e 6f74 6174  uteError(annotat
-000061e0: 696f 6e73 290a 2020 2020 6966 2061 6e6e  ions).    if ann
-000061f0: 6f74 6174 696f 6e73 2e67 6574 2827 6e61  otations.get('na
-00006200: 6d65 272c 2073 7472 2920 6e6f 7420 696e  me', str) not in
-00006210: 2028 2773 7472 272c 2073 7472 2c20 4f70   ('str', str, Op
-00006220: 7469 6f6e 616c 5b73 7472 5d29 3a0a 2020  tional[str]):.  
-00006230: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
-00006240: 2e52 6573 6572 7665 644d 6f64 756c 6541  .ReservedModuleA
-00006250: 7474 7269 6275 7465 4572 726f 7228 616e  ttributeError(an
-00006260: 6e6f 7461 7469 6f6e 7329 0a0a 2020 2020  notations)..    
-00006270: 2320 616e 7920 6e6f 6e2d 696e 6974 2066  # any non-init f
-00006280: 6965 6c64 2077 696c 6c20 6f6e 6c79 2062  ield will only b
-00006290: 6520 7365 7420 696e 2073 6574 7570 0a20  e set in setup. 
-000062a0: 2020 2023 2044 7572 696e 6720 5f5f 6861     # During __ha
-000062b0: 7368 5f5f 2061 6e64 205f 5f65 715f 5f20  sh__ and __eq__ 
-000062c0: 7468 6520 6669 656c 6420 6973 206e 6f74  the field is not
-000062d0: 2073 6574 2079 6574 0a20 2020 2023 2073   set yet.    # s
-000062e0: 6f20 6974 2073 686f 756c 6420 6e6f 7420  o it should not 
-000062f0: 6265 2075 7365 6420 696e 2063 6f6d 7061  be used in compa
-00006300: 7265 2c20 6861 7368 206f 7220 7265 7072  re, hash or repr
-00006310: 2e0a 2020 2020 666f 7220 6669 656c 6420  ..    for field 
-00006320: 696e 2061 6e6e 6f74 6174 696f 6e73 3a0a  in annotations:.
-00006330: 2020 2020 2020 6669 656c 645f 6d65 7461        field_meta
-00006340: 203d 2067 6574 6174 7472 2863 6c73 2c20   = getattr(cls, 
-00006350: 6669 656c 642c 204e 6f6e 6529 0a20 2020  field, None).   
-00006360: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
-00006370: 2866 6965 6c64 5f6d 6574 612c 2064 6174  (field_meta, dat
-00006380: 6163 6c61 7373 6573 2e46 6965 6c64 2920  aclasses.Field) 
-00006390: 616e 6420 6e6f 7420 6669 656c 645f 6d65  and not field_me
-000063a0: 7461 2e69 6e69 743a 0a20 2020 2020 2020  ta.init:.       
-000063b0: 2066 6965 6c64 5f6d 6574 612e 636f 6d70   field_meta.comp
-000063c0: 6172 6520 3d20 4661 6c73 650a 2020 2020  are = False.    
-000063d0: 2020 2020 6669 656c 645f 6d65 7461 2e68      field_meta.h
-000063e0: 6173 6820 3d20 4661 6c73 650a 2020 2020  ash = False.    
-000063f0: 2020 2020 6669 656c 645f 6d65 7461 2e72      field_meta.r
-00006400: 6570 7220 3d20 4661 6c73 650a 0a20 2020  epr = False..   
-00006410: 2065 7874 7261 5f66 6965 6c64 7320 3d20   extra_fields = 
-00006420: 5b28 2770 6172 656e 7427 2c20 5f50 6172  [('parent', _Par
-00006430: 656e 7454 7970 652c 0a20 2020 2020 2020  entType,.       
-00006440: 2020 2020 2020 2020 2020 2020 2020 6b77                kw
-00006450: 5f6f 6e6c 795f 6461 7461 636c 6173 7365  _only_dataclasse
-00006460: 732e 6669 656c 6428 0a20 2020 2020 2020  s.field(.       
-00006470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006480: 2020 7265 7072 3d46 616c 7365 2c20 6465    repr=False, de
-00006490: 6661 756c 743d 5f75 6e73 7065 6369 6669  fault=_unspecifi
-000064a0: 6564 5f70 6172 656e 742c 0a20 2020 2020  ed_parent,.     
-000064b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064c0: 2020 2020 6b77 5f6f 6e6c 793d 5472 7565      kw_only=True
-000064d0: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
-000064e0: 2020 2020 2020 2020 2827 6e61 6d65 272c          ('name',
-000064f0: 204f 7074 696f 6e61 6c5b 7374 725d 2c0a   Optional[str],.
-00006500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006510: 2020 2020 206b 775f 6f6e 6c79 5f64 6174       kw_only_dat
-00006520: 6163 6c61 7373 6573 2e66 6965 6c64 2864  aclasses.field(d
-00006530: 6566 6175 6c74 3d4e 6f6e 652c 206b 775f  efault=None, kw_
-00006540: 6f6e 6c79 3d54 7275 6529 295d 0a0a 2020  only=True))]..  
-00006550: 2020 6966 206b 775f 6f6e 6c79 3a0a 2020    if kw_only:.  
-00006560: 2020 2020 6966 2074 7570 6c65 2873 7973      if tuple(sys
-00006570: 2e76 6572 7369 6f6e 5f69 6e66 6f29 5b3a  .version_info)[:
-00006580: 335d 203e 3d20 2833 2c20 3130 2c20 3029  3] >= (3, 10, 0)
-00006590: 3a0a 2020 2020 2020 2020 666f 7220 6e61  :.        for na
-000065a0: 6d65 2c20 616e 6e6f 7461 7469 6f6e 2c20  me, annotation, 
-000065b0: 6465 6661 756c 7420 696e 2065 7874 7261  default in extra
-000065c0: 5f66 6965 6c64 733a 2020 2320 7079 7479  _fields:  # pyty
-000065d0: 7065 3a20 6469 7361 626c 653d 696e 7661  pe: disable=inva
-000065e0: 6c69 642d 616e 6e6f 7461 7469 6f6e 0a20  lid-annotation. 
-000065f0: 2020 2020 2020 2020 2073 6574 6174 7472           setattr
-00006600: 2863 6c73 2c20 6e61 6d65 2c20 6465 6661  (cls, name, defa
-00006610: 756c 7429 0a20 2020 2020 2020 2020 2063  ult).          c
-00006620: 6c73 2e5f 5f61 6e6e 6f74 6174 696f 6e73  ls.__annotations
-00006630: 5f5f 5b6e 616d 655d 203d 2061 6e6e 6f74  __[name] = annot
-00006640: 6174 696f 6e0a 2020 2020 2020 2020 6461  ation.        da
-00006650: 7461 636c 6173 7365 732e 6461 7461 636c  taclasses.datacl
-00006660: 6173 7328 0a20 2020 2020 2020 2020 2020  ass(.           
-00006670: 2075 6e73 6166 655f 6861 7368 3d27 5f5f   unsafe_hash='__
-00006680: 6861 7368 5f5f 2720 6e6f 7420 696e 2063  hash__' not in c
-00006690: 6c73 2e5f 5f64 6963 745f 5f2c 0a20 2020  ls.__dict__,.   
-000066a0: 2020 2020 2020 2020 2072 6570 723d 4661           repr=Fa
-000066b0: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
-000066c0: 206b 775f 6f6e 6c79 3d54 7275 652c 0a20   kw_only=True,. 
-000066d0: 2020 2020 2020 2029 2863 6c73 2920 2023         )(cls)  #
-000066e0: 2074 7970 653a 2069 676e 6f72 655b 6361   type: ignore[ca
-000066f0: 6c6c 2d6f 7665 726c 6f61 645d 0a20 2020  ll-overload].   
-00006700: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00006710: 2072 6169 7365 2054 7970 6545 7272 6f72   raise TypeError
-00006720: 2827 606b 775f 6f6e 6c79 6020 6973 206e  ('`kw_only` is n
-00006730: 6f74 2061 7661 696c 6162 6c65 2062 6566  ot available bef
-00006740: 6f72 6520 5079 2033 2e31 302e 2729 0a20  ore Py 3.10.'). 
-00006750: 2020 2065 6c73 653a 0a20 2020 2020 2023     else:.      #
-00006760: 204e 6f77 2061 7070 6c79 2064 6174 6163   Now apply datac
-00006770: 6c61 7373 2074 7261 6e73 666f 726d 2028  lass transform (
-00006780: 7768 6963 6820 6f70 6572 6174 6573 2069  which operates i
-00006790: 6e2d 706c 6163 6529 2e0a 2020 2020 2020  n-place)..      
-000067a0: 2320 446f 2067 656e 6572 6174 6520 6120  # Do generate a 
-000067b0: 6861 7368 2066 756e 6374 696f 6e20 6f6e  hash function on
-000067c0: 6c79 2069 6620 6e6f 7420 7072 6f76 6964  ly if not provid
-000067d0: 6564 2062 7920 7468 6520 636c 6173 732e  ed by the class.
-000067e0: 0a20 2020 2020 206b 775f 6f6e 6c79 5f64  .      kw_only_d
-000067f0: 6174 6163 6c61 7373 6573 2e64 6174 6163  ataclasses.datac
-00006800: 6c61 7373 280a 2020 2020 2020 2020 2020  lass(.          
-00006810: 636c 732c 0a20 2020 2020 2020 2020 2075  cls,.          u
-00006820: 6e73 6166 655f 6861 7368 3d27 5f5f 6861  nsafe_hash='__ha
-00006830: 7368 5f5f 2720 6e6f 7420 696e 2063 6c73  sh__' not in cls
-00006840: 2e5f 5f64 6963 745f 5f2c 0a20 2020 2020  .__dict__,.     
-00006850: 2020 2020 2072 6570 723d 4661 6c73 652c       repr=False,
-00006860: 0a20 2020 2020 2020 2020 2065 7874 7261  .          extra
-00006870: 5f66 6965 6c64 733d 6578 7472 615f 6669  _fields=extra_fi
-00006880: 656c 6473 2920 2023 2070 7974 7970 653a  elds)  # pytype:
-00006890: 2064 6973 6162 6c65 3d77 726f 6e67 2d6b   disable=wrong-k
-000068a0: 6579 776f 7264 2d61 7267 730a 0a20 2020  eyword-args..   
-000068b0: 2063 6c73 2e5f 5f68 6173 685f 5f20 3d20   cls.__hash__ = 
-000068c0: 5f77 7261 705f 6861 7368 2863 6c73 2e5f  _wrap_hash(cls._
-000068d0: 5f68 6173 685f 5f29 2020 2320 7479 7065  _hash__)  # type
-000068e0: 3a20 6967 6e6f 7265 5b6d 6574 686f 642d  : ignore[method-
-000068f0: 6173 7369 676e 5d0a 0a20 2040 636c 6173  assign]..  @clas
-00006900: 736d 6574 686f 640a 2020 6465 6620 5f76  smethod.  def _v
-00006910: 6572 6966 795f 7369 6e67 6c65 5f6f 725f  erify_single_or_
-00006920: 6e6f 5f63 6f6d 7061 6374 2863 6c73 293a  no_compact(cls):
-00006930: 0a20 2020 2022 2222 5374 6174 6963 616c  .    """Statical
-00006940: 6c79 2076 6572 6966 6965 7320 7468 6174  ly verifies that
-00006950: 2061 7420 6d6f 7374 2061 2073 696e 676c   at most a singl
-00006960: 6520 6d65 7468 6f64 2069 7320 6c61 6265  e method is labe
-00006970: 6c6c 6564 2063 6f6d 7061 6374 2e22 2222  lled compact."""
-00006980: 0a20 2020 206d 6574 686f 6473 203d 205b  .    methods = [
-00006990: 6d5b 305d 2066 6f72 206d 2069 6e20 696e  m[0] for m in in
-000069a0: 7370 6563 742e 6765 746d 656d 6265 7273  spect.getmembers
-000069b0: 2863 6c73 2c20 7072 6564 6963 6174 653d  (cls, predicate=
-000069c0: 6361 6c6c 6162 6c65 295d 0a20 2020 206e  callable)].    n
-000069d0: 5f63 6f6d 7061 6374 5f66 6e73 203d 206c  _compact_fns = l
-000069e0: 656e 285b 6d65 7468 6f64 5f6e 616d 6520  en([method_name 
-000069f0: 666f 7220 6d65 7468 6f64 5f6e 616d 6520  for method_name 
-00006a00: 696e 206d 6574 686f 6473 0a20 2020 2020  in methods.     
-00006a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006a20: 2020 2020 6966 2068 6173 6174 7472 2867      if hasattr(g
-00006a30: 6574 6174 7472 2863 6c73 2c20 6d65 7468  etattr(cls, meth
-00006a40: 6f64 5f6e 616d 6529 2c20 2763 6f6d 7061  od_name), 'compa
-00006a50: 6374 2729 5d29 0a20 2020 2069 6620 6e5f  ct')]).    if n_
-00006a60: 636f 6d70 6163 745f 666e 7320 3e20 313a  compact_fns > 1:
-00006a70: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
-00006a80: 6f72 732e 4d75 6c74 6970 6c65 4d65 7468  ors.MultipleMeth
-00006a90: 6f64 7343 6f6d 7061 6374 4572 726f 7228  odsCompactError(
-00006aa0: 290a 0a20 2040 636c 6173 736d 6574 686f  )..  @classmetho
-00006ab0: 640a 2020 6465 6620 5f77 7261 705f 6d6f  d.  def _wrap_mo
-00006ac0: 6475 6c65 5f61 7474 7269 6275 7465 7328  dule_attributes(
-00006ad0: 636c 7329 3a0a 2020 2020 2222 2257 7261  cls):.    """Wra
-00006ae0: 7073 2075 7365 722d 6465 6669 6e65 6420  ps user-defined 
-00006af0: 6e6f 6e2d 696e 6865 7269 7465 6420 6d65  non-inherited me
-00006b00: 7468 6f64 7320 616e 6420 6465 7363 7269  thods and descri
-00006b10: 7074 6f72 7320 7769 7468 2073 7461 7465  ptors with state
-00006b20: 0a20 2020 206d 616e 6167 656d 656e 7420  .    management 
-00006b30: 6675 6e63 7469 6f6e 732e 0a20 2020 2022  functions..    "
-00006b40: 2222 0a20 2020 2023 2077 7261 7020 6d65  "".    # wrap me
-00006b50: 7468 6f64 730a 2020 2020 6d65 7468 6f64  thods.    method
-00006b60: 5f65 7863 6c75 7369 6f6e 7320 3d20 285b  _exclusions = ([
-00006b70: 662e 6e61 6d65 2066 6f72 2066 2069 6e20  f.name for f in 
-00006b80: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
-00006b90: 6473 2863 6c73 295d 202b 0a20 2020 2020  ds(cls)] +.     
-00006ba0: 2020 2020 2020 2020 2020 2020 205b 275f               ['_
-00006bb0: 5f65 715f 5f27 2c20 275f 5f72 6570 725f  _eq__', '__repr_
-00006bc0: 5f27 2c20 275f 5f69 6e69 745f 5f27 2c20  _', '__init__', 
-00006bd0: 275f 5f68 6173 685f 5f27 2c0a 2020 2020  '__hash__',.    
-00006be0: 2020 2020 2020 2020 2020 2020 2020 2027                 '
-00006bf0: 5f5f 706f 7374 5f69 6e69 745f 5f27 5d29  __post_init__'])
-00006c00: 0a20 2020 2066 6f72 206b 6579 2069 6e20  .    for key in 
-00006c10: 5f67 6574 5f6c 6f63 616c 5f6d 6574 686f  _get_local_metho
-00006c20: 645f 6e61 6d65 7328 636c 732c 2065 7863  d_names(cls, exc
-00006c30: 6c75 6465 3d6d 6574 686f 645f 6578 636c  lude=method_excl
-00006c40: 7573 696f 6e73 293a 0a20 2020 2020 206d  usions):.      m
-00006c50: 6574 686f 6420 3d20 6765 7461 7474 7228  ethod = getattr(
-00006c60: 636c 732c 206b 6579 290a 2020 2020 2020  cls, key).      
-00006c70: 6966 2068 6173 6174 7472 286d 6574 686f  if hasattr(metho
-00006c80: 642c 2027 6e6f 7772 6170 2729 3a0a 2020  d, 'nowrap'):.  
-00006c90: 2020 2020 2020 636f 6e74 696e 7565 0a20        continue. 
-00006ca0: 2020 2020 2073 6574 6174 7472 2863 6c73       setattr(cls
-00006cb0: 2c20 6b65 792c 2077 7261 705f 6d65 7468  , key, wrap_meth
-00006cc0: 6f64 5f6f 6e63 6528 6d65 7468 6f64 2929  od_once(method))
-00006cd0: 0a0a 2020 2020 2320 7772 6170 2064 6573  ..    # wrap des
-00006ce0: 6372 6970 746f 7273 0a20 2020 2064 6573  criptors.    des
-00006cf0: 6372 6970 746f 725f 6578 636c 7573 696f  criptor_exclusio
-00006d00: 6e73 203d 2028 5b66 2e6e 616d 6520 666f  ns = ([f.name fo
-00006d10: 7220 6620 696e 2064 6174 6163 6c61 7373  r f in dataclass
-00006d20: 6573 2e66 6965 6c64 7328 636c 7329 5d20  es.fields(cls)] 
-00006d30: 2b0a 2020 2020 2020 2020 2020 2020 2020  +.              
-00006d40: 2020 2020 2020 2020 2020 2020 2020 205b                 [
-00006d50: 2770 6172 656e 7427 2c20 275f 5f64 6963  'parent', '__dic
-00006d60: 745f 5f27 5d29 0a20 2020 2066 6f72 206b  t__']).    for k
-00006d70: 6579 2069 6e20 5f67 6574 5f6c 6f63 616c  ey in _get_local
-00006d80: 5f64 6573 6372 6970 746f 725f 6e61 6d65  _descriptor_name
-00006d90: 7328 636c 732c 2064 6573 6372 6970 746f  s(cls, descripto
-00006da0: 725f 6578 636c 7573 696f 6e73 293a 0a20  r_exclusions):. 
-00006db0: 2020 2020 2023 2064 6f6e 2774 2075 7365       # don't use
-00006dc0: 2067 6574 6174 7472 2068 6572 652c 2073   getattr here, s
-00006dd0: 696e 6365 2069 7420 7769 6c6c 2063 616c  ince it will cal
-00006de0: 6c20 7468 6520 6465 7363 7269 7074 6f72  l the descriptor
-00006df0: 0a20 2020 2020 2064 6573 6372 6970 746f  .      descripto
-00006e00: 7220 3d20 636c 732e 5f5f 6469 6374 5f5f  r = cls.__dict__
-00006e10: 5b6b 6579 5d0a 2020 2020 2020 6966 2068  [key].      if h
-00006e20: 6173 6174 7472 2864 6573 6372 6970 746f  asattr(descripto
-00006e30: 722c 2027 6e6f 7772 6170 2729 3a0a 2020  r, 'nowrap'):.  
-00006e40: 2020 2020 2020 636f 6e74 696e 7565 0a20        continue. 
-00006e50: 2020 2020 2073 6574 6174 7472 2863 6c73       setattr(cls
-00006e60: 2c20 6b65 792c 2077 7261 705f 6465 7363  , key, wrap_desc
-00006e70: 7269 7074 6f72 5f6f 6e63 6528 6465 7363  riptor_once(desc
-00006e80: 7269 7074 6f72 2929 0a20 2020 2072 6574  riptor)).    ret
-00006e90: 7572 6e20 636c 730a 0a20 2064 6566 205f  urn cls..  def _
-00006ea0: 6361 6c6c 5f77 7261 7070 6564 5f6d 6574  call_wrapped_met
-00006eb0: 686f 6428 7365 6c66 2c20 6675 6e2c 2061  hod(self, fun, a
-00006ec0: 7267 732c 206b 7761 7267 7329 3a0a 2020  rgs, kwargs):.  
-00006ed0: 2020 2222 2222 4361 6c6c 7320 6120 7772    """"Calls a wr
-00006ee0: 6170 7065 6420 6d65 7468 6f64 2e0a 0a20  apped method... 
-00006ef0: 2020 2054 6869 7320 6675 6e63 7469 6f6e     This function
-00006f00: 2069 7320 7265 7370 6f6e 7369 626c 6520   is responsible 
-00006f10: 666f 7220 7365 7474 696e 6720 7570 2074  for setting up t
-00006f20: 6865 2074 6872 6561 6420 6c6f 6361 6c20  he thread local 
-00006f30: 7374 6174 650a 2020 2020 636f 7272 6563  state.    correc
-00006f40: 746c 7920 6265 666f 7265 2063 616c 6c69  tly before calli
-00006f50: 6e67 2074 6865 206d 6574 686f 6420 616e  ng the method an
-00006f60: 6420 636c 6561 6e69 6e67 2075 7020 6166  d cleaning up af
-00006f70: 7465 7277 6172 6473 2e0a 2020 2020 5468  terwards..    Th
-00006f80: 6973 2069 6e63 6c75 6465 7320 7374 6f72  is includes stor
-00006f90: 696e 6720 696e 7465 726d 6564 6961 7465  ing intermediate
-00006fa0: 732c 2073 6574 7570 206f 6620 7468 6520  s, setup of the 
-00006fb0: 636f 6d70 6163 7420 7363 6f70 652c 0a20  compact scope,. 
-00006fc0: 2020 2061 6e64 206d 616b 696e 6720 7375     and making su
-00006fd0: 7265 2073 6574 7570 2069 7320 6361 6c6c  re setup is call
-00006fe0: 6564 2062 6566 6f72 6520 616e 7920 6f74  ed before any ot
-00006ff0: 6865 7220 6d65 7468 6f64 2e0a 0a20 2020  her method...   
-00007000: 2041 7267 733a 0a20 2020 2020 2066 756e   Args:.      fun
-00007010: 3a20 5468 6520 7772 6170 7065 6420 6d65  : The wrapped me
-00007020: 7468 6f64 2e0a 2020 2020 2020 6172 6773  thod..      args
-00007030: 3a20 4e61 6d65 6420 6172 6775 6d65 6e74  : Named argument
-00007040: 7320 7061 7373 6564 2074 6f20 6060 6675  s passed to ``fu
-00007050: 6e60 602e 0a20 2020 2020 206b 7761 7267  n``..      kwarg
-00007060: 733a 204b 6579 776f 7264 2061 7267 756d  s: Keyword argum
-00007070: 656e 7473 2070 6173 7365 6420 746f 2060  ents passed to `
-00007080: 6066 756e 6060 2e0a 0a20 2020 2052 6574  `fun``...    Ret
-00007090: 7572 6e73 3a0a 2020 2020 2020 5468 6520  urns:.      The 
-000070a0: 7265 7375 6c74 7320 6f66 2063 616c 6c69  results of calli
-000070b0: 6e67 2060 6066 756e 6060 2e0a 2020 2020  ng ``fun``..    
-000070c0: 2222 220a 2020 2020 6973 5f63 6f6d 7061  """.    is_compa
-000070d0: 6374 5f6d 6574 686f 6420 3d20 6861 7361  ct_method = hasa
-000070e0: 7474 7228 6675 6e2c 2027 636f 6d70 6163  ttr(fun, 'compac
-000070f0: 7427 290a 2020 2020 6675 6e5f 6e61 6d65  t').    fun_name
-00007100: 203d 2067 6574 6174 7472 2866 756e 2c20   = getattr(fun, 
-00007110: 275f 5f6e 616d 655f 5f27 2c20 2775 6e6e  '__name__', 'unn
-00007120: 616d 6564 5f66 756e 6374 696f 6e27 290a  amed_function').
-00007130: 2020 2020 6973 5f73 6574 7570 5f6d 6574      is_setup_met
-00007140: 686f 6420 3d20 6675 6e5f 6e61 6d65 203d  hod = fun_name =
-00007150: 3d20 2773 6574 7570 270a 2020 2020 6164  = 'setup'.    ad
-00007160: 645f 6361 6c6c 5f69 6e66 6f20 3d20 6e6f  d_call_info = no
-00007170: 7420 6973 5f73 6574 7570 5f6d 6574 686f  t is_setup_metho
-00007180: 6420 616e 6420 6c65 6e28 5f63 6f6e 7465  d and len(_conte
-00007190: 7874 2e63 616c 6c5f 696e 666f 5f73 7461  xt.call_info_sta
-000071a0: 636b 2920 3e20 300a 2020 2020 2320 5765  ck) > 0.    # We
-000071b0: 206c 617a 696c 7920 6361 6c6c 2073 6574   lazily call set
-000071c0: 7570 2829 206f 6e6c 7920 7768 656e 206e  up() only when n
-000071d0: 6565 6465 642e 0a20 2020 2069 6620 6973  eeded..    if is
-000071e0: 5f73 6574 7570 5f6d 6574 686f 643a 0a20  _setup_method:. 
-000071f0: 2020 2020 2069 6620 7365 6c66 2e73 636f       if self.sco
-00007200: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-00007210: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
-00007220: 2e43 616c 6c53 6574 7570 556e 626f 756e  .CallSetupUnboun
-00007230: 644d 6f64 756c 6545 7272 6f72 2829 0a20  dModuleError(). 
-00007240: 2020 2020 2069 735f 7265 6375 7272 656e       is_recurren
-00007250: 7420 3d20 7365 6c66 2e5f 7374 6174 652e  t = self._state.
-00007260: 696e 5f73 6574 7570 0a20 2020 2020 2073  in_setup.      s
-00007270: 656c 662e 5f73 7461 7465 2e69 6e5f 7365  elf._state.in_se
-00007280: 7475 7020 3d20 5472 7565 0a20 2020 2065  tup = True.    e
-00007290: 6c73 653a 0a20 2020 2020 2073 656c 662e  lse:.      self.
-000072a0: 5f74 7279 5f73 6574 7570 2829 0a0a 2020  _try_setup()..  
-000072b0: 2020 6966 2069 735f 636f 6d70 6163 745f    if is_compact_
-000072c0: 6d65 7468 6f64 3a0a 2020 2020 2020 6966  method:.      if
-000072d0: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
-000072e0: 6f6e 653a 0a20 2020 2020 2020 2072 6169  one:.        rai
-000072f0: 7365 2065 7272 6f72 732e 4361 6c6c 436f  se errors.CallCo
-00007300: 6d70 6163 7455 6e62 6f75 6e64 4d6f 6475  mpactUnboundModu
-00007310: 6c65 4572 726f 7228 290a 2020 2020 2020  leError().      
-00007320: 6973 5f72 6563 7572 7265 6e74 203d 2073  is_recurrent = s
-00007330: 656c 662e 5f73 7461 7465 2e69 6e5f 636f  elf._state.in_co
-00007340: 6d70 6163 745f 6d65 7468 6f64 0a20 2020  mpact_method.   
-00007350: 2020 2073 656c 662e 5f73 7461 7465 2e69     self._state.i
-00007360: 6e5f 636f 6d70 6163 745f 6d65 7468 6f64  n_compact_method
-00007370: 203d 2054 7275 650a 2020 2020 5f63 6f6e   = True.    _con
-00007380: 7465 7874 2e6d 6f64 756c 655f 7374 6163  text.module_stac
-00007390: 6b2e 6170 7065 6e64 2873 656c 6629 0a20  k.append(self). 
-000073a0: 2020 2074 7279 3a0a 2020 2020 2020 2320     try:.      # 
-000073b0: 6765 7420 6361 6c6c 2069 6e66 6f0a 2020  get call info.  
-000073c0: 2020 2020 6966 2061 6464 5f63 616c 6c5f      if add_call_
-000073d0: 696e 666f 3a0a 2020 2020 2020 2020 6173  info:.        as
-000073e0: 7365 7274 2073 656c 662e 7363 6f70 6520  sert self.scope 
-000073f0: 6973 206e 6f74 204e 6f6e 650a 2020 2020  is not None.    
-00007400: 2020 2020 6361 6c6c 5f69 6e64 6578 203d      call_index =
-00007410: 205f 636f 6e74 6578 742e 6361 6c6c 5f69   _context.call_i
-00007420: 6e66 6f5f 7374 6163 6b5b 2d31 5d2e 6765  nfo_stack[-1].ge
-00007430: 745f 6361 6c6c 5f69 6e64 6578 2873 656c  t_call_index(sel
-00007440: 6629 0a20 2020 2020 2020 2073 636f 7065  f).        scope
-00007450: 5f70 6174 6820 3d20 6a61 782e 7472 6565  _path = jax.tree
-00007460: 5f75 7469 6c2e 7472 6565 5f6d 6170 285f  _util.tree_map(_
-00007470: 6669 785f 7061 7468 5f70 6172 742c 2073  fix_path_part, s
-00007480: 656c 662e 7363 6f70 652e 7061 7468 290a  elf.scope.path).
-00007490: 0a20 2020 2020 2023 2063 616c 6c20 6d65  .      # call me
-000074a0: 7468 6f64 0a20 2020 2020 2069 6620 5f75  thod.      if _u
-000074b0: 7365 5f6e 616d 6564 5f63 616c 6c3a 0a20  se_named_call:. 
-000074c0: 2020 2020 2020 2077 6974 6820 6a61 782e         with jax.
-000074d0: 6e61 6d65 645f 7363 6f70 6528 5f64 6572  named_scope(_der
-000074e0: 6976 655f 7072 6f66 696c 696e 675f 6e61  ive_profiling_na
-000074f0: 6d65 2873 656c 662c 2066 756e 2929 3a0a  me(self, fun)):.
-00007500: 2020 2020 2020 2020 2020 7920 3d20 6675            y = fu
-00007510: 6e28 7365 6c66 2c20 2a61 7267 732c 202a  n(self, *args, *
-00007520: 2a6b 7761 7267 7329 0a20 2020 2020 2065  *kwargs).      e
-00007530: 6c73 653a 0a20 2020 2020 2020 2079 203d  lse:.        y =
-00007540: 2066 756e 2873 656c 662c 202a 6172 6773   fun(self, *args
-00007550: 2c20 2a2a 6b77 6172 6773 290a 0a20 2020  , **kwargs)..   
-00007560: 2020 2069 6620 5f63 6f6e 7465 7874 2e63     if _context.c
-00007570: 6170 7475 7265 5f73 7461 636b 3a0a 2020  apture_stack:.  
-00007580: 2020 2020 2020 6669 6c74 6572 5f66 6e20        filter_fn 
-00007590: 3d20 5f63 6f6e 7465 7874 2e63 6170 7475  = _context.captu
-000075a0: 7265 5f73 7461 636b 5b2d 315d 0a20 2020  re_stack[-1].   
-000075b0: 2020 2020 2069 6620 6669 6c74 6572 5f66       if filter_f
-000075c0: 6e20 616e 6420 6669 6c74 6572 5f66 6e28  n and filter_fn(
-000075d0: 7365 6c66 2c20 6675 6e5f 6e61 6d65 293a  self, fun_name):
-000075e0: 0a20 2020 2020 2020 2020 2073 656c 662e  .          self.
-000075f0: 736f 7728 2769 6e74 6572 6d65 6469 6174  sow('intermediat
-00007600: 6573 272c 2066 756e 5f6e 616d 652c 2079  es', fun_name, y
-00007610: 290a 2020 2020 2020 6966 2061 6464 5f63  ).      if add_c
-00007620: 616c 6c5f 696e 666f 3a0a 2020 2020 2020  all_info:.      
-00007630: 2020 5f61 7267 732c 205f 6b77 6172 6773    _args, _kwargs
-00007640: 2c20 5f79 203d 2066 6c61 782e 6c69 6e65  , _y = flax.line
-00007650: 6e2e 7375 6d6d 6172 792e 5f72 6570 7265  n.summary._repre
-00007660: 7365 6e74 5f74 7265 6528 2861 7267 732c  sent_tree((args,
-00007670: 206b 7761 7267 732c 2079 2929 0a20 2020   kwargs, y)).   
-00007680: 2020 2020 205f 636f 6e74 6578 742e 6361       _context.ca
-00007690: 6c6c 5f69 6e66 6f5f 7374 6163 6b5b 2d31  ll_info_stack[-1
-000076a0: 5d2e 6361 6c6c 732e 6170 7065 6e64 280a  ].calls.append(.
-000076b0: 2020 2020 2020 2020 2020 5f43 616c 6c49            _CallI
-000076c0: 6e66 6f28 6361 6c6c 5f69 6e64 6578 2c20  nfo(call_index, 
-000076d0: 7363 6f70 655f 7061 7468 2c20 7479 7065  scope_path, type
-000076e0: 2873 656c 6629 2c20 6675 6e2e 5f5f 6e61  (self), fun.__na
-000076f0: 6d65 5f5f 2c20 5f61 7267 732c 205f 6b77  me__, _args, _kw
-00007700: 6172 6773 2c20 5f79 2929 0a20 2020 2020  args, _y)).     
-00007710: 2072 6574 7572 6e20 790a 2020 2020 6669   return y.    fi
-00007720: 6e61 6c6c 793a 0a20 2020 2020 205f 636f  nally:.      _co
-00007730: 6e74 6578 742e 6d6f 6475 6c65 5f73 7461  ntext.module_sta
-00007740: 636b 2e70 6f70 2829 0a20 2020 2020 2069  ck.pop().      i
-00007750: 6620 6973 5f63 6f6d 7061 6374 5f6d 6574  f is_compact_met
-00007760: 686f 643a 0a20 2020 2020 2020 206f 626a  hod:.        obj
-00007770: 6563 742e 5f5f 7365 7461 7474 725f 5f28  ect.__setattr__(
-00007780: 7365 6c66 2c20 2773 636f 7065 272c 2073  self, 'scope', s
-00007790: 656c 662e 7363 6f70 652e 7265 776f 756e  elf.scope.rewoun
-000077a0: 6428 2929 0a20 2020 2020 2023 2073 6574  d()).      # set
-000077b0: 7570 206f 7220 636f 6d70 6163 7420 6361  up or compact ca
-000077c0: 6c6c 7320 6361 6e20 6265 2072 6563 7572  lls can be recur
-000077d0: 7265 6e74 2066 6f72 2065 7861 6d70 6c65  rent for example
-000077e0: 2064 7565 2074 6f20 7375 7065 7220 6361   due to super ca
-000077f0: 6c6c 730a 2020 2020 2020 2320 7265 7365  lls.      # rese
-00007800: 7474 696e 6720 7468 6520 7374 6174 6520  tting the state 
-00007810: 776f 756c 6420 6361 7573 6520 6973 2063  would cause is c
-00007820: 6f6d 7061 6374 2f73 6574 7570 206d 6574  ompact/setup met
-00007830: 686f 640a 2020 2020 2020 2320 746f 2062  hod.      # to b
-00007840: 6520 7365 7420 746f 2046 616c 7365 2070  e set to False p
-00007850: 7265 6d61 7475 7265 6c79 2e0a 2020 2020  rematurely..    
-00007860: 2020 6966 2028 6973 5f63 6f6d 7061 6374    if (is_compact
-00007870: 5f6d 6574 686f 6420 6f72 2069 735f 7365  _method or is_se
-00007880: 7475 705f 6d65 7468 6f64 2920 616e 6420  tup_method) and 
-00007890: 6e6f 7420 6973 5f72 6563 7572 7265 6e74  not is_recurrent
-000078a0: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
-000078b0: 7374 6174 652e 7265 7365 7428 290a 0a20  state.reset().. 
-000078c0: 2064 6566 205f 5f73 6574 6174 7472 5f5f   def __setattr__
-000078d0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-000078e0: 2c20 7661 6c3a 2041 6e79 293a 0a20 2020  , val: Any):.   
-000078f0: 2022 2222 5365 7473 2061 6e20 6174 7472   """Sets an attr
-00007900: 6962 7574 6520 6f6e 2074 6869 7320 4d6f  ibute on this Mo
-00007910: 6475 6c65 2e0a 0a20 2020 2057 6520 6f76  dule...    We ov
-00007920: 6572 6c6f 6164 2073 6574 6174 7472 2073  erload setattr s
-00007930: 6f6c 656c 7920 746f 2073 7570 706f 7274  olely to support
-00007940: 2070 7974 686f 6e69 6320 6e61 6d69 6e67   pythonic naming
-00007950: 2076 6961 2061 7373 6967 6e6d 656e 7420   via assignment 
-00007960: 6f66 0a20 2020 2073 7562 6d6f 6475 6c65  of.    submodule
-00007970: 7320 696e 2074 6865 2073 7065 6369 616c  s in the special
-00007980: 203a 6d65 7468 3a60 7365 7475 7060 2066   :meth:`setup` f
-00007990: 756e 6374 696f 6e3a 3a0a 0a20 2020 2020  unction::..     
-000079a0: 2073 656c 662e 7375 626d 6f64 756c 655f   self.submodule_
-000079b0: 6e61 6d65 203d 204d 794d 6f64 756c 6528  name = MyModule(
-000079c0: 2e2e 2e29 0a0a 2020 2020 5765 2061 6c73  ...)..    We als
-000079d0: 6f20 7375 7070 6f72 7420 6c69 7374 7320  o support lists 
-000079e0: 616e 6420 6f74 6865 7220 6765 6e65 7261  and other genera
-000079f0: 6c20 7079 7472 6565 732c 2065 2e67 2e3a  l pytrees, e.g.:
-00007a00: 3a0a 0a20 2020 2020 2073 656c 662e 7375  :..      self.su
-00007a10: 626d 6f64 756c 6573 203d 205b 4d79 4d6f  bmodules = [MyMo
-00007a20: 6475 6c65 3028 2e2e 292c 204d 794d 6f64  dule0(..), MyMod
-00007a30: 756c 6531 282e 2e29 2c20 2e2e 2e5d 0a0a  ule1(..), ...]..
-00007a40: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-00007a50: 6e61 6d65 3a20 4174 7472 6962 7574 6520  name: Attribute 
-00007a60: 746f 2073 6574 2e0a 2020 2020 2020 7661  to set..      va
-00007a70: 6c3a 2056 616c 7565 206f 6620 7468 6520  l: Value of the 
-00007a80: 6174 7472 6962 7574 652e 0a20 2020 2022  attribute..    "
-00007a90: 2222 0a20 2020 2066 6965 6c64 7320 3d20  "".    fields = 
-00007aa0: 7365 6c66 2e5f 5f64 6174 6163 6c61 7373  self.__dataclass
-00007ab0: 5f66 6965 6c64 735f 5f20 2023 2070 7974  _fields__  # pyt
-00007ac0: 7970 653a 2064 6973 6162 6c65 3d61 7474  ype: disable=att
-00007ad0: 7269 6275 7465 2d65 7272 6f72 0a20 2020  ribute-error.   
-00007ae0: 2069 735f 6461 7461 636c 6173 735f 6174   is_dataclass_at
-00007af0: 7472 203d 206e 616d 6520 696e 2066 6965  tr = name in fie
-00007b00: 6c64 7320 616e 6420 6669 656c 6473 5b6e  lds and fields[n
-00007b10: 616d 655d 2e69 6e69 740a 0a20 2020 2069  ame].init..    i
-00007b20: 6620 6e6f 7420 7365 6c66 2e5f 7374 6174  f not self._stat
-00007b30: 652e 696e 5f73 6574 7570 3a0a 2020 2020  e.in_setup:.    
-00007b40: 2020 6966 206e 6f74 2073 656c 662e 5f73    if not self._s
-00007b50: 7461 7465 2e69 735f 696e 6974 6961 6c69  tate.is_initiali
-00007b60: 7a65 643a 0a20 2020 2020 2020 2023 2053  zed:.        # S
-00007b70: 6574 7469 6e67 2061 7474 7269 6275 7465  etting attribute
-00007b80: 7320 6265 666f 7265 2065 6e64 206f 6620  s before end of 
-00007b90: 4d6f 6475 6c65 2e5f 5f70 6f73 745f 696e  Module.__post_in
-00007ba0: 6974 5f5f 2829 0a20 2020 2020 2020 206f  it__().        o
-00007bb0: 626a 6563 742e 5f5f 7365 7461 7474 725f  bject.__setattr_
-00007bc0: 5f28 7365 6c66 2c20 6e61 6d65 2c20 7661  _(self, name, va
-00007bd0: 6c29 0a20 2020 2020 2020 2072 6574 7572  l).        retur
-00007be0: 6e0a 2020 2020 2020 656c 7365 3a0a 2020  n.      else:.  
-00007bf0: 2020 2020 2020 2320 5765 2772 6520 7061        # We're pa
-00007c00: 7374 2061 6c6c 2069 6e69 7469 616c 697a  st all initializ
-00007c10: 6174 696f 6e20 616e 6420 7365 7475 7020  ation and setup 
-00007c20: 6c6f 6769 633a 0a20 2020 2020 2020 2023  logic:.        #
-00007c30: 2052 6169 7365 7320 6120 5479 7065 4572   Raises a TypeEr
-00007c40: 726f 7220 6a75 7374 206c 696b 6520 6672  ror just like fr
-00007c50: 6f7a 656e 2070 7974 686f 6e20 6461 7461  ozen python data
-00007c60: 636c 6173 7365 732e 0a20 2020 2020 2020  classes..       
-00007c70: 2072 6169 7365 2065 7272 6f72 732e 5365   raise errors.Se
-00007c80: 7441 7474 7269 6275 7465 4672 6f7a 656e  tAttributeFrozen
-00007c90: 4d6f 6475 6c65 4572 726f 7228 0a20 2020  ModuleError(.   
-00007ca0: 2020 2020 2020 2020 2073 656c 662e 5f5f           self.__
-00007cb0: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
-00007cc0: 2c20 6e61 6d65 2c20 7661 6c29 0a0a 2020  , name, val)..  
-00007cd0: 2020 2320 5765 2772 6520 696e 7369 6465    # We're inside
-00007ce0: 2074 6865 2073 6574 7570 2829 206d 6574   the setup() met
-00007cf0: 686f 643a 0a20 2020 2069 6620 6973 5f64  hod:.    if is_d
-00007d00: 6174 6163 6c61 7373 5f61 7474 723a 0a20  ataclass_attr:. 
-00007d10: 2020 2020 2023 2054 6865 7365 206e 616d       # These nam
-00007d20: 6573 2061 7265 2073 7065 6369 6669 6564  es are specified
-00007d30: 2061 7320 6461 7461 636c 6173 7320 6669   as dataclass fi
-00007d40: 656c 6473 2e20 5468 6579 2073 686f 756c  elds. They shoul
-00007d50: 6420 6e6f 7420 6265 0a20 2020 2020 2023  d not be.      #
-00007d60: 2069 6e69 7469 616c 697a 6564 2077 6974   initialized wit
-00007d70: 6869 6e20 7468 6520 7365 7475 7028 2920  hin the setup() 
-00007d80: 6d65 7468 6f64 2c20 6275 7420 6361 6e20  method, but can 
-00007d90: 6265 206d 6f64 6966 6965 6420 6672 6565  be modified free
-00007da0: 6c79 0a20 2020 2020 2023 2062 6566 6f72  ly.      # befor
-00007db0: 6520 6974 2e0a 2020 2020 2020 7261 6973  e it..      rais
-00007dc0: 6520 6572 726f 7273 2e53 6574 4174 7472  e errors.SetAttr
-00007dd0: 6962 7574 6549 6e4d 6f64 756c 6553 6574  ibuteInModuleSet
-00007de0: 7570 4572 726f 7228 290a 0a20 2020 2023  upError()..    #
-00007df0: 2056 616c 7565 7320 2874 6861 7420 6d61   Values (that ma
-00007e00: 7920 6265 2076 6172 6961 626c 6573 206f  y be variables o
-00007e10: 7220 7375 626d 6f64 756c 6573 2920 6172  r submodules) ar
-00007e20: 6520 6265 696e 6720 6465 6669 6e65 6420  e being defined 
-00007e30: 616e 640a 2020 2020 2320 6174 7461 6368  and.    # attach
-00007e40: 6564 2069 6e20 7365 7475 7028 292c 2077  ed in setup(), w
-00007e50: 6520 7275 6e20 736f 6d65 2065 7874 7261  e run some extra
-00007e60: 206c 6f67 6963 2069 6e20 7468 6174 2063   logic in that c
-00007e70: 6173 652e 0a20 2020 2073 656c 662e 5f72  ase..    self._r
-00007e80: 6567 6973 7465 725f 7375 626d 6f64 756c  egister_submodul
-00007e90: 6573 286e 616d 652c 2076 616c 290a 0a20  es(name, val).. 
-00007ea0: 2064 6566 205f 5f67 6574 6174 7472 5f5f   def __getattr__
-00007eb0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-00007ec0: 2920 2d3e 2041 6e79 3a0a 2020 2020 2222  ) -> Any:.    ""
-00007ed0: 2243 616c 6c20 7365 7475 7028 2920 6265  "Call setup() be
-00007ee0: 666f 7265 2067 6574 7469 6e67 2061 6e79  fore getting any
-00007ef0: 2073 6574 7570 2d64 6566 696e 6564 2061   setup-defined a
-00007f00: 7474 7269 6275 7465 732e 2222 220a 2020  ttributes.""".  
-00007f10: 2020 2320 5765 2064 6f6e 2774 2077 616e    # We don't wan
-00007f20: 7420 746f 2072 6574 7572 6e20 616e 7974  t to return anyt
-00007f30: 6869 6e67 2066 6f72 2070 7974 686f 6e20  hing for python 
-00007f40: 636f 7079 202f 2070 6963 6b6c 6520 6d65  copy / pickle me
-00007f50: 7468 6f64 732e 0a20 2020 2069 6620 6e61  thods..    if na
-00007f60: 6d65 2069 6e20 5f55 4e44 4546 494e 4544  me in _UNDEFINED
-00007f70: 5f43 4f50 595f 5049 434b 4c45 5f4d 4554  _COPY_PICKLE_MET
-00007f80: 484f 4453 3a0a 2020 2020 2020 7261 6973  HODS:.      rais
-00007f90: 6520 4174 7472 6962 7574 6545 7272 6f72  e AttributeError
-00007fa0: 2829 0a20 2020 2073 656c 662e 5f74 7279  ().    self._try
-00007fb0: 5f73 6574 7570 2829 0a20 2020 2069 6620  _setup().    if 
-00007fc0: 6e61 6d65 2069 6e20 7365 6c66 2e5f 5f64  name in self.__d
-00007fd0: 6963 745f 5f3a 0a20 2020 2020 2072 6574  ict__:.      ret
-00007fe0: 7572 6e20 7365 6c66 2e5f 5f64 6963 745f  urn self.__dict_
-00007ff0: 5f5b 6e61 6d65 5d0a 2020 2020 656c 7365  _[name].    else
-00008000: 3a0a 2020 2020 2020 6d73 6720 3d20 6627  :.      msg = f'
-00008010: 227b 7365 6c66 2e5f 5f63 6c61 7373 5f5f  "{self.__class__
-00008020: 2e5f 5f6e 616d 655f 5f7d 2220 6f62 6a65  .__name__}" obje
-00008030: 6374 2068 6173 206e 6f20 6174 7472 6962  ct has no attrib
-00008040: 7574 6520 227b 6e61 6d65 7d22 2e27 0a20  ute "{name}".'. 
-00008050: 2020 2020 2069 6620 7365 6c66 2e73 636f       if self.sco
-00008060: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-00008070: 2020 2020 6d73 6720 2b3d 2028 6627 2049      msg += (f' I
-00008080: 6620 227b 6e61 6d65 7d22 2069 7320 6465  f "{name}" is de
-00008090: 6669 6e65 6420 696e 205c 272e 7365 7475  fined in \'.setu
-000080a0: 7028 295c 272c 2072 656d 656d 6265 7220  p()\', remember 
-000080b0: 7468 6573 6520 6669 656c 6473 2027 0a20  these fields '. 
-000080c0: 2020 2020 2020 2020 2027 6172 6520 6f6e           'are on
-000080d0: 6c79 2061 6363 6573 7369 626c 6520 6672  ly accessible fr
-000080e0: 6f6d 2069 6e73 6964 6520 5c27 696e 6974  om inside \'init
-000080f0: 5c27 206f 7220 5c27 6170 706c 795c 272e  \' or \'apply\'.
-00008100: 2729 0a20 2020 2020 2072 6169 7365 2041  ').      raise A
-00008110: 7474 7269 6275 7465 4572 726f 7228 6d73  ttributeError(ms
-00008120: 6729 0a0a 2020 6465 6620 5f5f 6469 725f  g)..  def __dir_
-00008130: 5f28 7365 6c66 2920 2d3e 204c 6973 745b  _(self) -> List[
-00008140: 7374 725d 3a0a 2020 2020 2222 2243 616c  str]:.    """Cal
-00008150: 6c20 7365 7475 7028 2920 6265 666f 7265  l setup() before
-00008160: 206c 6973 7469 6e67 2061 7474 7269 6275   listing attribu
-00008170: 7465 732e 2222 220a 2020 2020 7365 6c66  tes.""".    self
-00008180: 2e5f 7472 795f 7365 7475 7028 290a 2020  ._try_setup().  
-00008190: 2020 7265 7475 726e 206f 626a 6563 742e    return object.
-000081a0: 5f5f 6469 725f 5f28 7365 6c66 2920 2023  __dir__(self)  #
-000081b0: 2074 7970 653a 2069 676e 6f72 650a 0a20   type: ignore.. 
-000081c0: 2064 6566 205f 5f70 6f73 745f 696e 6974   def __post_init
-000081d0: 5f5f 2873 656c 6629 202d 3e20 4e6f 6e65  __(self) -> None
-000081e0: 3a0a 2020 2020 2320 444f 204e 4f54 2052  :.    # DO NOT R
-000081f0: 454d 4f56 4520 2d20 4d61 726b 6572 2066  EMOVE - Marker f
-00008200: 6f72 2069 6e74 6572 6e61 6c20 6c6f 6767  or internal logg
-00008210: 696e 672e 0a20 2020 2023 2049 6e20 6461  ing..    # In da
-00008220: 7461 636c 6173 7365 732c 205f 5f69 6e69  taclasses, __ini
-00008230: 745f 5f20 6973 206f 7665 7272 6964 6465  t__ is overridde
-00008240: 6e20 746f 2070 726f 6365 7373 2064 6174  n to process dat
-00008250: 6163 6c61 7373 2061 7267 756d 656e 7473  aclass arguments
-00008260: 2c0a 2020 2020 2320 616e 6420 5f5f 706f  ,.    # and __po
-00008270: 7374 5f69 6e69 745f 5f20 6973 2063 616c  st_init__ is cal
-00008280: 6c65 6420 696d 6d65 6469 6174 656c 7920  led immediately 
-00008290: 6166 7465 7277 6172 6473 2e20 4865 7265  afterwards. Here
-000082a0: 2c20 6465 7065 6e64 696e 6720 6f6e 2074  , depending on t
-000082b0: 6865 0a20 2020 2023 2074 7970 6520 6f66  he.    # type of
-000082c0: 2060 7061 7265 6e74 6020 7061 7373 6564   `parent` passed
-000082d0: 2074 6f20 696e 6974 6961 6c69 7a65 2074   to initialize t
-000082e0: 6865 204d 6f64 756c 652c 2077 6520 6569  he Module, we ei
-000082f0: 7468 6572 2064 6566 6572 0a20 2020 2023  ther defer.    #
-00008300: 2069 6e69 7469 616c 697a 6174 696f 6e2c   initialization,
-00008310: 2061 7474 6163 6820 7468 6973 204d 6f64   attach this Mod
-00008320: 756c 6520 6173 2061 2073 7562 6d6f 6475  ule as a submodu
-00008330: 6c65 206f 6620 6120 7061 7265 6e74 2c20  le of a parent, 
-00008340: 6f72 2062 696e 640a 2020 2020 2320 7468  or bind.    # th
-00008350: 6973 204d 6f64 756c 6520 6174 2074 6865  is Module at the
-00008360: 2074 6f70 2d6c 6576 656c 2074 6f20 7661   top-level to va
-00008370: 7269 6162 6c65 7320 616e 6420 726e 6773  riables and rngs
-00008380: 2e0a 0a20 2020 206f 626a 6563 742e 5f5f  ...    object.__
-00008390: 7365 7461 7474 725f 5f28 7365 6c66 2c20  setattr__(self, 
-000083a0: 275f 6964 272c 2075 7569 6428 2929 0a20  '_id', uuid()). 
-000083b0: 2020 206f 626a 6563 742e 5f5f 7365 7461     object.__seta
-000083c0: 7474 725f 5f28 7365 6c66 2c20 275f 7374  ttr__(self, '_st
-000083d0: 6174 6527 2c20 5f4d 6f64 756c 6549 6e74  ate', _ModuleInt
-000083e0: 6572 6e61 6c53 7461 7465 2829 290a 0a20  ernalState()).. 
-000083f0: 2020 2023 2054 7970 6963 616c 6c79 2077     # Typically w
-00008400: 6520 7365 7420 7468 6520 7061 7265 6e74  e set the parent
-00008410: 2062 6173 6564 206f 6e20 7468 6520 6479   based on the dy
-00008420: 6e61 6d69 6320 6d6f 6475 6c65 2063 6f6e  namic module con
-00008430: 7465 7874 2e0a 2020 2020 6966 2073 656c  text..    if sel
-00008440: 662e 7061 7265 6e74 2069 7320 5f75 6e73  f.parent is _uns
-00008450: 7065 6369 6669 6564 5f70 6172 656e 743a  pecified_parent:
-00008460: 2020 2320 7079 7479 7065 3a20 6469 7361    # pytype: disa
-00008470: 626c 653d 6174 7472 6962 7574 652d 6572  ble=attribute-er
-00008480: 726f 720a 2020 2020 2020 6f62 6a65 6374  ror.      object
-00008490: 2e5f 5f73 6574 6174 7472 5f5f 2873 656c  .__setattr__(sel
-000084a0: 662c 2027 7061 7265 6e74 272c 205f 636f  f, 'parent', _co
-000084b0: 6e74 6578 742e 6d6f 6475 6c65 5f73 7461  ntext.module_sta
-000084c0: 636b 5b2d 315d 290a 0a20 2020 2023 2049  ck[-1])..    # I
-000084d0: 6e69 7469 616c 697a 6174 696f 6e20 6973  nitialization is
-000084e0: 2064 6566 6572 7265 6420 666f 7220 746f   deferred for to
-000084f0: 7020 6c65 7665 6c20 4d6f 6475 6c65 7320  p level Modules 
-00008500: 6f72 2061 6e79 206f 7468 6572 2022 6f72  or any other "or
-00008510: 7068 616e 220a 2020 2020 2320 4d6f 6475  phan".    # Modu
-00008520: 6c65 7320 756e 7469 6c20 6174 7461 6368  les until attach
-00008530: 6d65 6e74 2062 7920 5f5f 7365 7461 7474  ment by __setatt
-00008540: 725f 5f20 692e 652e 204d 794d 6f64 756c  r__ i.e. MyModul
-00008550: 6528 2e2e 2e2c 2070 6172 656e 743d 4e6f  e(..., parent=No
-00008560: 6e65 290a 2020 2020 6966 2073 656c 662e  ne).    if self.
-00008570: 7061 7265 6e74 2069 7320 4e6f 6e65 3a0a  parent is None:.
-00008580: 2020 2020 2020 7265 7475 726e 0a0a 2020        return..  
-00008590: 2020 2320 5265 6769 7374 6572 2073 7562    # Register sub
-000085a0: 6d6f 6475 6c65 206f 6e20 7061 7265 6e74  module on parent
-000085b0: 204d 6f64 756c 652e 0a20 2020 2069 6620   Module..    if 
-000085c0: 6973 696e 7374 616e 6365 2873 656c 662e  isinstance(self.
-000085d0: 7061 7265 6e74 2c20 4d6f 6475 6c65 293a  parent, Module):
-000085e0: 0a20 2020 2020 2023 2057 6865 6e20 696e  .      # When in
-000085f0: 6974 6961 6c69 7a69 6e67 2061 6e20 756e  itializing an un
-00008600: 6e61 6d65 6420 4d6f 6475 6c65 2069 6e73  named Module ins
-00008610: 6964 6520 7365 7475 7028 290a 2020 2020  ide setup().    
-00008620: 2020 2320 696e 6974 6961 6c69 7a61 7469    # initializati
-00008630: 6f6e 2069 7320 6465 6665 7272 6564 2075  on is deferred u
-00008640: 6e74 696c 2061 7474 6163 686d 656e 7420  ntil attachment 
-00008650: 6279 205f 5f73 6574 6174 7472 5f5f 0a20  by __setattr__. 
-00008660: 2020 2020 2023 2069 2e65 2e20 7365 6c66       # i.e. self
-00008670: 2e6d 796d 6f64 756c 6520 3d20 4d79 4d6f  .mymodule = MyMo
-00008680: 6475 6c65 282e 2e2e 290a 2020 2020 2020  dule(...).      
-00008690: 7365 6c66 2e6e 616d 653a 204f 7074 696f  self.name: Optio
-000086a0: 6e61 6c5b 7374 725d 0a20 2020 2020 2069  nal[str].      i
-000086b0: 6620 7365 6c66 2e70 6172 656e 742e 5f73  f self.parent._s
-000086c0: 7461 7465 2e69 6e5f 7365 7475 7020 616e  tate.in_setup an
-000086d0: 6420 7365 6c66 2e6e 616d 6520 6973 204e  d self.name is N
-000086e0: 6f6e 653a 2020 2320 7079 7479 7065 3a20  one:  # pytype: 
-000086f0: 6469 7361 626c 653d 6174 7472 6962 7574  disable=attribut
-00008700: 652d 6572 726f 720a 2020 2020 2020 2020  e-error.        
-00008710: 7265 7475 726e 0a20 2020 2020 2069 6620  return.      if 
-00008720: 6e6f 7420 7365 6c66 2e70 6172 656e 742e  not self.parent.
-00008730: 5f69 6e69 7469 616c 697a 6174 696f 6e5f  _initialization_
-00008740: 616c 6c6f 7765 643a 0a20 2020 2020 2020  allowed:.       
-00008750: 2072 6169 7365 2065 7272 6f72 732e 4173   raise errors.As
-00008760: 7369 676e 5375 624d 6f64 756c 6545 7272  signSubModuleErr
-00008770: 6f72 2873 656c 662e 5f5f 636c 6173 735f  or(self.__class_
-00008780: 5f2e 5f5f 6e61 6d65 5f5f 290a 2020 2020  _.__name__).    
-00008790: 2020 2320 4175 746f 6e61 6d69 6e67 206f    # Autonaming o
-000087a0: 6620 7375 626d 6f64 756c 6573 2e0a 2020  f submodules..  
-000087b0: 2020 2020 6966 2073 656c 662e 6e61 6d65      if self.name
-000087c0: 2069 7320 4e6f 6e65 3a20 2023 2070 7974   is None:  # pyt
-000087d0: 7970 653a 2064 6973 6162 6c65 3d61 7474  ype: disable=att
-000087e0: 7269 6275 7465 2d65 7272 6f72 0a20 2020  ribute-error.   
-000087f0: 2020 2020 2070 7265 6669 7820 3d20 6627       prefix = f'
-00008800: 7b73 656c 662e 5f5f 636c 6173 735f 5f2e  {self.__class__.
-00008810: 5f5f 6e61 6d65 5f5f 7d27 0a20 2020 2020  __name__}'.     
-00008820: 2020 2063 7572 736f 7220 3d20 7365 6c66     cursor = self
-00008830: 2e70 6172 656e 742e 5f73 7461 7465 2e61  .parent._state.a
-00008840: 7574 6f6e 616d 655f 6375 7273 6f72 2e67  utoname_cursor.g
-00008850: 6574 2870 7265 6669 782c 2030 290a 2020  et(prefix, 0).  
-00008860: 2020 2020 2020 7365 6c66 2e6e 616d 6520        self.name 
-00008870: 3d20 6627 7b70 7265 6669 787d 5f7b 6375  = f'{prefix}_{cu
-00008880: 7273 6f72 7d27 0a20 2020 2020 2020 2073  rsor}'.        s
-00008890: 656c 662e 7061 7265 6e74 2e5f 7374 6174  elf.parent._stat
-000088a0: 652e 6175 746f 6e61 6d65 5f63 7572 736f  e.autoname_curso
-000088b0: 725b 7072 6566 6978 5d20 3d20 6375 7273  r[prefix] = curs
-000088c0: 6f72 202b 2031 0a20 2020 2020 2023 2041  or + 1.      # A
-000088d0: 6c6c 6f77 2073 636f 7065 2061 6c69 6173  llow scope alias
-000088e0: 696e 6720 756e 6465 7220 7472 616e 7366  ing under transf
-000088f0: 6f72 6d73 2066 6f72 2073 7562 6d6f 6475  orms for submodu
-00008900: 6c65 7320 6465 6669 6e65 6420 696e 2073  les defined in s
-00008910: 6574 7570 2e0a 2020 2020 2020 7265 7573  etup..      reus
-00008920: 655f 7363 6f70 6573 203d 2028 7365 6c66  e_scopes = (self
-00008930: 2e70 6172 656e 742e 5f73 7461 7465 2e69  .parent._state.i
-00008940: 6e5f 7365 7475 7020 616e 640a 2020 2020  n_setup and.    
-00008950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008960: 2020 7365 6c66 2e70 6172 656e 742e 5f73    self.parent._s
-00008970: 7461 7465 2e73 6574 7570 5f63 616c 6c65  tate.setup_calle
-00008980: 6420 3d3d 2053 6574 7570 5374 6174 652e  d == SetupState.
-00008990: 5452 414e 5346 4f52 4d45 4429 0a20 2020  TRANSFORMED).   
-000089a0: 2020 2023 2050 6572 666f 726d 206e 616d     # Perform nam
-000089b0: 652d 636f 6c6c 6973 696f 6e20 6368 6563  e-collision chec
-000089c0: 6b2e 0a20 2020 2020 2069 6620 7365 6c66  k..      if self
-000089d0: 2e70 6172 656e 742e 5f6e 616d 655f 7461  .parent._name_ta
-000089e0: 6b65 6e28 7365 6c66 2e6e 616d 652c 2073  ken(self.name, s
-000089f0: 656c 662c 2072 6575 7365 5f73 636f 7065  elf, reuse_scope
-00008a00: 733d 7265 7573 655f 7363 6f70 6573 293a  s=reuse_scopes):
-00008a10: 0a20 2020 2020 2020 2070 6172 656e 745f  .        parent_
-00008a20: 636c 6173 7320 3d20 7365 6c66 2e70 6172  class = self.par
-00008a30: 656e 742e 5f5f 636c 6173 735f 5f2e 5f5f  ent.__class__.__
-00008a40: 6e61 6d65 5f5f 0a20 2020 2020 2020 2072  name__.        r
-00008a50: 6169 7365 2065 7272 6f72 732e 4e61 6d65  aise errors.Name
-00008a60: 496e 5573 6545 7272 6f72 2827 7375 626d  InUseError('subm
-00008a70: 6f64 756c 6527 2c20 7365 6c66 2e6e 616d  odule', self.nam
-00008a80: 652c 2070 6172 656e 745f 636c 6173 7329  e, parent_class)
-00008a90: 0a20 2020 2020 2023 2046 696e 616c 697a  .      # Finaliz
-00008aa0: 6520 6174 7461 6368 6d65 6e74 2074 6f20  e attachment to 
-00008ab0: 7061 7265 6e74 2061 6e64 2073 636f 7065  parent and scope
-00008ac0: 2069 6e69 7469 616c 697a 6174 696f 6e2e   initialization.
-00008ad0: 0a20 2020 2020 2073 656c 662e 7061 7265  .      self.pare
-00008ae0: 6e74 2e5f 7374 6174 652e 6368 696c 6472  nt._state.childr
-00008af0: 656e 5b73 656c 662e 6e61 6d65 5d20 3d20  en[self.name] = 
-00008b00: 7365 6c66 0a20 2020 2020 2061 7373 6572  self.      asser
-00008b10: 7420 7365 6c66 2e70 6172 656e 742e 7363  t self.parent.sc
-00008b20: 6f70 6520 6973 206e 6f74 204e 6f6e 650a  ope is not None.
-00008b30: 2020 2020 2020 6f62 6a65 6374 2e5f 5f73        object.__s
-00008b40: 6574 6174 7472 5f5f 280a 2020 2020 2020  etattr__(.      
-00008b50: 2020 2020 7365 6c66 2c20 2773 636f 7065      self, 'scope
-00008b60: 272c 2073 656c 662e 7061 7265 6e74 2e73  ', self.parent.s
-00008b70: 636f 7065 2e70 7573 6828 7365 6c66 2e6e  cope.push(self.n
-00008b80: 616d 652c 2072 6575 7365 3d72 6575 7365  ame, reuse=reuse
-00008b90: 5f73 636f 7065 7329 290a 0a20 2020 2023  _scopes))..    #
-00008ba0: 2054 6f70 2d6c 6576 656c 2069 6e76 6f63   Top-level invoc
-00008bb0: 6174 696f 6e20 7769 7468 2061 2066 756e  ation with a fun
-00008bc0: 6374 696f 6e61 6c20 5363 6f70 652e 0a20  ctional Scope.. 
-00008bd0: 2020 2065 6c69 6620 6973 696e 7374 616e     elif isinstan
-00008be0: 6365 2873 656c 662e 7061 7265 6e74 2c20  ce(self.parent, 
-00008bf0: 5363 6f70 6529 3a0a 2020 2020 2020 6f62  Scope):.      ob
-00008c00: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
-00008c10: 2873 656c 662c 2027 7363 6f70 6527 2c20  (self, 'scope', 
-00008c20: 7365 6c66 2e70 6172 656e 7429 0a20 2020  self.parent).   
-00008c30: 2065 6c73 653a 0a20 2020 2020 2072 6169   else:.      rai
-00008c40: 7365 2056 616c 7565 4572 726f 7228 2770  se ValueError('p
-00008c50: 6172 656e 7420 6d75 7374 2062 6520 4e6f  arent must be No
-00008c60: 6e65 2c20 4d6f 6475 6c65 206f 7220 5363  ne, Module or Sc
-00008c70: 6f70 6527 290a 0a20 2020 2073 656c 662e  ope')..    self.
-00008c80: 5f73 7461 7465 2e69 735f 696e 6974 6961  _state.is_initia
-00008c90: 6c69 7a65 6420 3d20 5472 7565 0a0a 2020  lized = True..  
-00008ca0: 6465 6620 5f5f 7265 7072 5f5f 2873 656c  def __repr__(sel
-00008cb0: 6629 202d 3e20 7374 723a 0a20 2020 2072  f) -> str:.    r
-00008cc0: 6574 7572 6e20 5f6d 6f64 756c 655f 7265  eturn _module_re
-00008cd0: 7072 2873 656c 6629 0a0a 2020 6465 6620  pr(self)..  def 
-00008ce0: 7365 7475 7028 7365 6c66 2920 2d3e 204e  setup(self) -> N
-00008cf0: 6f6e 653a 0a20 2020 2022 2222 496e 6974  one:.    """Init
-00008d00: 6961 6c69 7a65 7320 6120 4d6f 6475 6c65  ializes a Module
-00008d10: 206c 617a 696c 7920 2873 696d 696c 6172   lazily (similar
-00008d20: 2074 6f20 6120 6c61 7a79 2060 605f 5f69   to a lazy ``__i
-00008d30: 6e69 745f 5f60 6029 2e0a 0a20 2020 2060  nit__``)...    `
-00008d40: 6073 6574 7570 6060 2069 7320 6361 6c6c  `setup`` is call
-00008d50: 6564 206f 6e63 6520 6c61 7a69 6c79 206f  ed once lazily o
-00008d60: 6e20 6120 6d6f 6475 6c65 2069 6e73 7461  n a module insta
-00008d70: 6e63 6520 7768 656e 2061 206d 6f64 756c  nce when a modul
-00008d80: 650a 2020 2020 6973 2062 6f75 6e64 2c20  e.    is bound, 
-00008d90: 696d 6d65 6469 6174 656c 7920 6265 666f  immediately befo
-00008da0: 7265 2061 6e79 206f 7468 6572 206d 6574  re any other met
-00008db0: 686f 6473 206c 696b 6520 6060 5f5f 6361  hods like ``__ca
-00008dc0: 6c6c 5f5f 6060 2061 7265 0a20 2020 2069  ll__`` are.    i
-00008dd0: 6e76 6f6b 6564 2c20 6f72 2062 6566 6f72  nvoked, or befor
-00008de0: 6520 6120 6060 7365 7475 7060 602d 6465  e a ``setup``-de
-00008df0: 6669 6e65 6420 6174 7472 6962 7574 6520  fined attribute 
-00008e00: 6f6e 2060 7365 6c66 6020 6973 2061 6363  on `self` is acc
-00008e10: 6573 7365 642e 0a0a 2020 2020 5468 6973  essed...    This
-00008e20: 2063 616e 2068 6170 7065 6e20 696e 2074   can happen in t
-00008e30: 6872 6565 2063 6173 6573 3a0a 0a20 2020  hree cases:..   
-00008e40: 2020 2031 2e20 496d 6d65 6469 6174 656c     1. Immediatel
-00008e50: 7920 7768 656e 2069 6e76 6f6b 696e 6720  y when invoking 
-00008e60: 3a6d 6574 683a 6061 7070 6c79 602c 203a  :meth:`apply`, :
-00008e70: 6d65 7468 3a60 696e 6974 6020 6f72 0a20  meth:`init` or. 
-00008e80: 2020 2020 2020 2020 3a6d 6574 683a 6069          :meth:`i
-00008e90: 6e69 745f 616e 645f 6f75 7470 7574 602e  nit_and_output`.
-00008ea0: 0a0a 2020 2020 2020 322e 204f 6e63 6520  ..      2. Once 
-00008eb0: 7468 6520 6d6f 6475 6c65 2069 7320 6769  the module is gi
-00008ec0: 7665 6e20 6120 6e61 6d65 2062 7920 6265  ven a name by be
-00008ed0: 696e 6720 6173 7369 676e 6564 2074 6f20  ing assigned to 
-00008ee0: 616e 2061 7474 7269 6275 7465 206f 660a  an attribute of.
-00008ef0: 2020 2020 2020 2020 2061 6e6f 7468 6572           another
-00008f00: 206d 6f64 756c 6520 696e 7369 6465 2074   module inside t
-00008f10: 6865 206f 7468 6572 206d 6f64 756c 6527  he other module'
-00008f20: 7320 6060 7365 7475 7060 6020 6d65 7468  s ``setup`` meth
-00008f30: 6f64 0a20 2020 2020 2020 2020 2873 6565  od.         (see
-00008f40: 203a 6d65 7468 3a60 5f5f 7365 7461 7474   :meth:`__setatt
-00008f50: 725f 5f60 293a 3a0a 0a20 2020 2020 2020  r__`)::..       
-00008f60: 2020 2020 636c 6173 7320 4d79 4d6f 6475      class MyModu
-00008f70: 6c65 286e 6e2e 4d6f 6475 6c65 293a 0a20  le(nn.Module):. 
-00008f80: 2020 2020 2020 2020 2020 2020 6465 6620              def 
-00008f90: 7365 7475 7028 7365 6c66 293a 0a20 2020  setup(self):.   
-00008fa0: 2020 2020 2020 2020 2020 2020 7375 626d              subm
-00008fb0: 6f64 756c 6520 3d20 436f 6e76 282e 2e2e  odule = Conv(...
-00008fc0: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
-00008fd0: 2020 2320 4163 6365 7373 696e 6720 6073    # Accessing `s
-00008fe0: 7562 6d6f 6475 6c65 6020 6174 7472 6962  ubmodule` attrib
-00008ff0: 7574 6573 2064 6f65 7320 6e6f 7420 7965  utes does not ye
-00009000: 7420 776f 726b 2068 6572 652e 0a0a 2020  t work here...  
-00009010: 2020 2020 2020 2020 2020 2020 2023 2054               # T
-00009020: 6865 2066 6f6c 6c6f 7769 6e67 206c 696e  he following lin
-00009030: 6520 696e 766f 6b65 7320 6073 656c 662e  e invokes `self.
-00009040: 5f5f 7365 7461 7474 725f 5f60 2c20 7768  __setattr__`, wh
-00009050: 6963 6820 6769 7665 730a 2020 2020 2020  ich gives.      
-00009060: 2020 2020 2020 2020 2023 2060 7375 626d           # `subm
-00009070: 6f64 756c 6560 2074 6865 206e 616d 6520  odule` the name 
-00009080: 2263 6f6e 7631 222e 0a20 2020 2020 2020  "conv1"..       
-00009090: 2020 2020 2020 2020 7365 6c66 2e63 6f6e          self.con
-000090a0: 7631 203d 2073 7562 6d6f 6475 6c65 0a0a  v1 = submodule..
-000090b0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-000090c0: 2041 6363 6573 7369 6e67 2060 7375 626d   Accessing `subm
-000090d0: 6f64 756c 6560 2061 7474 7269 6275 7465  odule` attribute
-000090e0: 7320 6f72 206d 6574 686f 6473 2069 7320  s or methods is 
-000090f0: 6e6f 7720 7361 6665 2061 6e64 0a20 2020  now safe and.   
-00009100: 2020 2020 2020 2020 2020 2020 2320 6569              # ei
-00009110: 7468 6572 2063 6175 7365 7320 7365 7475  ther causes setu
-00009120: 7028 2920 746f 2062 6520 6361 6c6c 6564  p() to be called
-00009130: 206f 6e63 652e 0a0a 2020 2020 2020 332e   once...      3.
-00009140: 204f 6e63 6520 6120 6d6f 6475 6c65 2069   Once a module i
-00009150: 7320 636f 6e73 7472 7563 7465 6420 696e  s constructed in
-00009160: 7369 6465 2061 206d 6574 686f 6420 7772  side a method wr
-00009170: 6170 7065 6420 7769 7468 0a20 2020 2020  apped with.     
-00009180: 2020 2020 3a6d 6574 683a 6063 6f6d 7061      :meth:`compa
-00009190: 6374 602c 2069 6d6d 6564 6961 7465 6c79  ct`, immediately
-000091a0: 2062 6566 6f72 6520 616e 6f74 6865 7220   before another 
-000091b0: 6d65 7468 6f64 2069 7320 6361 6c6c 6564  method is called
-000091c0: 206f 720a 2020 2020 2020 2020 2060 6073   or.         ``s
-000091d0: 6574 7570 6060 2064 6566 696e 6564 2061  etup`` defined a
-000091e0: 7474 7269 6275 7465 2069 7320 6163 6365  ttribute is acce
-000091f0: 7373 6564 2e0a 2020 2020 2222 220a 2020  ssed..    """.  
-00009200: 2020 7061 7373 0a0a 2020 6465 6620 5f72    pass..  def _r
-00009210: 6567 6973 7465 725f 7375 626d 6f64 756c  egister_submodul
-00009220: 6573 2873 656c 662c 206e 616d 652c 2076  es(self, name, v
-00009230: 616c 293a 0a20 2020 2022 2222 5265 6769  al):.    """Regi
-00009240: 7374 6572 7320 6120 7375 626d 6f64 756c  sters a submodul
-00009250: 652e 2222 220a 2020 2020 6173 7365 7274  e.""".    assert
-00009260: 2073 656c 662e 7363 6f70 652c 2027 5472   self.scope, 'Tr
-00009270: 7969 6e67 2074 6f20 7265 6769 7374 6572  ying to register
-00009280: 2073 7562 6d6f 6475 6c65 7320 6f6e 2075   submodules on u
-00009290: 6e62 6f75 6e64 2073 636f 7065 2e27 0a20  nbound scope.'. 
-000092a0: 2020 2072 6f6f 7420 3d20 7365 6c66 2e73     root = self.s
-000092b0: 636f 7065 2e72 6f6f 740a 2020 2020 6361  cope.root.    ca
-000092c0: 6368 6520 3d20 5f63 6163 6865 732e 6765  che = _caches.ge
-000092d0: 7428 726f 6f74 2c20 7765 616b 7265 662e  t(root, weakref.
-000092e0: 5765 616b 5661 6c75 6544 6963 7469 6f6e  WeakValueDiction
-000092f0: 6172 7928 2929 0a20 2020 205f 6361 6368  ary()).    _cach
-00009300: 6573 5b72 6f6f 745d 203d 2063 6163 6865  es[root] = cache
-00009310: 0a20 2020 2071 7565 7565 203d 205b 5d0a  .    queue = [].
-00009320: 2020 2020 7072 6573 6572 7665 5f61 646f      preserve_ado
-00009330: 7074 6564 5f6e 616d 6573 203d 2063 6f6e  pted_names = con
-00009340: 6669 672e 666c 6178 5f70 7265 7365 7276  fig.flax_preserv
-00009350: 655f 6164 6f70 7465 645f 6e61 6d65 730a  e_adopted_names.
-00009360: 2020 2020 6966 2068 6173 6174 7472 2873      if hasattr(s
-00009370: 656c 662c 2027 7072 6573 6572 7665 5f61  elf, 'preserve_a
-00009380: 646f 7074 6564 5f6e 616d 6573 2729 3a0a  dopted_names'):.
-00009390: 2020 2020 2020 7072 6573 6572 7665 5f61        preserve_a
-000093a0: 646f 7074 6564 5f6e 616d 6573 203d 2073  dopted_names = s
-000093b0: 656c 662e 7072 6573 6572 7665 5f61 646f  elf.preserve_ado
-000093c0: 7074 6564 5f6e 616d 6573 0a20 2020 2064  pted_names.    d
-000093d0: 6566 2061 646f 7074 5f61 7474 725f 6d6f  ef adopt_attr_mo
-000093e0: 6475 6c65 7328 6361 6368 652c 2071 7565  dules(cache, que
-000093f0: 7565 2c20 7375 6666 6978 2c20 7375 6276  ue, suffix, subv
-00009400: 616c 7565 293a 0a20 2020 2020 2069 6620  alue):.      if 
-00009410: 6973 696e 7374 616e 6365 2873 7562 7661  isinstance(subva
-00009420: 6c75 652c 204d 6f64 756c 6529 3a0a 2020  lue, Module):.  
-00009430: 2020 2020 2020 6164 6f70 7465 645f 6e61        adopted_na
-00009440: 6d65 203d 204e 6f6e 650a 2020 2020 2020  me = None.      
-00009450: 2020 6966 2073 7562 7661 6c75 652e 7061    if subvalue.pa
-00009460: 7265 6e74 2069 7320 4e6f 6e65 3a0a 2020  rent is None:.  
-00009470: 2020 2020 2020 2020 2320 5072 6573 6572          # Preser
-00009480: 7665 2073 6861 7269 6e67 2d62 792d 7265  ve sharing-by-re
-00009490: 6665 7265 6e63 6520 7265 6c61 7469 6f6e  ference relation
-000094a0: 7368 6970 7320 6475 7269 6e67 2061 646f  ships during ado
-000094b0: 7074 696f 6e0a 2020 2020 2020 2020 2020  ption.          
-000094c0: 2320 7669 6120 6361 6368 6520 6b65 7965  # via cache keye
-000094d0: 6420 6f6e 2075 6e69 7175 6520 696e 7374  d on unique inst
-000094e0: 616e 6365 2069 6473 2e0a 2020 2020 2020  ance ids..      
-000094f0: 2020 2020 6b65 7920 3d20 7375 6276 616c      key = subval
-00009500: 7565 2e5f 6964 0a20 2020 2020 2020 2020  ue._id.         
-00009510: 2023 204d 6f64 756c 6520 7761 7320 7061   # Module was pa
-00009520: 7373 6564 2066 726f 6d20 6f75 7473 6964  ssed from outsid
-00009530: 652e 2049 7420 6e65 6564 7320 746f 2062  e. It needs to b
-00009540: 6520 636c 6f6e 6564 2e0a 2020 2020 2020  e cloned..      
-00009550: 2020 2020 2320 4f75 7473 6964 6520 6d6f      # Outside mo
-00009560: 6475 6c65 7320 6172 6520 6e61 6d65 6420  dules are named 
-00009570: 6279 2061 7474 6163 686d 656e 742c 206e  by attachment, n
-00009580: 6f74 2061 6e20 6f75 7465 7220 6e61 6d65  ot an outer name
-00009590: 2c0a 2020 2020 2020 2020 2020 2320 554e  ,.          # UN
-000095a0: 4c45 5353 2077 6527 7265 2075 7369 6e67  LESS we're using
-000095b0: 206e 6577 2061 646f 7074 6564 206e 616d   new adopted nam
-000095c0: 6520 706f 6c69 6379 2c20 696e 2077 6869  e policy, in whi
-000095d0: 6368 2063 6173 6520 616e 2065 7869 7374  ch case an exist
-000095e0: 696e 670a 2020 2020 2020 2020 2020 2320  ing.          # 
-000095f0: 6e61 6d65 2077 696c 6c20 6265 2075 7365  name will be use
-00009600: 642c 2061 7320 6973 206f 6674 656e 2073  d, as is often s
-00009610: 7570 706c 6965 6420 6279 2063 6f6e 6669  upplied by confi
-00009620: 6720 7379 7374 656d 732e 0a20 2020 2020  g systems..     
-00009630: 2020 2020 2069 6620 7072 6573 6572 7665       if preserve
-00009640: 5f61 646f 7074 6564 5f6e 616d 6573 3a0a  _adopted_names:.
-00009650: 2020 2020 2020 2020 2020 2020 6164 6f70              adop
-00009660: 7465 645f 6e61 6d65 203d 206f 626a 6563  ted_name = objec
-00009670: 742e 5f5f 6765 7461 7474 7269 6275 7465  t.__getattribute
-00009680: 5f5f 2873 7562 7661 6c75 652c 2027 6e61  __(subvalue, 'na
-00009690: 6d65 2729 0a20 2020 2020 2020 2020 2069  me').          i
-000096a0: 6620 6b65 7920 696e 2063 6163 6865 3a0a  f key in cache:.
-000096b0: 2020 2020 2020 2020 2020 2020 7375 6276              subv
-000096c0: 616c 7565 203d 2063 6163 6865 5b6b 6579  alue = cache[key
-000096d0: 5d0a 2020 2020 2020 2020 2020 656c 7365  ].          else
-000096e0: 3a0a 2020 2020 2020 2020 2020 2020 7375  :.            su
-000096f0: 6276 616c 7565 203d 2073 7562 7661 6c75  bvalue = subvalu
-00009700: 652e 636c 6f6e 6528 6e61 6d65 3d4e 6f6e  e.clone(name=Non
-00009710: 6529 0a20 2020 2020 2020 2020 2020 2063  e).            c
-00009720: 6163 6865 5b6b 6579 5d20 3d20 7375 6276  ache[key] = subv
-00009730: 616c 7565 0a20 2020 2020 2020 2069 6620  alue.        if 
-00009740: 7375 6276 616c 7565 2e6e 616d 6520 6973  subvalue.name is
-00009750: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
-00009760: 206f 626a 6563 742e 5f5f 7365 7461 7474   object.__setatt
-00009770: 725f 5f28 7375 6276 616c 7565 2c20 2770  r__(subvalue, 'p
-00009780: 6172 656e 7427 2c20 7365 6c66 290a 2020  arent', self).  
-00009790: 2020 2020 2020 2020 6966 2061 646f 7074          if adopt
-000097a0: 6564 5f6e 616d 6520 6973 204e 6f6e 653a  ed_name is None:
-000097b0: 0a20 2020 2020 2020 2020 2020 2061 646f  .            ado
-000097c0: 7074 6564 5f6e 616d 6520 3d20 6627 7b6e  pted_name = f'{n
-000097d0: 616d 657d 7b73 7566 6669 787d 270a 2020  ame}{suffix}'.  
-000097e0: 2020 2020 2020 2020 6f62 6a65 6374 2e5f          object._
-000097f0: 5f73 6574 6174 7472 5f5f 2873 7562 7661  _setattr__(subva
-00009800: 6c75 652c 2027 6e61 6d65 272c 2061 646f  lue, 'name', ado
-00009810: 7074 6564 5f6e 616d 6529 0a20 2020 2020  pted_name).     
-00009820: 2020 2020 2071 7565 7565 2e61 7070 656e       queue.appen
-00009830: 6428 7375 6276 616c 7565 290a 2020 2020  d(subvalue).    
-00009840: 2020 7265 7475 726e 2073 7562 7661 6c75    return subvalu
-00009850: 650a 2020 2020 7661 6c20 3d20 5f66 7265  e.    val = _fre
-00009860: 657a 655f 6174 7472 285f 6d61 705f 6f76  eze_attr(_map_ov
-00009870: 6572 5f6d 6f64 756c 6573 5f69 6e5f 7472  er_modules_in_tr
-00009880: 6565 280a 2020 2020 2020 2020 6675 6e63  ee(.        func
-00009890: 746f 6f6c 732e 7061 7274 6961 6c28 6164  tools.partial(ad
-000098a0: 6f70 745f 6174 7472 5f6d 6f64 756c 6573  opt_attr_modules
-000098b0: 2c20 6361 6368 652c 2071 7565 7565 292c  , cache, queue),
-000098c0: 2076 616c 2929 0a20 2020 206f 626a 6563   val)).    objec
-000098d0: 742e 5f5f 7365 7461 7474 725f 5f28 7365  t.__setattr__(se
-000098e0: 6c66 2c20 6e61 6d65 2c20 7661 6c29 0a20  lf, name, val). 
-000098f0: 2020 2066 6f72 2078 2069 6e20 7175 6575     for x in queu
-00009900: 653a 0a20 2020 2020 2078 2e5f 5f70 6f73  e:.      x.__pos
-00009910: 745f 696e 6974 5f5f 2829 0a0a 2020 6465  t_init__()..  de
-00009920: 6620 5f74 7279 5f73 6574 7570 2873 656c  f _try_setup(sel
-00009930: 662c 2073 6861 6c6c 6f77 3a20 626f 6f6c  f, shallow: bool
-00009940: 203d 2046 616c 7365 2920 2d3e 204e 6f6e   = False) -> Non
-00009950: 653a 0a20 2020 2022 2222 5472 6965 7320  e:.    """Tries 
-00009960: 746f 2073 6574 7570 206d 6f64 756c 6520  to setup module 
-00009970: 6966 2073 636f 7065 2069 7320 6176 6169  if scope is avai
-00009980: 6c61 626c 6520 616e 6420 7365 7475 7020  lable and setup 
-00009990: 6861 7320 6e6f 7420 6265 656e 2063 616c  has not been cal
-000099a0: 6c65 6420 7965 742e 2222 220a 2020 2020  led yet.""".    
-000099b0: 6966 2028 7365 6c66 2e73 636f 7065 0a20  if (self.scope. 
-000099c0: 2020 2020 2020 2061 6e64 206e 6f74 2073         and not s
-000099d0: 656c 662e 5f73 7461 7465 2e69 6e5f 7365  elf._state.in_se
-000099e0: 7475 700a 2020 2020 2020 2020 616e 6420  tup.        and 
-000099f0: 7365 6c66 2e5f 7374 6174 652e 7365 7475  self._state.setu
-00009a00: 705f 6361 6c6c 6564 2021 3d20 5365 7475  p_called != Setu
-00009a10: 7053 7461 7465 2e44 4f4e 4529 3a0a 2020  pState.DONE):.  
-00009a20: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       
-00009a30: 2073 656c 662e 5f73 7461 7465 2e69 6e5f   self._state.in_
-00009a40: 7365 7475 7020 3d20 5472 7565 0a20 2020  setup = True.   
-00009a50: 2020 2020 2023 2041 2073 6861 6c6c 6f77       # A shallow
-00009a60: 2073 6574 7570 2077 696c 6c20 6f6e 6c79   setup will only
-00009a70: 2072 6567 6973 7465 7220 6174 7472 6962   register attrib
-00009a80: 7574 6520 7375 626d 6f64 756c 6573 2062  ute submodules b
-00009a90: 7574 2069 7420 646f 6573 0a20 2020 2020  ut it does.     
-00009aa0: 2020 2023 206e 6f74 2063 616c 6c20 7468     # not call th
-00009ab0: 6520 7573 6572 2773 2073 6574 7570 2e20  e user's setup. 
-00009ac0: 5468 6973 2061 766f 6964 7320 7275 6e6e  This avoids runn
-00009ad0: 696e 6720 6265 666f 7265 2061 0a20 2020  ing before a.   
-00009ae0: 2020 2020 2023 2074 7261 6e73 666f 726d       # transform
-00009af0: 6174 696f 6e2e 0a20 2020 2020 2020 2066  ation..        f
-00009b00: 6f72 2066 6965 6c64 2069 6e20 6461 7461  or field in data
-00009b10: 636c 6173 7365 732e 6669 656c 6473 2873  classes.fields(s
-00009b20: 656c 6629 3a0a 2020 2020 2020 2020 2020  elf):.          
-00009b30: 6966 2066 6965 6c64 2e6e 616d 6520 213d  if field.name !=
-00009b40: 2027 7061 7265 6e74 2720 616e 6420 6669   'parent' and fi
-00009b50: 656c 642e 696e 6974 3a0a 2020 2020 2020  eld.init:.      
-00009b60: 2020 2020 2020 7365 6c66 2e5f 7265 6769        self._regi
-00009b70: 7374 6572 5f73 7562 6d6f 6475 6c65 7328  ster_submodules(
-00009b80: 6669 656c 642e 6e61 6d65 2c20 6765 7461  field.name, geta
-00009b90: 7474 7228 7365 6c66 2c20 6669 656c 642e  ttr(self, field.
-00009ba0: 6e61 6d65 2929 0a20 2020 2020 2020 2069  name)).        i
-00009bb0: 6620 6e6f 7420 7368 616c 6c6f 773a 0a20  f not shallow:. 
-00009bc0: 2020 2020 2020 2020 2073 656c 662e 7365           self.se
-00009bd0: 7475 7028 290a 2020 2020 2020 2020 2320  tup().        # 
-00009be0: 5765 2072 756e 2073 7461 7469 6320 6368  We run static ch
-00009bf0: 6563 6b73 2061 6273 7472 6163 746c 7920  ecks abstractly 
-00009c00: 6f6e 6365 2066 6f72 2073 6574 7570 2062  once for setup b
-00009c10: 6566 6f72 6520 616e 7920 7472 616e 7366  efore any transf
-00009c20: 6f72 6d73 0a20 2020 2020 2020 2023 2074  orms.        # t
-00009c30: 6f20 6465 7465 6374 206e 616d 6520 636f  o detect name co
-00009c40: 6c6c 6973 696f 6e73 2061 6e64 206f 7468  llisions and oth
-00009c50: 6572 2070 7974 686f 6e20 6572 726f 7273  er python errors
-00009c60: 2e0a 2020 2020 2020 2020 656c 6966 2073  ..        elif s
-00009c70: 656c 662e 5f73 7461 7465 2e73 6574 7570  elf._state.setup
-00009c80: 5f63 616c 6c65 6420 3d3d 2053 6574 7570  _called == Setup
-00009c90: 5374 6174 652e 4e45 573a 0a20 2020 2020  State.NEW:.     
-00009ca0: 2020 2020 2073 656c 662e 5f76 616c 6964       self._valid
-00009cb0: 6174 655f 7365 7475 7028 290a 2020 2020  ate_setup().    
-00009cc0: 2020 6669 6e61 6c6c 793a 0a20 2020 2020    finally:.     
-00009cd0: 2020 2073 656c 662e 5f73 7461 7465 2e69     self._state.i
-00009ce0: 6e5f 7365 7475 7020 3d20 4661 6c73 650a  n_setup = False.
-00009cf0: 2020 2020 2020 2020 7365 6c66 2e5f 7374          self._st
-00009d00: 6174 652e 7365 7475 705f 6361 6c6c 6564  ate.setup_called
-00009d10: 203d 2053 6574 7570 5374 6174 652e 444f   = SetupState.DO
-00009d20: 4e45 0a0a 2020 6465 6620 5f76 616c 6964  NE..  def _valid
-00009d30: 6174 655f 7365 7475 7028 7365 6c66 2920  ate_setup(self) 
-00009d40: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    """
-00009d50: 4162 7374 7261 6374 6c79 2065 7661 6c75  Abstractly evalu
-00009d60: 6174 6573 2073 6574 7570 206f 6e6c 7920  ates setup only 
-00009d70: 746f 2072 756e 2073 7461 7469 6320 6368  to run static ch
-00009d80: 6563 6b73 2e22 2222 0a20 2020 2064 6566  ecks.""".    def
-00009d90: 2072 756e 5f73 6574 7570 5f6f 6e6c 7928   run_setup_only(
-00009da0: 7829 3a0a 2020 2020 2020 7772 6170 7065  x):.      wrappe
-00009db0: 645f 6964 203d 2077 7261 705f 6d65 7468  d_id = wrap_meth
-00009dc0: 6f64 5f6f 6e63 6528 6c61 6d62 6461 206d  od_once(lambda m
-00009dd0: 2c20 783a 2078 290a 2020 2020 2020 7769  , x: x).      wi
-00009de0: 7468 2054 6573 7453 636f 7065 287b 7d2c  th TestScope({},
-00009df0: 2072 6e67 733d 7b7d 2c20 6d75 7461 626c   rngs={}, mutabl
-00009e00: 653d 5472 7565 292e 7465 6d70 6f72 6172  e=True).temporar
-00009e10: 7928 2920 6173 2072 6f6f 743a 0a20 2020  y() as root:.   
-00009e20: 2020 2020 2072 6574 7572 6e20 7772 6170       return wrap
-00009e30: 7065 645f 6964 2873 656c 662e 636c 6f6e  ped_id(self.clon
-00009e40: 6528 7061 7265 6e74 3d72 6f6f 7429 2c20  e(parent=root), 
-00009e50: 7829 0a20 2020 205f 203d 206a 6178 2e65  x).    _ = jax.e
-00009e60: 7661 6c5f 7368 6170 6528 7275 6e5f 7365  val_shape(run_se
-00009e70: 7475 705f 6f6e 6c79 2c20 3029 0a0a 2020  tup_only, 0)..  
-00009e80: 6465 6620 5f6e 616d 655f 7461 6b65 6e28  def _name_taken(
-00009e90: 7365 6c66 2c0a 2020 2020 2020 2020 2020  self,.          
-00009ea0: 2020 2020 2020 2020 6e61 6d65 3a20 7374          name: st
-00009eb0: 722c 0a20 2020 2020 2020 2020 2020 2020  r,.             
-00009ec0: 2020 2020 206d 6f64 756c 653a 204f 7074       module: Opt
-00009ed0: 696f 6e61 6c5b 274d 6f64 756c 6527 5d20  ional['Module'] 
-00009ee0: 3d20 4e6f 6e65 2c0a 2020 2020 2020 2020  = None,.        
-00009ef0: 2020 2020 2020 2020 2020 7265 7573 655f            reuse_
-00009f00: 7363 6f70 6573 3a20 626f 6f6c 203d 2046  scopes: bool = F
-00009f10: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
-00009f20: 2020 2020 2020 2020 636f 6c6c 6563 7469          collecti
-00009f30: 6f6e 3a20 4f70 7469 6f6e 616c 5b73 7472  on: Optional[str
-00009f40: 5d20 3d20 4e6f 6e65 2920 2d3e 2062 6f6f  ] = None) -> boo
-00009f50: 6c3a 0a20 2020 2061 7373 6572 7420 7365  l:.    assert se
-00009f60: 6c66 2e73 636f 7065 2069 7320 6e6f 7420  lf.scope is not 
-00009f70: 4e6f 6e65 0a20 2020 2023 2077 6974 6820  None.    # with 
-00009f80: 7265 6c61 7865 6420 6e61 6d69 6e67 2064  relaxed naming d
-00009f90: 6f6e 2774 2066 6f72 6365 206e 6f6e 2d6f  on't force non-o
-00009fa0: 7665 726c 6170 2077 6974 6820 7079 7468  verlap with pyth
-00009fb0: 6f6e 2061 7474 7269 6275 7465 206e 616d  on attribute nam
-00009fc0: 6573 2e0a 2020 2020 6966 2063 6f6e 6669  es..    if confi
-00009fd0: 672e 666c 6178 5f72 656c 6178 6564 5f6e  g.flax_relaxed_n
-00009fe0: 616d 696e 673a 0a20 2020 2020 2069 6620  aming:.      if 
-00009ff0: 7265 7573 655f 7363 6f70 6573 3a0a 2020  reuse_scopes:.  
-0000a000: 2020 2020 2020 7265 7475 726e 2046 616c        return Fal
-0000a010: 7365 0a20 2020 2020 2072 6574 7572 6e20  se.      return 
-0000a020: 7365 6c66 2e73 636f 7065 2e6e 616d 655f  self.scope.name_
-0000a030: 7265 7365 7276 6564 286e 616d 652c 2063  reserved(name, c
-0000a040: 6f6c 6c65 6374 696f 6e29 0a20 2020 2069  ollection).    i
-0000a050: 6620 6e61 6d65 2069 6e20 5f61 6c6c 5f6e  f name in _all_n
-0000a060: 616d 6573 5f6f 6e5f 6f62 6a65 6374 2873  ames_on_object(s
-0000a070: 656c 6629 3a0a 2020 2020 2020 7661 6c20  elf):.      val 
-0000a080: 3d20 6765 7461 7474 7228 7365 6c66 2c20  = getattr(self, 
-0000a090: 6e61 6d65 2c20 4e6f 6e65 290a 2020 2020  name, None).    
-0000a0a0: 2020 6966 206d 6f64 756c 6520 6973 206e    if module is n
-0000a0b0: 6f74 204e 6f6e 6520 616e 6420 7661 6c20  ot None and val 
-0000a0c0: 6973 206d 6f64 756c 653a 0a20 2020 2020  is module:.     
-0000a0d0: 2020 2023 206e 616d 6520 6973 2074 616b     # name is tak
-0000a0e0: 656e 2062 7920 7468 6520 7661 6c75 6520  en by the value 
-0000a0f0: 6974 7365 6c66 2062 6563 6175 7365 0a20  itself because. 
-0000a100: 2020 2020 2020 2023 2066 6965 6c64 2061         # field a
-0000a110: 7373 6967 6e6d 656e 7420 6861 7070 656e  ssignment happen
-0000a120: 6564 2062 6566 6f72 6520 6e61 6d69 6e67  ed before naming
-0000a130: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-0000a140: 4661 6c73 650a 2020 2020 2020 7265 7475  False.      retu
-0000a150: 726e 2054 7275 650a 2020 2020 2320 4368  rn True.    # Ch
-0000a160: 6563 6b20 666f 7220 7468 6520 6578 6973  eck for the exis
-0000a170: 7465 6e63 6520 6f66 206e 616d 6520 696e  tence of name in
-0000a180: 2074 6865 2073 636f 7065 206f 626a 6563   the scope objec
-0000a190: 742e 0a20 2020 2069 6620 7265 7573 655f  t..    if reuse_
-0000a1a0: 7363 6f70 6573 3a0a 2020 2020 2020 7265  scopes:.      re
-0000a1b0: 7475 726e 2046 616c 7365 0a20 2020 2072  turn False.    r
-0000a1c0: 6574 7572 6e20 6e61 6d65 2069 6e20 7365  eturn name in se
-0000a1d0: 6c66 2e73 636f 7065 2e72 6573 6572 7661  lf.scope.reserva
-0000a1e0: 7469 6f6e 730a 0a20 2040 7072 6f70 6572  tions..  @proper
-0000a1f0: 7479 0a20 2064 6566 205f 696e 6974 6961  ty.  def _initia
-0000a200: 6c69 7a61 7469 6f6e 5f61 6c6c 6f77 6564  lization_allowed
-0000a210: 2873 656c 6629 3a0a 2020 2020 7265 7475  (self):.    retu
-0000a220: 726e 2073 656c 662e 5f73 7461 7465 2e69  rn self._state.i
-0000a230: 6e5f 7365 7475 7020 6f72 2073 656c 662e  n_setup or self.
-0000a240: 5f73 7461 7465 2e69 6e5f 636f 6d70 6163  _state.in_compac
-0000a250: 745f 6d65 7468 6f64 0a0a 2020 6465 6620  t_method..  def 
-0000a260: 636c 6f6e 6528 7365 6c66 3a20 4d2c 202a  clone(self: M, *
-0000a270: 2c0a 2020 2020 2020 2020 2020 2020 7061  ,.            pa
-0000a280: 7265 6e74 3a20 4f70 7469 6f6e 616c 5b55  rent: Optional[U
-0000a290: 6e69 6f6e 5b53 636f 7065 2c20 274d 6f64  nion[Scope, 'Mod
-0000a2a0: 756c 6527 5d5d 203d 204e 6f6e 652c 0a20  ule']] = None,. 
+00004fe0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00004ff0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00005000: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 0a23  -------------..#
+00005010: 2054 6865 204d 6f64 756c 6542 6173 6520   The ModuleBase 
+00005020: 636c 6173 7320 6973 2063 7265 6174 6564  class is created
+00005030: 206f 6e6c 7920 746f 206d 616b 6520 7374   only to make st
+00005040: 6174 6963 2061 6e61 6c79 7a65 7273 2068  atic analyzers h
+00005050: 6170 7079 0a23 206d 6169 6e6c 7920 7079  appy.# mainly py
+00005060: 7479 7065 2061 6e64 2070 7972 6967 6874  type and pyright
+00005070: 2e20 536f 6d65 206e 6f74 6573 3a0a 2320  . Some notes:.# 
+00005080: 2a20 7079 7269 6768 7420 2863 6f72 7265  * pyright (corre
+00005090: 6374 6c79 2920 636f 6d70 6c61 696e 7320  ctly) complains 
+000050a0: 7468 6174 204d 6f64 756c 6520 6974 7365  that Module itse
+000050b0: 6c66 2069 7320 6e6f 7420 6120 6461 7461  lf is not a data
+000050c0: 636c 6173 732c 2065 7665 6e0a 2320 2020  class, even.#   
+000050d0: 7468 6f75 6768 2061 6c6c 2069 7473 2073  though all its s
+000050e0: 7562 636c 6173 7365 7320 616e 6420 696e  ubclasses and in
+000050f0: 7461 6e63 6573 2041 5245 2064 6174 6163  tances ARE datac
+00005100: 6c61 7373 6573 2e20 4265 6361 7573 6520  lasses. Because 
+00005110: 7468 6572 6520 6973 206e 6f0a 2320 2020  there is no.#   
+00005120: 7761 7920 746f 2061 6e6e 6f74 6174 6520  way to annotate 
+00005130: 7468 6973 2069 6e20 6120 7761 7920 7468  this in a way th
+00005140: 6174 2070 7972 6967 6874 2075 6e64 6572  at pyright under
+00005150: 7374 616e 6473 2c20 7765 2063 7265 6174  stands, we creat
+00005160: 6520 610a 2320 2020 4d6f 6475 6c65 4261  e a.#   ModuleBa
+00005170: 7365 2063 6c61 7373 2064 6563 6f72 6174  se class decorat
+00005180: 6564 2077 6974 6820 6064 6174 6163 6c61  ed with `datacla
+00005190: 7373 5f74 7261 6e73 666f 726d 6020 7375  ss_transform` su
+000051a0: 6368 2074 6861 7420 7079 7269 6768 740a  ch that pyright.
+000051b0: 2320 2020 7468 696e 6b73 204d 6f64 756c  #   thinks Modul
+000051c0: 6520 6973 2061 2064 6174 6163 6c61 7373  e is a dataclass
+000051d0: 2028 696e 2072 6561 6c69 7479 206f 6e6c   (in reality onl
+000051e0: 7920 7375 6263 6c61 7373 6573 2061 7265  y subclasses are
+000051f0: 2069 6e73 7461 6e74 6961 7465 640a 2320   instantiated.# 
+00005200: 2020 736f 2074 6869 7320 6973 2066 696e    so this is fin
+00005210: 6529 2e0a 2320 2a20 5468 6520 605f 5f64  e)..# * The `__d
+00005220: 6174 6163 6c61 7373 5f66 6965 6c64 735f  ataclass_fields_
+00005230: 5f60 2061 7474 7269 6275 7465 2069 7320  _` attribute is 
+00005240: 6e65 6564 6564 2062 6563 6175 7365 2070  needed because p
+00005250: 7974 7970 6520 7365 656d 7320 746f 0a23  ytype seems to.#
+00005260: 2020 206e 6f74 2075 6e64 6572 7374 616e     not understan
+00005270: 6420 7468 6520 6064 6174 6163 6c61 7373  d the `dataclass
+00005280: 5f74 7261 6e73 666f 726d 6020 6465 636f  _transform` deco
+00005290: 7261 746f 722c 2074 6865 7265 666f 7265  rator, therefore
+000052a0: 2077 6520 6e65 6564 0a23 2020 2074 6f20   we need.#   to 
+000052b0: 6164 6420 7468 6520 6174 7472 6962 7574  add the attribut
+000052c0: 6520 6d61 6e75 616c 6c79 2e0a 2320 2a20  e manually..# * 
+000052d0: 4f74 6865 7220 6174 7472 6962 7574 6573  Other attributes
+000052e0: 2061 7265 2061 6e6e 6f74 6174 6564 2066   are annotated f
+000052f0: 6f72 2063 6f6d 706c 6574 656e 6573 732e  or completeness.
+00005300: 2042 6563 6175 7365 2077 6520 6172 6520   Because we are 
+00005310: 7573 696e 670a 2320 2020 7468 6520 6069  using.#   the `i
+00005320: 6620 7479 7069 6e67 2e54 5950 455f 4348  f typing.TYPE_CH
+00005330: 4543 4b49 4e47 6020 7061 7474 6572 6e2c  ECKING` pattern,
+00005340: 2074 6865 7365 2061 6e6e 6f74 6174 696f   these annotatio
+00005350: 6e73 2061 7265 206e 6f74 2070 7265 7365  ns are not prese
+00005360: 6e74 0a23 2020 2061 7420 7275 6e74 696d  nt.#   at runtim
+00005370: 6520 736f 2074 6865 7920 646f 6e27 7420  e so they don't 
+00005380: 6166 6665 6374 2074 6865 2064 6174 6163  affect the datac
+00005390: 6c61 7373 2062 6568 6176 696f 722e 0a40  lass behavior..@
+000053a0: 6461 7461 636c 6173 735f 7472 616e 7366  dataclass_transf
+000053b0: 6f72 6d28 290a 636c 6173 7320 4d6f 6475  orm().class Modu
+000053c0: 6c65 4261 7365 3a0a 2020 6966 2074 7970  leBase:.  if typ
+000053d0: 696e 672e 5459 5045 5f43 4845 434b 494e  ing.TYPE_CHECKIN
+000053e0: 473a 0a20 2020 2073 636f 7065 3a20 4f70  G:.    scope: Op
+000053f0: 7469 6f6e 616c 5b53 636f 7065 5d0a 2020  tional[Scope].  
+00005400: 2020 5f73 7461 7465 3a20 5f4d 6f64 756c    _state: _Modul
+00005410: 6549 6e74 6572 6e61 6c53 7461 7465 0a20  eInternalState. 
+00005420: 2020 205f 7061 7265 6e74 5f72 6566 3a20     _parent_ref: 
+00005430: 556e 696f 6e5b 274d 6f64 756c 6527 2c20  Union['Module', 
+00005440: 7765 616b 7265 662e 5265 6665 7265 6e63  weakref.Referenc
+00005450: 6554 7970 655b 274d 6f64 756c 6527 5d2c  eType['Module'],
+00005460: 204e 6f6e 655d 0a20 2020 2070 6172 656e   None].    paren
+00005470: 743a 2055 6e69 6f6e 5b27 4d6f 6475 6c65  t: Union['Module
+00005480: 272c 205f 5365 6e74 696e 656c 2c20 4e6f  ', _Sentinel, No
+00005490: 6e65 5d0a 2020 2020 5f5f 6461 7461 636c  ne].    __datacl
+000054a0: 6173 735f 6669 656c 6473 5f5f 3a20 4469  ass_fields__: Di
+000054b0: 6374 5b73 7472 2c20 6461 7461 636c 6173  ct[str, dataclas
+000054c0: 7365 732e 4669 656c 645d 0a0a 636c 6173  ses.Field]..clas
+000054d0: 7320 4d6f 6475 6c65 284d 6f64 756c 6542  s Module(ModuleB
+000054e0: 6173 6529 3a0a 2020 2222 2242 6173 6520  ase):.  """Base 
+000054f0: 636c 6173 7320 666f 7220 616c 6c20 6e65  class for all ne
+00005500: 7572 616c 206e 6574 776f 726b 206d 6f64  ural network mod
+00005510: 756c 6573 2e20 4c61 7965 7273 2061 6e64  ules. Layers and
+00005520: 206d 6f64 656c 7320 7368 6f75 6c64 2073   models should s
+00005530: 7562 636c 6173 7320 7468 6973 2063 6c61  ubclass this cla
+00005540: 7373 2e0a 0a20 2041 6c6c 2046 6c61 7820  ss...  All Flax 
+00005550: 4d6f 6475 6c65 7320 6172 6520 5079 7468  Modules are Pyth
+00005560: 6f6e 2033 2e37 0a20 2060 6461 7461 636c  on 3.7.  `datacl
+00005570: 6173 7365 7320 3c68 7474 7073 3a2f 2f64  asses <https://d
+00005580: 6f63 732e 7079 7468 6f6e 2e6f 7267 2f33  ocs.python.org/3
+00005590: 2f6c 6962 7261 7279 2f64 6174 6163 6c61  /library/datacla
+000055a0: 7373 6573 2e68 746d 6c3e 605f 2e20 5369  sses.html>`_. Si
+000055b0: 6e63 650a 2020 6461 7461 636c 6173 7365  nce.  dataclasse
+000055c0: 7320 7461 6b65 206f 7665 7220 6060 5f5f  s take over ``__
+000055d0: 696e 6974 5f5f 6060 2c20 796f 7520 7368  init__``, you sh
+000055e0: 6f75 6c64 2069 6e73 7465 6164 206f 7665  ould instead ove
+000055f0: 7272 6964 6520 3a6d 6574 683a 6073 6574  rride :meth:`set
+00005600: 7570 602c 0a20 2077 6869 6368 2069 7320  up`,.  which is 
+00005610: 6175 746f 6d61 7469 6361 6c6c 7920 6361  automatically ca
+00005620: 6c6c 6564 2074 6f20 696e 6974 6961 6c69  lled to initiali
+00005630: 7a65 2074 6865 206d 6f64 756c 652e 0a0a  ze the module...
+00005640: 2020 4d6f 6475 6c65 7320 6361 6e20 636f    Modules can co
+00005650: 6e74 6169 6e20 7375 626d 6f64 756c 6573  ntain submodules
+00005660: 2c20 616e 6420 696e 2074 6869 7320 7761  , and in this wa
+00005670: 7920 6361 6e20 6265 206e 6573 7465 6420  y can be nested 
+00005680: 696e 2061 2074 7265 650a 2020 7374 7275  in a tree.  stru
+00005690: 6374 7572 652e 2053 7562 6d6f 6465 6c73  cture. Submodels
+000056a0: 2063 616e 2062 6520 6173 7369 676e 6564   can be assigned
+000056b0: 2061 7320 7265 6775 6c61 7220 6174 7472   as regular attr
+000056c0: 6962 7574 6573 2069 6e73 6964 6520 7468  ibutes inside th
+000056d0: 650a 2020 3a6d 6574 683a 6073 6574 7570  e.  :meth:`setup
+000056e0: 6020 6d65 7468 6f64 2e0a 0a20 2059 6f75  ` method...  You
+000056f0: 2063 616e 2064 6566 696e 6520 6172 6269   can define arbi
+00005700: 7472 6172 7920 2266 6f72 7761 7264 2070  trary "forward p
+00005710: 6173 7322 206d 6574 686f 6473 206f 6e20  ass" methods on 
+00005720: 796f 7572 204d 6f64 756c 6520 7375 6263  your Module subc
+00005730: 6c61 7373 2e0a 2020 5768 696c 6520 6e6f  lass..  While no
+00005740: 206d 6574 686f 6473 2061 7265 2073 7065   methods are spe
+00005750: 6369 616c 2d63 6173 6564 2c20 6060 5f5f  cial-cased, ``__
+00005760: 6361 6c6c 5f5f 6060 2069 7320 6120 706f  call__`` is a po
+00005770: 7075 6c61 7220 6368 6f69 6365 2062 6563  pular choice bec
+00005780: 6175 7365 0a20 2069 7420 616c 6c6f 7773  ause.  it allows
+00005790: 2079 6f75 2074 6f20 7573 6520 6d6f 6475   you to use modu
+000057a0: 6c65 2069 6e73 7461 6e63 6573 2061 7320  le instances as 
+000057b0: 6966 2074 6865 7920 6172 6520 6675 6e63  if they are func
+000057c0: 7469 6f6e 733a 3a0a 0a20 2020 2066 726f  tions::..    fro
+000057d0: 6d20 666c 6178 2069 6d70 6f72 7420 6c69  m flax import li
+000057e0: 6e65 6e20 6173 206e 6e0a 0a20 2020 2063  nen as nn..    c
+000057f0: 6c61 7373 204d 6f64 756c 6528 6e6e 2e4d  lass Module(nn.M
+00005800: 6f64 756c 6529 3a0a 2020 2020 2020 6665  odule):.      fe
+00005810: 6174 7572 6573 3a20 5475 706c 655b 696e  atures: Tuple[in
+00005820: 742c 202e 2e2e 5d20 3d20 2831 362c 2034  t, ...] = (16, 4
+00005830: 290a 0a20 2020 2020 2064 6566 2073 6574  )..      def set
+00005840: 7570 2873 656c 6629 3a0a 2020 2020 2020  up(self):.      
+00005850: 2020 7365 6c66 2e64 656e 7365 3120 3d20    self.dense1 = 
+00005860: 4465 6e73 6528 7365 6c66 2e66 6561 7475  Dense(self.featu
+00005870: 7265 735b 305d 290a 2020 2020 2020 2020  res[0]).        
+00005880: 7365 6c66 2e64 656e 7365 3220 3d20 4465  self.dense2 = De
+00005890: 6e73 6528 7365 6c66 2e66 6561 7475 7265  nse(self.feature
+000058a0: 735b 315d 290a 0a20 2020 2020 2064 6566  s[1])..      def
+000058b0: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
+000058c0: 7829 3a0a 2020 2020 2020 2020 7265 7475  x):.        retu
+000058d0: 726e 2073 656c 662e 6465 6e73 6532 286e  rn self.dense2(n
+000058e0: 6e2e 7265 6c75 2873 656c 662e 6465 6e73  n.relu(self.dens
+000058f0: 6531 2878 2929 290a 0a20 204f 7074 696f  e1(x)))..  Optio
+00005900: 6e61 6c6c 792c 2066 6f72 206d 6f72 6520  nally, for more 
+00005910: 636f 6e63 6973 6520 6d6f 6475 6c65 2069  concise module i
+00005920: 6d70 6c65 6d65 6e74 6174 696f 6e73 2077  mplementations w
+00005930: 6865 7265 2073 7562 6d6f 6475 6c65 730a  here submodules.
+00005940: 2020 6465 6669 6e69 7469 6f6e 7320 6172    definitions ar
+00005950: 6520 636f 2d6c 6f63 6174 6564 2077 6974  e co-located wit
+00005960: 6820 7468 6569 7220 7573 6167 652c 2079  h their usage, y
+00005970: 6f75 2063 616e 2075 7365 2074 6865 0a20  ou can use the. 
+00005980: 203a 6d65 7468 3a60 636f 6d70 6163 7460   :meth:`compact`
+00005990: 2077 7261 7070 6572 2e0a 2020 2222 220a   wrapper..  """.
+000059a0: 0a20 2069 6620 7479 7069 6e67 2e54 5950  .  if typing.TYP
+000059b0: 455f 4348 4543 4b49 4e47 3a0a 2020 2020  E_CHECKING:.    
+000059c0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+000059d0: 662c 202a 6172 6773 2c20 2a2a 6b77 6172  f, *args, **kwar
+000059e0: 6773 293a 0a20 2020 2020 2023 2074 6869  gs):.      # thi
+000059f0: 7320 7374 7562 206d 616b 6573 2073 7572  s stub makes sur
+00005a00: 6520 7079 7479 7065 2061 6363 6570 7473  e pytype accepts
+00005a10: 2063 6f6e 7374 7275 6374 6f72 2061 7267   constructor arg
+00005a20: 756d 656e 7473 2e0a 2020 2020 2020 7061  uments..      pa
+00005a30: 7373 0a0a 2020 2020 6465 6620 5f5f 6361  ss..    def __ca
+00005a40: 6c6c 5f5f 2873 656c 662c 202a 6172 6773  ll__(self, *args
+00005a50: 2c20 2a2a 6b77 6172 6773 2920 2d3e 2041  , **kwargs) -> A
+00005a60: 6e79 3a0a 2020 2020 2020 2320 7468 6973  ny:.      # this
+00005a70: 2073 7475 6220 616c 6c6f 7773 2070 7974   stub allows pyt
+00005a80: 7970 6520 746f 2061 6363 6570 7420 4d6f  ype to accept Mo
+00005a90: 6475 6c65 7320 6173 2043 616c 6c61 626c  dules as Callabl
+00005aa0: 6573 2e0a 2020 2020 2020 7061 7373 0a0a  es..      pass..
+00005ab0: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
+00005ac0: 2064 6566 205f 5f69 6e69 745f 7375 6263   def __init_subc
+00005ad0: 6c61 7373 5f5f 2863 6c73 2c20 6b77 5f6f  lass__(cls, kw_o
+00005ae0: 6e6c 793a 2062 6f6f 6c20 3d20 4661 6c73  nly: bool = Fals
+00005af0: 652c 202a 2a6b 7761 7267 733a 2041 6e79  e, **kwargs: Any
+00005b00: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    "
+00005b10: 2222 4175 746f 6d61 7469 6361 6c6c 7920  ""Automatically 
+00005b20: 696e 6974 6961 6c69 7a65 7320 616c 6c20  initializes all 
+00005b30: 7375 6263 6c61 7373 6573 2061 7320 6375  subclasses as cu
+00005b40: 7374 6f6d 2064 6174 6163 6c61 7373 6573  stom dataclasses
+00005b50: 2e22 2222 0a20 2020 2073 7570 6572 2829  .""".    super()
+00005b60: 2e5f 5f69 6e69 745f 7375 6263 6c61 7373  .__init_subclass
+00005b70: 5f5f 282a 2a6b 7761 7267 7329 0a20 2020  __(**kwargs).   
+00005b80: 2023 2041 6c6c 2046 6c61 7820 4d6f 6475   # All Flax Modu
+00005b90: 6c65 7320 6172 6520 6461 7461 636c 6173  les are dataclas
+00005ba0: 7365 732e 2020 5765 2066 6f72 6365 2074  ses.  We force t
+00005bb0: 6869 7320 636f 6e76 656e 7469 6f6e 2073  his convention s
+00005bc0: 696e 6365 0a20 2020 2023 2069 7420 656e  ince.    # it en
+00005bd0: 636f 7572 6167 6573 2074 6865 2073 7461  courages the sta
+00005be0: 7465 6c65 7373 2062 6568 6176 696f 7220  teless behavior 
+00005bf0: 6e65 6564 6564 2074 6f20 636c 6f6e 6520  needed to clone 
+00005c00: 6d6f 6475 6c65 2069 6e73 7461 6e63 6573  module instances
+00005c10: 2066 6f72 0a20 2020 2023 2066 756e 6374   for.    # funct
+00005c20: 696f 6e61 6c20 7472 616e 7366 6f72 6d61  ional transforma
+00005c30: 7469 6f6e 2e20 2049 6e73 7465 6164 206f  tion.  Instead o
+00005c40: 6620 7573 696e 6720 6120 7079 7468 6f6e  f using a python
+00005c50: 206d 6574 6163 6c61 7373 2c20 7765 0a20   metaclass, we. 
+00005c60: 2020 2023 2061 7574 6f6d 6174 6963 616c     # automatical
+00005c70: 6c79 2074 7261 6e73 666f 726d 204d 6f64  ly transform Mod
+00005c80: 756c 6573 2069 6e74 6f20 6461 7461 636c  ules into datacl
+00005c90: 6173 7365 7320 6174 2073 7562 636c 6173  asses at subclas
+00005ca0: 7320 6372 6561 7469 6f6e 0a20 2020 2023  s creation.    #
+00005cb0: 2074 696d 652c 2061 6e64 2077 6520 7365   time, and we se
+00005cc0: 7420 7468 6520 6c61 7374 2064 6174 6163  t the last datac
+00005cd0: 6c61 7373 2061 7267 756d 656e 7473 2074  lass arguments t
+00005ce0: 6f20 6070 6172 656e 7460 2061 6e64 2060  o `parent` and `
+00005cf0: 6e61 6d65 602e 0a20 2020 2063 6c73 2e5f  name`..    cls._
+00005d00: 6375 7374 6f6d 697a 6564 5f64 6174 6163  customized_datac
+00005d10: 6c61 7373 5f74 7261 6e73 666f 726d 286b  lass_transform(k
+00005d20: 775f 6f6e 6c79 290a 2020 2020 2320 5765  w_only).    # We
+00005d30: 2077 7261 7020 7573 6572 2d64 6566 696e   wrap user-defin
+00005d40: 6564 206d 6574 686f 6473 2069 6e63 6c75  ed methods inclu
+00005d50: 6469 6e67 2073 6574 7570 2061 6e64 205f  ding setup and _
+00005d60: 5f63 616c 6c5f 5f20 746f 2065 6e66 6f72  _call__ to enfor
+00005d70: 6365 0a20 2020 2023 2061 206e 756d 6265  ce.    # a numbe
+00005d80: 7220 6f66 2064 6966 6665 7265 6e74 2063  r of different c
+00005d90: 6865 636b 7320 616e 6420 746f 2070 726f  hecks and to pro
+00005da0: 7669 6465 2063 6c65 6172 2065 7272 6f72  vide clear error
+00005db0: 206d 6573 7361 6765 732e 0a20 2020 2063   messages..    c
+00005dc0: 6c73 2e5f 7665 7269 6679 5f73 696e 676c  ls._verify_singl
+00005dd0: 655f 6f72 5f6e 6f5f 636f 6d70 6163 7428  e_or_no_compact(
+00005de0: 290a 2020 2020 636c 732e 5f77 7261 705f  ).    cls._wrap_
+00005df0: 6d6f 6475 6c65 5f61 7474 7269 6275 7465  module_attribute
+00005e00: 7328 290a 2020 2020 2320 5365 7420 656d  s().    # Set em
+00005e10: 7074 7920 636c 6173 7320 6465 6661 756c  pty class defaul
+00005e20: 7473 2e0a 2020 2020 636c 732e 5f73 7461  ts..    cls._sta
+00005e30: 7465 203d 205f 756e 696e 6974 6961 6c69  te = _uninitiali
+00005e40: 7a65 645f 6d6f 6475 6c65 5f69 6e74 6572  zed_module_inter
+00005e50: 6e61 6c5f 7374 6174 6520 2320 7479 7065  nal_state # type
+00005e60: 3a20 6967 6e6f 7265 5b61 7474 722d 6465  : ignore[attr-de
+00005e70: 6669 6e65 645d 0a20 2020 2063 6c73 2e73  fined].    cls.s
+00005e80: 636f 7065 3a20 4f70 7469 6f6e 616c 5b53  cope: Optional[S
+00005e90: 636f 7065 5d20 3d20 4e6f 6e65 2023 2074  cope] = None # t
+00005ea0: 7970 653a 2069 676e 6f72 650a 2020 2020  ype: ignore.    
+00005eb0: 2320 4861 6e64 6c65 7320 7765 616b 2072  # Handles weak r
+00005ec0: 6566 6572 656e 6369 6e67 206f 6620 7061  eferencing of pa
+00005ed0: 7265 6e74 204d 6f64 756c 6573 2074 6f20  rent Modules to 
+00005ee0: 7072 6576 656e 7420 7265 6665 7265 6e63  prevent referenc
+00005ef0: 6520 6379 636c 6573 2e0a 2020 2020 636c  e cycles..    cl
+00005f00: 732e 5f70 6172 656e 745f 7265 6620 3d20  s._parent_ref = 
+00005f10: 4e6f 6e65 2023 2074 7970 653a 2069 676e  None # type: ign
+00005f20: 6f72 655b 6174 7472 2d64 6566 696e 6564  ore[attr-defined
+00005f30: 5d0a 2020 2020 636c 732e 7061 7265 6e74  ].    cls.parent
+00005f40: 203d 2050 6172 656e 7444 6573 6372 6970   = ParentDescrip
+00005f50: 746f 7228 2920 2320 7479 7065 3a20 6967  tor() # type: ig
+00005f60: 6e6f 7265 5b61 7373 6967 6e6d 656e 745d  nore[assignment]
+00005f70: 0a0a 2020 4063 6c61 7373 6d65 7468 6f64  ..  @classmethod
+00005f80: 0a20 2064 6566 205f 6375 7374 6f6d 697a  .  def _customiz
+00005f90: 6564 5f64 6174 6163 6c61 7373 5f74 7261  ed_dataclass_tra
+00005fa0: 6e73 666f 726d 2863 6c73 2c20 6b77 5f6f  nsform(cls, kw_o
+00005fb0: 6e6c 793a 2062 6f6f 6c29 3a0a 2020 2020  nly: bool):.    
+00005fc0: 2222 2254 7261 6e73 666f 726d 7320 6063  """Transforms `c
+00005fd0: 6c73 6020 696e 746f 2061 2064 6174 6163  ls` into a datac
+00005fe0: 6c61 7373 2c20 7769 7468 2063 7573 746f  lass, with custo
+00005ff0: 6d20 6164 6469 7469 6f6e 616c 2062 6568  m additional beh
+00006000: 6176 696f 722e 0a0a 2020 2020 312e 2049  avior...    1. I
+00006010: 6e6a 6563 7420 6070 6172 656e 7460 2061  nject `parent` a
+00006020: 6e64 2060 6e61 6d65 6020 6669 656c 6473  nd `name` fields
+00006030: 2e20 2028 4966 2074 6865 7920 6172 6520  .  (If they are 
+00006040: 616c 7265 6164 7920 7072 6573 656e 742c  already present,
+00006050: 0a20 2020 2020 2020 7468 656e 2063 6865  .       then che
+00006060: 636b 2074 6861 7420 7468 6579 2068 6176  ck that they hav
+00006070: 6520 7468 6520 6578 7065 6374 6564 2074  e the expected t
+00006080: 7970 6573 2e29 0a20 2020 2032 2e20 5365  ypes.).    2. Se
+00006090: 7420 636f 6d70 6172 652c 2068 6173 682c  t compare, hash,
+000060a0: 2061 6e64 2072 6570 7220 746f 2046 616c   and repr to Fal
+000060b0: 7365 2066 6f72 206e 6f6e 2d69 6e69 7420  se for non-init 
+000060c0: 6669 656c 6473 2e0a 2020 2020 332e 2047  fields..    3. G
+000060d0: 656e 6572 6174 6520 6120 6861 7368 2066  enerate a hash f
+000060e0: 756e 6374 696f 6e20 2869 6620 6e6f 7420  unction (if not 
+000060f0: 7072 6f76 6964 6564 2062 7920 636c 7329  provided by cls)
+00006100: 2e0a 2020 2020 2222 220a 2020 2020 2320  ..    """.    # 
+00006110: 4368 6563 6b20 7265 7365 7276 6564 2061  Check reserved a
+00006120: 7474 7269 6275 7465 7320 6861 7665 2065  ttributes have e
+00006130: 7870 6563 7465 6420 7479 7065 2061 6e6e  xpected type ann
+00006140: 6f74 6174 696f 6e73 2e0a 2020 2020 616e  otations..    an
+00006150: 6e6f 7461 7469 6f6e 7320 3d20 6469 6374  notations = dict
+00006160: 2863 6c73 2e5f 5f64 6963 745f 5f2e 6765  (cls.__dict__.ge
+00006170: 7428 275f 5f61 6e6e 6f74 6174 696f 6e73  t('__annotations
+00006180: 5f5f 272c 207b 7d29 290a 2020 2020 6966  __', {})).    if
+00006190: 2061 6e6e 6f74 6174 696f 6e73 2e67 6574   annotations.get
+000061a0: 2827 7061 7265 6e74 272c 205f 5061 7265  ('parent', _Pare
+000061b0: 6e74 5479 7065 2920 213d 205f 5061 7265  ntType) != _Pare
+000061c0: 6e74 5479 7065 3a0a 2020 2020 2020 7261  ntType:.      ra
+000061d0: 6973 6520 6572 726f 7273 2e52 6573 6572  ise errors.Reser
+000061e0: 7665 644d 6f64 756c 6541 7474 7269 6275  vedModuleAttribu
+000061f0: 7465 4572 726f 7228 616e 6e6f 7461 7469  teError(annotati
+00006200: 6f6e 7329 0a20 2020 2069 6620 616e 6e6f  ons).    if anno
+00006210: 7461 7469 6f6e 732e 6765 7428 276e 616d  tations.get('nam
+00006220: 6527 2c20 7374 7229 206e 6f74 2069 6e20  e', str) not in 
+00006230: 2827 7374 7227 2c20 7374 722c 204f 7074  ('str', str, Opt
+00006240: 696f 6e61 6c5b 7374 725d 293a 0a20 2020  ional[str]):.   
+00006250: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
+00006260: 5265 7365 7276 6564 4d6f 6475 6c65 4174  ReservedModuleAt
+00006270: 7472 6962 7574 6545 7272 6f72 2861 6e6e  tributeError(ann
+00006280: 6f74 6174 696f 6e73 290a 0a20 2020 2023  otations)..    #
+00006290: 2061 6e79 206e 6f6e 2d69 6e69 7420 6669   any non-init fi
+000062a0: 656c 6420 7769 6c6c 206f 6e6c 7920 6265  eld will only be
+000062b0: 2073 6574 2069 6e20 7365 7475 700a 2020   set in setup.  
+000062c0: 2020 2320 4475 7269 6e67 205f 5f68 6173    # During __has
+000062d0: 685f 5f20 616e 6420 5f5f 6571 5f5f 2074  h__ and __eq__ t
+000062e0: 6865 2066 6965 6c64 2069 7320 6e6f 7420  he field is not 
+000062f0: 7365 7420 7965 740a 2020 2020 2320 736f  set yet.    # so
+00006300: 2069 7420 7368 6f75 6c64 206e 6f74 2062   it should not b
+00006310: 6520 7573 6564 2069 6e20 636f 6d70 6172  e used in compar
+00006320: 652c 2068 6173 6820 6f72 2072 6570 722e  e, hash or repr.
+00006330: 0a20 2020 2066 6f72 2066 6965 6c64 2069  .    for field i
+00006340: 6e20 616e 6e6f 7461 7469 6f6e 733a 0a20  n annotations:. 
+00006350: 2020 2020 2066 6965 6c64 5f6d 6574 6120       field_meta 
+00006360: 3d20 6765 7461 7474 7228 636c 732c 2066  = getattr(cls, f
+00006370: 6965 6c64 2c20 4e6f 6e65 290a 2020 2020  ield, None).    
+00006380: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+00006390: 6669 656c 645f 6d65 7461 2c20 6461 7461  field_meta, data
+000063a0: 636c 6173 7365 732e 4669 656c 6429 2061  classes.Field) a
+000063b0: 6e64 206e 6f74 2066 6965 6c64 5f6d 6574  nd not field_met
+000063c0: 612e 696e 6974 3a0a 2020 2020 2020 2020  a.init:.        
+000063d0: 6669 656c 645f 6d65 7461 2e63 6f6d 7061  field_meta.compa
+000063e0: 7265 203d 2046 616c 7365 0a20 2020 2020  re = False.     
+000063f0: 2020 2066 6965 6c64 5f6d 6574 612e 6861     field_meta.ha
+00006400: 7368 203d 2046 616c 7365 0a20 2020 2020  sh = False.     
+00006410: 2020 2066 6965 6c64 5f6d 6574 612e 7265     field_meta.re
+00006420: 7072 203d 2046 616c 7365 0a0a 2020 2020  pr = False..    
+00006430: 6578 7472 615f 6669 656c 6473 203d 205b  extra_fields = [
+00006440: 2827 7061 7265 6e74 272c 205f 5061 7265  ('parent', _Pare
+00006450: 6e74 5479 7065 2c0a 2020 2020 2020 2020  ntType,.        
+00006460: 2020 2020 2020 2020 2020 2020 206b 775f               kw_
+00006470: 6f6e 6c79 5f64 6174 6163 6c61 7373 6573  only_dataclasses
+00006480: 2e66 6965 6c64 280a 2020 2020 2020 2020  .field(.        
+00006490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000064a0: 2072 6570 723d 4661 6c73 652c 2064 6566   repr=False, def
+000064b0: 6175 6c74 3d5f 756e 7370 6563 6966 6965  ault=_unspecifie
+000064c0: 645f 7061 7265 6e74 2c0a 2020 2020 2020  d_parent,.      
+000064d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000064e0: 2020 206b 775f 6f6e 6c79 3d54 7275 6529     kw_only=True)
+000064f0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00006500: 2020 2020 2020 2028 276e 616d 6527 2c20         ('name', 
+00006510: 4f70 7469 6f6e 616c 5b73 7472 5d2c 0a20  Optional[str],. 
+00006520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006530: 2020 2020 6b77 5f6f 6e6c 795f 6461 7461      kw_only_data
+00006540: 636c 6173 7365 732e 6669 656c 6428 6465  classes.field(de
+00006550: 6661 756c 743d 4e6f 6e65 2c20 6b77 5f6f  fault=None, kw_o
+00006560: 6e6c 793d 5472 7565 2929 5d0a 0a20 2020  nly=True))]..   
+00006570: 2069 6620 6b77 5f6f 6e6c 793a 0a20 2020   if kw_only:.   
+00006580: 2020 2069 6620 7475 706c 6528 7379 732e     if tuple(sys.
+00006590: 7665 7273 696f 6e5f 696e 666f 295b 3a33  version_info)[:3
+000065a0: 5d20 3e3d 2028 332c 2031 302c 2030 293a  ] >= (3, 10, 0):
+000065b0: 0a20 2020 2020 2020 2066 6f72 206e 616d  .        for nam
+000065c0: 652c 2061 6e6e 6f74 6174 696f 6e2c 2064  e, annotation, d
+000065d0: 6566 6175 6c74 2069 6e20 6578 7472 615f  efault in extra_
+000065e0: 6669 656c 6473 3a20 2023 2070 7974 7970  fields:  # pytyp
+000065f0: 653a 2064 6973 6162 6c65 3d69 6e76 616c  e: disable=inval
+00006600: 6964 2d61 6e6e 6f74 6174 696f 6e0a 2020  id-annotation.  
+00006610: 2020 2020 2020 2020 7365 7461 7474 7228          setattr(
+00006620: 636c 732c 206e 616d 652c 2064 6566 6175  cls, name, defau
+00006630: 6c74 290a 2020 2020 2020 2020 2020 636c  lt).          cl
+00006640: 732e 5f5f 616e 6e6f 7461 7469 6f6e 735f  s.__annotations_
+00006650: 5f5b 6e61 6d65 5d20 3d20 616e 6e6f 7461  _[name] = annota
+00006660: 7469 6f6e 0a20 2020 2020 2020 2064 6174  tion.        dat
+00006670: 6163 6c61 7373 6573 2e64 6174 6163 6c61  aclasses.datacla
+00006680: 7373 280a 2020 2020 2020 2020 2020 2020  ss(.            
+00006690: 756e 7361 6665 5f68 6173 683d 275f 5f68  unsafe_hash='__h
+000066a0: 6173 685f 5f27 206e 6f74 2069 6e20 636c  ash__' not in cl
+000066b0: 732e 5f5f 6469 6374 5f5f 2c0a 2020 2020  s.__dict__,.    
+000066c0: 2020 2020 2020 2020 7265 7072 3d46 616c          repr=Fal
+000066d0: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
+000066e0: 6b77 5f6f 6e6c 793d 5472 7565 2c0a 2020  kw_only=True,.  
+000066f0: 2020 2020 2020 2928 636c 7329 2020 2320        )(cls)  # 
+00006700: 7479 7065 3a20 6967 6e6f 7265 5b63 616c  type: ignore[cal
+00006710: 6c2d 6f76 6572 6c6f 6164 5d0a 2020 2020  l-overload].    
+00006720: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+00006730: 7261 6973 6520 5479 7065 4572 726f 7228  raise TypeError(
+00006740: 2760 6b77 5f6f 6e6c 7960 2069 7320 6e6f  '`kw_only` is no
+00006750: 7420 6176 6169 6c61 626c 6520 6265 666f  t available befo
+00006760: 7265 2050 7920 332e 3130 2e27 290a 2020  re Py 3.10.').  
+00006770: 2020 656c 7365 3a0a 2020 2020 2020 2320    else:.      # 
+00006780: 4e6f 7720 6170 706c 7920 6461 7461 636c  Now apply datacl
+00006790: 6173 7320 7472 616e 7366 6f72 6d20 2877  ass transform (w
+000067a0: 6869 6368 206f 7065 7261 7465 7320 696e  hich operates in
+000067b0: 2d70 6c61 6365 292e 0a20 2020 2020 2023  -place)..      #
+000067c0: 2044 6f20 6765 6e65 7261 7465 2061 2068   Do generate a h
+000067d0: 6173 6820 6675 6e63 7469 6f6e 206f 6e6c  ash function onl
+000067e0: 7920 6966 206e 6f74 2070 726f 7669 6465  y if not provide
+000067f0: 6420 6279 2074 6865 2063 6c61 7373 2e0a  d by the class..
+00006800: 2020 2020 2020 6b77 5f6f 6e6c 795f 6461        kw_only_da
+00006810: 7461 636c 6173 7365 732e 6461 7461 636c  taclasses.datacl
+00006820: 6173 7328 0a20 2020 2020 2020 2020 2063  ass(.          c
+00006830: 6c73 2c0a 2020 2020 2020 2020 2020 756e  ls,.          un
+00006840: 7361 6665 5f68 6173 683d 275f 5f68 6173  safe_hash='__has
+00006850: 685f 5f27 206e 6f74 2069 6e20 636c 732e  h__' not in cls.
+00006860: 5f5f 6469 6374 5f5f 2c0a 2020 2020 2020  __dict__,.      
+00006870: 2020 2020 7265 7072 3d46 616c 7365 2c0a      repr=False,.
+00006880: 2020 2020 2020 2020 2020 6578 7472 615f            extra_
+00006890: 6669 656c 6473 3d65 7874 7261 5f66 6965  fields=extra_fie
+000068a0: 6c64 7329 2020 2320 7079 7479 7065 3a20  lds)  # pytype: 
+000068b0: 6469 7361 626c 653d 7772 6f6e 672d 6b65  disable=wrong-ke
+000068c0: 7977 6f72 642d 6172 6773 0a0a 2020 2020  yword-args..    
+000068d0: 636c 732e 5f5f 6861 7368 5f5f 203d 205f  cls.__hash__ = _
+000068e0: 7772 6170 5f68 6173 6828 636c 732e 5f5f  wrap_hash(cls.__
+000068f0: 6861 7368 5f5f 2920 2023 2074 7970 653a  hash__)  # type:
+00006900: 2069 676e 6f72 655b 6d65 7468 6f64 2d61   ignore[method-a
+00006910: 7373 6967 6e5d 0a0a 2020 4063 6c61 7373  ssign]..  @class
+00006920: 6d65 7468 6f64 0a20 2064 6566 205f 7665  method.  def _ve
+00006930: 7269 6679 5f73 696e 676c 655f 6f72 5f6e  rify_single_or_n
+00006940: 6f5f 636f 6d70 6163 7428 636c 7329 3a0a  o_compact(cls):.
+00006950: 2020 2020 2222 2253 7461 7469 6361 6c6c      """Staticall
+00006960: 7920 7665 7269 6669 6573 2074 6861 7420  y verifies that 
+00006970: 6174 206d 6f73 7420 6120 7369 6e67 6c65  at most a single
+00006980: 206d 6574 686f 6420 6973 206c 6162 656c   method is label
+00006990: 6c65 6420 636f 6d70 6163 742e 2222 220a  led compact.""".
+000069a0: 2020 2020 6d65 7468 6f64 7320 3d20 5b6d      methods = [m
+000069b0: 5b30 5d20 666f 7220 6d20 696e 2069 6e73  [0] for m in ins
+000069c0: 7065 6374 2e67 6574 6d65 6d62 6572 7328  pect.getmembers(
+000069d0: 636c 732c 2070 7265 6469 6361 7465 3d63  cls, predicate=c
+000069e0: 616c 6c61 626c 6529 5d0a 2020 2020 6e5f  allable)].    n_
+000069f0: 636f 6d70 6163 745f 666e 7320 3d20 6c65  compact_fns = le
+00006a00: 6e28 5b6d 6574 686f 645f 6e61 6d65 2066  n([method_name f
+00006a10: 6f72 206d 6574 686f 645f 6e61 6d65 2069  or method_name i
+00006a20: 6e20 6d65 7468 6f64 730a 2020 2020 2020  n methods.      
+00006a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006a40: 2020 2069 6620 6861 7361 7474 7228 6765     if hasattr(ge
+00006a50: 7461 7474 7228 636c 732c 206d 6574 686f  tattr(cls, metho
+00006a60: 645f 6e61 6d65 292c 2027 636f 6d70 6163  d_name), 'compac
+00006a70: 7427 295d 290a 2020 2020 6966 206e 5f63  t')]).    if n_c
+00006a80: 6f6d 7061 6374 5f66 6e73 203e 2031 3a0a  ompact_fns > 1:.
+00006a90: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
+00006aa0: 7273 2e4d 756c 7469 706c 654d 6574 686f  rs.MultipleMetho
+00006ab0: 6473 436f 6d70 6163 7445 7272 6f72 2829  dsCompactError()
+00006ac0: 0a0a 2020 4063 6c61 7373 6d65 7468 6f64  ..  @classmethod
+00006ad0: 0a20 2064 6566 205f 7772 6170 5f6d 6f64  .  def _wrap_mod
+00006ae0: 756c 655f 6174 7472 6962 7574 6573 2863  ule_attributes(c
+00006af0: 6c73 293a 0a20 2020 2022 2222 5772 6170  ls):.    """Wrap
+00006b00: 7320 7573 6572 2d64 6566 696e 6564 206e  s user-defined n
+00006b10: 6f6e 2d69 6e68 6572 6974 6564 206d 6574  on-inherited met
+00006b20: 686f 6473 2061 6e64 2064 6573 6372 6970  hods and descrip
+00006b30: 746f 7273 2077 6974 6820 7374 6174 650a  tors with state.
+00006b40: 2020 2020 6d61 6e61 6765 6d65 6e74 2066      management f
+00006b50: 756e 6374 696f 6e73 2e0a 2020 2020 2222  unctions..    ""
+00006b60: 220a 2020 2020 2320 7772 6170 206d 6574  ".    # wrap met
+00006b70: 686f 6473 0a20 2020 206d 6574 686f 645f  hods.    method_
+00006b80: 6578 636c 7573 696f 6e73 203d 2028 5b66  exclusions = ([f
+00006b90: 2e6e 616d 6520 666f 7220 6620 696e 2064  .name for f in d
+00006ba0: 6174 6163 6c61 7373 6573 2e66 6965 6c64  ataclasses.field
+00006bb0: 7328 636c 7329 5d20 2b0a 2020 2020 2020  s(cls)] +.      
+00006bc0: 2020 2020 2020 2020 2020 2020 5b27 5f5f              ['__
+00006bd0: 6571 5f5f 272c 2027 5f5f 7265 7072 5f5f  eq__', '__repr__
+00006be0: 272c 2027 5f5f 696e 6974 5f5f 272c 2027  ', '__init__', '
+00006bf0: 5f5f 6861 7368 5f5f 272c 0a20 2020 2020  __hash__',.     
+00006c00: 2020 2020 2020 2020 2020 2020 2020 275f                '_
+00006c10: 5f70 6f73 745f 696e 6974 5f5f 275d 290a  _post_init__']).
+00006c20: 2020 2020 666f 7220 6b65 7920 696e 205f      for key in _
+00006c30: 6765 745f 6c6f 6361 6c5f 6d65 7468 6f64  get_local_method
+00006c40: 5f6e 616d 6573 2863 6c73 2c20 6578 636c  _names(cls, excl
+00006c50: 7564 653d 6d65 7468 6f64 5f65 7863 6c75  ude=method_exclu
+00006c60: 7369 6f6e 7329 3a0a 2020 2020 2020 6d65  sions):.      me
+00006c70: 7468 6f64 203d 2067 6574 6174 7472 2863  thod = getattr(c
+00006c80: 6c73 2c20 6b65 7929 0a20 2020 2020 2069  ls, key).      i
+00006c90: 6620 6861 7361 7474 7228 6d65 7468 6f64  f hasattr(method
+00006ca0: 2c20 276e 6f77 7261 7027 293a 0a20 2020  , 'nowrap'):.   
+00006cb0: 2020 2020 2063 6f6e 7469 6e75 650a 2020       continue.  
+00006cc0: 2020 2020 7365 7461 7474 7228 636c 732c      setattr(cls,
+00006cd0: 206b 6579 2c20 7772 6170 5f6d 6574 686f   key, wrap_metho
+00006ce0: 645f 6f6e 6365 286d 6574 686f 6429 290a  d_once(method)).
+00006cf0: 0a20 2020 2023 2077 7261 7020 6465 7363  .    # wrap desc
+00006d00: 7269 7074 6f72 730a 2020 2020 6465 7363  riptors.    desc
+00006d10: 7269 7074 6f72 5f65 7863 6c75 7369 6f6e  riptor_exclusion
+00006d20: 7320 3d20 285b 662e 6e61 6d65 2066 6f72  s = ([f.name for
+00006d30: 2066 2069 6e20 6461 7461 636c 6173 7365   f in dataclasse
+00006d40: 732e 6669 656c 6473 2863 6c73 295d 202b  s.fields(cls)] +
+00006d50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00006d60: 2020 2020 2020 2020 2020 2020 2020 5b27                ['
+00006d70: 7061 7265 6e74 272c 2027 5f5f 6469 6374  parent', '__dict
+00006d80: 5f5f 275d 290a 2020 2020 666f 7220 6b65  __']).    for ke
+00006d90: 7920 696e 205f 6765 745f 6c6f 6361 6c5f  y in _get_local_
+00006da0: 6465 7363 7269 7074 6f72 5f6e 616d 6573  descriptor_names
+00006db0: 2863 6c73 2c20 6465 7363 7269 7074 6f72  (cls, descriptor
+00006dc0: 5f65 7863 6c75 7369 6f6e 7329 3a0a 2020  _exclusions):.  
+00006dd0: 2020 2020 2320 646f 6e27 7420 7573 6520      # don't use 
+00006de0: 6765 7461 7474 7220 6865 7265 2c20 7369  getattr here, si
+00006df0: 6e63 6520 6974 2077 696c 6c20 6361 6c6c  nce it will call
+00006e00: 2074 6865 2064 6573 6372 6970 746f 720a   the descriptor.
+00006e10: 2020 2020 2020 6465 7363 7269 7074 6f72        descriptor
+00006e20: 203d 2063 6c73 2e5f 5f64 6963 745f 5f5b   = cls.__dict__[
+00006e30: 6b65 795d 0a20 2020 2020 2069 6620 6861  key].      if ha
+00006e40: 7361 7474 7228 6465 7363 7269 7074 6f72  sattr(descriptor
+00006e50: 2c20 276e 6f77 7261 7027 293a 0a20 2020  , 'nowrap'):.   
+00006e60: 2020 2020 2063 6f6e 7469 6e75 650a 2020       continue.  
+00006e70: 2020 2020 7365 7461 7474 7228 636c 732c      setattr(cls,
+00006e80: 206b 6579 2c20 7772 6170 5f64 6573 6372   key, wrap_descr
+00006e90: 6970 746f 725f 6f6e 6365 2864 6573 6372  iptor_once(descr
+00006ea0: 6970 746f 7229 290a 2020 2020 7265 7475  iptor)).    retu
+00006eb0: 726e 2063 6c73 0a0a 2020 6465 6620 5f63  rn cls..  def _c
+00006ec0: 616c 6c5f 7772 6170 7065 645f 6d65 7468  all_wrapped_meth
+00006ed0: 6f64 2873 656c 662c 2066 756e 2c20 6172  od(self, fun, ar
+00006ee0: 6773 2c20 6b77 6172 6773 293a 0a20 2020  gs, kwargs):.   
+00006ef0: 2022 2222 2243 616c 6c73 2061 2077 7261   """"Calls a wra
+00006f00: 7070 6564 206d 6574 686f 642e 0a0a 2020  pped method...  
+00006f10: 2020 5468 6973 2066 756e 6374 696f 6e20    This function 
+00006f20: 6973 2072 6573 706f 6e73 6962 6c65 2066  is responsible f
+00006f30: 6f72 2073 6574 7469 6e67 2075 7020 7468  or setting up th
+00006f40: 6520 7468 7265 6164 206c 6f63 616c 2073  e thread local s
+00006f50: 7461 7465 0a20 2020 2063 6f72 7265 6374  tate.    correct
+00006f60: 6c79 2062 6566 6f72 6520 6361 6c6c 696e  ly before callin
+00006f70: 6720 7468 6520 6d65 7468 6f64 2061 6e64  g the method and
+00006f80: 2063 6c65 616e 696e 6720 7570 2061 6674   cleaning up aft
+00006f90: 6572 7761 7264 732e 0a20 2020 2054 6869  erwards..    Thi
+00006fa0: 7320 696e 636c 7564 6573 2073 746f 7269  s includes stori
+00006fb0: 6e67 2069 6e74 6572 6d65 6469 6174 6573  ng intermediates
+00006fc0: 2c20 7365 7475 7020 6f66 2074 6865 2063  , setup of the c
+00006fd0: 6f6d 7061 6374 2073 636f 7065 2c0a 2020  ompact scope,.  
+00006fe0: 2020 616e 6420 6d61 6b69 6e67 2073 7572    and making sur
+00006ff0: 6520 7365 7475 7020 6973 2063 616c 6c65  e setup is calle
+00007000: 6420 6265 666f 7265 2061 6e79 206f 7468  d before any oth
+00007010: 6572 206d 6574 686f 642e 0a0a 2020 2020  er method...    
+00007020: 4172 6773 3a0a 2020 2020 2020 6675 6e3a  Args:.      fun:
+00007030: 2054 6865 2077 7261 7070 6564 206d 6574   The wrapped met
+00007040: 686f 642e 0a20 2020 2020 2061 7267 733a  hod..      args:
+00007050: 204e 616d 6564 2061 7267 756d 656e 7473   Named arguments
+00007060: 2070 6173 7365 6420 746f 2060 6066 756e   passed to ``fun
+00007070: 6060 2e0a 2020 2020 2020 6b77 6172 6773  ``..      kwargs
+00007080: 3a20 4b65 7977 6f72 6420 6172 6775 6d65  : Keyword argume
+00007090: 6e74 7320 7061 7373 6564 2074 6f20 6060  nts passed to ``
+000070a0: 6675 6e60 602e 0a0a 2020 2020 5265 7475  fun``...    Retu
+000070b0: 726e 733a 0a20 2020 2020 2054 6865 2072  rns:.      The r
+000070c0: 6573 756c 7473 206f 6620 6361 6c6c 696e  esults of callin
+000070d0: 6720 6060 6675 6e60 602e 0a20 2020 2022  g ``fun``..    "
+000070e0: 2222 0a20 2020 2069 735f 636f 6d70 6163  "".    is_compac
+000070f0: 745f 6d65 7468 6f64 203d 2068 6173 6174  t_method = hasat
+00007100: 7472 2866 756e 2c20 2763 6f6d 7061 6374  tr(fun, 'compact
+00007110: 2729 0a20 2020 2066 756e 5f6e 616d 6520  ').    fun_name 
+00007120: 3d20 6765 7461 7474 7228 6675 6e2c 2027  = getattr(fun, '
+00007130: 5f5f 6e61 6d65 5f5f 272c 2027 756e 6e61  __name__', 'unna
+00007140: 6d65 645f 6675 6e63 7469 6f6e 2729 0a20  med_function'). 
+00007150: 2020 2069 735f 7365 7475 705f 6d65 7468     is_setup_meth
+00007160: 6f64 203d 2066 756e 5f6e 616d 6520 3d3d  od = fun_name ==
+00007170: 2027 7365 7475 7027 0a20 2020 2061 6464   'setup'.    add
+00007180: 5f63 616c 6c5f 696e 666f 203d 206e 6f74  _call_info = not
+00007190: 2069 735f 7365 7475 705f 6d65 7468 6f64   is_setup_method
+000071a0: 2061 6e64 206c 656e 285f 636f 6e74 6578   and len(_contex
+000071b0: 742e 6361 6c6c 5f69 6e66 6f5f 7374 6163  t.call_info_stac
+000071c0: 6b29 203e 2030 0a20 2020 2023 2057 6520  k) > 0.    # We 
+000071d0: 6c61 7a69 6c79 2063 616c 6c20 7365 7475  lazily call setu
+000071e0: 7028 2920 6f6e 6c79 2077 6865 6e20 6e65  p() only when ne
+000071f0: 6564 6564 2e0a 2020 2020 6966 2069 735f  eded..    if is_
+00007200: 7365 7475 705f 6d65 7468 6f64 3a0a 2020  setup_method:.  
+00007210: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
+00007220: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
+00007230: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
+00007240: 4361 6c6c 5365 7475 7055 6e62 6f75 6e64  CallSetupUnbound
+00007250: 4d6f 6475 6c65 4572 726f 7228 290a 2020  ModuleError().  
+00007260: 2020 2020 6973 5f72 6563 7572 7265 6e74      is_recurrent
+00007270: 203d 2073 656c 662e 5f73 7461 7465 2e69   = self._state.i
+00007280: 6e5f 7365 7475 700a 2020 2020 2020 7365  n_setup.      se
+00007290: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
+000072a0: 7570 203d 2054 7275 650a 2020 2020 656c  up = True.    el
+000072b0: 7365 3a0a 2020 2020 2020 7365 6c66 2e5f  se:.      self._
+000072c0: 7472 795f 7365 7475 7028 290a 0a20 2020  try_setup()..   
+000072d0: 2069 6620 6973 5f63 6f6d 7061 6374 5f6d   if is_compact_m
+000072e0: 6574 686f 643a 0a20 2020 2020 2069 6620  ethod:.      if 
+000072f0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+00007300: 6e65 3a0a 2020 2020 2020 2020 7261 6973  ne:.        rais
+00007310: 6520 6572 726f 7273 2e43 616c 6c43 6f6d  e errors.CallCom
+00007320: 7061 6374 556e 626f 756e 644d 6f64 756c  pactUnboundModul
+00007330: 6545 7272 6f72 2829 0a20 2020 2020 2069  eError().      i
+00007340: 735f 7265 6375 7272 656e 7420 3d20 7365  s_recurrent = se
+00007350: 6c66 2e5f 7374 6174 652e 696e 5f63 6f6d  lf._state.in_com
+00007360: 7061 6374 5f6d 6574 686f 640a 2020 2020  pact_method.    
+00007370: 2020 7365 6c66 2e5f 7374 6174 652e 696e    self._state.in
+00007380: 5f63 6f6d 7061 6374 5f6d 6574 686f 6420  _compact_method 
+00007390: 3d20 5472 7565 0a20 2020 205f 636f 6e74  = True.    _cont
+000073a0: 6578 742e 6d6f 6475 6c65 5f73 7461 636b  ext.module_stack
+000073b0: 2e61 7070 656e 6428 7365 6c66 290a 2020  .append(self).  
+000073c0: 2020 7472 793a 0a20 2020 2020 2023 2067    try:.      # g
+000073d0: 6574 2063 616c 6c20 696e 666f 0a20 2020  et call info.   
+000073e0: 2020 2069 6620 6164 645f 6361 6c6c 5f69     if add_call_i
+000073f0: 6e66 6f3a 0a20 2020 2020 2020 2061 7373  nfo:.        ass
+00007400: 6572 7420 7365 6c66 2e73 636f 7065 2069  ert self.scope i
+00007410: 7320 6e6f 7420 4e6f 6e65 0a20 2020 2020  s not None.     
+00007420: 2020 2063 616c 6c5f 696e 6465 7820 3d20     call_index = 
+00007430: 5f63 6f6e 7465 7874 2e63 616c 6c5f 696e  _context.call_in
+00007440: 666f 5f73 7461 636b 5b2d 315d 2e67 6574  fo_stack[-1].get
+00007450: 5f63 616c 6c5f 696e 6465 7828 7365 6c66  _call_index(self
+00007460: 290a 2020 2020 2020 2020 7363 6f70 655f  ).        scope_
+00007470: 7061 7468 203d 206a 6178 2e74 7265 655f  path = jax.tree_
+00007480: 7574 696c 2e74 7265 655f 6d61 7028 5f66  util.tree_map(_f
+00007490: 6978 5f70 6174 685f 7061 7274 2c20 7365  ix_path_part, se
+000074a0: 6c66 2e73 636f 7065 2e70 6174 6829 0a0a  lf.scope.path)..
+000074b0: 2020 2020 2020 2320 6361 6c6c 206d 6574        # call met
+000074c0: 686f 640a 2020 2020 2020 6966 205f 7573  hod.      if _us
+000074d0: 655f 6e61 6d65 645f 6361 6c6c 3a0a 2020  e_named_call:.  
+000074e0: 2020 2020 2020 7769 7468 206a 6178 2e6e        with jax.n
+000074f0: 616d 6564 5f73 636f 7065 285f 6465 7269  amed_scope(_deri
+00007500: 7665 5f70 726f 6669 6c69 6e67 5f6e 616d  ve_profiling_nam
+00007510: 6528 7365 6c66 2c20 6675 6e29 293a 0a20  e(self, fun)):. 
+00007520: 2020 2020 2020 2020 2079 203d 2066 756e           y = fun
+00007530: 2873 656c 662c 202a 6172 6773 2c20 2a2a  (self, *args, **
+00007540: 6b77 6172 6773 290a 2020 2020 2020 656c  kwargs).      el
+00007550: 7365 3a0a 2020 2020 2020 2020 7920 3d20  se:.        y = 
+00007560: 6675 6e28 7365 6c66 2c20 2a61 7267 732c  fun(self, *args,
+00007570: 202a 2a6b 7761 7267 7329 0a0a 2020 2020   **kwargs)..    
+00007580: 2020 6966 205f 636f 6e74 6578 742e 6361    if _context.ca
+00007590: 7074 7572 655f 7374 6163 6b3a 0a20 2020  pture_stack:.   
+000075a0: 2020 2020 2066 696c 7465 725f 666e 203d       filter_fn =
+000075b0: 205f 636f 6e74 6578 742e 6361 7074 7572   _context.captur
+000075c0: 655f 7374 6163 6b5b 2d31 5d0a 2020 2020  e_stack[-1].    
+000075d0: 2020 2020 6966 2066 696c 7465 725f 666e      if filter_fn
+000075e0: 2061 6e64 2066 696c 7465 725f 666e 2873   and filter_fn(s
+000075f0: 656c 662c 2066 756e 5f6e 616d 6529 3a0a  elf, fun_name):.
+00007600: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
+00007610: 6f77 2827 696e 7465 726d 6564 6961 7465  ow('intermediate
+00007620: 7327 2c20 6675 6e5f 6e61 6d65 2c20 7929  s', fun_name, y)
+00007630: 0a20 2020 2020 2069 6620 6164 645f 6361  .      if add_ca
+00007640: 6c6c 5f69 6e66 6f3a 0a20 2020 2020 2020  ll_info:.       
+00007650: 205f 6172 6773 2c20 5f6b 7761 7267 732c   _args, _kwargs,
+00007660: 205f 7920 3d20 666c 6178 2e6c 696e 656e   _y = flax.linen
+00007670: 2e73 756d 6d61 7279 2e5f 7265 7072 6573  .summary._repres
+00007680: 656e 745f 7472 6565 2828 6172 6773 2c20  ent_tree((args, 
+00007690: 6b77 6172 6773 2c20 7929 290a 2020 2020  kwargs, y)).    
+000076a0: 2020 2020 5f63 6f6e 7465 7874 2e63 616c      _context.cal
+000076b0: 6c5f 696e 666f 5f73 7461 636b 5b2d 315d  l_info_stack[-1]
+000076c0: 2e63 616c 6c73 2e61 7070 656e 6428 0a20  .calls.append(. 
+000076d0: 2020 2020 2020 2020 205f 4361 6c6c 496e           _CallIn
+000076e0: 666f 2863 616c 6c5f 696e 6465 782c 2073  fo(call_index, s
+000076f0: 636f 7065 5f70 6174 682c 2074 7970 6528  cope_path, type(
+00007700: 7365 6c66 292c 2066 756e 2e5f 5f6e 616d  self), fun.__nam
+00007710: 655f 5f2c 205f 6172 6773 2c20 5f6b 7761  e__, _args, _kwa
+00007720: 7267 732c 205f 7929 290a 2020 2020 2020  rgs, _y)).      
+00007730: 7265 7475 726e 2079 0a20 2020 2066 696e  return y.    fin
+00007740: 616c 6c79 3a0a 2020 2020 2020 5f63 6f6e  ally:.      _con
+00007750: 7465 7874 2e6d 6f64 756c 655f 7374 6163  text.module_stac
+00007760: 6b2e 706f 7028 290a 2020 2020 2020 6966  k.pop().      if
+00007770: 2069 735f 636f 6d70 6163 745f 6d65 7468   is_compact_meth
+00007780: 6f64 3a0a 2020 2020 2020 2020 6f62 6a65  od:.        obje
+00007790: 6374 2e5f 5f73 6574 6174 7472 5f5f 2873  ct.__setattr__(s
+000077a0: 656c 662c 2027 7363 6f70 6527 2c20 7365  elf, 'scope', se
+000077b0: 6c66 2e73 636f 7065 2e72 6577 6f75 6e64  lf.scope.rewound
+000077c0: 2829 290a 2020 2020 2020 2320 7365 7475  ()).      # setu
+000077d0: 7020 6f72 2063 6f6d 7061 6374 2063 616c  p or compact cal
+000077e0: 6c73 2063 616e 2062 6520 7265 6375 7272  ls can be recurr
+000077f0: 656e 7420 666f 7220 6578 616d 706c 6520  ent for example 
+00007800: 6475 6520 746f 2073 7570 6572 2063 616c  due to super cal
+00007810: 6c73 0a20 2020 2020 2023 2072 6573 6574  ls.      # reset
+00007820: 7469 6e67 2074 6865 2073 7461 7465 2077  ting the state w
+00007830: 6f75 6c64 2063 6175 7365 2069 7320 636f  ould cause is co
+00007840: 6d70 6163 742f 7365 7475 7020 6d65 7468  mpact/setup meth
+00007850: 6f64 0a20 2020 2020 2023 2074 6f20 6265  od.      # to be
+00007860: 2073 6574 2074 6f20 4661 6c73 6520 7072   set to False pr
+00007870: 656d 6174 7572 656c 792e 0a20 2020 2020  ematurely..     
+00007880: 2069 6620 2869 735f 636f 6d70 6163 745f   if (is_compact_
+00007890: 6d65 7468 6f64 206f 7220 6973 5f73 6574  method or is_set
+000078a0: 7570 5f6d 6574 686f 6429 2061 6e64 206e  up_method) and n
+000078b0: 6f74 2069 735f 7265 6375 7272 656e 743a  ot is_recurrent:
+000078c0: 0a20 2020 2020 2020 2073 656c 662e 5f73  .        self._s
+000078d0: 7461 7465 2e72 6573 6574 2829 0a0a 2020  tate.reset()..  
+000078e0: 6465 6620 5f5f 7365 7461 7474 725f 5f28  def __setattr__(
+000078f0: 7365 6c66 2c20 6e61 6d65 3a20 7374 722c  self, name: str,
+00007900: 2076 616c 3a20 416e 7929 3a0a 2020 2020   val: Any):.    
+00007910: 2222 2253 6574 7320 616e 2061 7474 7269  """Sets an attri
+00007920: 6275 7465 206f 6e20 7468 6973 204d 6f64  bute on this Mod
+00007930: 756c 652e 0a0a 2020 2020 5765 206f 7665  ule...    We ove
+00007940: 726c 6f61 6420 7365 7461 7474 7220 736f  rload setattr so
+00007950: 6c65 6c79 2074 6f20 7375 7070 6f72 7420  lely to support 
+00007960: 7079 7468 6f6e 6963 206e 616d 696e 6720  pythonic naming 
+00007970: 7669 6120 6173 7369 676e 6d65 6e74 206f  via assignment o
+00007980: 660a 2020 2020 7375 626d 6f64 756c 6573  f.    submodules
+00007990: 2069 6e20 7468 6520 7370 6563 6961 6c20   in the special 
+000079a0: 3a6d 6574 683a 6073 6574 7570 6020 6675  :meth:`setup` fu
+000079b0: 6e63 7469 6f6e 3a3a 0a0a 2020 2020 2020  nction::..      
+000079c0: 7365 6c66 2e73 7562 6d6f 6475 6c65 5f6e  self.submodule_n
+000079d0: 616d 6520 3d20 4d79 4d6f 6475 6c65 282e  ame = MyModule(.
+000079e0: 2e2e 290a 0a20 2020 2057 6520 616c 736f  ..)..    We also
+000079f0: 2073 7570 706f 7274 206c 6973 7473 2061   support lists a
+00007a00: 6e64 206f 7468 6572 2067 656e 6572 616c  nd other general
+00007a10: 2070 7974 7265 6573 2c20 652e 672e 3a3a   pytrees, e.g.::
+00007a20: 0a0a 2020 2020 2020 7365 6c66 2e73 7562  ..      self.sub
+00007a30: 6d6f 6475 6c65 7320 3d20 5b4d 794d 6f64  modules = [MyMod
+00007a40: 756c 6530 282e 2e29 2c20 4d79 4d6f 6475  ule0(..), MyModu
+00007a50: 6c65 3128 2e2e 292c 202e 2e2e 5d0a 0a20  le1(..), ...].. 
+00007a60: 2020 2041 7267 733a 0a20 2020 2020 206e     Args:.      n
+00007a70: 616d 653a 2041 7474 7269 6275 7465 2074  ame: Attribute t
+00007a80: 6f20 7365 742e 0a20 2020 2020 2076 616c  o set..      val
+00007a90: 3a20 5661 6c75 6520 6f66 2074 6865 2061  : Value of the a
+00007aa0: 7474 7269 6275 7465 2e0a 2020 2020 2222  ttribute..    ""
+00007ab0: 220a 2020 2020 6669 656c 6473 203d 2073  ".    fields = s
+00007ac0: 656c 662e 5f5f 6461 7461 636c 6173 735f  elf.__dataclass_
+00007ad0: 6669 656c 6473 5f5f 2020 2320 7079 7479  fields__  # pyty
+00007ae0: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
+00007af0: 6962 7574 652d 6572 726f 720a 2020 2020  ibute-error.    
+00007b00: 6973 5f64 6174 6163 6c61 7373 5f61 7474  is_dataclass_att
+00007b10: 7220 3d20 6e61 6d65 2069 6e20 6669 656c  r = name in fiel
+00007b20: 6473 2061 6e64 2066 6965 6c64 735b 6e61  ds and fields[na
+00007b30: 6d65 5d2e 696e 6974 0a0a 2020 2020 6966  me].init..    if
+00007b40: 206e 6f74 2073 656c 662e 5f73 7461 7465   not self._state
+00007b50: 2e69 6e5f 7365 7475 703a 0a20 2020 2020  .in_setup:.     
+00007b60: 2069 6620 6e6f 7420 7365 6c66 2e5f 7374   if not self._st
+00007b70: 6174 652e 6973 5f69 6e69 7469 616c 697a  ate.is_initializ
+00007b80: 6564 3a0a 2020 2020 2020 2020 2320 5365  ed:.        # Se
+00007b90: 7474 696e 6720 6174 7472 6962 7574 6573  tting attributes
+00007ba0: 2062 6566 6f72 6520 656e 6420 6f66 204d   before end of M
+00007bb0: 6f64 756c 652e 5f5f 706f 7374 5f69 6e69  odule.__post_ini
+00007bc0: 745f 5f28 290a 2020 2020 2020 2020 6f62  t__().        ob
+00007bd0: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
+00007be0: 2873 656c 662c 206e 616d 652c 2076 616c  (self, name, val
+00007bf0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+00007c00: 0a20 2020 2020 2065 6c73 653a 0a20 2020  .      else:.   
+00007c10: 2020 2020 2023 2057 6527 7265 2070 6173       # We're pas
+00007c20: 7420 616c 6c20 696e 6974 6961 6c69 7a61  t all initializa
+00007c30: 7469 6f6e 2061 6e64 2073 6574 7570 206c  tion and setup l
+00007c40: 6f67 6963 3a0a 2020 2020 2020 2020 2320  ogic:.        # 
+00007c50: 5261 6973 6573 2061 2054 7970 6545 7272  Raises a TypeErr
+00007c60: 6f72 206a 7573 7420 6c69 6b65 2066 726f  or just like fro
+00007c70: 7a65 6e20 7079 7468 6f6e 2064 6174 6163  zen python datac
+00007c80: 6c61 7373 6573 2e0a 2020 2020 2020 2020  lasses..        
+00007c90: 7261 6973 6520 6572 726f 7273 2e53 6574  raise errors.Set
+00007ca0: 4174 7472 6962 7574 6546 726f 7a65 6e4d  AttributeFrozenM
+00007cb0: 6f64 756c 6545 7272 6f72 280a 2020 2020  oduleError(.    
+00007cc0: 2020 2020 2020 2020 7365 6c66 2e5f 5f63          self.__c
+00007cd0: 6c61 7373 5f5f 2e5f 5f6e 616d 655f 5f2c  lass__.__name__,
+00007ce0: 206e 616d 652c 2076 616c 290a 0a20 2020   name, val)..   
+00007cf0: 2023 2057 6527 7265 2069 6e73 6964 6520   # We're inside 
+00007d00: 7468 6520 7365 7475 7028 2920 6d65 7468  the setup() meth
+00007d10: 6f64 3a0a 2020 2020 6966 2069 735f 6461  od:.    if is_da
+00007d20: 7461 636c 6173 735f 6174 7472 3a0a 2020  taclass_attr:.  
+00007d30: 2020 2020 2320 5468 6573 6520 6e61 6d65      # These name
+00007d40: 7320 6172 6520 7370 6563 6966 6965 6420  s are specified 
+00007d50: 6173 2064 6174 6163 6c61 7373 2066 6965  as dataclass fie
+00007d60: 6c64 732e 2054 6865 7920 7368 6f75 6c64  lds. They should
+00007d70: 206e 6f74 2062 650a 2020 2020 2020 2320   not be.      # 
+00007d80: 696e 6974 6961 6c69 7a65 6420 7769 7468  initialized with
+00007d90: 696e 2074 6865 2073 6574 7570 2829 206d  in the setup() m
+00007da0: 6574 686f 642c 2062 7574 2063 616e 2062  ethod, but can b
+00007db0: 6520 6d6f 6469 6669 6564 2066 7265 656c  e modified freel
+00007dc0: 790a 2020 2020 2020 2320 6265 666f 7265  y.      # before
+00007dd0: 2069 742e 0a20 2020 2020 2072 6169 7365   it..      raise
+00007de0: 2065 7272 6f72 732e 5365 7441 7474 7269   errors.SetAttri
+00007df0: 6275 7465 496e 4d6f 6475 6c65 5365 7475  buteInModuleSetu
+00007e00: 7045 7272 6f72 2829 0a0a 2020 2020 2320  pError()..    # 
+00007e10: 5661 6c75 6573 2028 7468 6174 206d 6179  Values (that may
+00007e20: 2062 6520 7661 7269 6162 6c65 7320 6f72   be variables or
+00007e30: 2073 7562 6d6f 6475 6c65 7329 2061 7265   submodules) are
+00007e40: 2062 6569 6e67 2064 6566 696e 6564 2061   being defined a
+00007e50: 6e64 0a20 2020 2023 2061 7474 6163 6865  nd.    # attache
+00007e60: 6420 696e 2073 6574 7570 2829 2c20 7765  d in setup(), we
+00007e70: 2072 756e 2073 6f6d 6520 6578 7472 6120   run some extra 
+00007e80: 6c6f 6769 6320 696e 2074 6861 7420 6361  logic in that ca
+00007e90: 7365 2e0a 2020 2020 7365 6c66 2e5f 7265  se..    self._re
+00007ea0: 6769 7374 6572 5f73 7562 6d6f 6475 6c65  gister_submodule
+00007eb0: 7328 6e61 6d65 2c20 7661 6c29 0a0a 2020  s(name, val)..  
+00007ec0: 6465 6620 5f5f 6765 7461 7474 725f 5f28  def __getattr__(
+00007ed0: 7365 6c66 2c20 6e61 6d65 3a20 7374 7229  self, name: str)
+00007ee0: 202d 3e20 416e 793a 0a20 2020 2022 2222   -> Any:.    """
+00007ef0: 4361 6c6c 2073 6574 7570 2829 2062 6566  Call setup() bef
+00007f00: 6f72 6520 6765 7474 696e 6720 616e 7920  ore getting any 
+00007f10: 7365 7475 702d 6465 6669 6e65 6420 6174  setup-defined at
+00007f20: 7472 6962 7574 6573 2e22 2222 0a20 2020  tributes.""".   
+00007f30: 2023 2057 6520 646f 6e27 7420 7761 6e74   # We don't want
+00007f40: 2074 6f20 7265 7475 726e 2061 6e79 7468   to return anyth
+00007f50: 696e 6720 666f 7220 7079 7468 6f6e 2063  ing for python c
+00007f60: 6f70 7920 2f20 7069 636b 6c65 206d 6574  opy / pickle met
+00007f70: 686f 6473 2e0a 2020 2020 6966 206e 616d  hods..    if nam
+00007f80: 6520 696e 205f 554e 4445 4649 4e45 445f  e in _UNDEFINED_
+00007f90: 434f 5059 5f50 4943 4b4c 455f 4d45 5448  COPY_PICKLE_METH
+00007fa0: 4f44 533a 0a20 2020 2020 2072 6169 7365  ODS:.      raise
+00007fb0: 2041 7474 7269 6275 7465 4572 726f 7228   AttributeError(
+00007fc0: 290a 2020 2020 7365 6c66 2e5f 7472 795f  ).    self._try_
+00007fd0: 7365 7475 7028 290a 2020 2020 6966 206e  setup().    if n
+00007fe0: 616d 6520 696e 2073 656c 662e 5f5f 6469  ame in self.__di
+00007ff0: 6374 5f5f 3a0a 2020 2020 2020 7265 7475  ct__:.      retu
+00008000: 726e 2073 656c 662e 5f5f 6469 6374 5f5f  rn self.__dict__
+00008010: 5b6e 616d 655d 0a20 2020 2065 6c73 653a  [name].    else:
+00008020: 0a20 2020 2020 206d 7367 203d 2066 2722  .      msg = f'"
+00008030: 7b73 656c 662e 5f5f 636c 6173 735f 5f2e  {self.__class__.
+00008040: 5f5f 6e61 6d65 5f5f 7d22 206f 626a 6563  __name__}" objec
+00008050: 7420 6861 7320 6e6f 2061 7474 7269 6275  t has no attribu
+00008060: 7465 2022 7b6e 616d 657d 222e 270a 2020  te "{name}".'.  
+00008070: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
+00008080: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
+00008090: 2020 206d 7367 202b 3d20 2866 2720 4966     msg += (f' If
+000080a0: 2022 7b6e 616d 657d 2220 6973 2064 6566   "{name}" is def
+000080b0: 696e 6564 2069 6e20 5c27 2e73 6574 7570  ined in \'.setup
+000080c0: 2829 5c27 2c20 7265 6d65 6d62 6572 2074  ()\', remember t
+000080d0: 6865 7365 2066 6965 6c64 7320 270a 2020  hese fields '.  
+000080e0: 2020 2020 2020 2020 2761 7265 206f 6e6c          'are onl
+000080f0: 7920 6163 6365 7373 6962 6c65 2066 726f  y accessible fro
+00008100: 6d20 696e 7369 6465 205c 2769 6e69 745c  m inside \'init\
+00008110: 2720 6f72 205c 2761 7070 6c79 5c27 2e27  ' or \'apply\'.'
+00008120: 290a 2020 2020 2020 7261 6973 6520 4174  ).      raise At
+00008130: 7472 6962 7574 6545 7272 6f72 286d 7367  tributeError(msg
+00008140: 290a 0a20 2064 6566 205f 5f64 6972 5f5f  )..  def __dir__
+00008150: 2873 656c 6629 202d 3e20 4c69 7374 5b73  (self) -> List[s
+00008160: 7472 5d3a 0a20 2020 2022 2222 4361 6c6c  tr]:.    """Call
+00008170: 2073 6574 7570 2829 2062 6566 6f72 6520   setup() before 
+00008180: 6c69 7374 696e 6720 6174 7472 6962 7574  listing attribut
+00008190: 6573 2e22 2222 0a20 2020 2073 656c 662e  es.""".    self.
+000081a0: 5f74 7279 5f73 6574 7570 2829 0a20 2020  _try_setup().   
+000081b0: 2072 6574 7572 6e20 6f62 6a65 6374 2e5f   return object._
+000081c0: 5f64 6972 5f5f 2873 656c 6629 2020 2320  _dir__(self)  # 
+000081d0: 7479 7065 3a20 6967 6e6f 7265 0a0a 2020  type: ignore..  
+000081e0: 6465 6620 5f5f 706f 7374 5f69 6e69 745f  def __post_init_
+000081f0: 5f28 7365 6c66 2920 2d3e 204e 6f6e 653a  _(self) -> None:
+00008200: 0a20 2020 2023 2044 4f20 4e4f 5420 5245  .    # DO NOT RE
+00008210: 4d4f 5645 202d 204d 6172 6b65 7220 666f  MOVE - Marker fo
+00008220: 7220 696e 7465 726e 616c 206c 6f67 6769  r internal loggi
+00008230: 6e67 2e0a 2020 2020 2320 496e 2064 6174  ng..    # In dat
+00008240: 6163 6c61 7373 6573 2c20 5f5f 696e 6974  aclasses, __init
+00008250: 5f5f 2069 7320 6f76 6572 7269 6464 656e  __ is overridden
+00008260: 2074 6f20 7072 6f63 6573 7320 6461 7461   to process data
+00008270: 636c 6173 7320 6172 6775 6d65 6e74 732c  class arguments,
+00008280: 0a20 2020 2023 2061 6e64 205f 5f70 6f73  .    # and __pos
+00008290: 745f 696e 6974 5f5f 2069 7320 6361 6c6c  t_init__ is call
+000082a0: 6564 2069 6d6d 6564 6961 7465 6c79 2061  ed immediately a
+000082b0: 6674 6572 7761 7264 732e 2048 6572 652c  fterwards. Here,
+000082c0: 2064 6570 656e 6469 6e67 206f 6e20 7468   depending on th
+000082d0: 650a 2020 2020 2320 7479 7065 206f 6620  e.    # type of 
+000082e0: 6070 6172 656e 7460 2070 6173 7365 6420  `parent` passed 
+000082f0: 746f 2069 6e69 7469 616c 697a 6520 7468  to initialize th
+00008300: 6520 4d6f 6475 6c65 2c20 7765 2065 6974  e Module, we eit
+00008310: 6865 7220 6465 6665 720a 2020 2020 2320  her defer.    # 
+00008320: 696e 6974 6961 6c69 7a61 7469 6f6e 2c20  initialization, 
+00008330: 6174 7461 6368 2074 6869 7320 4d6f 6475  attach this Modu
+00008340: 6c65 2061 7320 6120 7375 626d 6f64 756c  le as a submodul
+00008350: 6520 6f66 2061 2070 6172 656e 742c 206f  e of a parent, o
+00008360: 7220 6269 6e64 0a20 2020 2023 2074 6869  r bind.    # thi
+00008370: 7320 4d6f 6475 6c65 2061 7420 7468 6520  s Module at the 
+00008380: 746f 702d 6c65 7665 6c20 746f 2076 6172  top-level to var
+00008390: 6961 626c 6573 2061 6e64 2072 6e67 732e  iables and rngs.
+000083a0: 0a0a 2020 2020 6f62 6a65 6374 2e5f 5f73  ..    object.__s
+000083b0: 6574 6174 7472 5f5f 2873 656c 662c 2027  etattr__(self, '
+000083c0: 5f69 6427 2c20 7575 6964 2829 290a 2020  _id', uuid()).  
+000083d0: 2020 6f62 6a65 6374 2e5f 5f73 6574 6174    object.__setat
+000083e0: 7472 5f5f 2873 656c 662c 2027 5f73 7461  tr__(self, '_sta
+000083f0: 7465 272c 205f 4d6f 6475 6c65 496e 7465  te', _ModuleInte
+00008400: 726e 616c 5374 6174 6528 2929 0a0a 2020  rnalState())..  
+00008410: 2020 2320 5479 7069 6361 6c6c 7920 7765    # Typically we
+00008420: 2073 6574 2074 6865 2070 6172 656e 7420   set the parent 
+00008430: 6261 7365 6420 6f6e 2074 6865 2064 796e  based on the dyn
+00008440: 616d 6963 206d 6f64 756c 6520 636f 6e74  amic module cont
+00008450: 6578 742e 0a20 2020 2069 6620 7365 6c66  ext..    if self
+00008460: 2e70 6172 656e 7420 6973 205f 756e 7370  .parent is _unsp
+00008470: 6563 6966 6965 645f 7061 7265 6e74 3a20  ecified_parent: 
+00008480: 2023 2070 7974 7970 653a 2064 6973 6162   # pytype: disab
+00008490: 6c65 3d61 7474 7269 6275 7465 2d65 7272  le=attribute-err
+000084a0: 6f72 0a20 2020 2020 206f 626a 6563 742e  or.      object.
+000084b0: 5f5f 7365 7461 7474 725f 5f28 7365 6c66  __setattr__(self
+000084c0: 2c20 2770 6172 656e 7427 2c20 5f63 6f6e  , 'parent', _con
+000084d0: 7465 7874 2e6d 6f64 756c 655f 7374 6163  text.module_stac
+000084e0: 6b5b 2d31 5d29 0a0a 2020 2020 2320 496e  k[-1])..    # In
+000084f0: 6974 6961 6c69 7a61 7469 6f6e 2069 7320  itialization is 
+00008500: 6465 6665 7272 6564 2066 6f72 2074 6f70  deferred for top
+00008510: 206c 6576 656c 204d 6f64 756c 6573 206f   level Modules o
+00008520: 7220 616e 7920 6f74 6865 7220 226f 7270  r any other "orp
+00008530: 6861 6e22 0a20 2020 2023 204d 6f64 756c  han".    # Modul
+00008540: 6573 2075 6e74 696c 2061 7474 6163 686d  es until attachm
+00008550: 656e 7420 6279 205f 5f73 6574 6174 7472  ent by __setattr
+00008560: 5f5f 2069 2e65 2e20 4d79 4d6f 6475 6c65  __ i.e. MyModule
+00008570: 282e 2e2e 2c20 7061 7265 6e74 3d4e 6f6e  (..., parent=Non
+00008580: 6529 0a20 2020 2069 6620 7365 6c66 2e70  e).    if self.p
+00008590: 6172 656e 7420 6973 204e 6f6e 653a 0a20  arent is None:. 
+000085a0: 2020 2020 2072 6574 7572 6e0a 0a20 2020       return..   
+000085b0: 2023 2052 6567 6973 7465 7220 7375 626d   # Register subm
+000085c0: 6f64 756c 6520 6f6e 2070 6172 656e 7420  odule on parent 
+000085d0: 4d6f 6475 6c65 2e0a 2020 2020 6966 2069  Module..    if i
+000085e0: 7369 6e73 7461 6e63 6528 7365 6c66 2e70  sinstance(self.p
+000085f0: 6172 656e 742c 204d 6f64 756c 6529 3a0a  arent, Module):.
+00008600: 2020 2020 2020 2320 5768 656e 2069 6e69        # When ini
+00008610: 7469 616c 697a 696e 6720 616e 2075 6e6e  tializing an unn
+00008620: 616d 6564 204d 6f64 756c 6520 696e 7369  amed Module insi
+00008630: 6465 2073 6574 7570 2829 0a20 2020 2020  de setup().     
+00008640: 2023 2069 6e69 7469 616c 697a 6174 696f   # initializatio
+00008650: 6e20 6973 2064 6566 6572 7265 6420 756e  n is deferred un
+00008660: 7469 6c20 6174 7461 6368 6d65 6e74 2062  til attachment b
+00008670: 7920 5f5f 7365 7461 7474 725f 5f0a 2020  y __setattr__.  
+00008680: 2020 2020 2320 692e 652e 2073 656c 662e      # i.e. self.
+00008690: 6d79 6d6f 6475 6c65 203d 204d 794d 6f64  mymodule = MyMod
+000086a0: 756c 6528 2e2e 2e29 0a20 2020 2020 2073  ule(...).      s
+000086b0: 656c 662e 6e61 6d65 3a20 4f70 7469 6f6e  elf.name: Option
+000086c0: 616c 5b73 7472 5d0a 2020 2020 2020 6966  al[str].      if
+000086d0: 2073 656c 662e 7061 7265 6e74 2e5f 7374   self.parent._st
+000086e0: 6174 652e 696e 5f73 6574 7570 2061 6e64  ate.in_setup and
+000086f0: 2073 656c 662e 6e61 6d65 2069 7320 4e6f   self.name is No
+00008700: 6e65 3a20 2023 2070 7974 7970 653a 2064  ne:  # pytype: d
+00008710: 6973 6162 6c65 3d61 7474 7269 6275 7465  isable=attribute
+00008720: 2d65 7272 6f72 0a20 2020 2020 2020 2072  -error.        r
+00008730: 6574 7572 6e0a 2020 2020 2020 6966 206e  eturn.      if n
+00008740: 6f74 2073 656c 662e 7061 7265 6e74 2e5f  ot self.parent._
+00008750: 696e 6974 6961 6c69 7a61 7469 6f6e 5f61  initialization_a
+00008760: 6c6c 6f77 6564 3a0a 2020 2020 2020 2020  llowed:.        
+00008770: 7261 6973 6520 6572 726f 7273 2e41 7373  raise errors.Ass
+00008780: 6967 6e53 7562 4d6f 6475 6c65 4572 726f  ignSubModuleErro
+00008790: 7228 7365 6c66 2e5f 5f63 6c61 7373 5f5f  r(self.__class__
+000087a0: 2e5f 5f6e 616d 655f 5f29 0a20 2020 2020  .__name__).     
+000087b0: 2023 2041 7574 6f6e 616d 696e 6720 6f66   # Autonaming of
+000087c0: 2073 7562 6d6f 6475 6c65 732e 0a20 2020   submodules..   
+000087d0: 2020 2069 6620 7365 6c66 2e6e 616d 6520     if self.name 
+000087e0: 6973 204e 6f6e 653a 2020 2320 7079 7479  is None:  # pyty
+000087f0: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
+00008800: 6962 7574 652d 6572 726f 720a 2020 2020  ibute-error.    
+00008810: 2020 2020 7072 6566 6978 203d 2066 277b      prefix = f'{
+00008820: 7365 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f  self.__class__._
+00008830: 5f6e 616d 655f 5f7d 270a 2020 2020 2020  _name__}'.      
+00008840: 2020 6375 7273 6f72 203d 2073 656c 662e    cursor = self.
+00008850: 7061 7265 6e74 2e5f 7374 6174 652e 6175  parent._state.au
+00008860: 746f 6e61 6d65 5f63 7572 736f 722e 6765  toname_cursor.ge
+00008870: 7428 7072 6566 6978 2c20 3029 0a20 2020  t(prefix, 0).   
+00008880: 2020 2020 2073 656c 662e 6e61 6d65 203d       self.name =
+00008890: 2066 277b 7072 6566 6978 7d5f 7b63 7572   f'{prefix}_{cur
+000088a0: 736f 727d 270a 2020 2020 2020 2020 7365  sor}'.        se
+000088b0: 6c66 2e70 6172 656e 742e 5f73 7461 7465  lf.parent._state
+000088c0: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
+000088d0: 5b70 7265 6669 785d 203d 2063 7572 736f  [prefix] = curso
+000088e0: 7220 2b20 310a 2020 2020 2020 2320 416c  r + 1.      # Al
+000088f0: 6c6f 7720 7363 6f70 6520 616c 6961 7369  low scope aliasi
+00008900: 6e67 2075 6e64 6572 2074 7261 6e73 666f  ng under transfo
+00008910: 726d 7320 666f 7220 7375 626d 6f64 756c  rms for submodul
+00008920: 6573 2064 6566 696e 6564 2069 6e20 7365  es defined in se
+00008930: 7475 702e 0a20 2020 2020 2072 6575 7365  tup..      reuse
+00008940: 5f73 636f 7065 7320 3d20 2873 656c 662e  _scopes = (self.
+00008950: 7061 7265 6e74 2e5f 7374 6174 652e 696e  parent._state.in
+00008960: 5f73 6574 7570 2061 6e64 0a20 2020 2020  _setup and.     
+00008970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008980: 2073 656c 662e 7061 7265 6e74 2e5f 7374   self.parent._st
+00008990: 6174 652e 7365 7475 705f 6361 6c6c 6564  ate.setup_called
+000089a0: 203d 3d20 5365 7475 7053 7461 7465 2e54   == SetupState.T
+000089b0: 5241 4e53 464f 524d 4544 290a 2020 2020  RANSFORMED).    
+000089c0: 2020 2320 5065 7266 6f72 6d20 6e61 6d65    # Perform name
+000089d0: 2d63 6f6c 6c69 7369 6f6e 2063 6865 636b  -collision check
+000089e0: 2e0a 2020 2020 2020 6966 2073 656c 662e  ..      if self.
+000089f0: 7061 7265 6e74 2e5f 6e61 6d65 5f74 616b  parent._name_tak
+00008a00: 656e 2873 656c 662e 6e61 6d65 2c20 7365  en(self.name, se
+00008a10: 6c66 2c20 7265 7573 655f 7363 6f70 6573  lf, reuse_scopes
+00008a20: 3d72 6575 7365 5f73 636f 7065 7329 3a0a  =reuse_scopes):.
+00008a30: 2020 2020 2020 2020 7061 7265 6e74 5f63          parent_c
+00008a40: 6c61 7373 203d 2073 656c 662e 7061 7265  lass = self.pare
+00008a50: 6e74 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e  nt.__class__.__n
+00008a60: 616d 655f 5f0a 2020 2020 2020 2020 7261  ame__.        ra
+00008a70: 6973 6520 6572 726f 7273 2e4e 616d 6549  ise errors.NameI
+00008a80: 6e55 7365 4572 726f 7228 2773 7562 6d6f  nUseError('submo
+00008a90: 6475 6c65 272c 2073 656c 662e 6e61 6d65  dule', self.name
+00008aa0: 2c20 7061 7265 6e74 5f63 6c61 7373 290a  , parent_class).
+00008ab0: 2020 2020 2020 2320 4669 6e61 6c69 7a65        # Finalize
+00008ac0: 2061 7474 6163 686d 656e 7420 746f 2070   attachment to p
+00008ad0: 6172 656e 7420 616e 6420 7363 6f70 6520  arent and scope 
+00008ae0: 696e 6974 6961 6c69 7a61 7469 6f6e 2e0a  initialization..
+00008af0: 2020 2020 2020 7365 6c66 2e70 6172 656e        self.paren
+00008b00: 742e 5f73 7461 7465 2e63 6869 6c64 7265  t._state.childre
+00008b10: 6e5b 7365 6c66 2e6e 616d 655d 203d 2073  n[self.name] = s
+00008b20: 656c 660a 2020 2020 2020 6173 7365 7274  elf.      assert
+00008b30: 2073 656c 662e 7061 7265 6e74 2e73 636f   self.parent.sco
+00008b40: 7065 2069 7320 6e6f 7420 4e6f 6e65 0a20  pe is not None. 
+00008b50: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
+00008b60: 7461 7474 725f 5f28 0a20 2020 2020 2020  tattr__(.       
+00008b70: 2020 2073 656c 662c 2027 7363 6f70 6527     self, 'scope'
+00008b80: 2c20 7365 6c66 2e70 6172 656e 742e 7363  , self.parent.sc
+00008b90: 6f70 652e 7075 7368 2873 656c 662e 6e61  ope.push(self.na
+00008ba0: 6d65 2c20 7265 7573 653d 7265 7573 655f  me, reuse=reuse_
+00008bb0: 7363 6f70 6573 2929 0a0a 2020 2020 2320  scopes))..    # 
+00008bc0: 546f 702d 6c65 7665 6c20 696e 766f 6361  Top-level invoca
+00008bd0: 7469 6f6e 2077 6974 6820 6120 6675 6e63  tion with a func
+00008be0: 7469 6f6e 616c 2053 636f 7065 2e0a 2020  tional Scope..  
+00008bf0: 2020 656c 6966 2069 7369 6e73 7461 6e63    elif isinstanc
+00008c00: 6528 7365 6c66 2e70 6172 656e 742c 2053  e(self.parent, S
+00008c10: 636f 7065 293a 0a20 2020 2020 206f 626a  cope):.      obj
+00008c20: 6563 742e 5f5f 7365 7461 7474 725f 5f28  ect.__setattr__(
+00008c30: 7365 6c66 2c20 2773 636f 7065 272c 2073  self, 'scope', s
+00008c40: 656c 662e 7061 7265 6e74 290a 2020 2020  elf.parent).    
+00008c50: 656c 7365 3a0a 2020 2020 2020 7261 6973  else:.      rais
+00008c60: 6520 5661 6c75 6545 7272 6f72 2827 7061  e ValueError('pa
+00008c70: 7265 6e74 206d 7573 7420 6265 204e 6f6e  rent must be Non
+00008c80: 652c 204d 6f64 756c 6520 6f72 2053 636f  e, Module or Sco
+00008c90: 7065 2729 0a0a 2020 2020 2320 6561 6765  pe')..    # eage
+00008ca0: 726c 7920 6269 6e64 2073 7562 6d6f 6475  rly bind submodu
+00008cb0: 6c65 7320 6966 2073 636f 7065 2069 7320  les if scope is 
+00008cc0: 6176 6169 6c61 626c 650a 2020 2020 6966  available.    if
+00008cd0: 2073 656c 662e 7363 6f70 6520 6973 206e   self.scope is n
+00008ce0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
+00008cf0: 2066 6f72 2066 6965 6c64 2069 6e20 6461   for field in da
+00008d00: 7461 636c 6173 7365 732e 6669 656c 6473  taclasses.fields
+00008d10: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00008d20: 2020 6966 2066 6965 6c64 2e6e 616d 6520    if field.name 
+00008d30: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
+00008d40: 2c20 276e 616d 6527 2920 616e 6420 6669  , 'name') and fi
+00008d50: 656c 642e 696e 6974 3a0a 2020 2020 2020  eld.init:.      
+00008d60: 2020 2020 2020 7365 6c66 2e5f 7265 6769        self._regi
+00008d70: 7374 6572 5f73 7562 6d6f 6475 6c65 7328  ster_submodules(
+00008d80: 6669 656c 642e 6e61 6d65 2c20 6765 7461  field.name, geta
+00008d90: 7474 7228 7365 6c66 2c20 6669 656c 642e  ttr(self, field.
+00008da0: 6e61 6d65 2929 0a0a 2020 2020 7365 6c66  name))..    self
+00008db0: 2e5f 7374 6174 652e 6973 5f69 6e69 7469  ._state.is_initi
+00008dc0: 616c 697a 6564 203d 2054 7275 650a 0a20  alized = True.. 
+00008dd0: 2064 6566 205f 5f72 6570 725f 5f28 7365   def __repr__(se
+00008de0: 6c66 2920 2d3e 2073 7472 3a0a 2020 2020  lf) -> str:.    
+00008df0: 7265 7475 726e 205f 6d6f 6475 6c65 5f72  return _module_r
+00008e00: 6570 7228 7365 6c66 290a 0a20 2064 6566  epr(self)..  def
+00008e10: 2073 6574 7570 2873 656c 6629 202d 3e20   setup(self) -> 
+00008e20: 4e6f 6e65 3a0a 2020 2020 2222 2249 6e69  None:.    """Ini
+00008e30: 7469 616c 697a 6573 2061 204d 6f64 756c  tializes a Modul
+00008e40: 6520 6c61 7a69 6c79 2028 7369 6d69 6c61  e lazily (simila
+00008e50: 7220 746f 2061 206c 617a 7920 6060 5f5f  r to a lazy ``__
+00008e60: 696e 6974 5f5f 6060 292e 0a0a 2020 2020  init__``)...    
+00008e70: 6060 7365 7475 7060 6020 6973 2063 616c  ``setup`` is cal
+00008e80: 6c65 6420 6f6e 6365 206c 617a 696c 7920  led once lazily 
+00008e90: 6f6e 2061 206d 6f64 756c 6520 696e 7374  on a module inst
+00008ea0: 616e 6365 2077 6865 6e20 6120 6d6f 6475  ance when a modu
+00008eb0: 6c65 0a20 2020 2069 7320 626f 756e 642c  le.    is bound,
+00008ec0: 2069 6d6d 6564 6961 7465 6c79 2062 6566   immediately bef
+00008ed0: 6f72 6520 616e 7920 6f74 6865 7220 6d65  ore any other me
+00008ee0: 7468 6f64 7320 6c69 6b65 2060 605f 5f63  thods like ``__c
+00008ef0: 616c 6c5f 5f60 6020 6172 650a 2020 2020  all__`` are.    
+00008f00: 696e 766f 6b65 642c 206f 7220 6265 666f  invoked, or befo
+00008f10: 7265 2061 2060 6073 6574 7570 6060 2d64  re a ``setup``-d
+00008f20: 6566 696e 6564 2061 7474 7269 6275 7465  efined attribute
+00008f30: 206f 6e20 6073 656c 6660 2069 7320 6163   on `self` is ac
+00008f40: 6365 7373 6564 2e0a 0a20 2020 2054 6869  cessed...    Thi
+00008f50: 7320 6361 6e20 6861 7070 656e 2069 6e20  s can happen in 
+00008f60: 7468 7265 6520 6361 7365 733a 0a0a 2020  three cases:..  
+00008f70: 2020 2020 312e 2049 6d6d 6564 6961 7465      1. Immediate
+00008f80: 6c79 2077 6865 6e20 696e 766f 6b69 6e67  ly when invoking
+00008f90: 203a 6d65 7468 3a60 6170 706c 7960 2c20   :meth:`apply`, 
+00008fa0: 3a6d 6574 683a 6069 6e69 7460 206f 720a  :meth:`init` or.
+00008fb0: 2020 2020 2020 2020 203a 6d65 7468 3a60           :meth:`
+00008fc0: 696e 6974 5f61 6e64 5f6f 7574 7075 7460  init_and_output`
+00008fd0: 2e0a 0a20 2020 2020 2032 2e20 4f6e 6365  ...      2. Once
+00008fe0: 2074 6865 206d 6f64 756c 6520 6973 2067   the module is g
+00008ff0: 6976 656e 2061 206e 616d 6520 6279 2062  iven a name by b
+00009000: 6569 6e67 2061 7373 6967 6e65 6420 746f  eing assigned to
+00009010: 2061 6e20 6174 7472 6962 7574 6520 6f66   an attribute of
+00009020: 0a20 2020 2020 2020 2020 616e 6f74 6865  .         anothe
+00009030: 7220 6d6f 6475 6c65 2069 6e73 6964 6520  r module inside 
+00009040: 7468 6520 6f74 6865 7220 6d6f 6475 6c65  the other module
+00009050: 2773 2060 6073 6574 7570 6060 206d 6574  's ``setup`` met
+00009060: 686f 640a 2020 2020 2020 2020 2028 7365  hod.         (se
+00009070: 6520 3a6d 6574 683a 605f 5f73 6574 6174  e :meth:`__setat
+00009080: 7472 5f5f 6029 3a3a 0a0a 2020 2020 2020  tr__`)::..      
+00009090: 2020 2020 2063 6c61 7373 204d 794d 6f64       class MyMod
+000090a0: 756c 6528 6e6e 2e4d 6f64 756c 6529 3a0a  ule(nn.Module):.
+000090b0: 2020 2020 2020 2020 2020 2020 2064 6566               def
+000090c0: 2073 6574 7570 2873 656c 6629 3a0a 2020   setup(self):.  
+000090d0: 2020 2020 2020 2020 2020 2020 2073 7562               sub
+000090e0: 6d6f 6475 6c65 203d 2043 6f6e 7628 2e2e  module = Conv(..
+000090f0: 2e29 0a0a 2020 2020 2020 2020 2020 2020  .)..            
+00009100: 2020 2023 2041 6363 6573 7369 6e67 2060     # Accessing `
+00009110: 7375 626d 6f64 756c 6560 2061 7474 7269  submodule` attri
+00009120: 6275 7465 7320 646f 6573 206e 6f74 2079  butes does not y
+00009130: 6574 2077 6f72 6b20 6865 7265 2e0a 0a20  et work here... 
+00009140: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00009150: 5468 6520 666f 6c6c 6f77 696e 6720 6c69  The following li
+00009160: 6e65 2069 6e76 6f6b 6573 2060 7365 6c66  ne invokes `self
+00009170: 2e5f 5f73 6574 6174 7472 5f5f 602c 2077  .__setattr__`, w
+00009180: 6869 6368 2067 6976 6573 0a20 2020 2020  hich gives.     
+00009190: 2020 2020 2020 2020 2020 2320 6073 7562            # `sub
+000091a0: 6d6f 6475 6c65 6020 7468 6520 6e61 6d65  module` the name
+000091b0: 2022 636f 6e76 3122 2e0a 2020 2020 2020   "conv1"..      
+000091c0: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
+000091d0: 6e76 3120 3d20 7375 626d 6f64 756c 650a  nv1 = submodule.
+000091e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000091f0: 2320 4163 6365 7373 696e 6720 6073 7562  # Accessing `sub
+00009200: 6d6f 6475 6c65 6020 6174 7472 6962 7574  module` attribut
+00009210: 6573 206f 7220 6d65 7468 6f64 7320 6973  es or methods is
+00009220: 206e 6f77 2073 6166 6520 616e 640a 2020   now safe and.  
+00009230: 2020 2020 2020 2020 2020 2020 2023 2065               # e
+00009240: 6974 6865 7220 6361 7573 6573 2073 6574  ither causes set
+00009250: 7570 2829 2074 6f20 6265 2063 616c 6c65  up() to be calle
+00009260: 6420 6f6e 6365 2e0a 0a20 2020 2020 2033  d once...      3
+00009270: 2e20 4f6e 6365 2061 206d 6f64 756c 6520  . Once a module 
+00009280: 6973 2063 6f6e 7374 7275 6374 6564 2069  is constructed i
+00009290: 6e73 6964 6520 6120 6d65 7468 6f64 2077  nside a method w
+000092a0: 7261 7070 6564 2077 6974 680a 2020 2020  rapped with.    
+000092b0: 2020 2020 203a 6d65 7468 3a60 636f 6d70       :meth:`comp
+000092c0: 6163 7460 2c20 696d 6d65 6469 6174 656c  act`, immediatel
+000092d0: 7920 6265 666f 7265 2061 6e6f 7468 6572  y before another
+000092e0: 206d 6574 686f 6420 6973 2063 616c 6c65   method is calle
+000092f0: 6420 6f72 0a20 2020 2020 2020 2020 6060  d or.         ``
+00009300: 7365 7475 7060 6020 6465 6669 6e65 6420  setup`` defined 
+00009310: 6174 7472 6962 7574 6520 6973 2061 6363  attribute is acc
+00009320: 6573 7365 642e 0a20 2020 2022 2222 0a20  essed..    """. 
+00009330: 2020 2070 6173 730a 0a20 2064 6566 205f     pass..  def _
+00009340: 7265 6769 7374 6572 5f73 7562 6d6f 6475  register_submodu
+00009350: 6c65 7328 7365 6c66 2c20 6e61 6d65 2c20  les(self, name, 
+00009360: 7661 6c29 3a0a 2020 2020 2222 2252 6567  val):.    """Reg
+00009370: 6973 7465 7273 2061 2073 7562 6d6f 6475  isters a submodu
+00009380: 6c65 2e22 2222 0a20 2020 2061 7373 6572  le.""".    asser
+00009390: 7420 7365 6c66 2e73 636f 7065 2c20 2754  t self.scope, 'T
+000093a0: 7279 696e 6720 746f 2072 6567 6973 7465  rying to registe
+000093b0: 7220 7375 626d 6f64 756c 6573 206f 6e20  r submodules on 
+000093c0: 756e 626f 756e 6420 7363 6f70 652e 270a  unbound scope.'.
+000093d0: 2020 2020 726f 6f74 203d 2073 656c 662e      root = self.
+000093e0: 7363 6f70 652e 726f 6f74 0a20 2020 2063  scope.root.    c
+000093f0: 6163 6865 203d 205f 6361 6368 6573 2e67  ache = _caches.g
+00009400: 6574 2872 6f6f 742c 2077 6561 6b72 6566  et(root, weakref
+00009410: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
+00009420: 6e61 7279 2829 290a 2020 2020 5f63 6163  nary()).    _cac
+00009430: 6865 735b 726f 6f74 5d20 3d20 6361 6368  hes[root] = cach
+00009440: 650a 2020 2020 7175 6575 6520 3d20 5b5d  e.    queue = []
+00009450: 0a20 2020 2070 7265 7365 7276 655f 6164  .    preserve_ad
+00009460: 6f70 7465 645f 6e61 6d65 7320 3d20 636f  opted_names = co
+00009470: 6e66 6967 2e66 6c61 785f 7072 6573 6572  nfig.flax_preser
+00009480: 7665 5f61 646f 7074 6564 5f6e 616d 6573  ve_adopted_names
+00009490: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
+000094a0: 7479 7065 2873 656c 6629 2c20 2770 7265  type(self), 'pre
+000094b0: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
+000094c0: 6d65 7327 293a 0a20 2020 2020 2070 7265  mes'):.      pre
+000094d0: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
+000094e0: 6d65 7320 3d20 7479 7065 2873 656c 6629  mes = type(self)
+000094f0: 2e70 7265 7365 7276 655f 6164 6f70 7465  .preserve_adopte
+00009500: 645f 6e61 6d65 730a 2020 2020 6465 6620  d_names.    def 
+00009510: 6164 6f70 745f 6174 7472 5f6d 6f64 756c  adopt_attr_modul
+00009520: 6573 2863 6163 6865 2c20 7175 6575 652c  es(cache, queue,
+00009530: 2073 7566 6669 782c 2073 7562 7661 6c75   suffix, subvalu
+00009540: 6529 3a0a 2020 2020 2020 6966 2069 7369  e):.      if isi
+00009550: 6e73 7461 6e63 6528 7375 6276 616c 7565  nstance(subvalue
+00009560: 2c20 4d6f 6475 6c65 293a 0a20 2020 2020  , Module):.     
+00009570: 2020 2061 646f 7074 6564 5f6e 616d 6520     adopted_name 
+00009580: 3d20 4e6f 6e65 0a20 2020 2020 2020 2069  = None.        i
+00009590: 6620 7375 6276 616c 7565 2e70 6172 656e  f subvalue.paren
+000095a0: 7420 6973 204e 6f6e 653a 0a20 2020 2020  t is None:.     
+000095b0: 2020 2020 2023 2050 7265 7365 7276 6520       # Preserve 
+000095c0: 7368 6172 696e 672d 6279 2d72 6566 6572  sharing-by-refer
+000095d0: 656e 6365 2072 656c 6174 696f 6e73 6869  ence relationshi
+000095e0: 7073 2064 7572 696e 6720 6164 6f70 7469  ps during adopti
+000095f0: 6f6e 0a20 2020 2020 2020 2020 2023 2076  on.          # v
+00009600: 6961 2063 6163 6865 206b 6579 6564 206f  ia cache keyed o
+00009610: 6e20 756e 6971 7565 2069 6e73 7461 6e63  n unique instanc
+00009620: 6520 6964 732e 0a20 2020 2020 2020 2020  e ids..         
+00009630: 206b 6579 203d 2073 7562 7661 6c75 652e   key = subvalue.
+00009640: 5f69 640a 2020 2020 2020 2020 2020 2320  _id.          # 
+00009650: 4d6f 6475 6c65 2077 6173 2070 6173 7365  Module was passe
+00009660: 6420 6672 6f6d 206f 7574 7369 6465 2e20  d from outside. 
+00009670: 4974 206e 6565 6473 2074 6f20 6265 2063  It needs to be c
+00009680: 6c6f 6e65 642e 0a20 2020 2020 2020 2020  loned..         
+00009690: 2023 204f 7574 7369 6465 206d 6f64 756c   # Outside modul
+000096a0: 6573 2061 7265 206e 616d 6564 2062 7920  es are named by 
+000096b0: 6174 7461 6368 6d65 6e74 2c20 6e6f 7420  attachment, not 
+000096c0: 616e 206f 7574 6572 206e 616d 652c 0a20  an outer name,. 
+000096d0: 2020 2020 2020 2020 2023 2055 4e4c 4553           # UNLES
+000096e0: 5320 7765 2772 6520 7573 696e 6720 6e65  S we're using ne
+000096f0: 7720 6164 6f70 7465 6420 6e61 6d65 2070  w adopted name p
+00009700: 6f6c 6963 792c 2069 6e20 7768 6963 6820  olicy, in which 
+00009710: 6361 7365 2061 6e20 6578 6973 7469 6e67  case an existing
+00009720: 0a20 2020 2020 2020 2020 2023 206e 616d  .          # nam
+00009730: 6520 7769 6c6c 2062 6520 7573 6564 2c20  e will be used, 
+00009740: 6173 2069 7320 6f66 7465 6e20 7375 7070  as is often supp
+00009750: 6c69 6564 2062 7920 636f 6e66 6967 2073  lied by config s
+00009760: 7973 7465 6d73 2e0a 2020 2020 2020 2020  ystems..        
+00009770: 2020 6966 2070 7265 7365 7276 655f 6164    if preserve_ad
+00009780: 6f70 7465 645f 6e61 6d65 733a 0a20 2020  opted_names:.   
+00009790: 2020 2020 2020 2020 2061 646f 7074 6564           adopted
+000097a0: 5f6e 616d 6520 3d20 6f62 6a65 6374 2e5f  _name = object._
+000097b0: 5f67 6574 6174 7472 6962 7574 655f 5f28  _getattribute__(
+000097c0: 7375 6276 616c 7565 2c20 276e 616d 6527  subvalue, 'name'
+000097d0: 290a 2020 2020 2020 2020 2020 6966 206b  ).          if k
+000097e0: 6579 2069 6e20 6361 6368 653a 0a20 2020  ey in cache:.   
+000097f0: 2020 2020 2020 2020 2073 7562 7661 6c75           subvalu
+00009800: 6520 3d20 6361 6368 655b 6b65 795d 0a20  e = cache[key]. 
+00009810: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+00009820: 2020 2020 2020 2020 2020 2073 7562 7661             subva
+00009830: 6c75 6520 3d20 7375 6276 616c 7565 2e63  lue = subvalue.c
+00009840: 6c6f 6e65 286e 616d 653d 4e6f 6e65 290a  lone(name=None).
+00009850: 2020 2020 2020 2020 2020 2020 6361 6368              cach
+00009860: 655b 6b65 795d 203d 2073 7562 7661 6c75  e[key] = subvalu
+00009870: 650a 2020 2020 2020 2020 6966 2073 7562  e.        if sub
+00009880: 7661 6c75 652e 6e61 6d65 2069 7320 4e6f  value.name is No
+00009890: 6e65 3a0a 2020 2020 2020 2020 2020 6f62  ne:.          ob
+000098a0: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
+000098b0: 2873 7562 7661 6c75 652c 2027 7061 7265  (subvalue, 'pare
+000098c0: 6e74 272c 2073 656c 6629 0a20 2020 2020  nt', self).     
+000098d0: 2020 2020 2069 6620 6164 6f70 7465 645f       if adopted_
+000098e0: 6e61 6d65 2069 7320 4e6f 6e65 3a0a 2020  name is None:.  
+000098f0: 2020 2020 2020 2020 2020 6164 6f70 7465            adopte
+00009900: 645f 6e61 6d65 203d 2066 277b 6e61 6d65  d_name = f'{name
+00009910: 7d7b 7375 6666 6978 7d27 0a20 2020 2020  }{suffix}'.     
+00009920: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
+00009930: 7461 7474 725f 5f28 7375 6276 616c 7565  tattr__(subvalue
+00009940: 2c20 276e 616d 6527 2c20 6164 6f70 7465  , 'name', adopte
+00009950: 645f 6e61 6d65 290a 2020 2020 2020 2020  d_name).        
+00009960: 2020 7175 6575 652e 6170 7065 6e64 2873    queue.append(s
+00009970: 7562 7661 6c75 6529 0a20 2020 2020 2072  ubvalue).      r
+00009980: 6574 7572 6e20 7375 6276 616c 7565 0a20  eturn subvalue. 
+00009990: 2020 2076 616c 203d 205f 6672 6565 7a65     val = _freeze
+000099a0: 5f61 7474 7228 5f6d 6170 5f6f 7665 725f  _attr(_map_over_
+000099b0: 6d6f 6475 6c65 735f 696e 5f74 7265 6528  modules_in_tree(
+000099c0: 0a20 2020 2020 2020 2066 756e 6374 6f6f  .        functoo
+000099d0: 6c73 2e70 6172 7469 616c 2861 646f 7074  ls.partial(adopt
+000099e0: 5f61 7474 725f 6d6f 6475 6c65 732c 2063  _attr_modules, c
+000099f0: 6163 6865 2c20 7175 6575 6529 2c20 7661  ache, queue), va
+00009a00: 6c29 290a 2020 2020 6f62 6a65 6374 2e5f  l)).    object._
+00009a10: 5f73 6574 6174 7472 5f5f 2873 656c 662c  _setattr__(self,
+00009a20: 206e 616d 652c 2076 616c 290a 2020 2020   name, val).    
+00009a30: 666f 7220 7820 696e 2071 7565 7565 3a0a  for x in queue:.
+00009a40: 2020 2020 2020 782e 5f5f 706f 7374 5f69        x.__post_i
+00009a50: 6e69 745f 5f28 290a 0a20 2064 6566 205f  nit__()..  def _
+00009a60: 7472 795f 7365 7475 7028 7365 6c66 2c20  try_setup(self, 
+00009a70: 7368 616c 6c6f 773a 2062 6f6f 6c20 3d20  shallow: bool = 
+00009a80: 4661 6c73 6529 202d 3e20 4e6f 6e65 3a0a  False) -> None:.
+00009a90: 2020 2020 2222 2254 7269 6573 2074 6f20      """Tries to 
+00009aa0: 7365 7475 7020 6d6f 6475 6c65 2069 6620  setup module if 
+00009ab0: 7363 6f70 6520 6973 2061 7661 696c 6162  scope is availab
+00009ac0: 6c65 2061 6e64 2073 6574 7570 2068 6173  le and setup has
+00009ad0: 206e 6f74 2062 6565 6e20 6361 6c6c 6564   not been called
+00009ae0: 2079 6574 2e22 2222 0a20 2020 2069 6620   yet.""".    if 
+00009af0: 2873 656c 662e 7363 6f70 650a 2020 2020  (self.scope.    
+00009b00: 2020 2020 616e 6420 6e6f 7420 7365 6c66      and not self
+00009b10: 2e5f 7374 6174 652e 696e 5f73 6574 7570  ._state.in_setup
+00009b20: 0a20 2020 2020 2020 2061 6e64 2073 656c  .        and sel
+00009b30: 662e 5f73 7461 7465 2e73 6574 7570 5f63  f._state.setup_c
+00009b40: 616c 6c65 6420 213d 2053 6574 7570 5374  alled != SetupSt
+00009b50: 6174 652e 444f 4e45 293a 0a20 2020 2020  ate.DONE):.     
+00009b60: 2074 7279 3a0a 2020 2020 2020 2020 7365   try:.        se
+00009b70: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
+00009b80: 7570 203d 2054 7275 650a 2020 2020 2020  up = True.      
+00009b90: 2020 2320 4120 7368 616c 6c6f 7720 7365    # A shallow se
+00009ba0: 7475 7020 7769 6c6c 206f 6e6c 7920 7265  tup will only re
+00009bb0: 6769 7374 6572 2061 7474 7269 6275 7465  gister attribute
+00009bc0: 2073 7562 6d6f 6475 6c65 7320 6275 7420   submodules but 
+00009bd0: 6974 2064 6f65 730a 2020 2020 2020 2020  it does.        
+00009be0: 2320 6e6f 7420 6361 6c6c 2074 6865 2075  # not call the u
+00009bf0: 7365 7227 7320 7365 7475 702e 2054 6869  ser's setup. Thi
+00009c00: 7320 6176 6f69 6473 2072 756e 6e69 6e67  s avoids running
+00009c10: 2062 6566 6f72 6520 610a 2020 2020 2020   before a.      
+00009c20: 2020 2320 7472 616e 7366 6f72 6d61 7469    # transformati
+00009c30: 6f6e 2e0a 2020 2020 2020 2020 666f 7220  on..        for 
+00009c40: 6669 656c 6420 696e 2064 6174 6163 6c61  field in datacla
+00009c50: 7373 6573 2e66 6965 6c64 7328 7365 6c66  sses.fields(self
+00009c60: 293a 0a20 2020 2020 2020 2020 2069 6620  ):.          if 
+00009c70: 6669 656c 642e 6e61 6d65 206e 6f74 2069  field.name not i
+00009c80: 6e20 2827 7061 7265 6e74 272c 2027 6e61  n ('parent', 'na
+00009c90: 6d65 2729 2061 6e64 2066 6965 6c64 2e69  me') and field.i
+00009ca0: 6e69 743a 0a20 2020 2020 2020 2020 2020  nit:.           
+00009cb0: 2073 656c 662e 5f72 6567 6973 7465 725f   self._register_
+00009cc0: 7375 626d 6f64 756c 6573 2866 6965 6c64  submodules(field
+00009cd0: 2e6e 616d 652c 2067 6574 6174 7472 2873  .name, getattr(s
+00009ce0: 656c 662c 2066 6965 6c64 2e6e 616d 6529  elf, field.name)
+00009cf0: 290a 2020 2020 2020 2020 6966 206e 6f74  ).        if not
+00009d00: 2073 6861 6c6c 6f77 3a0a 2020 2020 2020   shallow:.      
+00009d10: 2020 2020 7365 6c66 2e73 6574 7570 2829      self.setup()
+00009d20: 0a20 2020 2020 2020 2023 2057 6520 7275  .        # We ru
+00009d30: 6e20 7374 6174 6963 2063 6865 636b 7320  n static checks 
+00009d40: 6162 7374 7261 6374 6c79 206f 6e63 6520  abstractly once 
+00009d50: 666f 7220 7365 7475 7020 6265 666f 7265  for setup before
+00009d60: 2061 6e79 2074 7261 6e73 666f 726d 730a   any transforms.
+00009d70: 2020 2020 2020 2020 2320 746f 2064 6574          # to det
+00009d80: 6563 7420 6e61 6d65 2063 6f6c 6c69 7369  ect name collisi
+00009d90: 6f6e 7320 616e 6420 6f74 6865 7220 7079  ons and other py
+00009da0: 7468 6f6e 2065 7272 6f72 732e 0a20 2020  thon errors..   
+00009db0: 2020 2020 2065 6c69 6620 7365 6c66 2e5f       elif self._
+00009dc0: 7374 6174 652e 7365 7475 705f 6361 6c6c  state.setup_call
+00009dd0: 6564 203d 3d20 5365 7475 7053 7461 7465  ed == SetupState
+00009de0: 2e4e 4557 3a0a 2020 2020 2020 2020 2020  .NEW:.          
+00009df0: 7365 6c66 2e5f 7661 6c69 6461 7465 5f73  self._validate_s
+00009e00: 6574 7570 2829 0a20 2020 2020 2066 696e  etup().      fin
+00009e10: 616c 6c79 3a0a 2020 2020 2020 2020 7365  ally:.        se
+00009e20: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
+00009e30: 7570 203d 2046 616c 7365 0a20 2020 2020  up = False.     
+00009e40: 2020 2069 6620 6e6f 7420 7368 616c 6c6f     if not shallo
+00009e50: 773a 0a20 2020 2020 2020 2020 2073 656c  w:.          sel
+00009e60: 662e 5f73 7461 7465 2e73 6574 7570 5f63  f._state.setup_c
+00009e70: 616c 6c65 6420 3d20 5365 7475 7053 7461  alled = SetupSta
+00009e80: 7465 2e44 4f4e 450a 0a20 2064 6566 205f  te.DONE..  def _
+00009e90: 7661 6c69 6461 7465 5f73 6574 7570 2873  validate_setup(s
+00009ea0: 656c 6629 202d 3e20 4e6f 6e65 3a0a 2020  elf) -> None:.  
+00009eb0: 2020 2222 2241 6273 7472 6163 746c 7920    """Abstractly 
+00009ec0: 6576 616c 7561 7465 7320 7365 7475 7020  evaluates setup 
+00009ed0: 6f6e 6c79 2074 6f20 7275 6e20 7374 6174  only to run stat
+00009ee0: 6963 2063 6865 636b 732e 2222 220a 2020  ic checks.""".  
+00009ef0: 2020 6465 6620 7275 6e5f 7365 7475 705f    def run_setup_
+00009f00: 6f6e 6c79 2878 293a 0a20 2020 2020 2077  only(x):.      w
+00009f10: 7261 7070 6564 5f69 6420 3d20 7772 6170  rapped_id = wrap
+00009f20: 5f6d 6574 686f 645f 6f6e 6365 286c 616d  _method_once(lam
+00009f30: 6264 6120 6d2c 2078 3a20 7829 0a20 2020  bda m, x: x).   
+00009f40: 2020 2077 6974 6820 5465 7374 5363 6f70     with TestScop
+00009f50: 6528 7b7d 2c20 726e 6773 3d7b 7d2c 206d  e({}, rngs={}, m
+00009f60: 7574 6162 6c65 3d54 7275 6529 2e74 656d  utable=True).tem
+00009f70: 706f 7261 7279 2829 2061 7320 726f 6f74  porary() as root
+00009f80: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
+00009f90: 2077 7261 7070 6564 5f69 6428 7365 6c66   wrapped_id(self
+00009fa0: 2e63 6c6f 6e65 2870 6172 656e 743d 726f  .clone(parent=ro
+00009fb0: 6f74 292c 2078 290a 2020 2020 5f20 3d20  ot), x).    _ = 
+00009fc0: 6a61 782e 6576 616c 5f73 6861 7065 2872  jax.eval_shape(r
+00009fd0: 756e 5f73 6574 7570 5f6f 6e6c 792c 2030  un_setup_only, 0
+00009fe0: 290a 0a20 2064 6566 205f 6e61 6d65 5f74  )..  def _name_t
+00009ff0: 616b 656e 2873 656c 662c 0a20 2020 2020  aken(self,.     
+0000a000: 2020 2020 2020 2020 2020 2020 206e 616d               nam
+0000a010: 653a 2073 7472 2c0a 2020 2020 2020 2020  e: str,.        
+0000a020: 2020 2020 2020 2020 2020 6d6f 6475 6c65            module
+0000a030: 3a20 4f70 7469 6f6e 616c 5b27 4d6f 6475  : Optional['Modu
+0000a040: 6c65 275d 203d 204e 6f6e 652c 0a20 2020  le'] = None,.   
+0000a050: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+0000a060: 6575 7365 5f73 636f 7065 733a 2062 6f6f  euse_scopes: boo
+0000a070: 6c20 3d20 4661 6c73 652c 0a20 2020 2020  l = False,.     
+0000a080: 2020 2020 2020 2020 2020 2020 2063 6f6c               col
+0000a090: 6c65 6374 696f 6e3a 204f 7074 696f 6e61  lection: Optiona
+0000a0a0: 6c5b 7374 725d 203d 204e 6f6e 6529 202d  l[str] = None) -
+0000a0b0: 3e20 626f 6f6c 3a0a 2020 2020 6173 7365  > bool:.    asse
+0000a0c0: 7274 2073 656c 662e 7363 6f70 6520 6973  rt self.scope is
+0000a0d0: 206e 6f74 204e 6f6e 650a 2020 2020 6966   not None.    if
+0000a0e0: 2072 6575 7365 5f73 636f 7065 733a 0a20   reuse_scopes:. 
+0000a0f0: 2020 2020 2072 6574 7572 6e20 4661 6c73       return Fals
+0000a100: 650a 2020 2020 7265 7475 726e 2073 656c  e.    return sel
+0000a110: 662e 7363 6f70 652e 6e61 6d65 5f72 6573  f.scope.name_res
+0000a120: 6572 7665 6428 6e61 6d65 2c20 636f 6c6c  erved(name, coll
+0000a130: 6563 7469 6f6e 290a 0a20 2040 7072 6f70  ection)..  @prop
+0000a140: 6572 7479 0a20 2064 6566 205f 696e 6974  erty.  def _init
+0000a150: 6961 6c69 7a61 7469 6f6e 5f61 6c6c 6f77  ialization_allow
+0000a160: 6564 2873 656c 6629 3a0a 2020 2020 7265  ed(self):.    re
+0000a170: 7475 726e 2028 6e6f 7420 7365 6c66 2e5f  turn (not self._
+0000a180: 7374 6174 652e 6973 5f69 6e69 7469 616c  state.is_initial
+0000a190: 697a 6564 2020 2320 616c 6c6f 7720 6561  ized  # allow ea
+0000a1a0: 6765 7220 6174 7461 6368 6d65 6e74 2069  ger attachment i
+0000a1b0: 6e20 706f 7374 2d69 6e69 740a 2020 2020  n post-init.    
+0000a1c0: 2020 2020 2020 2020 6f72 2073 656c 662e          or self.
+0000a1d0: 5f73 7461 7465 2e69 6e5f 7365 7475 700a  _state.in_setup.
+0000a1e0: 2020 2020 2020 2020 2020 2020 6f72 2073              or s
+0000a1f0: 656c 662e 5f73 7461 7465 2e69 6e5f 636f  elf._state.in_co
+0000a200: 6d70 6163 745f 6d65 7468 6f64 290a 0a20  mpact_method).. 
+0000a210: 2064 6566 2063 6c6f 6e65 2873 656c 663a   def clone(self:
+0000a220: 204d 2c20 2a2c 0a20 2020 2020 2020 2020   M, *,.         
+0000a230: 2020 2070 6172 656e 743a 204f 7074 696f     parent: Optio
+0000a240: 6e61 6c5b 556e 696f 6e5b 5363 6f70 652c  nal[Union[Scope,
+0000a250: 2027 4d6f 6475 6c65 275d 5d20 3d20 4e6f   'Module']] = No
+0000a260: 6e65 2c0a 2020 2020 2020 2020 2020 2020  ne,.            
+0000a270: 5f64 6565 705f 636c 6f6e 653a 2055 6e69  _deep_clone: Uni
+0000a280: 6f6e 5b62 6f6f 6c2c 2077 6561 6b72 6566  on[bool, weakref
+0000a290: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
+0000a2a0: 6e61 7279 5d20 3d20 4661 6c73 652c 0a20  nary] = False,. 
 0000a2b0: 2020 2020 2020 2020 2020 202a 2a75 7064             **upd
 0000a2c0: 6174 6573 2920 2d3e 204d 3a0a 2020 2020  ates) -> M:.    
 0000a2d0: 2222 2243 7265 6174 6573 2061 2063 6c6f  """Creates a clo
 0000a2e0: 6e65 206f 6620 7468 6973 204d 6f64 756c  ne of this Modul
 0000a2f0: 652c 2077 6974 6820 6f70 7469 6f6e 616c  e, with optional
 0000a300: 6c79 2075 7064 6174 6564 2061 7267 756d  ly updated argum
 0000a310: 656e 7473 2e0a 0a20 2020 2041 7267 733a  ents...    Args:
 0000a320: 0a20 2020 2020 2070 6172 656e 743a 2054  .      parent: T
 0000a330: 6865 2070 6172 656e 7420 6f66 2074 6865  he parent of the
 0000a340: 2063 6c6f 6e65 2e20 5468 6520 636c 6f6e   clone. The clon
 0000a350: 6520 7769 6c6c 2068 6176 6520 6e6f 2070  e will have no p
 0000a360: 6172 656e 7420 6966 206e 6f0a 2020 2020  arent if no.    
 0000a370: 2020 2020 6578 706c 6963 6974 2070 6172      explicit par
 0000a380: 656e 7420 6973 2073 7065 6369 6669 6564  ent is specified
-0000a390: 2e0a 2020 2020 2020 2a2a 7570 6461 7465  ..      **update
-0000a3a0: 733a 2041 7474 7269 6275 7465 2075 7064  s: Attribute upd
-0000a3b0: 6174 6573 2e0a 2020 2020 5265 7475 726e  ates..    Return
-0000a3c0: 733a 0a20 2020 2020 2041 2063 6c6f 6e65  s:.      A clone
-0000a3d0: 206f 6620 7468 6520 7468 6973 204d 6f64   of the this Mod
-0000a3e0: 756c 6520 7769 7468 2074 6865 2075 7064  ule with the upd
-0000a3f0: 6174 6564 2061 7474 7269 6275 7465 7320  ated attributes 
-0000a400: 616e 6420 7061 7265 6e74 2e0a 2020 2020  and parent..    
-0000a410: 2222 220a 2020 2020 6174 7472 7320 3d20  """.    attrs = 
-0000a420: 7b66 2e6e 616d 653a 2067 6574 6174 7472  {f.name: getattr
-0000a430: 2873 656c 662c 2066 2e6e 616d 6529 2066  (self, f.name) f
-0000a440: 6f72 2066 2069 6e20 6461 7461 636c 6173  or f in dataclas
-0000a450: 7365 732e 6669 656c 6473 2873 656c 6629  ses.fields(self)
-0000a460: 2069 6620 662e 696e 6974 7d0a 2020 2020   if f.init}.    
-0000a470: 6174 7472 732e 7570 6461 7465 2870 6172  attrs.update(par
-0000a480: 656e 743d 7061 7265 6e74 2c20 2a2a 7570  ent=parent, **up
-0000a490: 6461 7465 7329 0a20 2020 2072 6574 7572  dates).    retur
-0000a4a0: 6e20 7365 6c66 2e5f 5f63 6c61 7373 5f5f  n self.__class__
-0000a4b0: 282a 2a61 7474 7273 290a 0a20 2064 6566  (**attrs)..  def
-0000a4c0: 2076 6172 6961 626c 6528 7365 6c66 2c20   variable(self, 
-0000a4d0: 636f 6c3a 2073 7472 2c20 6e61 6d65 3a20  col: str, name: 
-0000a4e0: 7374 722c 0a20 2020 2020 2020 2020 2020  str,.           
-0000a4f0: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
-0000a500: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
-0000a510: 2e2e 2c20 416e 795d 5d20 3d20 4e6f 6e65  .., Any]] = None
-0000a520: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000a530: 202a 696e 6974 5f61 7267 732c 0a20 2020   *init_args,.   
-0000a540: 2020 2020 2020 2020 2020 2020 756e 626f              unbo
-0000a550: 783a 2062 6f6f 6c20 3d20 5472 7565 2920  x: bool = True) 
-0000a560: 2d3e 2056 6172 6961 626c 653a 0a20 2020  -> Variable:.   
-0000a570: 2022 2222 4465 636c 6172 6573 2061 6e64   """Declares and
-0000a580: 2072 6574 7572 6e73 2061 2076 6172 6961   returns a varia
-0000a590: 626c 6520 696e 2074 6869 7320 4d6f 6475  ble in this Modu
-0000a5a0: 6c65 2e0a 0a20 2020 2053 6565 203a 6d6f  le...    See :mo
-0000a5b0: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
-0000a5c0: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
-0000a5d0: 2069 6e66 6f72 6d61 7469 6f6e 2e20 5365   information. Se
-0000a5e0: 6520 616c 736f 203a 6d65 7468 3a60 7061  e also :meth:`pa
-0000a5f0: 7261 6d60 0a20 2020 2066 6f72 2061 2073  ram`.    for a s
-0000a600: 686f 7274 6861 6e64 2077 6179 2074 6f20  horthand way to 
-0000a610: 6465 6669 6e65 2072 6561 642d 6f6e 6c79  define read-only
-0000a620: 2076 6172 6961 626c 6573 2069 6e20 7468   variables in th
-0000a630: 6520 2270 6172 616d 7322 0a20 2020 2063  e "params".    c
-0000a640: 6f6c 6c65 6374 696f 6e2e 0a0a 2020 2020  ollection...    
-0000a650: 436f 6e74 7261 7279 2074 6f20 3a6d 6574  Contrary to :met
-0000a660: 683a 6070 6172 616d 602c 2061 6c6c 2061  h:`param`, all a
-0000a670: 7267 756d 656e 7473 2070 6173 7369 6e67  rguments passing
-0000a680: 2075 7369 6e67 2060 696e 6974 5f66 6e60   using `init_fn`
-0000a690: 2073 686f 756c 6420 6265 0a20 2020 2070   should be.    p
-0000a6a0: 6173 7365 6420 6f6e 2065 7870 6c69 6369  assed on explici
-0000a6b0: 746c 793a 3a0a 0a20 2020 2020 206b 6579  tly::..      key
-0000a6c0: 203d 2073 656c 662e 6d61 6b65 5f72 6e67   = self.make_rng
-0000a6d0: 2827 7374 6174 7327 290a 2020 2020 2020  ('stats').      
-0000a6e0: 6d65 616e 203d 2073 656c 662e 7661 7269  mean = self.vari
-0000a6f0: 6162 6c65 2827 7374 6174 7327 2c20 276d  able('stats', 'm
-0000a700: 6561 6e27 2c20 6c65 6375 6e5f 6e6f 726d  ean', lecun_norm
-0000a710: 616c 2829 2c20 6b65 792c 2028 322c 2032  al(), key, (2, 2
-0000a720: 2929 0a0a 2020 2020 496e 2074 6865 2065  ))..    In the e
-0000a730: 7861 6d70 6c65 2061 626f 7665 2c20 7468  xample above, th
-0000a740: 6520 6675 6e63 7469 6f6e 2060 6c65 6375  e function `lecu
-0000a750: 6e5f 6e6f 726d 616c 6020 6578 7065 6374  n_normal` expect
-0000a760: 7320 7477 6f20 6172 6775 6d65 6e74 733a  s two arguments:
-0000a770: 0a20 2020 2060 6b65 7960 2061 6e64 2060  .    `key` and `
-0000a780: 7368 6170 6560 2c20 616e 6420 626f 7468  shape`, and both
-0000a790: 2068 6176 6520 746f 2062 6520 7061 7373   have to be pass
-0000a7a0: 6564 206f 6e2e 2054 6865 2050 524e 4720  ed on. The PRNG 
-0000a7b0: 666f 7220 6073 7461 7473 6020 6861 730a  for `stats` has.
-0000a7c0: 2020 2020 746f 2062 6520 7072 6f76 6964      to be provid
-0000a7d0: 6564 2065 7870 6c69 6369 746c 7920 7768  ed explicitly wh
-0000a7e0: 656e 2063 616c 6c69 6e67 203a 6d65 7468  en calling :meth
-0000a7f0: 3a60 696e 6974 6020 616e 6420 3a6d 6574  :`init` and :met
-0000a800: 683a 6061 7070 6c79 602e 0a0a 2020 2020  h:`apply`...    
-0000a810: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
-0000a820: 2054 6865 2076 6172 6961 626c 6520 636f   The variable co
-0000a830: 6c6c 6563 7469 6f6e 206e 616d 652e 0a20  llection name.. 
-0000a840: 2020 2020 206e 616d 653a 2054 6865 2076       name: The v
-0000a850: 6172 6961 626c 6520 6e61 6d65 2e0a 2020  ariable name..  
-0000a860: 2020 2020 696e 6974 5f66 6e3a 2054 6865      init_fn: The
-0000a870: 2066 756e 6374 696f 6e20 7468 6174 2077   function that w
-0000a880: 696c 6c20 6265 2063 616c 6c65 6420 746f  ill be called to
-0000a890: 2063 6f6d 7075 7465 2074 6865 2069 6e69   compute the ini
-0000a8a0: 7469 616c 2076 616c 7565 0a20 2020 2020  tial value.     
-0000a8b0: 2020 206f 6620 7468 6973 2076 6172 6961     of this varia
-0000a8c0: 626c 652e 2054 6869 7320 6675 6e63 7469  ble. This functi
-0000a8d0: 6f6e 2077 696c 6c20 6f6e 6c79 2062 6520  on will only be 
-0000a8e0: 6361 6c6c 6564 2074 6865 2066 6972 7374  called the first
-0000a8f0: 2074 696d 650a 2020 2020 2020 2020 7468   time.        th
-0000a900: 6973 2076 6172 6961 626c 6520 6973 2075  is variable is u
-0000a910: 7365 6420 696e 2074 6869 7320 6d6f 6475  sed in this modu
-0000a920: 6c65 2e20 4966 204e 6f6e 652c 2074 6865  le. If None, the
-0000a930: 2076 6172 6961 626c 6520 6d75 7374 0a20   variable must. 
-0000a940: 2020 2020 2020 2061 6c72 6561 6479 2062         already b
-0000a950: 6520 696e 6974 6961 6c69 7a65 6420 6f74  e initialized ot
-0000a960: 6865 7277 6973 6520 616e 2065 7272 6f72  herwise an error
-0000a970: 2069 7320 7261 6973 6564 2e0a 2020 2020   is raised..    
-0000a980: 2020 2a69 6e69 745f 6172 6773 3a20 5468    *init_args: Th
-0000a990: 6520 6172 6775 6d65 6e74 7320 746f 2070  e arguments to p
-0000a9a0: 6173 7320 746f 2069 6e69 745f 666e 2e0a  ass to init_fn..
-0000a9b0: 2020 2020 2020 756e 626f 783a 2049 6620        unbox: If 
-0000a9c0: 5472 7565 2c20 6060 4178 6973 4d65 7461  True, ``AxisMeta
-0000a9d0: 6461 7461 6060 2069 6e73 7461 6e63 6573  data`` instances
-0000a9e0: 2061 7265 2072 6570 6c61 6365 6420 6279   are replaced by
-0000a9f0: 2074 6865 6972 2075 6e62 6f78 6564 0a20   their unboxed. 
-0000aa00: 2020 2020 2020 2076 616c 7565 2c20 7365         value, se
-0000aa10: 6520 6060 666c 6178 2e6e 6e2e 6d65 7461  e ``flax.nn.meta
-0000aa20: 2e75 6e62 6f78 6060 2028 6465 6661 756c  .unbox`` (defaul
-0000aa30: 743a 2054 7275 6529 2e0a 0a20 2020 2052  t: True)...    R
-0000aa40: 6574 7572 6e73 3a0a 2020 2020 2020 4120  eturns:.      A 
-0000aa50: 3a63 6c61 7373 3a60 666c 6178 2e63 6f72  :class:`flax.cor
-0000aa60: 652e 7661 7269 6162 6c65 732e 5661 7269  e.variables.Vari
-0000aa70: 6162 6c65 6020 7468 6174 2063 616e 2062  able` that can b
-0000aa80: 6520 7265 6164 206f 7220 7365 7420 7669  e read or set vi
-0000aa90: 610a 2020 2020 2020 222e 7661 6c75 6522  a.      ".value"
-0000aaa0: 2061 7474 7269 6275 7465 2e20 5468 726f   attribute. Thro
-0000aab0: 7773 2061 6e20 6572 726f 7220 6966 2074  ws an error if t
-0000aac0: 6865 2076 6172 6961 626c 6520 6578 6973  he variable exis
-0000aad0: 7473 2061 6c72 6561 6479 2e0a 2020 2020  ts already..    
-0000aae0: 2222 220a 2020 2020 6966 206e 6f74 2073  """.    if not s
-0000aaf0: 656c 662e 5f69 6e69 7469 616c 697a 6174  elf._initializat
-0000ab00: 696f 6e5f 616c 6c6f 7765 643a 0a20 2020  ion_allowed:.   
-0000ab10: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-0000ab20: 726f 7228 0a20 2020 2020 2020 2020 2027  ror(.          '
-0000ab30: 5661 7269 6162 6c65 7320 6d75 7374 2062  Variables must b
-0000ab40: 6520 696e 6974 6961 6c69 7a65 6420 696e  e initialized in
-0000ab50: 2060 7365 7475 7028 2960 206f 7220 696e   `setup()` or in
-0000ab60: 2061 206d 6574 686f 6420 270a 2020 2020   a method '.    
-0000ab70: 2020 2020 2020 2777 7261 7070 6564 2069        'wrapped i
-0000ab80: 6e20 6040 636f 6d70 6163 7460 2729 0a20  n `@compact`'). 
-0000ab90: 2020 2069 6620 7365 6c66 2e5f 6e61 6d65     if self._name
-0000aba0: 5f74 616b 656e 286e 616d 652c 2063 6f6c  _taken(name, col
-0000abb0: 6c65 6374 696f 6e3d 636f 6c29 3a0a 2020  lection=col):.  
-0000abc0: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
-0000abd0: 2e4e 616d 6549 6e55 7365 4572 726f 7228  .NameInUseError(
-0000abe0: 2776 6172 6961 626c 6527 2c20 6e61 6d65  'variable', name
-0000abf0: 2c20 7365 6c66 2e5f 5f63 6c61 7373 5f5f  , self.__class__
-0000ac00: 2e5f 5f6e 616d 655f 5f29 0a20 2020 2061  .__name__).    a
-0000ac10: 7373 6572 7420 7365 6c66 2e73 636f 7065  ssert self.scope
-0000ac20: 2069 7320 6e6f 7420 4e6f 6e65 0a20 2020   is not None.   
-0000ac30: 2076 203d 2073 656c 662e 7363 6f70 652e   v = self.scope.
-0000ac40: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
-0000ac50: 6d65 2c20 696e 6974 5f66 6e2c 202a 696e  me, init_fn, *in
-0000ac60: 6974 5f61 7267 732c 2075 6e62 6f78 3d75  it_args, unbox=u
-0000ac70: 6e62 6f78 290a 2020 2020 7365 6c66 2e5f  nbox).    self._
-0000ac80: 7374 6174 652e 6368 696c 6472 656e 5b6e  state.children[n
-0000ac90: 616d 655d 203d 2063 6f6c 0a20 2020 2072  ame] = col.    r
-0000aca0: 6574 7572 6e20 760a 0a20 2064 6566 2070  eturn v..  def p
-0000acb0: 6172 616d 2873 656c 662c 206e 616d 653a  aram(self, name:
-0000acc0: 2073 7472 2c20 696e 6974 5f66 6e3a 2043   str, init_fn: C
-0000acd0: 616c 6c61 626c 655b 2e2e 2e2c 2054 5d2c  allable[..., T],
-0000ace0: 202a 696e 6974 5f61 7267 732c 0a20 2020   *init_args,.   
-0000acf0: 2020 2020 2020 2020 2075 6e62 6f78 3a20           unbox: 
-0000ad00: 626f 6f6c 203d 2054 7275 6529 202d 3e20  bool = True) -> 
-0000ad10: 543a 0a20 2020 2022 2222 4465 636c 6172  T:.    """Declar
-0000ad20: 6573 2061 6e64 2072 6574 7572 6e73 2061  es and returns a
-0000ad30: 2070 6172 616d 6574 6572 2069 6e20 7468   parameter in th
-0000ad40: 6973 204d 6f64 756c 652e 0a0a 2020 2020  is Module...    
-0000ad50: 5061 7261 6d65 7465 7273 2061 7265 2072  Parameters are r
-0000ad60: 6561 642d 6f6e 6c79 2076 6172 6961 626c  ead-only variabl
-0000ad70: 6573 2069 6e20 7468 6520 636f 6c6c 6563  es in the collec
-0000ad80: 7469 6f6e 206e 616d 6564 2022 7061 7261  tion named "para
-0000ad90: 6d73 222e 2053 6565 0a20 2020 203a 6d6f  ms". See.    :mo
-0000ada0: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
-0000adb0: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
-0000adc0: 2064 6574 6169 6c73 206f 6e20 7661 7269   details on vari
-0000add0: 6162 6c65 732e 0a0a 2020 2020 5468 6520  ables...    The 
-0000ade0: 6669 7273 7420 6172 6775 6d65 6e74 206f  first argument o
-0000adf0: 6620 6069 6e69 745f 666e 6020 6973 2061  f `init_fn` is a
-0000ae00: 7373 756d 6564 2074 6f20 6265 2061 2050  ssumed to be a P
-0000ae10: 524e 4720 6b65 792c 2077 6869 6368 2069  RNG key, which i
-0000ae20: 730a 2020 2020 7072 6f76 6964 6564 2061  s.    provided a
-0000ae30: 7574 6f6d 6174 6963 616c 6c79 2061 6e64  utomatically and
-0000ae40: 2064 6f65 7320 6e6f 7420 6861 7665 2074   does not have t
-0000ae50: 6f20 6265 2070 6173 7365 6420 7573 696e  o be passed usin
-0000ae60: 6720 6069 6e69 745f 6172 6773 603a 3a0a  g `init_args`::.
-0000ae70: 0a20 2020 2020 206d 6561 6e20 3d20 7365  .      mean = se
-0000ae80: 6c66 2e70 6172 616d 2827 6d65 616e 272c  lf.param('mean',
-0000ae90: 206c 6563 756e 5f6e 6f72 6d61 6c28 292c   lecun_normal(),
-0000aea0: 2028 322c 2032 2929 0a0a 2020 2020 496e   (2, 2))..    In
-0000aeb0: 2074 6865 2065 7861 6d70 6c65 2061 626f   the example abo
-0000aec0: 7665 2c20 7468 6520 6675 6e63 7469 6f6e  ve, the function
-0000aed0: 2060 6c65 6375 6e5f 6e6f 726d 616c 6020   `lecun_normal` 
-0000aee0: 6578 7065 6374 7320 7477 6f20 6172 6775  expects two argu
-0000aef0: 6d65 6e74 733a 0a20 2020 2060 6b65 7960  ments:.    `key`
-0000af00: 2061 6e64 2060 7368 6170 6560 2c20 6275   and `shape`, bu
-0000af10: 7420 6f6e 6c79 2060 7368 6170 6560 2068  t only `shape` h
-0000af20: 6173 2074 6f20 6265 2070 726f 7669 6465  as to be provide
-0000af30: 6420 6578 706c 6963 6974 6c79 3b20 606b  d explicitly; `k
-0000af40: 6579 600a 2020 2020 6973 2073 6574 2061  ey`.    is set a
-0000af50: 7574 6f6d 6174 6963 616c 6c79 2075 7369  utomatically usi
-0000af60: 6e67 2074 6865 2050 524e 4720 666f 7220  ng the PRNG for 
-0000af70: 6070 6172 616d 7360 2074 6861 7420 6973  `params` that is
-0000af80: 2070 6173 7365 6420 7768 656e 0a20 2020   passed when.   
-0000af90: 2069 6e69 7469 616c 697a 696e 6720 7468   initializing th
-0000afa0: 6520 6d6f 6475 6c65 2075 7369 6e67 203a  e module using :
-0000afb0: 6d65 7468 3a60 696e 6974 602e 0a0a 2020  meth:`init`...  
-0000afc0: 2020 4172 6773 3a0a 2020 2020 2020 6e61    Args:.      na
-0000afd0: 6d65 3a20 5468 6520 7061 7261 6d65 7465  me: The paramete
-0000afe0: 7220 6e61 6d65 2e0a 2020 2020 2020 696e  r name..      in
-0000aff0: 6974 5f66 6e3a 2054 6865 2066 756e 6374  it_fn: The funct
-0000b000: 696f 6e20 7468 6174 2077 696c 6c20 6265  ion that will be
-0000b010: 2063 616c 6c65 6420 746f 2063 6f6d 7075   called to compu
-0000b020: 7465 2074 6865 2069 6e69 7469 616c 2076  te the initial v
-0000b030: 616c 7565 0a20 2020 2020 2020 206f 6620  alue.        of 
-0000b040: 7468 6973 2076 6172 6961 626c 652e 2054  this variable. T
-0000b050: 6869 7320 6675 6e63 7469 6f6e 2077 696c  his function wil
-0000b060: 6c20 6f6e 6c79 2062 6520 6361 6c6c 6564  l only be called
-0000b070: 2074 6865 2066 6972 7374 2074 696d 650a   the first time.
-0000b080: 2020 2020 2020 2020 7468 6973 2070 6172          this par
-0000b090: 616d 6574 6572 2069 7320 7573 6564 2069  ameter is used i
-0000b0a0: 6e20 7468 6973 206d 6f64 756c 652e 0a20  n this module.. 
-0000b0b0: 2020 2020 202a 696e 6974 5f61 7267 733a       *init_args:
-0000b0c0: 2054 6865 2061 7267 756d 656e 7473 2074   The arguments t
-0000b0d0: 6f20 7061 7373 2074 6f20 696e 6974 5f66  o pass to init_f
-0000b0e0: 6e2e 0a20 2020 2020 2075 6e62 6f78 3a20  n..      unbox: 
-0000b0f0: 4966 2054 7275 652c 2060 6041 7869 734d  If True, ``AxisM
-0000b100: 6574 6164 6174 6160 6020 696e 7374 616e  etadata`` instan
-0000b110: 6365 7320 6172 6520 7265 706c 6163 6564  ces are replaced
-0000b120: 2062 7920 7468 6569 7220 756e 626f 7865   by their unboxe
-0000b130: 640a 2020 2020 2020 2020 7661 6c75 652c  d.        value,
-0000b140: 2073 6565 2060 6066 6c61 782e 6e6e 2e6d   see ``flax.nn.m
-0000b150: 6574 612e 756e 626f 7860 6020 2864 6566  eta.unbox`` (def
-0000b160: 6175 6c74 3a20 5472 7565 292e 0a0a 2020  ault: True)...  
-0000b170: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
-0000b180: 2054 6865 2076 616c 7565 206f 6620 7468   The value of th
-0000b190: 6520 696e 6974 6961 6c69 7a65 6420 7061  e initialized pa
-0000b1a0: 7261 6d65 7465 722e 2054 6872 6f77 7320  rameter. Throws 
-0000b1b0: 616e 2065 7272 6f72 2069 6620 7468 6520  an error if the 
-0000b1c0: 7061 7261 6d65 7465 720a 2020 2020 2020  parameter.      
-0000b1d0: 6578 6973 7473 2061 6c72 6561 6479 2e0a  exists already..
-0000b1e0: 2020 2020 2222 220a 2020 2020 6966 206e      """.    if n
-0000b1f0: 6f74 2073 656c 662e 5f69 6e69 7469 616c  ot self._initial
-0000b200: 697a 6174 696f 6e5f 616c 6c6f 7765 643a  ization_allowed:
-0000b210: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
-0000b220: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-0000b230: 2020 2027 5061 7261 6d65 7465 7273 206d     'Parameters m
-0000b240: 7573 7420 6265 2069 6e69 7469 616c 697a  ust be initializ
-0000b250: 6564 2069 6e20 6073 6574 7570 2829 6020  ed in `setup()` 
-0000b260: 6f72 2069 6e20 6120 6d65 7468 6f64 2027  or in a method '
-0000b270: 0a20 2020 2020 2020 2020 2027 7772 6170  .          'wrap
-0000b280: 7065 6420 696e 2060 4063 6f6d 7061 6374  ped in `@compact
-0000b290: 6027 290a 2020 2020 6966 2073 656c 662e  `').    if self.
-0000b2a0: 5f6e 616d 655f 7461 6b65 6e28 6e61 6d65  _name_taken(name
-0000b2b0: 2c20 636f 6c6c 6563 7469 6f6e 3d27 7061  , collection='pa
-0000b2c0: 7261 6d73 2729 3a0a 2020 2020 2020 7261  rams'):.      ra
-0000b2d0: 6973 6520 6572 726f 7273 2e4e 616d 6549  ise errors.NameI
-0000b2e0: 6e55 7365 4572 726f 7228 2770 6172 616d  nUseError('param
-0000b2f0: 272c 206e 616d 652c 2073 656c 662e 5f5f  ', name, self.__
-0000b300: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
-0000b310: 290a 2020 2020 6173 7365 7274 2073 656c  ).    assert sel
-0000b320: 662e 7363 6f70 6520 6973 206e 6f74 204e  f.scope is not N
-0000b330: 6f6e 650a 2020 2020 7620 3d20 7365 6c66  one.    v = self
-0000b340: 2e73 636f 7065 2e70 6172 616d 286e 616d  .scope.param(nam
-0000b350: 652c 2069 6e69 745f 666e 2c20 2a69 6e69  e, init_fn, *ini
-0000b360: 745f 6172 6773 2c20 756e 626f 783d 756e  t_args, unbox=un
-0000b370: 626f 7829 0a20 2020 2073 656c 662e 5f73  box).    self._s
-0000b380: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
-0000b390: 6d65 5d20 3d20 2770 6172 616d 7327 0a20  me] = 'params'. 
-0000b3a0: 2020 2072 6574 7572 6e20 760a 0a20 2064     return v..  d
-0000b3b0: 6566 2068 6173 5f76 6172 6961 626c 6528  ef has_variable(
-0000b3c0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
-0000b3d0: 6e61 6d65 3a20 7374 7229 202d 3e20 626f  name: str) -> bo
-0000b3e0: 6f6c 3a0a 2020 2020 2222 2243 6865 636b  ol:.    """Check
-0000b3f0: 7320 6966 2061 2076 6172 6961 626c 6520  s if a variable 
-0000b400: 6f66 2067 6976 656e 2063 6f6c 6c65 6374  of given collect
-0000b410: 696f 6e20 616e 6420 6e61 6d65 2065 7869  ion and name exi
-0000b420: 7374 7320 696e 2074 6869 7320 4d6f 6475  sts in this Modu
-0000b430: 6c65 2e0a 0a20 2020 2053 6565 203a 6d6f  le...    See :mo
-0000b440: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
-0000b450: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
-0000b460: 2065 7870 6c61 6e61 7469 6f6e 206f 6e20   explanation on 
-0000b470: 7661 7269 6162 6c65 7320 616e 640a 2020  variables and.  
-0000b480: 2020 636f 6c6c 6563 7469 6f6e 732e 0a0a    collections...
-0000b490: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-0000b4a0: 636f 6c3a 2054 6865 2076 6172 6961 626c  col: The variabl
-0000b4b0: 6520 636f 6c6c 6563 7469 6f6e 206e 616d  e collection nam
-0000b4c0: 652e 0a20 2020 2020 206e 616d 653a 2054  e..      name: T
-0000b4d0: 6865 206e 616d 6520 6f66 2074 6865 2076  he name of the v
-0000b4e0: 6172 6961 626c 652e 0a20 2020 2052 6574  ariable..    Ret
-0000b4f0: 7572 6e73 3a0a 2020 2020 2020 5472 7565  urns:.      True
-0000b500: 2069 6620 7468 6520 7661 7269 6162 6c65   if the variable
-0000b510: 2065 7869 7374 732e 0a20 2020 2022 2222   exists..    """
-0000b520: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-0000b530: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-0000b540: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0000b550: 6f72 2822 4361 6e27 7420 6163 6365 7373  or("Can't access
-0000b560: 2076 6172 6961 626c 6573 206f 6e20 756e   variables on un
-0000b570: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
-0000b580: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-0000b590: 7363 6f70 652e 6861 735f 7661 7269 6162  scope.has_variab
-0000b5a0: 6c65 2863 6f6c 2c20 6e61 6d65 290a 0a20  le(col, name).. 
-0000b5b0: 2064 6566 2069 735f 6d75 7461 626c 655f   def is_mutable_
-0000b5c0: 636f 6c6c 6563 7469 6f6e 2873 656c 662c  collection(self,
-0000b5d0: 2063 6f6c 3a20 7374 7229 202d 3e20 626f   col: str) -> bo
-0000b5e0: 6f6c 3a0a 2020 2020 2222 2252 6574 7572  ol:.    """Retur
-0000b5f0: 6e73 2074 7275 6520 6966 2074 6865 2063  ns true if the c
-0000b600: 6f6c 6c65 6374 696f 6e20 6063 6f6c 6020  ollection `col` 
-0000b610: 6973 206d 7574 6162 6c65 2e22 2222 0a20  is mutable.""". 
-0000b620: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
-0000b630: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-0000b640: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000b650: 2822 4361 6e27 7420 6368 6563 6b20 6d75  ("Can't check mu
-0000b660: 7461 6269 6c69 7479 206f 6e20 756e 626f  tability on unbo
-0000b670: 756e 6420 6d6f 6475 6c65 7322 290a 2020  und modules").  
-0000b680: 2020 7265 7475 726e 2073 656c 662e 7363    return self.sc
-0000b690: 6f70 652e 6973 5f6d 7574 6162 6c65 5f63  ope.is_mutable_c
-0000b6a0: 6f6c 6c65 6374 696f 6e28 636f 6c29 0a0a  ollection(col)..
-0000b6b0: 2020 6465 6620 6861 735f 726e 6728 7365    def has_rng(se
-0000b6c0: 6c66 2c20 6e61 6d65 3a20 7374 7229 202d  lf, name: str) -
-0000b6d0: 3e20 626f 6f6c 3a0a 2020 2020 2222 2252  > bool:.    """R
-0000b6e0: 6574 7572 6e73 2074 7275 6520 6966 2061  eturns true if a
-0000b6f0: 2050 524e 4753 6571 7565 6e63 6520 7769   PRNGSequence wi
-0000b700: 7468 206e 616d 6520 606e 616d 6560 2065  th name `name` e
-0000b710: 7869 7374 732e 2222 220a 2020 2020 6966  xists.""".    if
-0000b720: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
-0000b730: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
-0000b740: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
-0000b750: 2774 2071 7565 7279 2066 6f72 2052 4e47  't query for RNG
-0000b760: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
-0000b770: 756c 6573 2229 0a20 2020 2072 6574 7572  ules").    retur
-0000b780: 6e20 7365 6c66 2e73 636f 7065 2e68 6173  n self.scope.has
-0000b790: 5f72 6e67 286e 616d 6529 0a0a 2020 6465  _rng(name)..  de
-0000b7a0: 6620 6d61 6b65 5f72 6e67 2873 656c 662c  f make_rng(self,
-0000b7b0: 206e 616d 653a 2073 7472 2920 2d3e 2050   name: str) -> P
-0000b7c0: 524e 474b 6579 3a0a 2020 2020 2222 2252  RNGKey:.    """R
-0000b7d0: 6574 7572 6e73 2061 206e 6577 2052 4e47  eturns a new RNG
-0000b7e0: 206b 6579 2066 726f 6d20 6120 6769 7665   key from a give
-0000b7f0: 6e20 524e 4720 7365 7175 656e 6365 2066  n RNG sequence f
-0000b800: 6f72 2074 6869 7320 4d6f 6475 6c65 2e0a  or this Module..
-0000b810: 0a20 2020 2054 6865 206e 6577 2052 4e47  .    The new RNG
-0000b820: 206b 6579 2069 7320 7370 6c69 7420 6672   key is split fr
-0000b830: 6f6d 2074 6865 2070 7265 7669 6f75 7320  om the previous 
-0000b840: 6f6e 652e 2054 6875 732c 2065 7665 7279  one. Thus, every
-0000b850: 2063 616c 6c20 746f 0a20 2020 2060 6d61   call to.    `ma
-0000b860: 6b65 5f72 6e67 6020 7265 7475 726e 7320  ke_rng` returns 
-0000b870: 6120 6e65 7720 524e 4720 6b65 792c 2077  a new RNG key, w
-0000b880: 6869 6c65 2073 7469 6c6c 2067 7561 7261  hile still guara
-0000b890: 6e74 6565 696e 6720 6675 6c6c 0a20 2020  nteeing full.   
-0000b8a0: 2072 6570 726f 6475 6369 6269 6c69 7479   reproducibility
-0000b8b0: 2e0a 0a20 2020 2054 4f44 4f3a 204c 696e  ...    TODO: Lin
-0000b8c0: 6b20 746f 2046 6c61 7820 524e 4720 6465  k to Flax RNG de
-0000b8d0: 7369 676e 206e 6f74 652e 0a0a 2020 2020  sign note...    
-0000b8e0: 4172 6773 3a0a 2020 2020 2020 6e61 6d65  Args:.      name
-0000b8f0: 3a20 5468 6520 524e 4720 7365 7175 656e  : The RNG sequen
-0000b900: 6365 206e 616d 652e 0a20 2020 2052 6574  ce name..    Ret
-0000b910: 7572 6e73 3a0a 2020 2020 2020 5468 6520  urns:.      The 
-0000b920: 6e65 776c 7920 6765 6e65 7261 7465 6420  newly generated 
-0000b930: 524e 4720 6b65 792e 0a20 2020 2022 2222  RNG key..    """
-0000b940: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-0000b950: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-0000b960: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0000b970: 6f72 2822 4361 6e27 7420 7573 6520 524e  or("Can't use RN
-0000b980: 4773 206f 6e20 756e 626f 756e 6420 6d6f  Gs on unbound mo
-0000b990: 6475 6c65 7322 290a 2020 2020 7265 7475  dules").    retu
-0000b9a0: 726e 2073 656c 662e 7363 6f70 652e 6d61  rn self.scope.ma
-0000b9b0: 6b65 5f72 6e67 286e 616d 6529 0a0a 2020  ke_rng(name)..  
-0000b9c0: 6465 6620 6973 5f69 6e69 7469 616c 697a  def is_initializ
-0000b9d0: 696e 6728 7365 6c66 2920 2d3e 2062 6f6f  ing(self) -> boo
-0000b9e0: 6c3a 0a20 2020 2022 2222 5265 7475 726e  l:.    """Return
-0000b9f0: 7320 5472 7565 2069 6620 7275 6e6e 696e  s True if runnin
-0000ba00: 6720 756e 6465 7220 7365 6c66 2e69 6e69  g under self.ini
-0000ba10: 7428 2e2e 2e29 206f 7220 6e6e 2e69 6e69  t(...) or nn.ini
-0000ba20: 7428 2e2e 2e29 2829 2e0a 0a20 2020 2054  t(...)()...    T
-0000ba30: 6869 7320 6973 2061 2068 656c 7065 7220  his is a helper 
-0000ba40: 6d65 7468 6f64 2074 6f20 6861 6e64 6c65  method to handle
-0000ba50: 2074 6865 2063 6f6d 6d6f 6e20 6361 7365   the common case
-0000ba60: 206f 6620 7369 6d70 6c65 2069 6e69 7469   of simple initi
-0000ba70: 616c 697a 6174 696f 6e0a 2020 2020 7768  alization.    wh
-0000ba80: 6572 6520 7765 2077 6973 6820 746f 2068  ere we wish to h
-0000ba90: 6176 6520 7365 7475 7020 6c6f 6769 6320  ave setup logic 
-0000baa0: 6f63 6375 7220 7768 656e 206f 6e6c 7920  occur when only 
-0000bab0: 6361 6c6c 6564 2075 6e64 6572 0a20 2020  called under.   
-0000bac0: 2060 606d 6f64 756c 652e 696e 6974 6060   ``module.init``
-0000bad0: 206f 7220 6060 6e6e 2e69 6e69 7460 602e   or ``nn.init``.
-0000bae0: 2020 466f 7220 6d6f 7265 2063 6f6d 706c    For more compl
-0000baf0: 6963 6174 6564 206d 756c 7469 2d70 6861  icated multi-pha
-0000bb00: 7365 0a20 2020 2069 6e69 7469 616c 697a  se.    initializ
-0000bb10: 6174 696f 6e20 7363 656e 6172 696f 7320  ation scenarios 
-0000bb20: 6974 2069 7320 6265 7474 6572 2074 6f20  it is better to 
-0000bb30: 7465 7374 2066 6f72 2074 6865 206d 7574  test for the mut
-0000bb40: 6162 696c 6974 7920 6f66 0a20 2020 2070  ability of.    p
-0000bb50: 6172 7469 6375 6c61 7220 7661 7269 6162  articular variab
-0000bb60: 6c65 2063 6f6c 6c65 6374 696f 6e73 206f  le collections o
-0000bb70: 7220 666f 7220 7468 6520 7072 6573 656e  r for the presen
-0000bb80: 6365 206f 6620 7061 7274 6963 756c 6172  ce of particular
-0000bb90: 0a20 2020 2076 6172 6961 626c 6573 2074  .    variables t
-0000bba0: 6861 7420 706f 7465 6e74 6961 6c6c 7920  hat potentially 
-0000bbb0: 6e65 6564 2074 6f20 6265 2069 6e69 7469  need to be initi
-0000bbc0: 616c 697a 6564 2e0a 2020 2020 2222 220a  alized..    """.
-0000bbd0: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
-0000bbe0: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
-0000bbf0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-0000bc00: 7228 2243 616e 2774 2063 6865 636b 2069  r("Can't check i
-0000bc10: 6620 7275 6e6e 696e 6720 756e 6465 7220  f running under 
-0000bc20: 696e 6974 2829 206f 6e20 756e 626f 756e  init() on unboun
-0000bc30: 6420 6d6f 6475 6c65 7322 290a 2020 2020  d modules").    
-0000bc40: 7265 7475 726e 2073 656c 662e 7363 6f70  return self.scop
-0000bc50: 652e 6765 745f 666c 6167 2827 696e 6974  e.get_flag('init
-0000bc60: 6961 6c69 7a69 6e67 272c 2046 616c 7365  ializing', False
-0000bc70: 290a 0a20 2064 6566 205f 6d6f 6475 6c65  )..  def _module
-0000bc80: 5f63 6865 636b 7328 7365 6c66 293a 0a20  _checks(self):. 
-0000bc90: 2020 2022 2222 5275 6e20 7374 616e 6461     """Run standa
-0000bca0: 7264 2072 756e 7469 6d65 2063 6865 636b  rd runtime check
-0000bcb0: 732e 2222 220a 0a20 2020 2069 6620 6e6f  s."""..    if no
-0000bcc0: 7420 6973 696e 7374 616e 6365 2873 656c  t isinstance(sel
-0000bcd0: 662c 204d 6f64 756c 6529 3a0a 2020 2020  f, Module):.    
-0000bce0: 2020 7261 6973 6520 6572 726f 7273 2e49    raise errors.I
-0000bcf0: 6e76 616c 6964 496e 7374 616e 6365 4d6f  nvalidInstanceMo
-0000bd00: 6475 6c65 4572 726f 7228 290a 0a20 2020  duleError()..   
-0000bd10: 206f 7665 7272 6964 6465 6e5f 706f 7374   overridden_post
-0000bd20: 5f69 6e69 7420 3d20 7365 6c66 2e5f 5f70  _init = self.__p
-0000bd30: 6f73 745f 696e 6974 5f5f 2021 3d20 4d6f  ost_init__ != Mo
-0000bd40: 6475 6c65 2e5f 5f70 6f73 745f 696e 6974  dule.__post_init
-0000bd50: 5f5f 0a20 2020 2069 6620 6f76 6572 7269  __.    if overri
-0000bd60: 6464 656e 5f70 6f73 745f 696e 6974 2061  dden_post_init a
-0000bd70: 6e64 206e 6f74 2068 6173 6174 7472 2873  nd not hasattr(s
-0000bd80: 656c 662c 2022 5f69 6422 293a 0a20 2020  elf, "_id"):.   
-0000bd90: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-0000bda0: 496e 636f 7272 6563 7450 6f73 7449 6e69  IncorrectPostIni
-0000bdb0: 744f 7665 7272 6964 6545 7272 6f72 2829  tOverrideError()
-0000bdc0: 0a0a 2020 4074 7261 6365 6261 636b 5f75  ..  @traceback_u
-0000bdd0: 7469 6c2e 6170 695f 626f 756e 6461 7279  til.api_boundary
-0000bde0: 0a20 2064 6566 2062 696e 6428 7365 6c66  .  def bind(self
-0000bdf0: 3a20 4d2c 0a20 2020 2020 2020 2020 2020  : M,.           
-0000be00: 7661 7269 6162 6c65 733a 2056 6172 6961  variables: Varia
-0000be10: 626c 6544 6963 742c 0a20 2020 2020 2020  bleDict,.       
-0000be20: 2020 2020 2a61 7267 732c 0a20 2020 2020      *args,.     
-0000be30: 2020 2020 2020 726e 6773 3a20 4f70 7469        rngs: Opti
-0000be40: 6f6e 616c 5b52 4e47 5365 7175 656e 6365  onal[RNGSequence
-0000be50: 735d 203d 204e 6f6e 652c 0a20 2020 2020  s] = None,.     
-0000be60: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
-0000be70: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
-0000be80: 3d20 4661 6c73 6529 202d 3e20 4d3a 0a20  = False) -> M:. 
-0000be90: 2020 2022 2222 4372 6561 7465 7320 616e     """Creates an
-0000bea0: 2069 6e74 6572 6163 7469 7665 204d 6f64   interactive Mod
-0000beb0: 756c 6520 696e 7374 616e 6365 2062 7920  ule instance by 
-0000bec0: 6269 6e64 696e 6720 7661 7269 6162 6c65  binding variable
-0000bed0: 7320 616e 6420 524e 4773 2e0a 0a20 2020  s and RNGs...   
-0000bee0: 2060 6062 696e 6460 6020 7072 6f76 6964   ``bind`` provid
-0000bef0: 6573 2061 6e20 2269 6e74 6572 6163 7469  es an "interacti
-0000bf00: 7665 2220 696e 7374 616e 6365 206f 6620  ve" instance of 
-0000bf10: 6120 4d6f 6475 6c65 2064 6972 6563 746c  a Module directl
-0000bf20: 7920 7769 7468 6f75 740a 2020 2020 7472  y without.    tr
-0000bf30: 616e 7366 6f72 6d69 6e67 2061 2066 756e  ansforming a fun
-0000bf40: 6374 696f 6e20 7769 7468 2060 6061 7070  ction with ``app
-0000bf50: 6c79 6060 2e20 5468 6973 2069 7320 7061  ly``. This is pa
-0000bf60: 7274 6963 756c 6172 6c79 2075 7365 6675  rticularly usefu
-0000bf70: 6c20 666f 720a 2020 2020 6465 6275 6767  l for.    debugg
-0000bf80: 696e 6720 616e 6420 696e 7465 7261 6374  ing and interact
-0000bf90: 6976 6520 7573 6520 6361 7365 7320 6c69  ive use cases li
-0000bfa0: 6b65 206e 6f74 6562 6f6f 6b73 2077 6865  ke notebooks whe
-0000bfb0: 7265 2061 2066 756e 6374 696f 6e20 776f  re a function wo
-0000bfc0: 756c 640a 2020 2020 6c69 6d69 7420 7468  uld.    limit th
-0000bfd0: 6520 6162 696c 6974 7920 746f 2073 706c  e ability to spl
-0000bfe0: 6974 2075 7020 636f 6465 2069 6e74 6f20  it up code into 
-0000bff0: 6469 6666 6572 656e 7420 6365 6c6c 732e  different cells.
-0000c000: 0a0a 2020 2020 4f6e 6365 2074 6865 2076  ..    Once the v
-0000c010: 6172 6961 626c 6573 2028 616e 6420 6f70  ariables (and op
-0000c020: 7469 6f6e 616c 6c79 2052 4e47 7329 2061  tionally RNGs) a
-0000c030: 7265 2062 6f75 6e64 2074 6f20 6120 6060  re bound to a ``
-0000c040: 4d6f 6475 6c65 6060 2069 740a 2020 2020  Module`` it.    
-0000c050: 6265 636f 6d65 7320 6120 7374 6174 6566  becomes a statef
-0000c060: 756c 206f 626a 6563 742e 204e 6f74 6520  ul object. Note 
-0000c070: 7468 6174 2069 6469 6f6d 6174 6963 204a  that idiomatic J
-0000c080: 4158 2069 7320 6675 6e63 7469 6f6e 616c  AX is functional
-0000c090: 2061 6e64 0a20 2020 2074 6865 7265 666f   and.    therefo
-0000c0a0: 7265 2061 6e20 696e 7465 7261 6374 6976  re an interactiv
-0000c0b0: 6520 696e 7374 616e 6365 2064 6f65 7320  e instance does 
-0000c0c0: 6e6f 7420 6d69 7820 7765 6c6c 2077 6974  not mix well wit
-0000c0d0: 6820 7661 6e69 6c6c 6120 4a41 5820 4150  h vanilla JAX AP
-0000c0e0: 4973 2e0a 2020 2020 6060 6269 6e64 2829  Is..    ``bind()
-0000c0f0: 6060 2073 686f 756c 6420 6f6e 6c79 2062  `` should only b
-0000c100: 6520 7573 6564 2066 6f72 2069 6e74 6572  e used for inter
-0000c110: 6163 7469 7665 2065 7870 6572 696d 656e  active experimen
-0000c120: 7461 7469 6f6e 2c20 616e 6420 696e 2061  tation, and in a
-0000c130: 6c6c 0a20 2020 206f 7468 6572 2063 6173  ll.    other cas
-0000c140: 6573 2077 6520 7374 726f 6e67 6c79 2065  es we strongly e
-0000c150: 6e63 6f75 7261 6765 2075 7365 7273 2074  ncourage users t
-0000c160: 6f20 7573 6520 6060 6170 706c 7928 2960  o use ``apply()`
-0000c170: 6020 696e 7374 6561 642e 0a0a 2020 2020  ` instead...    
-0000c180: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
-0000c190: 2069 6d70 6f72 7420 6a61 780a 2020 2020   import jax.    
-0000c1a0: 2020 696d 706f 7274 206a 6178 2e6e 756d    import jax.num
-0000c1b0: 7079 2061 7320 6a6e 700a 2020 2020 2020  py as jnp.      
-0000c1c0: 696d 706f 7274 2066 6c61 782e 6c69 6e65  import flax.line
-0000c1d0: 6e20 6173 206e 6e0a 0a20 2020 2020 2063  n as nn..      c
-0000c1e0: 6c61 7373 2041 7574 6f45 6e63 6f64 6572  lass AutoEncoder
-0000c1f0: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
-0000c200: 2020 2020 2064 6566 2073 6574 7570 2873       def setup(s
-0000c210: 656c 6629 3a0a 2020 2020 2020 2020 2020  elf):.          
-0000c220: 7365 6c66 2e65 6e63 6f64 6572 203d 206e  self.encoder = n
-0000c230: 6e2e 4465 6e73 6528 3329 0a20 2020 2020  n.Dense(3).     
-0000c240: 2020 2020 2073 656c 662e 6465 636f 6465       self.decode
-0000c250: 7220 3d20 6e6e 2e44 656e 7365 2835 290a  r = nn.Dense(5).
-0000c260: 0a20 2020 2020 2020 2064 6566 205f 5f63  .        def __c
-0000c270: 616c 6c5f 5f28 7365 6c66 2c20 7829 3a0a  all__(self, x):.
-0000c280: 2020 2020 2020 2020 2020 7265 7475 726e            return
-0000c290: 2073 656c 662e 6465 636f 6465 7228 7365   self.decoder(se
-0000c2a0: 6c66 2e65 6e63 6f64 6572 2878 2929 0a0a  lf.encoder(x))..
-0000c2b0: 2020 2020 2020 7820 3d20 6a6e 702e 6f6e        x = jnp.on
-0000c2c0: 6573 2828 3136 2c20 3929 290a 2020 2020  es((16, 9)).    
-0000c2d0: 2020 6165 203d 2041 7574 6f45 6e63 6f64    ae = AutoEncod
-0000c2e0: 6572 2829 0a20 2020 2020 2076 6172 6961  er().      varia
-0000c2f0: 626c 6573 203d 2061 652e 696e 6974 286a  bles = ae.init(j
-0000c300: 6178 2e72 616e 646f 6d2e 5052 4e47 4b65  ax.random.PRNGKe
-0000c310: 7928 3029 2c20 7829 0a20 2020 2020 206d  y(0), x).      m
-0000c320: 6f64 656c 203d 2061 652e 6269 6e64 2876  odel = ae.bind(v
-0000c330: 6172 6961 626c 6573 290a 2020 2020 2020  ariables).      
-0000c340: 7a20 3d20 6d6f 6465 6c2e 656e 636f 6465  z = model.encode
-0000c350: 7228 7829 0a20 2020 2020 2078 5f72 6563  r(x).      x_rec
-0000c360: 6f6e 7374 7275 6374 6564 203d 206d 6f64  onstructed = mod
-0000c370: 656c 2e64 6563 6f64 6572 287a 290a 0a20  el.decoder(z).. 
-0000c380: 2020 2041 7267 733a 0a20 2020 2020 2076     Args:.      v
-0000c390: 6172 6961 626c 6573 3a20 4120 6469 6374  ariables: A dict
-0000c3a0: 696f 6e61 7279 2063 6f6e 7461 696e 696e  ionary containin
-0000c3b0: 6720 7661 7269 6162 6c65 7320 6b65 7965  g variables keye
-0000c3c0: 6420 6279 2076 6172 6961 626c 650a 2020  d by variable.  
-0000c3d0: 2020 2020 2020 636f 6c6c 6563 7469 6f6e        collection
-0000c3e0: 732e 2053 6565 203a 6d6f 643a 6066 6c61  s. See :mod:`fla
-0000c3f0: 782e 636f 7265 2e76 6172 6961 626c 6573  x.core.variables
-0000c400: 6020 666f 7220 6d6f 7265 2064 6574 6169  ` for more detai
-0000c410: 6c73 0a20 2020 2020 2020 2061 626f 7574  ls.        about
-0000c420: 2076 6172 6961 626c 6573 2e0a 2020 2020   variables..    
-0000c430: 2020 2a61 7267 733a 204e 616d 6564 2061    *args: Named a
-0000c440: 7267 756d 656e 7473 2028 6e6f 7420 7573  rguments (not us
-0000c450: 6564 292e 0a20 2020 2020 2072 6e67 733a  ed)..      rngs:
-0000c460: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
-0000c470: 6579 7320 746f 2069 6e69 7469 616c 697a  eys to initializ
-0000c480: 6520 7468 6520 5052 4e47 2073 6571 7565  e the PRNG seque
-0000c490: 6e63 6573 2e0a 2020 2020 2020 6d75 7461  nces..      muta
-0000c4a0: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
-0000c4b0: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
-0000c4c0: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
-0000c4d0: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
-0000c4e0: 6c64 2062 650a 2020 2020 2020 2020 7472  ld be.        tr
-0000c4f0: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
-0000c500: 3a0a 2020 2020 2020 2020 2020 6060 626f  :.          ``bo
-0000c510: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
-0000c520: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
-0000c530: 6162 6c65 2e0a 2020 2020 2020 2020 2020  able..          
-0000c540: 6060 7374 7260 603a 2054 6865 206e 616d  ``str``: The nam
-0000c550: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
-0000c560: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
-0000c570: 2e0a 2020 2020 2020 2020 2020 6060 6c69  ..          ``li
-0000c580: 7374 6060 3a20 4120 6c69 7374 206f 6620  st``: A list of 
-0000c590: 6e61 6d65 7320 6f66 206d 7574 6162 6c65  names of mutable
-0000c5a0: 2063 6f6c 6c65 6374 696f 6e73 2e0a 0a20   collections... 
-0000c5b0: 2020 2052 6574 7572 6e73 3a0a 2020 2020     Returns:.    
-0000c5c0: 2020 4120 636f 7079 206f 6620 7468 6973    A copy of this
-0000c5d0: 2069 6e73 7461 6e63 6520 7769 7468 2062   instance with b
-0000c5e0: 6f75 6e64 2076 6172 6961 626c 6573 2061  ound variables a
-0000c5f0: 6e64 2052 4e47 732e 0a20 2020 2022 2222  nd RNGs..    """
-0000c600: 0a20 2020 204d 6f64 756c 652e 5f6d 6f64  .    Module._mod
-0000c610: 756c 655f 6368 6563 6b73 2873 656c 6629  ule_checks(self)
-0000c620: 0a0a 2020 2020 6465 6c20 6172 6773 0a20  ..    del args. 
-0000c630: 2020 2073 636f 7065 203d 2063 6f72 652e     scope = core.
-0000c640: 6269 6e64 2876 6172 6961 626c 6573 2c20  bind(variables, 
-0000c650: 726e 6773 3d72 6e67 732c 206d 7574 6162  rngs=rngs, mutab
-0000c660: 6c65 3d6d 7574 6162 6c65 290a 2020 2020  le=mutable).    
-0000c670: 7265 7475 726e 2073 656c 662e 636c 6f6e  return self.clon
-0000c680: 6528 7061 7265 6e74 3d73 636f 7065 290a  e(parent=scope).
-0000c690: 0a20 2064 6566 2075 6e62 696e 6428 7365  .  def unbind(se
-0000c6a0: 6c66 3a20 4d29 202d 3e20 5475 706c 655b  lf: M) -> Tuple[
-0000c6b0: 4d2c 2056 6172 6961 626c 6544 6963 745d  M, VariableDict]
-0000c6c0: 3a0a 2020 2020 2222 2252 6574 7572 6e73  :.    """Returns
-0000c6d0: 2061 6e20 756e 626f 756e 6420 636f 7079   an unbound copy
-0000c6e0: 206f 6620 6120 4d6f 6475 6c65 2061 6e64   of a Module and
-0000c6f0: 2069 7473 2076 6172 6961 626c 6573 2e0a   its variables..
-0000c700: 0a20 2020 2060 6075 6e62 696e 6460 6020  .    ``unbind`` 
-0000c710: 6865 6c70 7320 6372 6561 7465 2061 2073  helps create a s
-0000c720: 7461 7465 6c65 7373 2076 6572 7369 6f6e  tateless version
-0000c730: 206f 6620 6120 626f 756e 6420 4d6f 6475   of a bound Modu
-0000c740: 6c65 2e0a 0a20 2020 2041 6e20 6578 616d  le...    An exam
-0000c750: 706c 6520 6f66 2061 2063 6f6d 6d6f 6e20  ple of a common 
-0000c760: 7573 6520 6361 7365 3a20 746f 2065 7874  use case: to ext
-0000c770: 7261 6374 2061 2073 7562 2d4d 6f64 756c  ract a sub-Modul
-0000c780: 6520 6465 6669 6e65 6420 696e 7369 6465  e defined inside
-0000c790: 0a20 2020 2060 6073 6574 7570 2829 6060  .    ``setup()``
-0000c7a0: 2061 6e64 2069 7473 2063 6f72 7265 7370   and its corresp
-0000c7b0: 6f6e 6469 6e67 2076 6172 6961 626c 6573  onding variables
-0000c7c0: 3a20 3129 2074 656d 706f 7261 7269 6c79  : 1) temporarily
-0000c7d0: 2060 6062 696e 6460 6020 7468 6520 7061   ``bind`` the pa
-0000c7e0: 7265 6e74 0a20 2020 204d 6f64 756c 653b  rent.    Module;
-0000c7f0: 2061 6e64 2074 6865 6e20 3229 2060 6075   and then 2) ``u
-0000c800: 6e62 696e 6460 6020 7468 6520 6465 7369  nbind`` the desi
-0000c810: 7265 6420 7375 622d 4d6f 6475 6c65 2e20  red sub-Module. 
-0000c820: 2852 6563 616c 6c20 7468 6174 2060 6073  (Recall that ``s
-0000c830: 6574 7570 2829 6060 0a20 2020 2069 7320  etup()``.    is 
-0000c840: 6f6e 6c79 2063 616c 6c65 6420 7768 656e  only called when
-0000c850: 2074 6865 204d 6f64 756c 6520 6973 2062   the Module is b
-0000c860: 6f75 6e64 2e29 3a3a 0a0a 2020 2020 2020  ound.)::..      
-0000c870: 636c 6173 7320 4175 746f 456e 636f 6465  class AutoEncode
-0000c880: 7228 6e6e 2e4d 6f64 756c 6529 3a0a 2020  r(nn.Module):.  
-0000c890: 2020 2020 2020 6465 6620 7365 7475 7028        def setup(
-0000c8a0: 7365 6c66 293a 0a20 2020 2020 2020 2020  self):.         
-0000c8b0: 2073 656c 662e 656e 636f 6465 7220 3d20   self.encoder = 
-0000c8c0: 456e 636f 6465 7228 290a 2020 2020 2020  Encoder().      
-0000c8d0: 2020 2020 7365 6c66 2e64 6563 6f64 6572      self.decoder
-0000c8e0: 203d 2044 6563 6f64 6572 2829 0a0a 2020   = Decoder()..  
-0000c8f0: 2020 2020 2020 6465 6620 5f5f 6361 6c6c        def __call
-0000c900: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
-0000c910: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
-0000c920: 6c66 2e64 6563 6f64 6572 2873 656c 662e  lf.decoder(self.
-0000c930: 656e 636f 6465 7228 7829 290a 0a20 2020  encoder(x))..   
-0000c940: 2020 206d 6f64 756c 6520 3d20 4175 746f     module = Auto
-0000c950: 456e 636f 6465 7228 290a 2020 2020 2020  Encoder().      
-0000c960: 7661 7269 6162 6c65 7320 3d20 6d6f 6475  variables = modu
-0000c970: 6c65 2e69 6e69 7428 6a61 782e 7261 6e64  le.init(jax.rand
-0000c980: 6f6d 2e50 524e 474b 6579 2830 292c 206a  om.PRNGKey(0), j
-0000c990: 6e70 2e6f 6e65 7328 2831 2c20 3738 3429  np.ones((1, 784)
-0000c9a0: 2929 0a20 2020 2020 202e 2e2e 0a20 2020  )).      ....   
-0000c9b0: 2020 2023 2045 7874 7261 6374 2074 6865     # Extract the
-0000c9c0: 2045 6e63 6f64 6572 2073 7562 2d4d 6f64   Encoder sub-Mod
-0000c9d0: 756c 6520 616e 6420 6974 7320 7661 7269  ule and its vari
-0000c9e0: 6162 6c65 730a 2020 2020 2020 656e 636f  ables.      enco
-0000c9f0: 6465 722c 2065 6e63 6f64 6572 5f76 6172  der, encoder_var
-0000ca00: 7320 3d20 6d6f 6475 6c65 2e62 696e 6428  s = module.bind(
-0000ca10: 7661 7269 6162 6c65 7329 2e65 6e63 6f64  variables).encod
-0000ca20: 6572 2e75 6e62 696e 6428 290a 0a20 2020  er.unbind()..   
-0000ca30: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000ca40: 4120 7475 706c 6520 7769 7468 2061 6e20  A tuple with an 
-0000ca50: 756e 626f 756e 6420 636f 7079 206f 6620  unbound copy of 
-0000ca60: 7468 6973 204d 6f64 756c 6520 616e 6420  this Module and 
-0000ca70: 6974 7320 7661 7269 6162 6c65 732e 0a20  its variables.. 
-0000ca80: 2020 2022 2222 0a20 2020 204d 6f64 756c     """.    Modul
-0000ca90: 652e 5f6d 6f64 756c 655f 6368 6563 6b73  e._module_checks
-0000caa0: 2873 656c 6629 0a0a 2020 2020 6966 2073  (self)..    if s
-0000cab0: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-0000cac0: 653a 0a20 2020 2020 2072 6169 7365 2065  e:.      raise e
-0000cad0: 7272 6f72 732e 4361 6c6c 556e 6269 6e64  rrors.CallUnbind
-0000cae0: 4f6e 556e 626f 756e 644d 6f64 756c 6545  OnUnboundModuleE
-0000caf0: 7272 6f72 2829 0a0a 2020 2020 7661 7269  rror()..    vari
-0000cb00: 6162 6c65 7320 3d20 7365 6c66 2e76 6172  ables = self.var
-0000cb10: 6961 626c 6573 0a20 2020 206d 6f64 756c  iables.    modul
-0000cb20: 6520 3d20 7365 6c66 2e63 6c6f 6e65 2829  e = self.clone()
-0000cb30: 0a20 2020 2072 6574 7572 6e20 6d6f 6475  .    return modu
-0000cb40: 6c65 2c20 7661 7269 6162 6c65 730a 0a20  le, variables.. 
-0000cb50: 2040 7472 6163 6562 6163 6b5f 7574 696c   @traceback_util
-0000cb60: 2e61 7069 5f62 6f75 6e64 6172 790a 2020  .api_boundary.  
-0000cb70: 6465 6620 6170 706c 7928 7365 6c66 2c0a  def apply(self,.
-0000cb80: 2020 2020 2020 2020 2020 2020 7661 7269              vari
-0000cb90: 6162 6c65 733a 2056 6172 6961 626c 6544  ables: VariableD
-0000cba0: 6963 742c 0a20 2020 2020 2020 2020 2020  ict,.           
-0000cbb0: 202a 6172 6773 2c0a 2020 2020 2020 2020   *args,.        
-0000cbc0: 2020 2020 726e 6773 3a20 4f70 7469 6f6e      rngs: Option
-0000cbd0: 616c 5b52 4e47 5365 7175 656e 6365 735d  al[RNGSequences]
-0000cbe0: 203d 204e 6f6e 652c 0a20 2020 2020 2020   = None,.       
-0000cbf0: 2020 2020 206d 6574 686f 643a 2055 6e69       method: Uni
-0000cc00: 6f6e 5b43 616c 6c61 626c 655b 2e2e 2e2c  on[Callable[...,
-0000cc10: 2041 6e79 5d2c 2073 7472 2c20 4e6f 6e65   Any], str, None
-0000cc20: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 2020  ] = None,.      
-0000cc30: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
-0000cc40: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
-0000cc50: 3d20 4661 6c73 652c 0a20 2020 2020 2020  = False,.       
-0000cc60: 2020 2020 2063 6170 7475 7265 5f69 6e74       capture_int
-0000cc70: 6572 6d65 6469 6174 6573 3a20 556e 696f  ermediates: Unio
-0000cc80: 6e5b 626f 6f6c 2c20 4361 6c6c 6162 6c65  n[bool, Callable
-0000cc90: 5b5b 274d 6f64 756c 6527 2c20 7374 725d  [['Module', str]
-0000cca0: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
-0000ccb0: 2c0a 2020 2020 2020 2020 2020 2020 2a2a  ,.            **
-0000ccc0: 6b77 6172 6773 2920 2d3e 2055 6e69 6f6e  kwargs) -> Union
-0000ccd0: 5b41 6e79 2c20 5475 706c 655b 416e 792c  [Any, Tuple[Any,
-0000cce0: 2055 6e69 6f6e 5b46 726f 7a65 6e56 6172   Union[FrozenVar
-0000ccf0: 6961 626c 6544 6963 742c 2044 6963 745b  iableDict, Dict[
-0000cd00: 7374 722c 2041 6e79 5d5d 5d5d 3a0a 2020  str, Any]]]]:.  
-0000cd10: 2020 2222 2241 7070 6c69 6573 2061 206d    """Applies a m
-0000cd20: 6f64 756c 6520 6d65 7468 6f64 2074 6f20  odule method to 
-0000cd30: 7661 7269 6162 6c65 7320 616e 6420 7265  variables and re
-0000cd40: 7475 726e 7320 6f75 7470 7574 2061 6e64  turns output and
-0000cd50: 206d 6f64 6966 6965 6420 7661 7269 6162   modified variab
-0000cd60: 6c65 732e 0a0a 2020 2020 4e6f 7465 2074  les...    Note t
-0000cd70: 6861 7420 606d 6574 686f 6460 2073 686f  hat `method` sho
-0000cd80: 756c 6420 6265 2073 6574 2069 6620 6f6e  uld be set if on
-0000cd90: 6520 776f 756c 6420 6c69 6b65 2074 6f20  e would like to 
-0000cda0: 6361 6c6c 2060 6170 706c 7960 206f 6e20  call `apply` on 
-0000cdb0: 610a 2020 2020 6469 6666 6572 656e 7420  a.    different 
-0000cdc0: 636c 6173 7320 6d65 7468 6f64 2074 6861  class method tha
-0000cdd0: 6e20 6060 5f5f 6361 6c6c 5f5f 6060 2e20  n ``__call__``. 
-0000cde0: 466f 7220 696e 7374 616e 6365 2c20 7375  For instance, su
-0000cdf0: 7070 6f73 6520 610a 2020 2020 5472 616e  ppose a.    Tran
-0000ce00: 7366 6f72 6d65 7220 6d6f 6475 6c65 7320  sformer modules 
-0000ce10: 6861 7320 6120 6d65 7468 6f64 2063 616c  has a method cal
-0000ce20: 6c65 6420 6065 6e63 6f64 6560 2c20 7468  led `encode`, th
-0000ce30: 656e 2074 6865 2066 6f6c 6c6f 7769 6e67  en the following
-0000ce40: 2063 616c 6c73 0a20 2020 2060 6170 706c   calls.    `appl
-0000ce50: 7960 206f 6e20 7468 6174 206d 6574 686f  y` on that metho
-0000ce60: 643a 3a0a 0a20 2020 2020 206d 6f64 656c  d::..      model
-0000ce70: 203d 2054 7261 6e73 666f 726d 6572 2829   = Transformer()
-0000ce80: 0a20 2020 2020 2065 6e63 6f64 6564 203d  .      encoded =
-0000ce90: 206d 6f64 656c 2e61 7070 6c79 287b 2770   model.apply({'p
-0000cea0: 6172 616d 7327 3a20 7061 7261 6d73 7d2c  arams': params},
-0000ceb0: 2078 2c20 6d65 7468 6f64 3d54 7261 6e73   x, method=Trans
-0000cec0: 666f 726d 6572 2e65 6e63 6f64 6529 0a0a  former.encode)..
-0000ced0: 2020 2020 4966 2061 2066 756e 6374 696f      If a functio
-0000cee0: 6e20 696e 7374 616e 6365 2069 7320 7072  n instance is pr
-0000cef0: 6f76 6964 6564 2c20 7468 6520 756e 626f  ovided, the unbo
-0000cf00: 756e 6420 6675 6e63 7469 6f6e 2069 7320  und function is 
-0000cf10: 7573 6564 2e20 466f 720a 2020 2020 696e  used. For.    in
-0000cf20: 7374 616e 6365 2c20 7468 6520 6578 616d  stance, the exam
-0000cf30: 706c 6520 6265 6c6f 7720 6973 2065 7175  ple below is equ
-0000cf40: 6976 616c 656e 7420 746f 2074 6865 206f  ivalent to the o
-0000cf50: 6e65 2061 626f 7665 3a3a 0a0a 2020 2020  ne above::..    
-0000cf60: 2020 656e 636f 6465 6420 3d20 6d6f 6465    encoded = mode
-0000cf70: 6c2e 6170 706c 7928 7b27 7061 7261 6d73  l.apply({'params
-0000cf80: 273a 2070 6172 616d 737d 2c20 782c 206d  ': params}, x, m
-0000cf90: 6574 686f 643d 6d6f 6465 6c2e 656e 636f  ethod=model.enco
-0000cfa0: 6465 290a 0a20 2020 2059 6f75 2063 616e  de)..    You can
-0000cfb0: 2061 6c73 6f20 7061 7373 2061 2073 7472   also pass a str
-0000cfc0: 696e 6720 746f 2061 2063 616c 6c61 626c  ing to a callabl
-0000cfd0: 6520 6174 7472 6962 7574 6520 6f66 2074  e attribute of t
-0000cfe0: 6865 206d 6f64 756c 652e 2046 6f72 0a20  he module. For. 
-0000cff0: 2020 2065 7861 6d70 6c65 2c20 7468 6520     example, the 
-0000d000: 7072 6576 696f 7573 2063 616e 2062 6520  previous can be 
-0000d010: 7772 6974 7465 6e20 6173 3a3a 0a0a 2020  written as::..  
-0000d020: 2020 2020 656e 636f 6465 6420 3d20 6d6f      encoded = mo
-0000d030: 6465 6c2e 6170 706c 7928 7b27 7061 7261  del.apply({'para
-0000d040: 6d73 273a 2070 6172 616d 737d 2c20 782c  ms': params}, x,
-0000d050: 206d 6574 686f 643d 2765 6e63 6f64 6527   method='encode'
-0000d060: 290a 0a20 2020 204e 6f74 6520 6060 6d65  )..    Note ``me
-0000d070: 7468 6f64 6060 2063 616e 2061 6c73 6f20  thod`` can also 
-0000d080: 6265 2061 2066 756e 6374 696f 6e20 7468  be a function th
-0000d090: 6174 2069 7320 6e6f 7420 6465 6669 6e65  at is not define
-0000d0a0: 6420 696e 0a20 2020 2060 6054 7261 6e73  d in.    ``Trans
-0000d0b0: 666f 726d 6572 6060 2e20 496e 2074 6861  former``. In tha
-0000d0c0: 7420 6361 7365 2c20 7468 6520 6675 6e63  t case, the func
-0000d0d0: 7469 6f6e 2073 686f 756c 6420 6861 7665  tion should have
-0000d0e0: 2061 7420 6c65 6173 7420 6f6e 650a 2020   at least one.  
-0000d0f0: 2020 6172 6775 6d65 6e74 2072 6570 7265    argument repre
-0000d100: 7365 6e74 696e 6720 616e 2069 6e73 7461  senting an insta
-0000d110: 6e63 6520 6f66 2074 6865 204d 6f64 756c  nce of the Modul
-0000d120: 6520 636c 6173 733a 3a0a 0a20 2020 2020  e class::..     
-0000d130: 2064 6566 206f 7468 6572 5f66 6e28 696e   def other_fn(in
-0000d140: 7374 616e 6365 2c20 2e2e 2e29 3a0a 2020  stance, ...):.  
-0000d150: 2020 2020 2020 696e 7374 616e 6365 2e73        instance.s
-0000d160: 6f6d 655f 6d6f 6475 6c65 5f61 7474 7228  ome_module_attr(
-0000d170: 2e2e 2e29 0a20 2020 2020 2020 202e 2e2e  ...).        ...
-0000d180: 0a0a 2020 2020 2020 6d6f 6465 6c2e 6170  ..      model.ap
-0000d190: 706c 7928 7b27 7061 7261 6d73 273a 2070  ply({'params': p
-0000d1a0: 6172 616d 737d 2c20 782c 206d 6574 686f  arams}, x, metho
-0000d1b0: 643d 6f74 6865 725f 666e 290a 0a20 2020  d=other_fn)..   
-0000d1c0: 2041 7267 733a 0a20 2020 2020 2076 6172   Args:.      var
-0000d1d0: 6961 626c 6573 3a20 4120 6469 6374 696f  iables: A dictio
-0000d1e0: 6e61 7279 2063 6f6e 7461 696e 696e 6720  nary containing 
-0000d1f0: 7661 7269 6162 6c65 7320 6b65 7965 6420  variables keyed 
-0000d200: 6279 2076 6172 6961 626c 650a 2020 2020  by variable.    
-0000d210: 2020 2020 636f 6c6c 6563 7469 6f6e 732e      collections.
-0000d220: 2053 6565 203a 6d6f 643a 6066 6c61 782e   See :mod:`flax.
-0000d230: 636f 7265 2e76 6172 6961 626c 6573 6020  core.variables` 
-0000d240: 666f 7220 6d6f 7265 2064 6574 6169 6c73  for more details
-0000d250: 0a20 2020 2020 2020 2061 626f 7574 2076  .        about v
-0000d260: 6172 6961 626c 6573 2e0a 2020 2020 2020  ariables..      
-0000d270: 2a61 7267 733a 204e 616d 6564 2061 7267  *args: Named arg
-0000d280: 756d 656e 7473 2070 6173 7365 6420 746f  uments passed to
-0000d290: 2074 6865 2073 7065 6369 6669 6564 2061   the specified a
-0000d2a0: 7070 6c79 206d 6574 686f 642e 0a20 2020  pply method..   
-0000d2b0: 2020 2072 6e67 733a 2061 2064 6963 7420     rngs: a dict 
-0000d2c0: 6f66 2050 524e 474b 6579 7320 746f 2069  of PRNGKeys to i
-0000d2d0: 6e69 7469 616c 697a 6520 7468 6520 5052  nitialize the PR
-0000d2e0: 4e47 2073 6571 7565 6e63 6573 2e0a 2020  NG sequences..  
-0000d2f0: 2020 2020 2020 5468 6520 2270 6172 616d        The "param
-0000d300: 7322 2050 524e 4720 7365 7175 656e 6365  s" PRNG sequence
-0000d310: 2069 7320 7573 6564 2074 6f20 696e 6974   is used to init
-0000d320: 6961 6c69 7a65 2070 6172 616d 6574 6572  ialize parameter
-0000d330: 732e 0a20 2020 2020 206d 6574 686f 643a  s..      method:
-0000d340: 2041 2066 756e 6374 696f 6e20 746f 2063   A function to c
-0000d350: 616c 6c20 6170 706c 7920 6f6e 2e20 5468  all apply on. Th
-0000d360: 6973 2069 7320 6765 6e65 7261 6c6c 7920  is is generally 
-0000d370: 6120 6675 6e63 7469 6f6e 2069 6e20 7468  a function in th
-0000d380: 650a 2020 2020 2020 2020 6d6f 6475 6c65  e.        module
-0000d390: 2e20 4966 2070 726f 7669 6465 642c 2061  . If provided, a
-0000d3a0: 7070 6c69 6573 2074 6869 7320 6d65 7468  pplies this meth
-0000d3b0: 6f64 2e20 4966 206e 6f74 2070 726f 7669  od. If not provi
-0000d3c0: 6465 642c 2061 7070 6c69 6573 2074 6865  ded, applies the
-0000d3d0: 0a20 2020 2020 2020 2060 605f 5f63 616c  .        ``__cal
-0000d3e0: 6c5f 5f60 6020 6d65 7468 6f64 206f 6620  l__`` method of 
-0000d3f0: 7468 6520 6d6f 6475 6c65 2e20 4120 7374  the module. A st
-0000d400: 7269 6e67 2063 616e 2061 6c73 6f20 6265  ring can also be
-0000d410: 2070 726f 7669 6465 6420 746f 0a20 2020   provided to.   
-0000d420: 2020 2020 2073 7065 6369 6679 2061 206d       specify a m
-0000d430: 6574 686f 6420 6279 206e 616d 652e 0a20  ethod by name.. 
-0000d440: 2020 2020 206d 7574 6162 6c65 3a20 4361       mutable: Ca
-0000d450: 6e20 6265 2062 6f6f 6c2c 2073 7472 2c20  n be bool, str, 
-0000d460: 6f72 206c 6973 742e 2053 7065 6369 6669  or list. Specifi
-0000d470: 6573 2077 6869 6368 2063 6f6c 6c65 6374  es which collect
-0000d480: 696f 6e73 2073 686f 756c 6420 6265 0a20  ions should be. 
-0000d490: 2020 2020 2020 2020 2020 2020 2020 7472                tr
-0000d4a0: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
-0000d4b0: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
-0000d4c0: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
-0000d4d0: 7265 206d 7574 6162 6c65 2e0a 2020 2020  re mutable..    
-0000d4e0: 2020 2020 2020 2020 2020 2060 6073 7472             ``str
-0000d4f0: 6060 3a20 5468 6520 6e61 6d65 206f 6620  ``: The name of 
-0000d500: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
-0000d510: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
-0000d520: 6973 7460 603a 2041 0a20 2020 2020 2020  ist``: A.       
-0000d530: 2020 2020 2020 2020 6c69 7374 206f 6620          list of 
-0000d540: 6e61 6d65 7320 6f66 206d 7574 6162 6c65  names of mutable
-0000d550: 2063 6f6c 6c65 6374 696f 6e73 2e0a 2020   collections..  
-0000d560: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-0000d570: 726d 6564 6961 7465 733a 2049 6620 6054  rmediates: If `T
-0000d580: 7275 6560 2c20 6361 7074 7572 6573 2069  rue`, captures i
-0000d590: 6e74 6572 6d65 6469 6174 6520 7265 7475  ntermediate retu
-0000d5a0: 726e 2076 616c 7565 730a 2020 2020 2020  rn values.      
-0000d5b0: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
-0000d5c0: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
-0000d5d0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
-0000d5e0: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
-0000d5f0: 6c74 206f 6e6c 790a 2020 2020 2020 2020  lt only.        
-0000d600: 7468 6520 7265 7475 726e 2076 616c 7565  the return value
-0000d610: 7320 6f66 2061 6c6c 2060 605f 5f63 616c  s of all ``__cal
-0000d620: 6c5f 5f60 6020 6d65 7468 6f64 7320 6172  l__`` methods ar
-0000d630: 6520 7374 6f72 6564 2e20 4120 6675 6e63  e stored. A func
-0000d640: 7469 6f6e 2063 616e 0a20 2020 2020 2020  tion can.       
-0000d650: 2062 6520 7061 7373 6564 2074 6f20 6368   be passed to ch
-0000d660: 616e 6765 2074 6865 2066 696c 7465 7220  ange the filter 
-0000d670: 6265 6861 7669 6f72 2e20 5468 6520 6669  behavior. The fi
-0000d680: 6c74 6572 2066 756e 6374 696f 6e20 7461  lter function ta
-0000d690: 6b65 730a 2020 2020 2020 2020 7468 6520  kes.        the 
-0000d6a0: 4d6f 6475 6c65 2069 6e73 7461 6e63 6520  Module instance 
-0000d6b0: 616e 6420 6d65 7468 6f64 206e 616d 6520  and method name 
-0000d6c0: 616e 6420 7265 7475 726e 7320 6120 626f  and returns a bo
-0000d6d0: 6f6c 2069 6e64 6963 6174 696e 670a 2020  ol indicating.  
-0000d6e0: 2020 2020 2020 7768 6574 6865 7220 7468        whether th
-0000d6f0: 6520 6f75 7470 7574 206f 6620 7468 6174  e output of that
-0000d700: 206d 6574 686f 6420 696e 766f 6361 7469   method invocati
-0000d710: 6f6e 2073 686f 756c 6420 6265 2073 746f  on should be sto
-0000d720: 7265 642e 0a20 2020 2020 202a 2a6b 7761  red..      **kwa
-0000d730: 7267 733a 204b 6579 776f 7264 2061 7267  rgs: Keyword arg
-0000d740: 756d 656e 7473 2070 6173 7365 6420 746f  uments passed to
-0000d750: 2074 6865 2073 7065 6369 6669 6564 2061   the specified a
-0000d760: 7070 6c79 206d 6574 686f 642e 0a20 2020  pply method..   
-0000d770: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000d780: 4966 2060 606d 7574 6162 6c65 6060 2069  If ``mutable`` i
-0000d790: 7320 4661 6c73 652c 2072 6574 7572 6e73  s False, returns
-0000d7a0: 206f 7574 7075 742e 2049 6620 616e 7920   output. If any 
-0000d7b0: 636f 6c6c 6563 7469 6f6e 7320 6172 650a  collections are.
-0000d7c0: 2020 2020 2020 6d75 7461 626c 652c 2072        mutable, r
-0000d7d0: 6574 7572 6e73 2060 6028 6f75 7470 7574  eturns ``(output
-0000d7e0: 2c20 7661 7273 2960 602c 2077 6865 7265  , vars)``, where
-0000d7f0: 2060 6076 6172 7360 6020 6172 6520 6973   ``vars`` are is
-0000d800: 2061 2064 6963 740a 2020 2020 2020 6f66   a dict.      of
-0000d810: 2074 6865 206d 6f64 6966 6965 6420 636f   the modified co
-0000d820: 6c6c 6563 7469 6f6e 732e 0a20 2020 2022  llections..    "
-0000d830: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
-0000d840: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
-0000d850: 6629 0a0a 2020 2020 6966 2069 7369 6e73  f)..    if isins
-0000d860: 7461 6e63 6528 6d65 7468 6f64 2c20 7374  tance(method, st
-0000d870: 7229 3a0a 2020 2020 2020 6174 7472 6962  r):.      attrib
-0000d880: 7574 655f 6e61 6d65 203d 206d 6574 686f  ute_name = metho
-0000d890: 640a 2020 2020 2020 6d65 7468 6f64 203d  d.      method =
-0000d8a0: 2067 6574 6174 7472 2873 656c 662c 2061   getattr(self, a
-0000d8b0: 7474 7269 6275 7465 5f6e 616d 6529 0a20  ttribute_name). 
-0000d8c0: 2020 2020 2069 6620 6e6f 7420 6361 6c6c       if not call
-0000d8d0: 6162 6c65 286d 6574 686f 6429 3a0a 2020  able(method):.  
-0000d8e0: 2020 2020 2020 636c 6173 735f 6e61 6d65        class_name
-0000d8f0: 203d 2074 7970 6528 7365 6c66 292e 5f5f   = type(self).__
-0000d900: 6e61 6d65 5f5f 0a20 2020 2020 2020 2072  name__.        r
-0000d910: 6169 7365 2054 7970 6545 7272 6f72 2866  aise TypeError(f
-0000d920: 275c 277b 636c 6173 735f 6e61 6d65 7d2e  '\'{class_name}.
-0000d930: 7b61 7474 7269 6275 7465 5f6e 616d 657d  {attribute_name}
-0000d940: 5c27 206d 7573 7420 6265 2061 2063 616c  \' must be a cal
-0000d950: 6c61 626c 652c 2067 6f74 207b 7479 7065  lable, got {type
-0000d960: 286d 6574 686f 6429 7d2e 2729 0a20 2020  (method)}.').   
-0000d970: 2065 6c69 6620 6d65 7468 6f64 2069 7320   elif method is 
-0000d980: 4e6f 6e65 3a0a 2020 2020 2020 6d65 7468  None:.      meth
-0000d990: 6f64 203d 2073 656c 662e 5f5f 6361 6c6c  od = self.__call
-0000d9a0: 5f5f 0a20 2020 206d 6574 686f 6420 3d20  __.    method = 
-0000d9b0: 5f67 6574 5f75 6e62 6f75 6e64 5f66 6e28  _get_unbound_fn(
-0000d9c0: 6d65 7468 6f64 290a 2020 2020 7265 7475  method).    retu
-0000d9d0: 726e 2061 7070 6c79 280a 2020 2020 2020  rn apply(.      
-0000d9e0: 2020 6d65 7468 6f64 2c20 7365 6c66 2c0a    method, self,.
-0000d9f0: 2020 2020 2020 2020 6d75 7461 626c 653d          mutable=
-0000da00: 6d75 7461 626c 652c 0a20 2020 2020 2020  mutable,.       
-0000da10: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-0000da20: 6469 6174 6573 3d63 6170 7475 7265 5f69  diates=capture_i
-0000da30: 6e74 6572 6d65 6469 6174 6573 2c0a 2020  ntermediates,.  
-0000da40: 2020 2928 7661 7269 6162 6c65 732c 202a    )(variables, *
-0000da50: 6172 6773 2c20 2a2a 6b77 6172 6773 2c20  args, **kwargs, 
-0000da60: 726e 6773 3d72 6e67 7329 0a0a 2020 4074  rngs=rngs)..  @t
-0000da70: 7261 6365 6261 636b 5f75 7469 6c2e 6170  raceback_util.ap
-0000da80: 695f 626f 756e 6461 7279 0a20 2064 6566  i_boundary.  def
-0000da90: 2069 6e69 745f 7769 7468 5f6f 7574 7075   init_with_outpu
-0000daa0: 7428 7365 6c66 2c0a 2020 2020 2020 2020  t(self,.        
-0000dab0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-0000dac0: 6e67 733a 2055 6e69 6f6e 5b50 524e 474b  ngs: Union[PRNGK
-0000dad0: 6579 2c20 524e 4753 6571 7565 6e63 6573  ey, RNGSequences
-0000dae0: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-0000daf0: 2020 2020 2020 2020 2020 2a61 7267 732c            *args,
-0000db00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000db10: 2020 2020 2020 2020 6d65 7468 6f64 3a20          method: 
-0000db20: 556e 696f 6e5b 4361 6c6c 6162 6c65 5b2e  Union[Callable[.
-0000db30: 2e2e 2c20 416e 795d 2c20 7374 722c 204e  .., Any], str, N
-0000db40: 6f6e 655d 203d 204e 6f6e 652c 0a20 2020  one] = None,.   
-0000db50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000db60: 2020 2020 6d75 7461 626c 653a 2043 6f6c      mutable: Col
-0000db70: 6c65 6374 696f 6e46 696c 7465 7220 3d20  lectionFilter = 
-0000db80: 4465 6e79 4c69 7374 2827 696e 7465 726d  DenyList('interm
-0000db90: 6564 6961 7465 7327 292c 0a20 2020 2020  ediates'),.     
-0000dba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000dbb0: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
-0000dbc0: 6564 6961 7465 733a 2055 6e69 6f6e 5b62  ediates: Union[b
-0000dbd0: 6f6f 6c2c 2043 616c 6c61 626c 655b 5b27  ool, Callable[['
-0000dbe0: 4d6f 6475 6c65 272c 2073 7472 5d2c 2062  Module', str], b
-0000dbf0: 6f6f 6c5d 5d20 3d20 4661 6c73 652c 0a20  ool]] = False,. 
-0000dc00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000dc10: 2020 2020 2020 2a2a 6b77 6172 6773 2920        **kwargs) 
-0000dc20: 2d3e 2054 7570 6c65 5b41 6e79 2c20 556e  -> Tuple[Any, Un
-0000dc30: 696f 6e5b 4672 6f7a 656e 5661 7269 6162  ion[FrozenVariab
-0000dc40: 6c65 4469 6374 2c20 4469 6374 5b73 7472  leDict, Dict[str
-0000dc50: 2c20 416e 795d 5d5d 3a0a 2020 2020 2222  , Any]]]:.    ""
-0000dc60: 2249 6e69 7469 616c 697a 6573 2061 206d  "Initializes a m
-0000dc70: 6f64 756c 6520 6d65 7468 6f64 2077 6974  odule method wit
-0000dc80: 6820 7661 7269 6162 6c65 7320 616e 6420  h variables and 
-0000dc90: 7265 7475 726e 7320 6f75 7470 7574 2061  returns output a
-0000dca0: 6e64 206d 6f64 6966 6965 6420 7661 7269  nd modified vari
-0000dcb0: 6162 6c65 732e 0a0a 2020 2020 4172 6773  ables...    Args
-0000dcc0: 3a0a 2020 2020 2020 726e 6773 3a20 5468  :.      rngs: Th
-0000dcd0: 6520 726e 6773 2066 6f72 2074 6865 2076  e rngs for the v
-0000dce0: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
-0000dcf0: 6f6e 732e 0a20 2020 2020 202a 6172 6773  ons..      *args
-0000dd00: 3a20 4e61 6d65 6420 6172 6775 6d65 6e74  : Named argument
-0000dd10: 7320 7061 7373 6564 2074 6f20 7468 6520  s passed to the 
-0000dd20: 696e 6974 2066 756e 6374 696f 6e2e 0a20  init function.. 
-0000dd30: 2020 2020 206d 6574 686f 643a 2041 6e20       method: An 
-0000dd40: 6f70 7469 6f6e 616c 206d 6574 686f 642e  optional method.
-0000dd50: 2049 6620 7072 6f76 6964 6564 2c20 6170   If provided, ap
-0000dd60: 706c 6965 7320 7468 6973 206d 6574 686f  plies this metho
-0000dd70: 642e 2049 6620 6e6f 740a 2020 2020 2020  d. If not.      
-0000dd80: 2020 7072 6f76 6964 6564 2c20 6170 706c    provided, appl
-0000dd90: 6965 7320 7468 6520 6060 5f5f 6361 6c6c  ies the ``__call
-0000dda0: 5f5f 6060 206d 6574 686f 642e 2041 2073  __`` method. A s
-0000ddb0: 7472 696e 6720 6361 6e20 616c 736f 2062  tring can also b
-0000ddc0: 6527 0a20 2020 2020 2020 2070 726f 7669  e'.        provi
-0000ddd0: 6465 6420 746f 2073 7065 6369 6679 2061  ded to specify a
-0000dde0: 206d 6574 686f 6420 6279 206e 616d 652e   method by name.
-0000ddf0: 0a20 2020 2020 206d 7574 6162 6c65 3a20  .      mutable: 
-0000de00: 4361 6e20 6265 2062 6f6f 6c2c 2073 7472  Can be bool, str
-0000de10: 2c20 6f72 206c 6973 742e 2053 7065 6369  , or list. Speci
-0000de20: 6669 6573 2077 6869 6368 2063 6f6c 6c65  fies which colle
-0000de30: 6374 696f 6e73 2073 686f 756c 6420 6265  ctions should be
-0000de40: 0a20 2020 2020 2020 2074 7265 6174 6564  .        treated
-0000de50: 2061 7320 6d75 7461 626c 653a 2060 6062   as mutable: ``b
-0000de60: 6f6f 6c60 603a 2061 6c6c 2f6e 6f20 636f  ool``: all/no co
-0000de70: 6c6c 6563 7469 6f6e 7320 6172 6520 6d75  llections are mu
-0000de80: 7461 626c 652e 0a20 2020 2020 2020 2060  table..        `
-0000de90: 6073 7472 6060 3a20 5468 6520 6e61 6d65  `str``: The name
-0000dea0: 206f 6620 6120 7369 6e67 6c65 206d 7574   of a single mut
-0000deb0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e2e  able collection.
-0000dec0: 2060 606c 6973 7460 603a 2041 0a20 2020   ``list``: A.   
-0000ded0: 2020 2020 206c 6973 7420 6f66 206e 616d       list of nam
-0000dee0: 6573 206f 6620 6d75 7461 626c 6520 636f  es of mutable co
-0000def0: 6c6c 6563 7469 6f6e 732e 2042 7920 6465  llections. By de
-0000df00: 6661 756c 7420 616c 6c20 636f 6c6c 6563  fault all collec
-0000df10: 7469 6f6e 730a 2020 2020 2020 2020 6578  tions.        ex
-0000df20: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
-0000df30: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
-0000df40: 2e0a 2020 2020 2020 6361 7074 7572 655f  ..      capture_
-0000df50: 696e 7465 726d 6564 6961 7465 733a 2049  intermediates: I
-0000df60: 6620 6054 7275 6560 2c20 6361 7074 7572  f `True`, captur
-0000df70: 6573 2069 6e74 6572 6d65 6469 6174 6520  es intermediate 
-0000df80: 7265 7475 726e 2076 616c 7565 730a 2020  return values.  
-0000df90: 2020 2020 2020 6f66 2061 6c6c 204d 6f64        of all Mod
-0000dfa0: 756c 6573 2069 6e73 6964 6520 7468 6520  ules inside the 
-0000dfb0: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
-0000dfc0: 636f 6c6c 6563 7469 6f6e 2e20 4279 2064  collection. By d
-0000dfd0: 6566 6175 6c74 206f 6e6c 790a 2020 2020  efault only.    
-0000dfe0: 2020 2020 7468 6520 7265 7475 726e 2076      the return v
-0000dff0: 616c 7565 7320 6f66 2061 6c6c 2060 605f  alues of all ``_
-0000e000: 5f63 616c 6c5f 5f60 6020 6d65 7468 6f64  _call__`` method
-0000e010: 7320 6172 6520 7374 6f72 6564 2e20 4120  s are stored. A 
-0000e020: 6675 6e63 7469 6f6e 2063 616e 0a20 2020  function can.   
-0000e030: 2020 2020 2062 6520 7061 7373 6564 2074       be passed t
-0000e040: 6f20 6368 616e 6765 2074 6865 2066 696c  o change the fil
-0000e050: 7465 7220 6265 6861 7669 6f72 2e20 5468  ter behavior. Th
-0000e060: 6520 6669 6c74 6572 2066 756e 6374 696f  e filter functio
-0000e070: 6e20 7461 6b65 730a 2020 2020 2020 2020  n takes.        
-0000e080: 7468 6520 4d6f 6475 6c65 2069 6e73 7461  the Module insta
-0000e090: 6e63 6520 616e 6420 6d65 7468 6f64 206e  nce and method n
-0000e0a0: 616d 6520 616e 6420 7265 7475 726e 7320  ame and returns 
-0000e0b0: 6120 626f 6f6c 2069 6e64 6963 6174 696e  a bool indicatin
-0000e0c0: 670a 2020 2020 2020 2020 7768 6574 6865  g.        whethe
-0000e0d0: 7220 7468 6520 6f75 7470 7574 206f 6620  r the output of 
-0000e0e0: 7468 6174 206d 6574 686f 6420 696e 766f  that method invo
-0000e0f0: 6361 7469 6f6e 2073 686f 756c 6420 6265  cation should be
-0000e100: 2073 746f 7265 642e 0a20 2020 2020 202a   stored..      *
-0000e110: 2a6b 7761 7267 733a 204b 6579 776f 7264  *kwargs: Keyword
-0000e120: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
-0000e130: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
-0000e140: 6e63 7469 6f6e 2e0a 2020 2020 5265 7475  nction..    Retu
-0000e150: 726e 733a 0a20 2020 2020 2060 286f 7574  rns:.      `(out
-0000e160: 7075 742c 2076 6172 7329 6060 2c20 7768  put, vars)``, wh
-0000e170: 6572 6520 6060 7661 7273 6060 2061 7265  ere ``vars`` are
-0000e180: 2069 7320 6120 6469 6374 206f 6620 7468   is a dict of th
-0000e190: 6520 6d6f 6469 6669 6564 0a20 2020 2020  e modified.     
-0000e1a0: 2063 6f6c 6c65 6374 696f 6e73 2e0a 2020   collections..  
-0000e1b0: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
-0000e1c0: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
-0000e1d0: 7365 6c66 290a 0a20 2020 2069 6620 6e6f  self)..    if no
-0000e1e0: 7420 6973 696e 7374 616e 6365 2872 6e67  t isinstance(rng
-0000e1f0: 732c 2064 6963 7429 3a0a 2020 2020 2020  s, dict):.      
-0000e200: 6966 206e 6f74 2063 6f72 652e 7363 6f70  if not core.scop
-0000e210: 652e 5f69 735f 7661 6c69 645f 726e 6728  e._is_valid_rng(
-0000e220: 726e 6773 293a 0a20 2020 2020 2020 2072  rngs):.        r
-0000e230: 6169 7365 2065 7272 6f72 732e 496e 7661  aise errors.Inva
-0000e240: 6c69 6452 6e67 4572 726f 7228 0a20 2020  lidRngError(.   
-0000e250: 2020 2020 2020 2020 2027 524e 4773 2073           'RNGs s
-0000e260: 686f 756c 6420 6265 206f 6620 7368 6170  hould be of shap
-0000e270: 6520 2832 2c29 206f 7220 4b65 7941 7272  e (2,) or KeyArr
-0000e280: 6179 2069 6e20 4d6f 6475 6c65 2027 0a20  ay in Module '. 
-0000e290: 2020 2020 2020 2020 2020 2066 277b 7365             f'{se
-0000e2a0: 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e  lf.__class__.__n
-0000e2b0: 616d 655f 5f7d 2c20 6275 7420 726e 6773  ame__}, but rngs
-0000e2c0: 2061 7265 3a20 7b72 6e67 737d 2729 0a20   are: {rngs}'). 
-0000e2d0: 2020 2020 2072 6e67 7320 3d20 7b27 7061       rngs = {'pa
-0000e2e0: 7261 6d73 273a 2072 6e67 737d 0a0a 2020  rams': rngs}..  
-0000e2f0: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-0000e300: 6d65 7468 6f64 2c20 7374 7229 3a0a 2020  method, str):.  
-0000e310: 2020 2020 6174 7472 6962 7574 655f 6e61      attribute_na
-0000e320: 6d65 203d 206d 6574 686f 640a 2020 2020  me = method.    
-0000e330: 2020 6d65 7468 6f64 203d 2067 6574 6174    method = getat
-0000e340: 7472 2873 656c 662c 2061 7474 7269 6275  tr(self, attribu
-0000e350: 7465 5f6e 616d 6529 0a20 2020 2020 2069  te_name).      i
-0000e360: 6620 6e6f 7420 6361 6c6c 6162 6c65 286d  f not callable(m
-0000e370: 6574 686f 6429 3a0a 2020 2020 2020 2020  ethod):.        
-0000e380: 636c 6173 735f 6e61 6d65 203d 2074 7970  class_name = typ
-0000e390: 6528 7365 6c66 292e 5f5f 6e61 6d65 5f5f  e(self).__name__
-0000e3a0: 0a20 2020 2020 2020 2072 6169 7365 2054  .        raise T
-0000e3b0: 7970 6545 7272 6f72 2866 275c 277b 636c  ypeError(f'\'{cl
-0000e3c0: 6173 735f 6e61 6d65 7d2e 7b61 7474 7269  ass_name}.{attri
-0000e3d0: 6275 7465 5f6e 616d 657d 5c27 206d 7573  bute_name}\' mus
-0000e3e0: 7420 6265 2061 2063 616c 6c61 626c 652c  t be a callable,
-0000e3f0: 2067 6f74 207b 7479 7065 286d 6574 686f   got {type(metho
-0000e400: 6429 7d2e 2729 0a20 2020 2065 6c69 6620  d)}.').    elif 
-0000e410: 6d65 7468 6f64 2069 7320 4e6f 6e65 3a0a  method is None:.
-0000e420: 2020 2020 2020 6d65 7468 6f64 203d 2073        method = s
-0000e430: 656c 662e 5f5f 6361 6c6c 5f5f 0a20 2020  elf.__call__.   
-0000e440: 206d 6574 686f 6420 3d20 5f67 6574 5f75   method = _get_u
-0000e450: 6e62 6f75 6e64 5f66 6e28 6d65 7468 6f64  nbound_fn(method
-0000e460: 290a 2020 2020 7265 7475 726e 2069 6e69  ).    return ini
-0000e470: 745f 7769 7468 5f6f 7574 7075 7428 0a20  t_with_output(. 
-0000e480: 2020 2020 2020 206d 6574 686f 642c 0a20         method,. 
-0000e490: 2020 2020 2020 2073 656c 662c 0a20 2020         self,.   
-0000e4a0: 2020 2020 206d 7574 6162 6c65 3d6d 7574       mutable=mut
-0000e4b0: 6162 6c65 2c0a 2020 2020 2020 2020 6361  able,.        ca
-0000e4c0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-0000e4d0: 7465 733d 6361 7074 7572 655f 696e 7465  tes=capture_inte
-0000e4e0: 726d 6564 6961 7465 730a 2020 2020 2928  rmediates.    )(
-0000e4f0: 726e 6773 2c20 2a61 7267 732c 202a 2a6b  rngs, *args, **k
-0000e500: 7761 7267 7329 0a0a 2020 4074 7261 6365  wargs)..  @trace
-0000e510: 6261 636b 5f75 7469 6c2e 6170 695f 626f  back_util.api_bo
-0000e520: 756e 6461 7279 0a20 2064 6566 2069 6e69  undary.  def ini
-0000e530: 7428 7365 6c66 2c0a 2020 2020 2020 2020  t(self,.        
-0000e540: 2020 2072 6e67 733a 2055 6e69 6f6e 5b50     rngs: Union[P
-0000e550: 524e 474b 6579 2c20 524e 4753 6571 7565  RNGKey, RNGSeque
-0000e560: 6e63 6573 5d2c 0a20 2020 2020 2020 2020  nces],.         
-0000e570: 2020 2a61 7267 732c 0a20 2020 2020 2020    *args,.       
-0000e580: 2020 2020 6d65 7468 6f64 3a20 556e 696f      method: Unio
-0000e590: 6e5b 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  n[Callable[..., 
-0000e5a0: 416e 795d 2c20 7374 722c 204e 6f6e 655d  Any], str, None]
-0000e5b0: 203d 204e 6f6e 652c 0a20 2020 2020 2020   = None,.       
-0000e5c0: 2020 2020 6d75 7461 626c 653a 2043 6f6c      mutable: Col
-0000e5d0: 6c65 6374 696f 6e46 696c 7465 7220 3d20  lectionFilter = 
-0000e5e0: 4465 6e79 4c69 7374 2827 696e 7465 726d  DenyList('interm
-0000e5f0: 6564 6961 7465 7327 292c 0a20 2020 2020  ediates'),.     
-0000e600: 2020 2020 2020 6361 7074 7572 655f 696e        capture_in
-0000e610: 7465 726d 6564 6961 7465 733a 2055 6e69  termediates: Uni
-0000e620: 6f6e 5b62 6f6f 6c2c 2043 616c 6c61 626c  on[bool, Callabl
-0000e630: 655b 5b27 4d6f 6475 6c65 272c 2073 7472  e[['Module', str
-0000e640: 5d2c 2062 6f6f 6c5d 5d20 3d20 4661 6c73  ], bool]] = Fals
-0000e650: 652c 0a20 2020 2020 2020 2020 2020 2a2a  e,.           **
-0000e660: 6b77 6172 6773 2920 2d3e 2055 6e69 6f6e  kwargs) -> Union
-0000e670: 5b46 726f 7a65 6e56 6172 6961 626c 6544  [FrozenVariableD
-0000e680: 6963 742c 2044 6963 745b 7374 722c 2041  ict, Dict[str, A
-0000e690: 6e79 5d5d 3a0a 2020 2020 2222 2249 6e69  ny]]:.    """Ini
-0000e6a0: 7469 616c 697a 6573 2061 206d 6f64 756c  tializes a modul
-0000e6b0: 6520 6d65 7468 6f64 2077 6974 6820 7661  e method with va
-0000e6c0: 7269 6162 6c65 7320 616e 6420 7265 7475  riables and retu
-0000e6d0: 726e 7320 6d6f 6469 6669 6564 2076 6172  rns modified var
-0000e6e0: 6961 626c 6573 2e0a 0a20 2020 2060 6069  iables...    ``i
-0000e6f0: 6e69 7460 6020 7461 6b65 7320 6173 2066  nit`` takes as f
-0000e700: 6972 7374 2061 7267 756d 656e 7420 6569  irst argument ei
-0000e710: 7468 6572 2061 2073 696e 676c 6520 6060  ther a single ``
-0000e720: 5052 4e47 4b65 7960 602c 206f 7220 6120  PRNGKey``, or a 
-0000e730: 6469 6374 696f 6e61 7279 206d 6170 7069  dictionary mappi
-0000e740: 6e67 2076 6172 6961 626c 6520 636f 6c6c  ng variable coll
-0000e750: 6563 7469 6f6e 7320 6e61 6d65 7320 746f  ections names to
-0000e760: 2074 6865 6972 2060 6050 524e 474b 6579   their ``PRNGKey
-0000e770: 7360 602c 2061 6e64 2077 696c 6c20 6361  s``, and will ca
-0000e780: 6c6c 2060 606d 6574 686f 6460 6020 2877  ll ``method`` (w
-0000e790: 6869 6368 2069 7320 7468 6520 6d6f 6475  hich is the modu
-0000e7a0: 6c65 2773 2060 605f 5f63 616c 6c5f 5f60  le's ``__call__`
-0000e7b0: 6020 6675 6e63 7469 6f6e 2062 7920 6465  ` function by de
-0000e7c0: 6661 756c 7429 2070 6173 7369 6e67 2060  fault) passing `
-0000e7d0: 602a 6172 6773 6060 2061 6e64 2060 602a  `*args`` and ``*
-0000e7e0: 2a6b 7761 7267 7360 602c 2061 6e64 2072  *kwargs``, and r
-0000e7f0: 6574 7572 6e73 0a20 2020 2061 2064 6963  eturns.    a dic
-0000e800: 7469 6f6e 6172 7920 6f66 2069 6e69 7469  tionary of initi
-0000e810: 616c 697a 6564 2076 6172 6961 626c 6573  alized variables
-0000e820: 2e0a 0a20 2020 2045 7861 6d70 6c65 3a3a  ...    Example::
-0000e830: 0a0a 2020 2020 2020 3e3e 3e20 696d 706f  ..      >>> impo
-0000e840: 7274 2066 6c61 782e 6c69 6e65 6e20 6173  rt flax.linen as
-0000e850: 206e 6e0a 2020 2020 2020 3e3e 3e20 696d   nn.      >>> im
-0000e860: 706f 7274 206a 6178 2e6e 756d 7079 2061  port jax.numpy a
-0000e870: 7320 6a6e 700a 2020 2020 2020 3e3e 3e20  s jnp.      >>> 
-0000e880: 696d 706f 7274 206a 6178 0a20 2020 2020  import jax.     
-0000e890: 202e 2e2e 0a20 2020 2020 203e 3e3e 2063   ....      >>> c
-0000e8a0: 6c61 7373 2046 6f6f 286e 6e2e 4d6f 6475  lass Foo(nn.Modu
-0000e8b0: 6c65 293a 0a20 2020 2020 202e 2e2e 2020  le):.      ...  
-0000e8c0: 2040 6e6e 2e63 6f6d 7061 6374 0a20 2020   @nn.compact.   
-0000e8d0: 2020 202e 2e2e 2020 2064 6566 205f 5f63     ...   def __c
-0000e8e0: 616c 6c5f 5f28 7365 6c66 2c20 782c 2074  all__(self, x, t
-0000e8f0: 7261 696e 293a 0a20 2020 2020 202e 2e2e  rain):.      ...
-0000e900: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
-0000e910: 6528 3136 2928 7829 0a20 2020 2020 202e  e(16)(x).      .
-0000e920: 2e2e 2020 2020 2078 203d 206e 6e2e 4261  ..     x = nn.Ba
-0000e930: 7463 684e 6f72 6d28 7573 655f 7275 6e6e  tchNorm(use_runn
-0000e940: 696e 675f 6176 6572 6167 653d 6e6f 7420  ing_average=not 
-0000e950: 7472 6169 6e29 2878 290a 2020 2020 2020  train)(x).      
-0000e960: 2e2e 2e20 2020 2020 7820 3d20 6e6e 2e72  ...     x = nn.r
-0000e970: 656c 7528 7829 0a20 2020 2020 202e 2e2e  elu(x).      ...
-0000e980: 2020 2020 2072 6574 7572 6e20 6e6e 2e44       return nn.D
-0000e990: 656e 7365 2831 2928 7829 0a20 2020 2020  ense(1)(x).     
-0000e9a0: 202e 2e2e 0a20 2020 2020 203e 3e3e 206d   ....      >>> m
-0000e9b0: 6f64 756c 6520 3d20 466f 6f28 290a 2020  odule = Foo().  
-0000e9c0: 2020 2020 3e3e 3e20 6b65 7920 3d20 6a61      >>> key = ja
-0000e9d0: 782e 7261 6e64 6f6d 2e50 524e 474b 6579  x.random.PRNGKey
-0000e9e0: 2830 290a 2020 2020 2020 3e3e 3e20 7661  (0).      >>> va
-0000e9f0: 7269 6162 6c65 7320 3d20 6d6f 6475 6c65  riables = module
-0000ea00: 2e69 6e69 7428 6b65 792c 206a 6e70 2e65  .init(key, jnp.e
-0000ea10: 6d70 7479 2828 312c 2037 2929 2c20 7472  mpty((1, 7)), tr
-0000ea20: 6169 6e3d 4661 6c73 6529 0a0a 2020 2020  ain=False)..    
-0000ea30: 4966 2079 6f75 2070 6173 7320 6120 7369  If you pass a si
-0000ea40: 6e67 6c65 2060 6050 524e 474b 6579 6060  ngle ``PRNGKey``
-0000ea50: 2c20 466c 6178 2077 696c 6c20 7573 6520  , Flax will use 
-0000ea60: 6974 2074 6f20 6665 6564 2074 6865 2060  it to feed the `
-0000ea70: 6027 7061 7261 6d73 2760 6020 524e 4720  `'params'`` RNG 
-0000ea80: 7374 7265 616d 2e0a 2020 2020 4966 2079  stream..    If y
-0000ea90: 6f75 2077 616e 7420 746f 2075 7365 2061  ou want to use a
-0000eaa0: 2064 6966 6665 7265 6e74 2052 4e47 2073   different RNG s
-0000eab0: 7472 6561 6d20 6f72 206e 6565 6420 746f  tream or need to
-0000eac0: 2075 7365 206d 756c 7469 706c 6520 7374   use multiple st
-0000ead0: 7265 616d 732c 2079 6f75 206d 7573 7420  reams, you must 
-0000eae0: 7061 7373 2061 0a20 2020 2064 6963 7469  pass a.    dicti
-0000eaf0: 6f6e 6172 7920 6d61 7070 696e 6720 6561  onary mapping ea
-0000eb00: 6368 2052 4e47 2073 7472 6561 6d20 6e61  ch RNG stream na
-0000eb10: 6d65 2074 6f20 6974 7320 636f 7272 6573  me to its corres
-0000eb20: 706f 6e64 696e 6720 6060 5052 4e47 4b65  ponding ``PRNGKe
-0000eb30: 7960 6020 746f 2060 6069 6e69 7460 602e  y`` to ``init``.
-0000eb40: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
-0000eb50: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
-0000eb60: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
-0000eb70: 0a20 2020 2020 202e 2e2e 2020 2040 6e6e  .      ...   @nn
-0000eb80: 2e63 6f6d 7061 6374 0a20 2020 2020 202e  .compact.      .
-0000eb90: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
-0000eba0: 5f28 7365 6c66 2c20 782c 2074 7261 696e  _(self, x, train
-0000ebb0: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
-0000ebc0: 2078 203d 206e 6e2e 4465 6e73 6528 3136   x = nn.Dense(16
-0000ebd0: 2928 7829 0a20 2020 2020 202e 2e2e 2020  )(x).      ...  
-0000ebe0: 2020 2078 203d 206e 6e2e 4261 7463 684e     x = nn.BatchN
-0000ebf0: 6f72 6d28 7573 655f 7275 6e6e 696e 675f  orm(use_running_
-0000ec00: 6176 6572 6167 653d 6e6f 7420 7472 6169  average=not trai
-0000ec10: 6e29 2878 290a 2020 2020 2020 2e2e 2e20  n)(x).      ... 
-0000ec20: 2020 2020 7820 3d20 6e6e 2e72 656c 7528      x = nn.relu(
-0000ec30: 7829 0a20 2020 2020 202e 2e2e 0a20 2020  x).      ....   
-0000ec40: 2020 202e 2e2e 2020 2020 2023 2041 6464     ...     # Add
-0000ec50: 2067 6175 7373 6961 6e20 6e6f 6973 650a   gaussian noise.
-0000ec60: 2020 2020 2020 2e2e 2e20 2020 2020 6e6f        ...     no
-0000ec70: 6973 655f 6b65 7920 3d20 7365 6c66 2e6d  ise_key = self.m
-0000ec80: 616b 655f 726e 6728 276e 6f69 7365 2729  ake_rng('noise')
-0000ec90: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
-0000eca0: 203d 2078 202b 206a 6178 2e72 616e 646f   = x + jax.rando
-0000ecb0: 6d2e 6e6f 726d 616c 286e 6f69 7365 5f6b  m.normal(noise_k
-0000ecc0: 6579 2c20 782e 7368 6170 6529 0a20 2020  ey, x.shape).   
-0000ecd0: 2020 202e 2e2e 0a20 2020 2020 202e 2e2e     ....      ...
-0000ece0: 2020 2020 2072 6574 7572 6e20 6e6e 2e44       return nn.D
-0000ecf0: 656e 7365 2831 2928 7829 0a20 2020 2020  ense(1)(x).     
-0000ed00: 202e 2e2e 0a20 2020 2020 203e 3e3e 206d   ....      >>> m
-0000ed10: 6f64 756c 6520 3d20 466f 6f28 290a 2020  odule = Foo().  
-0000ed20: 2020 2020 3e3e 3e20 726e 6773 203d 207b      >>> rngs = {
-0000ed30: 2770 6172 616d 7327 3a20 6a61 782e 7261  'params': jax.ra
-0000ed40: 6e64 6f6d 2e50 524e 474b 6579 2830 292c  ndom.PRNGKey(0),
-0000ed50: 2027 6e6f 6973 6527 3a20 6a61 782e 7261   'noise': jax.ra
-0000ed60: 6e64 6f6d 2e50 524e 474b 6579 2831 297d  ndom.PRNGKey(1)}
-0000ed70: 0a20 2020 2020 203e 3e3e 2076 6172 6961  .      >>> varia
-0000ed80: 626c 6573 203d 206d 6f64 756c 652e 696e  bles = module.in
-0000ed90: 6974 2872 6e67 732c 206a 6e70 2e65 6d70  it(rngs, jnp.emp
-0000eda0: 7479 2828 312c 2037 2929 2c20 7472 6169  ty((1, 7)), trai
-0000edb0: 6e3d 4661 6c73 6529 0a0a 2020 2020 4a69  n=False)..    Ji
-0000edc0: 7474 696e 6720 6069 6e69 7460 2069 6e69  tting `init` ini
-0000edd0: 7469 616c 697a 6573 2061 206d 6f64 656c  tializes a model
-0000ede0: 206c 617a 696c 7920 7573 696e 6720 6f6e   lazily using on
-0000edf0: 6c79 2074 6865 2073 6861 7065 7320 6f66  ly the shapes of
-0000ee00: 2074 6865 0a20 2020 2070 726f 7669 6465   the.    provide
-0000ee10: 6420 6172 6775 6d65 6e74 732c 2061 6e64  d arguments, and
-0000ee20: 2061 766f 6964 7320 636f 6d70 7574 696e   avoids computin
-0000ee30: 6720 7468 6520 666f 7277 6172 6420 7061  g the forward pa
-0000ee40: 7373 2077 6974 6820 6163 7475 616c 0a20  ss with actual. 
-0000ee50: 2020 2076 616c 7565 732e 2045 7861 6d70     values. Examp
-0000ee60: 6c65 3a3a 0a0a 2020 2020 2020 3e3e 3e20  le::..      >>> 
-0000ee70: 6d6f 6475 6c65 203d 206e 6e2e 4465 6e73  module = nn.Dens
-0000ee80: 6528 3129 0a20 2020 2020 203e 3e3e 2069  e(1).      >>> i
-0000ee90: 6e69 745f 6a69 7420 3d20 6a61 782e 6a69  nit_jit = jax.ji
-0000eea0: 7428 6d6f 6475 6c65 2e69 6e69 7429 0a20  t(module.init). 
-0000eeb0: 2020 2020 203e 3e3e 2076 6172 6961 626c       >>> variabl
-0000eec0: 6573 203d 2069 6e69 745f 6a69 7428 6a61  es = init_jit(ja
-0000eed0: 782e 7261 6e64 6f6d 2e50 524e 474b 6579  x.random.PRNGKey
-0000eee0: 2830 292c 206a 6e70 2e65 6d70 7479 2828  (0), jnp.empty((
-0000eef0: 312c 2037 2929 290a 0a20 2020 2060 6069  1, 7)))..    ``i
-0000ef00: 6e69 7460 6020 6973 2061 206c 6967 6874  nit`` is a light
-0000ef10: 2077 7261 7070 6572 206f 7665 7220 6060   wrapper over ``
-0000ef20: 6170 706c 7960 602c 2073 6f20 6f74 6865  apply``, so othe
-0000ef30: 7220 6060 6170 706c 7960 6020 6172 6775  r ``apply`` argu
-0000ef40: 6d65 6e74 7320 6c69 6b65 0a20 2020 2060  ments like.    `
-0000ef50: 606d 6574 686f 6460 602c 2060 606d 7574  `method``, ``mut
-0000ef60: 6162 6c65 6060 2c20 616e 6420 6060 6361  able``, and ``ca
-0000ef70: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-0000ef80: 7465 7360 6020 6172 6520 616c 736f 2061  tes`` are also a
-0000ef90: 7661 696c 6162 6c65 2e0a 0a20 2020 2041  vailable...    A
-0000efa0: 7267 733a 0a20 2020 2020 2072 6e67 733a  rgs:.      rngs:
-0000efb0: 2054 6865 2072 6e67 7320 666f 7220 7468   The rngs for th
-0000efc0: 6520 7661 7269 6162 6c65 2063 6f6c 6c65  e variable colle
-0000efd0: 6374 696f 6e73 2e0a 2020 2020 2020 2a61  ctions..      *a
-0000efe0: 7267 733a 204e 616d 6564 2061 7267 756d  rgs: Named argum
-0000eff0: 656e 7473 2070 6173 7365 6420 746f 2074  ents passed to t
-0000f000: 6865 2069 6e69 7420 6675 6e63 7469 6f6e  he init function
-0000f010: 2e0a 2020 2020 2020 6d65 7468 6f64 3a20  ..      method: 
-0000f020: 416e 206f 7074 696f 6e61 6c20 6d65 7468  An optional meth
-0000f030: 6f64 2e20 4966 2070 726f 7669 6465 642c  od. If provided,
-0000f040: 2061 7070 6c69 6573 2074 6869 7320 6d65   applies this me
-0000f050: 7468 6f64 2e20 4966 206e 6f74 0a20 2020  thod. If not.   
-0000f060: 2020 2020 2070 726f 7669 6465 642c 2061       provided, a
-0000f070: 7070 6c69 6573 2074 6865 2060 605f 5f63  pplies the ``__c
-0000f080: 616c 6c5f 5f60 6020 6d65 7468 6f64 2e20  all__`` method. 
-0000f090: 4120 7374 7269 6e67 2063 616e 2061 6c73  A string can als
-0000f0a0: 6f20 6265 0a20 2020 2020 2020 2070 726f  o be.        pro
-0000f0b0: 7669 6465 6420 746f 2073 7065 6369 6679  vided to specify
-0000f0c0: 2061 206d 6574 686f 6420 6279 206e 616d   a method by nam
-0000f0d0: 652e 0a20 2020 2020 206d 7574 6162 6c65  e..      mutable
-0000f0e0: 3a20 4361 6e20 6265 2062 6f6f 6c2c 2073  : Can be bool, s
-0000f0f0: 7472 2c20 6f72 206c 6973 742e 2053 7065  tr, or list. Spe
-0000f100: 6369 6669 6573 2077 6869 6368 2063 6f6c  cifies which col
-0000f110: 6c65 6374 696f 6e73 2073 686f 756c 6420  lections should 
-0000f120: 6265 0a20 2020 2020 2020 2074 7265 6174  be.        treat
-0000f130: 6564 2061 7320 6d75 7461 626c 653a 2060  ed as mutable: `
-0000f140: 6062 6f6f 6c60 603a 2061 6c6c 2f6e 6f20  `bool``: all/no 
-0000f150: 636f 6c6c 6563 7469 6f6e 7320 6172 6520  collections are 
-0000f160: 6d75 7461 626c 652e 0a20 2020 2020 2020  mutable..       
-0000f170: 2060 6073 7472 6060 3a20 5468 6520 6e61   ``str``: The na
-0000f180: 6d65 206f 6620 6120 7369 6e67 6c65 206d  me of a single m
-0000f190: 7574 6162 6c65 2063 6f6c 6c65 6374 696f  utable collectio
-0000f1a0: 6e2e 2060 606c 6973 7460 603a 2041 0a20  n. ``list``: A. 
-0000f1b0: 2020 2020 2020 206c 6973 7420 6f66 206e         list of n
-0000f1c0: 616d 6573 206f 6620 6d75 7461 626c 6520  ames of mutable 
-0000f1d0: 636f 6c6c 6563 7469 6f6e 732e 2042 7920  collections. By 
-0000f1e0: 6465 6661 756c 7420 616c 6c20 636f 6c6c  default all coll
-0000f1f0: 6563 7469 6f6e 730a 2020 2020 2020 2020  ections.        
-0000f200: 6578 6365 7074 2022 696e 7465 726d 6564  except "intermed
-0000f210: 6961 7465 7322 2061 7265 206d 7574 6162  iates" are mutab
-0000f220: 6c65 2e0a 2020 2020 2020 6361 7074 7572  le..      captur
-0000f230: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
-0000f240: 2049 6620 6054 7275 6560 2c20 6361 7074   If `True`, capt
-0000f250: 7572 6573 2069 6e74 6572 6d65 6469 6174  ures intermediat
-0000f260: 6520 7265 7475 726e 2076 616c 7565 730a  e return values.
-0000f270: 2020 2020 2020 2020 6f66 2061 6c6c 204d          of all M
-0000f280: 6f64 756c 6573 2069 6e73 6964 6520 7468  odules inside th
-0000f290: 6520 2269 6e74 6572 6d65 6469 6174 6573  e "intermediates
-0000f2a0: 2220 636f 6c6c 6563 7469 6f6e 2e20 4279  " collection. By
-0000f2b0: 2064 6566 6175 6c74 206f 6e6c 790a 2020   default only.  
-0000f2c0: 2020 2020 2020 7468 6520 7265 7475 726e        the return
-0000f2d0: 2076 616c 7565 7320 6f66 2061 6c6c 2060   values of all `
-0000f2e0: 605f 5f63 616c 6c5f 5f60 6020 6d65 7468  `__call__`` meth
-0000f2f0: 6f64 7320 6172 6520 7374 6f72 6564 2e20  ods are stored. 
-0000f300: 4120 6675 6e63 7469 6f6e 2063 616e 0a20  A function can. 
-0000f310: 2020 2020 2020 2062 6520 7061 7373 6564         be passed
-0000f320: 2074 6f20 6368 616e 6765 2074 6865 2066   to change the f
-0000f330: 696c 7465 7220 6265 6861 7669 6f72 2e20  ilter behavior. 
-0000f340: 5468 6520 6669 6c74 6572 2066 756e 6374  The filter funct
-0000f350: 696f 6e20 7461 6b65 730a 2020 2020 2020  ion takes.      
-0000f360: 2020 7468 6520 4d6f 6475 6c65 2069 6e73    the Module ins
-0000f370: 7461 6e63 6520 616e 6420 6d65 7468 6f64  tance and method
-0000f380: 206e 616d 6520 616e 6420 7265 7475 726e   name and return
-0000f390: 7320 6120 626f 6f6c 2069 6e64 6963 6174  s a bool indicat
-0000f3a0: 696e 670a 2020 2020 2020 2020 7768 6574  ing.        whet
-0000f3b0: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
-0000f3c0: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
-0000f3d0: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
-0000f3e0: 6265 2073 746f 7265 642e 0a20 2020 2020  be stored..     
-0000f3f0: 202a 2a6b 7761 7267 733a 204b 6579 776f   **kwargs: Keywo
-0000f400: 7264 2061 7267 756d 656e 7473 2070 6173  rd arguments pas
-0000f410: 7365 6420 746f 2074 6865 2069 6e69 7420  sed to the init 
-0000f420: 6675 6e63 7469 6f6e 2e0a 2020 2020 5265  function..    Re
-0000f430: 7475 726e 733a 0a20 2020 2020 2054 6865  turns:.      The
-0000f440: 2069 6e69 7469 616c 697a 6564 2076 6172   initialized var
-0000f450: 6961 626c 6520 6469 6374 2e0a 2020 2020  iable dict..    
-0000f460: 2222 220a 2020 2020 4d6f 6475 6c65 2e5f  """.    Module._
-0000f470: 6d6f 6475 6c65 5f63 6865 636b 7328 7365  module_checks(se
-0000f480: 6c66 290a 0a20 2020 205f 2c20 765f 6f75  lf)..    _, v_ou
-0000f490: 7420 3d20 7365 6c66 2e69 6e69 745f 7769  t = self.init_wi
-0000f4a0: 7468 5f6f 7574 7075 7428 0a20 2020 2020  th_output(.     
-0000f4b0: 2020 2072 6e67 732c 0a20 2020 2020 2020     rngs,.       
-0000f4c0: 202a 6172 6773 2c0a 2020 2020 2020 2020   *args,.        
-0000f4d0: 6d65 7468 6f64 3d6d 6574 686f 642c 0a20  method=method,. 
-0000f4e0: 2020 2020 2020 206d 7574 6162 6c65 3d6d         mutable=m
-0000f4f0: 7574 6162 6c65 2c0a 2020 2020 2020 2020  utable,.        
-0000f500: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-0000f510: 6961 7465 733d 6361 7074 7572 655f 696e  iates=capture_in
-0000f520: 7465 726d 6564 6961 7465 732c 0a20 2020  termediates,.   
-0000f530: 2020 2020 202a 2a6b 7761 7267 7329 0a20       **kwargs). 
-0000f540: 2020 2072 6574 7572 6e20 765f 6f75 740a     return v_out.
-0000f550: 0a20 2040 7472 6163 6562 6163 6b5f 7574  .  @traceback_ut
-0000f560: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
-0000f570: 2020 6465 6620 6c61 7a79 5f69 6e69 7428    def lazy_init(
-0000f580: 7365 6c66 2c0a 2020 2020 2020 2020 2020  self,.          
-0000f590: 2072 6e67 733a 2055 6e69 6f6e 5b50 524e   rngs: Union[PRN
-0000f5a0: 474b 6579 2c20 524e 4753 6571 7565 6e63  GKey, RNGSequenc
-0000f5b0: 6573 5d2c 0a20 2020 2020 2020 2020 2020  es],.           
-0000f5c0: 2a61 7267 732c 0a20 2020 2020 2020 2020  *args,.         
-0000f5d0: 2020 6d65 7468 6f64 3a20 4f70 7469 6f6e    method: Option
-0000f5e0: 616c 5b43 616c 6c61 626c 655b 2e2e 2e2c  al[Callable[...,
-0000f5f0: 2041 6e79 5d5d 203d 204e 6f6e 652c 0a20   Any]] = None,. 
-0000f600: 2020 2020 2020 2020 2020 6d75 7461 626c            mutabl
-0000f610: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
-0000f620: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
-0000f630: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
-0000f640: 0a20 2020 2020 2020 2020 2020 2a2a 6b77  .           **kw
-0000f650: 6172 6773 2920 2d3e 2046 726f 7a65 6e56  args) -> FrozenV
-0000f660: 6172 6961 626c 6544 6963 743a 0a20 2020  ariableDict:.   
-0000f670: 2022 2222 496e 6974 6961 6c69 7a65 7320   """Initializes 
-0000f680: 6120 6d6f 6475 6c65 2077 6974 686f 7574  a module without
-0000f690: 2063 6f6d 7075 7469 6e67 206f 6e20 616e   computing on an
-0000f6a0: 2061 6374 7561 6c20 696e 7075 742e 0a0a   actual input...
-0000f6b0: 2020 2020 6c61 7a79 5f69 6e69 7420 7769      lazy_init wi
-0000f6c0: 6c6c 2069 6e69 7469 616c 697a 6520 7468  ll initialize th
-0000f6d0: 6520 7661 7269 6162 6c65 7320 7769 7468  e variables with
-0000f6e0: 6f75 7420 646f 696e 6720 756e 6e65 6365  out doing unnece
-0000f6f0: 7373 6172 7920 636f 6d70 7574 652e 0a20  ssary compute.. 
-0000f700: 2020 2054 6865 2069 6e70 7574 2064 6174     The input dat
-0000f710: 6120 7368 6f75 6c64 2062 6520 7061 7373  a should be pass
-0000f720: 6564 2061 7320 6120 6060 6a61 782e 5368  ed as a ``jax.Sh
-0000f730: 6170 6544 7479 7065 5374 7275 6374 6060  apeDtypeStruct``
-0000f740: 2077 6869 6368 2073 7065 6369 6669 6573   which specifies
-0000f750: 0a20 2020 2074 6865 2073 6861 7065 2061  .    the shape a
-0000f760: 6e64 2064 7479 7065 206f 6620 7468 6520  nd dtype of the 
-0000f770: 696e 7075 7420 6275 7420 6e6f 2063 6f6e  input but no con
-0000f780: 6372 6574 6520 6461 7461 2e0a 0a20 2020  crete data...   
-0000f790: 2045 7861 6d70 6c65 3a3a 0a0a 2020 2020   Example::..    
-0000f7a0: 2020 6d6f 6465 6c20 3d20 6e6e 2e44 656e    model = nn.Den
-0000f7b0: 7365 2866 6561 7475 7265 733d 3235 3629  se(features=256)
-0000f7c0: 0a20 2020 2020 2076 6172 6961 626c 6573  .      variables
-0000f7d0: 203d 206d 6f64 656c 2e6c 617a 795f 696e   = model.lazy_in
-0000f7e0: 6974 2872 6e67 2c20 6a61 782e 5368 6170  it(rng, jax.Shap
-0000f7f0: 6544 7479 7065 5374 7275 6374 2828 312c  eDtypeStruct((1,
-0000f800: 2031 3238 292c 206a 6e70 2e66 6c6f 6174   128), jnp.float
-0000f810: 3332 2929 0a0a 2020 2020 5468 6520 6172  32))..    The ar
-0000f820: 6773 2061 6e64 206b 7761 7267 7320 6172  gs and kwargs ar
-0000f830: 6773 2070 6173 7365 6420 746f 2060 606c  gs passed to ``l
-0000f840: 617a 795f 696e 6974 6060 2063 616e 2062  azy_init`` can b
-0000f850: 6520 6120 6d69 7820 6f66 0a20 2020 2063  e a mix of.    c
-0000f860: 6f6e 6372 6574 6520 286a 6178 2061 7272  oncrete (jax arr
-0000f870: 6179 732c 2073 6361 6c61 7273 2c20 626f  ays, scalars, bo
-0000f880: 6f6c 7329 2061 6e64 2061 6273 7472 6163  ols) and abstrac
-0000f890: 7420 2853 6861 7065 4474 7970 6553 7472  t (ShapeDtypeStr
-0000f8a0: 7563 7429 2076 616c 7565 732e 0a20 2020  uct) values..   
-0000f8b0: 2043 6f6e 6372 6574 6520 7661 6c75 6573   Concrete values
-0000f8c0: 2061 7265 206f 6e6c 7920 6e65 6365 7373   are only necess
-0000f8d0: 6172 7920 666f 7220 6172 6775 6d65 6e74  ary for argument
-0000f8e0: 7320 7468 6174 2061 6666 6563 740a 2020  s that affect.  
-0000f8f0: 2020 7468 6520 696e 6974 6961 6c69 7a61    the initializa
-0000f900: 7469 6f6e 206f 6620 7661 7269 6162 6c65  tion of variable
-0000f910: 732e 2046 6f72 2065 7861 6d70 6c65 2c20  s. For example, 
-0000f920: 7468 6520 6d6f 6465 6c20 6d69 6768 7420  the model might 
-0000f930: 6578 7065 6374 0a20 2020 2061 206b 6579  expect.    a key
-0000f940: 776f 7264 2061 7267 2074 6861 7420 656e  word arg that en
-0000f950: 6162 6c65 732f 6469 7361 626c 6573 2061  ables/disables a
-0000f960: 2073 7562 7061 7274 206f 6620 7468 6520   subpart of the 
-0000f970: 6d6f 6465 6c2e 0a20 2020 2049 6e20 7468  model..    In th
-0000f980: 6973 2063 6173 652c 2061 6e20 6578 706c  is case, an expl
-0000f990: 6963 6974 2076 616c 7565 2028 5472 7565  icit value (True
-0000f9a0: 2f46 6c61 7365 2920 7368 6f75 6c64 2062  /Flase) should b
-0000f9b0: 6520 7061 7373 6564 206f 7468 6572 7769  e passed otherwi
-0000f9c0: 7365 0a20 2020 2060 606c 617a 795f 696e  se.    ``lazy_in
-0000f9d0: 6974 6060 2063 616e 6e6f 7420 696e 6665  it`` cannot infe
-0000f9e0: 7220 7768 6963 6820 7661 7269 6162 6c65  r which variable
-0000f9f0: 7320 7368 6f75 6c64 2062 6520 696e 6974  s should be init
-0000fa00: 6961 6c69 7a65 642e 0a0a 2020 2020 4172  ialized...    Ar
-0000fa10: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
-0000fa20: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
-0000fa30: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-0000fa40: 7469 6f6e 732e 0a20 2020 2020 202a 6172  tions..      *ar
-0000fa50: 6773 3a20 6172 6775 6d65 6e74 7320 7061  gs: arguments pa
-0000fa60: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
-0000fa70: 2066 756e 6374 696f 6e2e 0a20 2020 2020   function..     
-0000fa80: 206d 6574 686f 643a 2041 6e20 6f70 7469   method: An opti
-0000fa90: 6f6e 616c 206d 6574 686f 642e 2049 6620  onal method. If 
-0000faa0: 7072 6f76 6964 6564 2c20 6170 706c 6965  provided, applie
-0000fab0: 7320 7468 6973 206d 6574 686f 642e 2049  s this method. I
-0000fac0: 6620 6e6f 740a 2020 2020 2020 2020 7072  f not.        pr
-0000fad0: 6f76 6964 6564 2c20 6170 706c 6965 7320  ovided, applies 
-0000fae0: 7468 6520 6060 5f5f 6361 6c6c 5f5f 6060  the ``__call__``
-0000faf0: 206d 6574 686f 642e 0a20 2020 2020 206d   method..      m
-0000fb00: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
-0000fb10: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
-0000fb20: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
-0000fb30: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
-0000fb40: 686f 756c 6420 6265 0a20 2020 2020 2020  hould be.       
-0000fb50: 2074 7265 6174 6564 2061 7320 6d75 7461   treated as muta
-0000fb60: 626c 653a 2060 6062 6f6f 6c60 603a 2061  ble: ``bool``: a
-0000fb70: 6c6c 2f6e 6f20 636f 6c6c 6563 7469 6f6e  ll/no collection
-0000fb80: 7320 6172 6520 6d75 7461 626c 652e 0a20  s are mutable.. 
-0000fb90: 2020 2020 2020 2060 6073 7472 6060 3a20         ``str``: 
-0000fba0: 5468 6520 6e61 6d65 206f 6620 6120 7369  The name of a si
-0000fbb0: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
-0000fbc0: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
-0000fbd0: 603a 2041 0a20 2020 2020 2020 206c 6973  `: A.        lis
-0000fbe0: 7420 6f66 206e 616d 6573 206f 6620 6d75  t of names of mu
-0000fbf0: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
-0000fc00: 732e 2042 7920 6465 6661 756c 7420 616c  s. By default al
-0000fc10: 6c20 636f 6c6c 6563 7469 6f6e 730a 2020  l collections.  
-0000fc20: 2020 2020 2020 6578 6365 7074 2022 696e        except "in
-0000fc30: 7465 726d 6564 6961 7465 7322 2061 7265  termediates" are
-0000fc40: 206d 7574 6162 6c65 2e0a 2020 2020 2020   mutable..      
-0000fc50: 2a2a 6b77 6172 6773 3a20 4b65 7977 6f72  **kwargs: Keywor
-0000fc60: 6420 6172 6775 6d65 6e74 7320 7061 7373  d arguments pass
-0000fc70: 6564 2074 6f20 7468 6520 696e 6974 2066  ed to the init f
-0000fc80: 756e 6374 696f 6e2e 0a20 2020 2052 6574  unction..    Ret
-0000fc90: 7572 6e73 3a0a 2020 2020 2020 5468 6520  urns:.      The 
-0000fca0: 696e 6974 6961 6c69 7a65 6420 7661 7269  initialized vari
-0000fcb0: 6162 6c65 2064 6963 742e 0a20 2020 2022  able dict..    "
-0000fcc0: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
-0000fcd0: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
-0000fce0: 6629 0a20 2020 2064 6566 206c 617a 795f  f).    def lazy_
-0000fcf0: 7772 6170 7065 7228 726e 6773 2c20 2a61  wrapper(rngs, *a
-0000fd00: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
-0000fd10: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-0000fd20: 662e 696e 6974 2872 6e67 732c 202a 6172  f.init(rngs, *ar
-0000fd30: 6773 2c20 6d65 7468 6f64 3d6d 6574 686f  gs, method=metho
-0000fd40: 642c 206d 7574 6162 6c65 3d6d 7574 6162  d, mutable=mutab
-0000fd50: 6c65 2c20 2a2a 6b77 6172 6773 290a 2020  le, **kwargs).  
-0000fd60: 2020 7265 7475 726e 2070 6172 7469 616c    return partial
-0000fd70: 5f65 7661 6c2e 6c61 7a79 5f69 6e69 7428  _eval.lazy_init(
-0000fd80: 6c61 7a79 5f77 7261 7070 6572 2928 726e  lazy_wrapper)(rn
-0000fd90: 6773 2c20 2a61 7267 732c 202a 2a6b 7761  gs, *args, **kwa
-0000fda0: 7267 7329 0a0a 2020 4070 726f 7065 7274  rgs)..  @propert
-0000fdb0: 790a 2020 6465 6620 7661 7269 6162 6c65  y.  def variable
-0000fdc0: 7328 7365 6c66 2920 2d3e 2056 6172 6961  s(self) -> Varia
-0000fdd0: 626c 6544 6963 743a 0a20 2020 2022 2222  bleDict:.    """
-0000fde0: 5265 7475 726e 7320 7468 6520 7661 7269  Returns the vari
-0000fdf0: 6162 6c65 7320 696e 2074 6869 7320 6d6f  ables in this mo
-0000fe00: 6475 6c65 2e22 2222 0a20 2020 2069 6620  dule.""".    if 
-0000fe10: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
-0000fe20: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
-0000fe30: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
-0000fe40: 7420 6163 6365 7373 2076 6172 6961 626c  t access variabl
-0000fe50: 6573 206f 6e20 756e 626f 756e 6420 6d6f  es on unbound mo
-0000fe60: 6475 6c65 7322 290a 2020 2020 7265 7475  dules").    retu
-0000fe70: 726e 2073 656c 662e 7363 6f70 652e 7661  rn self.scope.va
-0000fe80: 7269 6162 6c65 7328 290a 0a20 2064 6566  riables()..  def
-0000fe90: 2067 6574 5f76 6172 6961 626c 6528 7365   get_variable(se
-0000fea0: 6c66 2c20 636f 6c3a 2073 7472 2c20 6e61  lf, col: str, na
-0000feb0: 6d65 3a20 7374 722c 2064 6566 6175 6c74  me: str, default
-0000fec0: 3a20 4f70 7469 6f6e 616c 5b54 5d20 3d20  : Optional[T] = 
-0000fed0: 4e6f 6e65 2920 2d3e 2054 3a0a 2020 2020  None) -> T:.    
-0000fee0: 2222 2252 6574 7269 6576 6573 2074 6865  """Retrieves the
-0000fef0: 2076 616c 7565 206f 6620 6120 5661 7269   value of a Vari
-0000ff00: 6162 6c65 2e0a 0a20 2020 2041 7267 733a  able...    Args:
-0000ff10: 0a20 2020 2020 2063 6f6c 3a20 7468 6520  .      col: the 
-0000ff20: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
-0000ff30: 696f 6e2e 0a20 2020 2020 206e 616d 653a  ion..      name:
-0000ff40: 2074 6865 206e 616d 6520 6f66 2074 6865   the name of the
-0000ff50: 2076 6172 6961 626c 652e 0a20 2020 2020   variable..     
-0000ff60: 2064 6566 6175 6c74 3a20 7468 6520 6465   default: the de
-0000ff70: 6661 756c 7420 7661 6c75 6520 746f 2072  fault value to r
-0000ff80: 6574 7572 6e20 6966 2074 6865 2076 6172  eturn if the var
-0000ff90: 6961 626c 6520 646f 6573 206e 6f74 2065  iable does not e
-0000ffa0: 7869 7374 2069 6e0a 2020 2020 2020 2020  xist in.        
-0000ffb0: 7468 6973 2073 636f 7065 2e0a 0a20 2020  this scope...   
-0000ffc0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000ffd0: 5468 6520 7661 6c75 6520 6f66 2074 6865  The value of the
-0000ffe0: 2069 6e70 7574 2076 6172 6961 626c 652c   input variable,
-0000fff0: 206f 6620 7468 6520 6465 6661 756c 7420   of the default 
-00010000: 7661 6c75 6520 6966 2074 6865 2076 6172  value if the var
-00010010: 6961 626c 650a 2020 2020 2020 646f 6573  iable.      does
-00010020: 6e27 7420 6578 6973 7420 696e 2074 6869  n't exist in thi
-00010030: 7320 7363 6f70 652e 0a20 2020 2022 2222  s scope..    """
-00010040: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-00010050: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-00010060: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-00010070: 6f72 2822 4361 6e27 7420 6163 6365 7373  or("Can't access
-00010080: 2076 6172 6961 626c 6573 206f 6e20 756e   variables on un
-00010090: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
-000100a0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-000100b0: 7363 6f70 652e 6765 745f 7661 7269 6162  scope.get_variab
-000100c0: 6c65 2863 6f6c 2c20 6e61 6d65 2c20 6465  le(col, name, de
-000100d0: 6661 756c 7429 0a0a 2020 6465 6620 7075  fault)..  def pu
-000100e0: 745f 7661 7269 6162 6c65 2873 656c 662c  t_variable(self,
-000100f0: 2063 6f6c 3a20 7374 722c 206e 616d 653a   col: str, name:
-00010100: 2073 7472 2c20 7661 6c75 653a 2041 6e79   str, value: Any
-00010110: 293a 0a20 2020 2022 2222 5570 6461 7465  ):.    """Update
-00010120: 7320 7468 6520 7661 6c75 6520 6f66 2074  s the value of t
-00010130: 6865 2067 6976 656e 2076 6172 6961 626c  he given variabl
-00010140: 6520 6966 2069 7420 6973 206d 7574 6162  e if it is mutab
-00010150: 6c65 2c20 6f72 2061 6e20 6572 726f 7220  le, or an error 
-00010160: 6f74 6865 7277 6973 652e 0a0a 2020 2020  otherwise...    
-00010170: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
-00010180: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
-00010190: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
-000101a0: 6e61 6d65 3a20 7468 6520 6e61 6d65 206f  name: the name o
-000101b0: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
-000101c0: 2020 2020 2020 7661 6c75 653a 2074 6865        value: the
-000101d0: 206e 6577 2076 616c 7565 206f 6620 7468   new value of th
-000101e0: 6520 7661 7269 6162 6c65 2e0a 2020 2020  e variable..    
-000101f0: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
-00010200: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-00010210: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-00010220: 4572 726f 7228 2243 616e 2774 2061 6363  Error("Can't acc
-00010230: 6573 7320 7661 7269 6162 6c65 7320 6f6e  ess variables on
-00010240: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-00010250: 2229 0a20 2020 2073 656c 662e 7363 6f70  ").    self.scop
-00010260: 652e 7075 745f 7661 7269 6162 6c65 2863  e.put_variable(c
-00010270: 6f6c 2c20 6e61 6d65 2c20 7661 6c75 6529  ol, name, value)
-00010280: 0a0a 2020 406f 7665 726c 6f61 640a 2020  ..  @overload.  
-00010290: 6465 6620 736f 7728 7365 6c66 2c20 636f  def sow(self, co
-000102a0: 6c3a 2073 7472 2c20 6e61 6d65 3a20 7374  l: str, name: st
-000102b0: 722c 2076 616c 7565 3a20 416e 7929 202d  r, value: Any) -
-000102c0: 3e20 626f 6f6c 3a0a 2020 2020 2e2e 2e0a  > bool:.    ....
-000102d0: 0a20 2040 6f76 6572 6c6f 6164 0a20 2064  .  @overload.  d
-000102e0: 6566 2073 6f77 2873 656c 662c 2063 6f6c  ef sow(self, col
-000102f0: 3a20 7374 722c 206e 616d 653a 2073 7472  : str, name: str
-00010300: 2c20 7661 6c75 653a 2054 2c0a 2020 2020  , value: T,.    
-00010310: 2020 2020 2020 7265 6475 6365 5f66 6e3a        reduce_fn:
-00010320: 2043 616c 6c61 626c 655b 5b4b 2c20 545d   Callable[[K, T]
-00010330: 2c20 4b5d 203d 2074 7570 6c65 5f72 6564  , K] = tuple_red
-00010340: 7563 652c 0a20 2020 2020 2020 2020 2069  uce,.          i
-00010350: 6e69 745f 666e 3a20 4361 6c6c 6162 6c65  nit_fn: Callable
-00010360: 5b5b 5d2c 204b 5d20 3d20 7475 706c 655f  [[], K] = tuple_
-00010370: 696e 6974 2920 2d3e 2062 6f6f 6c3a 2023  init) -> bool: #
-00010380: 2074 7970 653a 2069 676e 6f72 650a 2020   type: ignore.  
-00010390: 2020 2e2e 2e0a 0a20 2064 6566 2073 6f77    .....  def sow
-000103a0: 2873 656c 662c 2063 6f6c 3a20 7374 722c  (self, col: str,
-000103b0: 206e 616d 653a 2073 7472 2c20 7661 6c75   name: str, valu
-000103c0: 653a 2054 2c0a 2020 2020 2020 2020 2020  e: T,.          
-000103d0: 7265 6475 6365 5f66 6e3a 2043 616c 6c61  reduce_fn: Calla
-000103e0: 626c 655b 5b4b 2c20 545d 2c20 4b5d 203d  ble[[K, T], K] =
-000103f0: 2074 7570 6c65 5f72 6564 7563 652c 0a20   tuple_reduce,. 
-00010400: 2020 2020 2020 2020 2069 6e69 745f 666e           init_fn
-00010410: 3a20 4361 6c6c 6162 6c65 5b5b 5d2c 204b  : Callable[[], K
-00010420: 5d20 3d20 7475 706c 655f 696e 6974 2920  ] = tuple_init) 
-00010430: 2d3e 2062 6f6f 6c3a 2023 2074 7970 653a  -> bool: # type:
-00010440: 2069 676e 6f72 650a 2020 2020 2222 2253   ignore.    """S
-00010450: 746f 7265 7320 6120 7661 6c75 6520 696e  tores a value in
-00010460: 2061 2063 6f6c 6c65 6374 696f 6e2e 0a0a   a collection...
-00010470: 2020 2020 436f 6c6c 6563 7469 6f6e 7320      Collections 
-00010480: 6361 6e20 6265 2075 7365 6420 746f 2063  can be used to c
-00010490: 6f6c 6c65 6374 2069 6e74 6572 6d65 6469  ollect intermedi
-000104a0: 6174 6520 7661 6c75 6573 2077 6974 686f  ate values witho
-000104b0: 7574 0a20 2020 2074 6865 206f 7665 7268  ut.    the overh
-000104c0: 6561 6420 6f66 2065 7870 6c69 6369 746c  ead of explicitl
-000104d0: 7920 7061 7373 696e 6720 6120 636f 6e74  y passing a cont
-000104e0: 6169 6e65 7220 7468 726f 7567 6820 6561  ainer through ea
-000104f0: 6368 204d 6f64 756c 6520 6361 6c6c 2e0a  ch Module call..
-00010500: 0a20 2020 2049 6620 7468 6520 7461 7267  .    If the targ
-00010510: 6574 2063 6f6c 6c65 6374 696f 6e20 6973  et collection is
-00010520: 206e 6f74 206d 7574 6162 6c65 2060 736f   not mutable `so
-00010530: 7760 2062 6568 6176 6573 206c 696b 6520  w` behaves like 
-00010540: 6120 6e6f 2d6f 700a 2020 2020 616e 6420  a no-op.    and 
-00010550: 7265 7475 726e 7320 6046 616c 7365 602e  returns `False`.
-00010560: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
-00010570: 0a20 2020 2020 2069 6d70 6f72 7420 6a61  .      import ja
-00010580: 780a 2020 2020 2020 696d 706f 7274 206a  x.      import j
-00010590: 6178 2e6e 756d 7079 2061 7320 6a6e 700a  ax.numpy as jnp.
-000105a0: 2020 2020 2020 696d 706f 7274 2066 6c61        import fla
-000105b0: 782e 6c69 6e65 6e20 6173 206e 6e0a 0a20  x.linen as nn.. 
-000105c0: 2020 2020 2063 6c61 7373 2046 6f6f 286e       class Foo(n
-000105d0: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
-000105e0: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
-000105f0: 2020 2020 2020 2064 6566 205f 5f63 616c         def __cal
-00010600: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
-00010610: 2020 2020 2020 2020 6820 3d20 6e6e 2e44          h = nn.D
-00010620: 656e 7365 2834 2928 7829 0a20 2020 2020  ense(4)(x).     
-00010630: 2020 2020 2073 656c 662e 736f 7728 2769       self.sow('i
-00010640: 6e74 6572 6d65 6469 6174 6573 272c 2027  ntermediates', '
-00010650: 6827 2c20 6829 0a20 2020 2020 2020 2020  h', h).         
-00010660: 2072 6574 7572 6e20 6e6e 2e44 656e 7365   return nn.Dense
-00010670: 2832 2928 6829 0a0a 2020 2020 2020 7820  (2)(h)..      x 
-00010680: 3d20 6a6e 702e 6f6e 6573 2828 3136 2c20  = jnp.ones((16, 
-00010690: 3929 290a 2020 2020 2020 6d6f 6465 6c20  9)).      model 
-000106a0: 3d20 466f 6f28 290a 2020 2020 2020 7661  = Foo().      va
-000106b0: 7269 6162 6c65 7320 3d20 6d6f 6465 6c2e  riables = model.
-000106c0: 696e 6974 286a 6178 2e72 616e 646f 6d2e  init(jax.random.
-000106d0: 5052 4e47 4b65 7928 3029 2c20 7829 0a20  PRNGKey(0), x). 
-000106e0: 2020 2020 2079 2c20 7374 6174 6520 3d20       y, state = 
-000106f0: 6d6f 6465 6c2e 6170 706c 7928 7661 7269  model.apply(vari
-00010700: 6162 6c65 732c 2078 2c20 6d75 7461 626c  ables, x, mutabl
-00010710: 653d 5b27 696e 7465 726d 6564 6961 7465  e=['intermediate
-00010720: 7327 5d29 0a20 2020 2020 2070 7269 6e74  s']).      print
-00010730: 2873 7461 7465 5b27 696e 7465 726d 6564  (state['intermed
-00010740: 6961 7465 7327 5d29 2020 2320 7b27 6827  iates'])  # {'h'
-00010750: 3a20 282e 2e2e 2c29 7d0a 0a20 2020 2042  : (...,)}..    B
-00010760: 7920 6465 6661 756c 7420 7468 6520 7661  y default the va
-00010770: 6c75 6573 2061 7265 2073 746f 7265 6420  lues are stored 
-00010780: 696e 2061 2074 7570 6c65 2061 6e64 2065  in a tuple and e
-00010790: 6163 6820 7374 6f72 6564 2076 616c 7565  ach stored value
-000107a0: 0a20 2020 2069 7320 6170 7065 6e64 6564  .    is appended
-000107b0: 2061 7420 7468 6520 656e 642e 2054 6869   at the end. Thi
-000107c0: 7320 7761 7920 616c 6c20 696e 7465 726d  s way all interm
-000107d0: 6564 6961 7465 7320 6361 6e20 6265 2074  ediates can be t
-000107e0: 7261 636b 6564 2077 6865 6e0a 2020 2020  racked when.    
-000107f0: 7468 6520 7361 6d65 206d 6f64 756c 6520  the same module 
-00010800: 6973 2063 616c 6c65 6420 6d75 6c74 6970  is called multip
-00010810: 6c65 2074 696d 6573 2e20 416c 7465 726e  le times. Altern
-00010820: 6174 6976 656c 792c 2061 2063 7573 746f  atively, a custo
-00010830: 6d0a 2020 2020 696e 6974 2f72 6564 7563  m.    init/reduc
-00010840: 6520 6675 6e63 7469 6f6e 2063 616e 2062  e function can b
-00010850: 6520 7061 7373 6564 3a3a 0a0a 2020 2020  e passed::..    
-00010860: 2020 636c 6173 7320 466f 6f32 286e 6e2e    class Foo2(nn.
-00010870: 4d6f 6475 6c65 293a 0a20 2020 2020 2020  Module):.       
-00010880: 2040 6e6e 2e63 6f6d 7061 6374 0a20 2020   @nn.compact.   
-00010890: 2020 2020 2064 6566 205f 5f63 616c 6c5f       def __call_
-000108a0: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
-000108b0: 2020 2020 2020 696e 6974 5f66 6e20 3d20        init_fn = 
-000108c0: 6c61 6d62 6461 3a20 300a 2020 2020 2020  lambda: 0.      
-000108d0: 2020 2020 7265 6475 6365 5f66 6e20 3d20      reduce_fn = 
-000108e0: 6c61 6d62 6461 2061 2c20 623a 2061 202b  lambda a, b: a +
-000108f0: 2062 0a20 2020 2020 2020 2020 2073 656c   b.          sel
-00010900: 662e 736f 7728 2769 6e74 6572 6d65 6469  f.sow('intermedi
-00010910: 6174 6573 272c 2027 6827 2c20 782c 0a20  ates', 'h', x,. 
-00010920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010930: 2020 696e 6974 5f66 6e3d 696e 6974 5f66    init_fn=init_f
-00010940: 6e2c 2072 6564 7563 655f 666e 3d72 6564  n, reduce_fn=red
-00010950: 7563 655f 666e 290a 2020 2020 2020 2020  uce_fn).        
-00010960: 2020 7365 6c66 2e73 6f77 2827 696e 7465    self.sow('inte
-00010970: 726d 6564 6961 7465 7327 2c20 2768 272c  rmediates', 'h',
-00010980: 2078 202a 2032 2c0a 2020 2020 2020 2020   x * 2,.        
-00010990: 2020 2020 2020 2020 2020 2069 6e69 745f             init_
-000109a0: 666e 3d69 6e69 745f 666e 2c20 7265 6475  fn=init_fn, redu
-000109b0: 6365 5f66 6e3d 7265 6475 6365 5f66 6e29  ce_fn=reduce_fn)
-000109c0: 0a20 2020 2020 2020 2020 2072 6574 7572  .          retur
-000109d0: 6e20 780a 0a20 2020 2020 206d 6f64 656c  n x..      model
-000109e0: 203d 2046 6f6f 3228 290a 2020 2020 2020   = Foo2().      
-000109f0: 7661 7269 6162 6c65 7320 3d20 6d6f 6465  variables = mode
-00010a00: 6c2e 696e 6974 286a 6178 2e72 616e 646f  l.init(jax.rando
-00010a10: 6d2e 5052 4e47 4b65 7928 3029 2c20 7829  m.PRNGKey(0), x)
-00010a20: 0a20 2020 2020 2079 2c20 7374 6174 6520  .      y, state 
-00010a30: 3d20 6d6f 6465 6c2e 6170 706c 7928 7661  = model.apply(va
-00010a40: 7269 6162 6c65 732c 206a 6e70 2e6f 6e65  riables, jnp.one
-00010a50: 7328 2831 2c20 3129 292c 206d 7574 6162  s((1, 1)), mutab
-00010a60: 6c65 3d5b 2769 6e74 6572 6d65 6469 6174  le=['intermediat
-00010a70: 6573 275d 290a 2020 2020 2020 7072 696e  es']).      prin
-00010a80: 7428 7374 6174 655b 2769 6e74 6572 6d65  t(state['interme
-00010a90: 6469 6174 6573 275d 2920 2023 203d 3d3e  diates'])  # ==>
-00010aa0: 207b 2768 273a 205b 5b33 2e5d 5d7d 0a0a   {'h': [[3.]]}..
-00010ab0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-00010ac0: 636f 6c3a 2054 6865 206e 616d 6520 6f66  col: The name of
-00010ad0: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
-00010ae0: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
-00010af0: 6e61 6d65 3a20 5468 6520 6e61 6d65 206f  name: The name o
-00010b00: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
-00010b10: 2020 2020 2020 7661 6c75 653a 2054 6865        value: The
-00010b20: 2076 616c 7565 206f 6620 7468 6520 7661   value of the va
-00010b30: 7269 6162 6c65 2e0a 2020 2020 2020 7265  riable..      re
-00010b40: 6475 6365 5f66 6e3a 2054 6865 2066 756e  duce_fn: The fun
-00010b50: 6374 696f 6e20 7573 6564 2074 6f20 636f  ction used to co
-00010b60: 6d62 696e 6520 7468 6520 6578 6973 7469  mbine the existi
-00010b70: 6e67 2076 616c 7565 2077 6974 680a 2020  ng value with.  
-00010b80: 2020 2020 2020 7468 6520 6e65 7720 7661        the new va
-00010b90: 6c75 652e 2054 6865 2064 6566 6175 6c74  lue. The default
-00010ba0: 2069 7320 746f 2061 7070 656e 6420 7468   is to append th
-00010bb0: 6520 7661 6c75 6520 746f 2061 2074 7570  e value to a tup
-00010bc0: 6c65 2e0a 2020 2020 2020 696e 6974 5f66  le..      init_f
-00010bd0: 6e3a 2046 6f72 2074 6865 2066 6972 7374  n: For the first
-00010be0: 2076 616c 7565 2073 746f 7265 642c 2060   value stored, `
-00010bf0: 7265 6475 6365 5f66 6e60 2077 696c 6c20  reduce_fn` will 
-00010c00: 6265 2070 6173 7365 640a 2020 2020 2020  be passed.      
-00010c10: 2020 7468 6520 7265 7375 6c74 206f 6620    the result of 
-00010c20: 6069 6e69 745f 666e 6020 746f 6765 7468  `init_fn` togeth
-00010c30: 6572 2077 6974 6820 7468 6520 7661 6c75  er with the valu
-00010c40: 6520 746f 2062 6520 7374 6f72 6564 2e0a  e to be stored..
-00010c50: 2020 2020 2020 2020 5468 6520 6465 6661          The defa
-00010c60: 756c 7420 6973 2061 6e20 656d 7074 7920  ult is an empty 
-00010c70: 7475 706c 652e 0a0a 2020 2020 5265 7475  tuple...    Retu
-00010c80: 726e 733a 0a20 2020 2020 2060 5472 7565  rns:.      `True
-00010c90: 6020 6966 2074 6865 2076 616c 7565 2068  ` if the value h
-00010ca0: 6173 2062 6565 6e20 7374 6f72 6564 2073  as been stored s
-00010cb0: 7563 6365 7373 6675 6c6c 792c 2060 4661  uccessfully, `Fa
-00010cc0: 6c73 6560 206f 7468 6572 7769 7365 2e0a  lse` otherwise..
-00010cd0: 2020 2020 2222 220a 2020 2020 6966 2073      """.    if s
-00010ce0: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-00010cf0: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
-00010d00: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
-00010d10: 2073 746f 7265 2076 6172 6961 626c 6573   store variables
-00010d20: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
-00010d30: 6c65 7322 290a 2020 2020 6966 206e 6f74  les").    if not
-00010d40: 2073 656c 662e 7363 6f70 652e 6973 5f6d   self.scope.is_m
-00010d50: 7574 6162 6c65 5f63 6f6c 6c65 6374 696f  utable_collectio
-00010d60: 6e28 636f 6c29 3a0a 2020 2020 2020 7265  n(col):.      re
-00010d70: 7475 726e 2046 616c 7365 0a20 2020 2069  turn False.    i
-00010d80: 6620 7365 6c66 2e73 636f 7065 2e68 6173  f self.scope.has
-00010d90: 5f76 6172 6961 626c 6528 636f 6c2c 206e  _variable(col, n
-00010da0: 616d 6529 3a0a 2020 2020 2020 7873 203d  ame):.      xs =
-00010db0: 2073 656c 662e 7363 6f70 652e 6765 745f   self.scope.get_
-00010dc0: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
-00010dd0: 6d65 290a 2020 2020 656c 7365 3a0a 2020  me).    else:.  
-00010de0: 2020 2020 7365 6c66 2e73 636f 7065 2e72      self.scope.r
-00010df0: 6573 6572 7665 286e 616d 6529 0a20 2020  eserve(name).   
-00010e00: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
-00010e10: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
-00010e20: 636f 6c0a 2020 2020 2020 7873 203d 2069  col.      xs = i
-00010e30: 6e69 745f 666e 2829 0a20 2020 2078 7320  nit_fn().    xs 
-00010e40: 3d20 7265 6475 6365 5f66 6e28 7873 2c20  = reduce_fn(xs, 
-00010e50: 7661 6c75 6529 0a20 2020 2073 656c 662e  value).    self.
-00010e60: 7363 6f70 652e 7075 745f 7661 7269 6162  scope.put_variab
-00010e70: 6c65 2863 6f6c 2c20 6e61 6d65 2c20 7873  le(col, name, xs
-00010e80: 290a 2020 2020 7265 7475 726e 2054 7275  ).    return Tru
-00010e90: 650a 0a20 2064 6566 2070 6572 7475 7262  e..  def perturb
-00010ea0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-00010eb0: 2c20 7661 6c75 653a 2054 2c20 636f 6c6c  , value: T, coll
-00010ec0: 6563 7469 6f6e 3a20 7374 7220 3d20 2770  ection: str = 'p
-00010ed0: 6572 7475 7262 6174 696f 6e73 2729 202d  erturbations') -
-00010ee0: 3e20 543a 0a20 2020 2022 2222 4164 6420  > T:.    """Add 
-00010ef0: 616e 207a 6572 6f2d 7661 6c75 6520 7661  an zero-value va
-00010f00: 7269 6162 6c65 2028 2770 6572 7475 7262  riable ('perturb
-00010f10: 6174 696f 6e27 2920 746f 2074 6865 2069  ation') to the i
-00010f20: 6e74 6572 6d65 6469 6174 6520 7661 6c75  ntermediate valu
-00010f30: 652e 0a0a 2020 2020 5468 6520 6772 6164  e...    The grad
-00010f40: 6965 6e74 206f 6620 6076 616c 7565 6020  ient of `value` 
-00010f50: 776f 756c 6420 6265 2074 6865 2073 616d  would be the sam
-00010f60: 6520 6173 2074 6865 2067 7261 6469 656e  e as the gradien
-00010f70: 7420 6f66 2074 6869 730a 2020 2020 7065  t of this.    pe
-00010f80: 7274 7572 6261 7469 6f6e 2076 6172 6961  rturbation varia
-00010f90: 626c 652e 2054 6865 7265 666f 7265 2c20  ble. Therefore, 
-00010fa0: 6966 2079 6f75 2064 6566 696e 6520 796f  if you define yo
-00010fb0: 7572 206c 6f73 7320 6675 6e63 7469 6f6e  ur loss function
-00010fc0: 2077 6974 680a 2020 2020 626f 7468 2070   with.    both p
-00010fd0: 6172 616d 7320 616e 6420 7065 7274 7572  arams and pertur
-00010fe0: 6261 7469 6f6e 7320 6173 2073 7461 6e64  bations as stand
-00010ff0: 616c 6f6e 6520 6172 6775 6d65 6e74 732c  alone arguments,
-00011000: 2079 6f75 2063 616e 2067 6574 2074 6865   you can get the
-00011010: 0a20 2020 2069 6e74 6572 6d65 6469 6174  .    intermediat
-00011020: 6520 6772 6164 6965 6e74 7320 6f66 2060  e gradients of `
-00011030: 7661 6c75 6560 2062 7920 7275 6e6e 696e  value` by runnin
-00011040: 6720 606a 6178 2e67 7261 6460 206f 6e20  g `jax.grad` on 
-00011050: 7468 6520 7065 7274 7572 6261 7469 6f6e  the perturbation
-00011060: 0a20 2020 2061 7267 756d 656e 742e 0a0a  .    argument...
-00011070: 2020 2020 4e6f 7465 3a20 7468 6973 2069      Note: this i
-00011080: 7320 616e 2065 7870 6572 696d 656e 7461  s an experimenta
-00011090: 6c20 4150 4920 616e 6420 6d61 7920 6265  l API and may be
-000110a0: 2074 7765 616b 6564 206c 6174 6572 2066   tweaked later f
-000110b0: 6f72 2062 6574 7465 720a 2020 2020 7065  or better.    pe
-000110c0: 7266 6f72 6d61 6e63 6520 616e 6420 7573  rformance and us
-000110d0: 6162 696c 6974 792e 0a20 2020 2041 7420  ability..    At 
-000110e0: 6974 7320 6375 7272 656e 7420 7374 6167  its current stag
-000110f0: 652c 2069 7420 6372 6561 7465 7320 6578  e, it creates ex
-00011100: 7472 6120 6475 6d6d 7920 7661 7269 6162  tra dummy variab
-00011110: 6c65 7320 7468 6174 206f 6363 7570 6965  les that occupie
-00011120: 7320 6578 7472 610a 2020 2020 6d65 6d6f  s extra.    memo
-00011130: 7279 2073 7061 6365 2e20 5573 6520 6974  ry space. Use it
-00011140: 206f 6e6c 7920 746f 2064 6562 7567 2067   only to debug g
-00011150: 7261 6469 656e 7473 2069 6e20 7472 6169  radients in trai
-00011160: 6e69 6e67 2e0a 0a20 2020 2045 7861 6d70  ning...    Examp
-00011170: 6c65 3a3a 0a0a 2020 2020 2020 696d 706f  le::..      impo
-00011180: 7274 206a 6178 0a20 2020 2020 2069 6d70  rt jax.      imp
-00011190: 6f72 7420 6a61 782e 6e75 6d70 7920 6173  ort jax.numpy as
-000111a0: 206a 6e70 0a20 2020 2020 2069 6d70 6f72   jnp.      impor
-000111b0: 7420 666c 6178 2e6c 696e 656e 2061 7320  t flax.linen as 
-000111c0: 6e6e 0a0a 2020 2020 2020 636c 6173 7320  nn..      class 
-000111d0: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
-000111e0: 2020 2020 2020 2020 2020 406e 6e2e 636f            @nn.co
-000111f0: 6d70 6163 740a 2020 2020 2020 2020 2020  mpact.          
-00011200: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
-00011210: 662c 2078 293a 0a20 2020 2020 2020 2020  f, x):.         
-00011220: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
-00011230: 6528 3329 2878 290a 2020 2020 2020 2020  e(3)(x).        
-00011240: 2020 2020 2020 7820 3d20 7365 6c66 2e70        x = self.p
-00011250: 6572 7475 7262 2827 6465 6e73 6533 272c  erturb('dense3',
-00011260: 2078 290a 2020 2020 2020 2020 2020 2020   x).            
-00011270: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
-00011280: 6528 3229 2878 290a 0a20 2020 2020 2064  e(2)(x)..      d
-00011290: 6566 206c 6f73 7328 7061 7261 6d73 2c20  ef loss(params, 
-000112a0: 7065 7274 7572 6261 7469 6f6e 732c 2069  perturbations, i
-000112b0: 6e70 7574 732c 2074 6172 6765 7473 293a  nputs, targets):
-000112c0: 0a20 2020 2020 2020 2076 6172 6961 626c  .        variabl
-000112d0: 6573 203d 207b 2770 6172 616d 7327 3a20  es = {'params': 
-000112e0: 7061 7261 6d73 2c20 2770 6572 7475 7262  params, 'perturb
-000112f0: 6174 696f 6e73 273a 2070 6572 7475 7262  ations': perturb
-00011300: 6174 696f 6e73 7d0a 2020 2020 2020 2020  ations}.        
-00011310: 7072 6564 7320 3d20 6d6f 6465 6c2e 6170  preds = model.ap
-00011320: 706c 7928 7661 7269 6162 6c65 732c 2069  ply(variables, i
-00011330: 6e70 7574 7329 0a20 2020 2020 2020 2072  nputs).        r
-00011340: 6574 7572 6e20 6a6e 702e 7371 7561 7265  eturn jnp.square
-00011350: 2870 7265 6473 202d 2074 6172 6765 7473  (preds - targets
-00011360: 292e 6d65 616e 2829 0a0a 2020 2020 2020  ).mean()..      
-00011370: 7820 3d20 6a6e 702e 6f6e 6573 2828 322c  x = jnp.ones((2,
-00011380: 2039 2929 0a20 2020 2020 2079 203d 206a   9)).      y = j
-00011390: 6e70 2e6f 6e65 7328 2832 2c20 3229 290a  np.ones((2, 2)).
-000113a0: 2020 2020 2020 6d6f 6465 6c20 3d20 466f        model = Fo
-000113b0: 6f28 290a 2020 2020 2020 7661 7269 6162  o().      variab
-000113c0: 6c65 7320 3d20 6d6f 6465 6c2e 696e 6974  les = model.init
-000113d0: 286a 6178 2e72 616e 646f 6d2e 5052 4e47  (jax.random.PRNG
-000113e0: 4b65 7928 3029 2c20 7829 0a20 2020 2020  Key(0), x).     
-000113f0: 2069 6e74 6d5f 6772 6164 7320 3d20 6a61   intm_grads = ja
-00011400: 782e 6772 6164 286c 6f73 732c 2061 7267  x.grad(loss, arg
-00011410: 6e75 6d73 3d31 2928 7661 7269 6162 6c65  nums=1)(variable
-00011420: 735b 2770 6172 616d 7327 5d2c 2076 6172  s['params'], var
-00011430: 6961 626c 6573 5b27 7065 7274 7572 6261  iables['perturba
-00011440: 7469 6f6e 7327 5d2c 2078 2c20 7929 0a20  tions'], x, y). 
-00011450: 2020 2020 2070 7269 6e74 2869 6e74 6d5f       print(intm_
-00011460: 6772 6164 735b 2764 656e 7365 3327 5d29  grads['dense3'])
-00011470: 2023 203d 3d3e 205b 5b2d 312e 3435 3639   # ==> [[-1.4569
-00011480: 3234 2020 202d 302e 3434 3333 3235 3337  24   -0.44332537
-00011490: 2020 302e 3032 3432 3238 3437 5d0a 2020    0.02422847].  
-000114a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000114b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000114c0: 2320 2020 2020 205b 2d31 2e34 3536 3932  #      [-1.45692
-000114d0: 3420 2020 2d30 2e34 3433 3332 3533 3720  4   -0.44332537 
-000114e0: 2030 2e30 3234 3232 3834 375d 5d0a 0a20   0.02422847]].. 
-000114f0: 2020 2049 6620 7065 7274 7572 6261 7469     If perturbati
-00011500: 6f6e 7320 6172 6520 6e6f 7420 7061 7373  ons are not pass
-00011510: 6564 2074 6f20 6061 7070 6c79 602c 2060  ed to `apply`, `
-00011520: 7065 7274 7572 6260 2062 6568 6176 6573  perturb` behaves
-00011530: 206c 696b 6520 6120 6e6f 2d6f 700a 2020   like a no-op.  
-00011540: 2020 736f 2079 6f75 2063 616e 2065 6173    so you can eas
-00011550: 696c 7920 6469 7361 626c 6520 7468 6520  ily disable the 
-00011560: 6265 6861 7669 6f72 2077 6865 6e20 6e6f  behavior when no
-00011570: 7420 6e65 6564 6564 3a3a 0a0a 2020 2020  t needed::..    
-00011580: 2020 6d6f 6465 6c2e 6170 706c 7928 7b27    model.apply({'
-00011590: 7061 7261 6d73 273a 2070 6172 616d 732c  params': params,
-000115a0: 2027 7065 7274 7572 6261 7469 6f6e 7327   'perturbations'
-000115b0: 3a20 7065 7274 7572 6261 7469 6f6e 737d  : perturbations}
-000115c0: 2c20 7829 2023 2077 6f72 6b73 2061 7320  , x) # works as 
-000115d0: 6578 7065 6374 6564 0a20 2020 2020 206d  expected.      m
-000115e0: 6f64 656c 2e61 7070 6c79 287b 2770 6172  odel.apply({'par
-000115f0: 616d 7327 3a20 7061 7261 6d73 7d2c 2078  ams': params}, x
-00011600: 2920 2320 6265 6861 7665 7320 6c69 6b65  ) # behaves like
-00011610: 2061 206e 6f2d 6f70 0a0a 2020 2020 2222   a no-op..    ""
-00011620: 220a 2020 2020 6465 6620 5f72 6f6f 745f  ".    def _root_
-00011630: 6861 735f 636f 6c6c 6563 7469 6f6e 2829  has_collection()
-00011640: 3a0a 2020 2020 2020 2222 2252 6574 7572  :.      """Retur
-00011650: 6e73 2054 7275 6520 6966 2074 6865 2072  ns True if the r
-00011660: 6f6f 7420 7363 6f70 6520 6861 7320 7468  oot scope has th
-00011670: 6520 636f 6c6c 6563 7469 6f6e 2e22 2222  e collection."""
-00011680: 0a20 2020 2020 2061 7373 6572 7420 7365  .      assert se
-00011690: 6c66 2e73 636f 7065 2069 7320 6e6f 7420  lf.scope is not 
-000116a0: 4e6f 6e65 0a20 2020 2020 2072 6574 7572  None.      retur
-000116b0: 6e20 636f 6c6c 6563 7469 6f6e 2069 6e20  n collection in 
-000116c0: 7365 6c66 2e73 636f 7065 2e72 6f6f 742e  self.scope.root.
-000116d0: 5f76 6172 6961 626c 6573 0a20 2020 2023  _variables.    #
-000116e0: 2077 6520 7769 6c6c 206f 6e6c 7920 6164   we will only ad
-000116f0: 6420 7468 6520 7065 7274 7572 6261 7469  d the perturbati
-00011700: 6f6e 2076 6172 6961 626c 6520 6966 2074  on variable if t
-00011710: 6865 2063 6f6c 6c65 6374 696f 6e20 6973  he collection is
-00011720: 206d 7574 6162 6c65 0a20 2020 2023 2028   mutable.    # (
-00011730: 652e 672e 2064 7572 696e 6720 6069 6e69  e.g. during `ini
-00011740: 7460 2920 6f72 2069 6620 7468 6520 636f  t`) or if the co
-00011750: 6c6c 6563 7469 6f6e 2077 6173 2070 6173  llection was pas
-00011760: 7365 6420 746f 2060 6170 706c 7960 2028  sed to `apply` (
-00011770: 636f 6e74 6169 6e65 6420 696e 0a20 2020  contained in.   
-00011780: 2023 2074 6865 2072 6f6f 7420 7363 6f70   # the root scop
-00011790: 6529 2e0a 2020 2020 6966 2073 656c 662e  e)..    if self.
-000117a0: 6973 5f6d 7574 6162 6c65 5f63 6f6c 6c65  is_mutable_colle
-000117b0: 6374 696f 6e28 636f 6c6c 6563 7469 6f6e  ction(collection
-000117c0: 2920 6f72 205f 726f 6f74 5f68 6173 5f63  ) or _root_has_c
-000117d0: 6f6c 6c65 6374 696f 6e28 293a 0a20 2020  ollection():.   
-000117e0: 2020 2076 616c 7565 202b 3d20 7365 6c66     value += self
-000117f0: 2e76 6172 6961 626c 6528 636f 6c6c 6563  .variable(collec
-00011800: 7469 6f6e 2c20 6e61 6d65 2c20 6c61 6d62  tion, name, lamb
-00011810: 6461 3a20 6a6e 702e 7a65 726f 735f 6c69  da: jnp.zeros_li
-00011820: 6b65 2876 616c 7565 2929 2e76 616c 7565  ke(value)).value
-00011830: 2023 2074 7970 653a 2069 676e 6f72 650a   # type: ignore.
-00011840: 2020 2020 7265 7475 726e 2076 616c 7565      return value
-00011850: 0a0a 2020 6465 6620 7461 6275 6c61 7465  ..  def tabulate
-00011860: 280a 2020 2020 7365 6c66 2c0a 2020 2020  (.    self,.    
-00011870: 726e 6773 3a20 556e 696f 6e5b 5052 4e47  rngs: Union[PRNG
-00011880: 4b65 792c 2052 4e47 5365 7175 656e 6365  Key, RNGSequence
-00011890: 735d 2c0a 2020 2020 2a61 7267 732c 0a20  s],.    *args,. 
-000118a0: 2020 2064 6570 7468 3a20 4f70 7469 6f6e     depth: Option
-000118b0: 616c 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a  al[int] = None,.
-000118c0: 2020 2020 7368 6f77 5f72 6570 6561 7465      show_repeate
-000118d0: 643a 2062 6f6f 6c20 3d20 4661 6c73 652c  d: bool = False,
-000118e0: 0a20 2020 206d 7574 6162 6c65 3a20 436f  .    mutable: Co
-000118f0: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
-00011900: 2054 7275 652c 0a20 2020 2063 6f6e 736f   True,.    conso
-00011910: 6c65 5f6b 7761 7267 733a 204f 7074 696f  le_kwargs: Optio
-00011920: 6e61 6c5b 4d61 7070 696e 675b 7374 722c  nal[Mapping[str,
-00011930: 2041 6e79 5d5d 203d 204e 6f6e 652c 0a20   Any]] = None,. 
-00011940: 2020 202a 2a6b 7761 7267 7329 202d 3e20     **kwargs) -> 
-00011950: 7374 723a 0a20 2020 2022 2222 4372 6561  str:.    """Crea
-00011960: 7465 7320 6120 7375 6d6d 6172 7920 6f66  tes a summary of
-00011970: 2074 6865 204d 6f64 756c 6520 7265 7072   the Module repr
-00011980: 6573 656e 7465 6420 6173 2061 2074 6162  esented as a tab
-00011990: 6c65 2e0a 0a20 2020 2054 6869 7320 6d65  le...    This me
-000119a0: 7468 6f64 2068 6173 2074 6865 2073 616d  thod has the sam
-000119b0: 6520 7369 676e 6174 7572 6520 616e 6420  e signature and 
-000119c0: 696e 7465 726e 616c 6c79 2063 616c 6c73  internally calls
-000119d0: 2060 4d6f 6475 6c65 2e69 6e69 7460 2c0a   `Module.init`,.
-000119e0: 2020 2020 6275 7420 696e 7374 6561 6420      but instead 
-000119f0: 6f66 2072 6574 7572 6e69 6e67 2074 6865  of returning the
-00011a00: 2076 6172 6961 626c 6573 2c20 6974 2072   variables, it r
-00011a10: 6574 7572 6e73 2074 6865 2073 7472 696e  eturns the strin
-00011a20: 6720 7375 6d6d 6172 697a 696e 670a 2020  g summarizing.  
-00011a30: 2020 7468 6520 4d6f 6475 6c65 2069 6e20    the Module in 
-00011a40: 6120 7461 626c 652e 2060 7461 6275 6c61  a table. `tabula
-00011a50: 7465 6020 7573 6573 2060 6a61 782e 6576  te` uses `jax.ev
-00011a60: 616c 5f73 6861 7065 6020 746f 2072 756e  al_shape` to run
-00011a70: 2074 6865 2066 6f72 7761 7264 0a20 2020   the forward.   
-00011a80: 2063 6f6d 7075 7461 7469 6f6e 2077 6974   computation wit
-00011a90: 686f 7574 2063 6f6e 7375 6d69 6e67 2061  hout consuming a
-00011aa0: 6e79 2046 4c4f 5073 206f 7220 616c 6c6f  ny FLOPs or allo
-00011ab0: 6361 7469 6e67 206d 656d 6f72 792e 0a0a  cating memory...
-00011ac0: 2020 2020 4164 6469 7469 6f6e 616c 2061      Additional a
-00011ad0: 7267 756d 656e 7473 2063 616e 2062 6520  rguments can be 
-00011ae0: 7061 7373 6564 2069 6e74 6f20 7468 6520  passed into the 
-00011af0: 6063 6f6e 736f 6c65 5f6b 7761 7267 7360  `console_kwargs`
-00011b00: 2061 7267 756d 656e 742c 2066 6f72 2065   argument, for e
-00011b10: 7861 6d70 6c65 2c0a 2020 2020 607b 2777  xample,.    `{'w
-00011b20: 6964 7468 273a 2031 3230 7d60 2e20 466f  idth': 120}`. Fo
-00011b30: 7220 6120 6675 6c6c 206c 6973 7420 6f66  r a full list of
-00011b40: 2060 636f 6e73 6f6c 655f 6b77 6172 6773   `console_kwargs
-00011b50: 6020 6172 6775 6d65 6e74 732c 2073 6565  ` arguments, see
-00011b60: 3a0a 2020 2020 6874 7470 733a 2f2f 7269  :.    https://ri
-00011b70: 6368 2e72 6561 6474 6865 646f 6373 2e69  ch.readthedocs.i
-00011b80: 6f2f 656e 2f73 7461 626c 652f 7265 6665  o/en/stable/refe
-00011b90: 7265 6e63 652f 636f 6e73 6f6c 652e 6874  rence/console.ht
-00011ba0: 6d6c 2372 6963 682e 636f 6e73 6f6c 652e  ml#rich.console.
-00011bb0: 436f 6e73 6f6c 650a 0a20 2020 2045 7861  Console..    Exa
-00011bc0: 6d70 6c65 3a3a 0a0a 2020 2020 2020 696d  mple::..      im
-00011bd0: 706f 7274 206a 6178 0a20 2020 2020 2069  port jax.      i
-00011be0: 6d70 6f72 7420 6a61 782e 6e75 6d70 7920  mport jax.numpy 
-00011bf0: 6173 206a 6e70 0a20 2020 2020 2069 6d70  as jnp.      imp
-00011c00: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-00011c10: 7320 6e6e 0a0a 2020 2020 2020 636c 6173  s nn..      clas
-00011c20: 7320 466f 6f28 6e6e 2e4d 6f64 756c 6529  s Foo(nn.Module)
-00011c30: 3a0a 2020 2020 2020 2020 2020 406e 6e2e  :.          @nn.
-00011c40: 636f 6d70 6163 740a 2020 2020 2020 2020  compact.        
-00011c50: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-00011c60: 656c 662c 2078 293a 0a20 2020 2020 2020  elf, x):.       
-00011c70: 2020 2020 2020 2068 203d 206e 6e2e 4465         h = nn.De
-00011c80: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
-00011c90: 2020 2020 2020 2020 7265 7475 726e 206e          return n
-00011ca0: 6e2e 4465 6e73 6528 3229 2868 290a 0a20  n.Dense(2)(h).. 
-00011cb0: 2020 2020 2078 203d 206a 6e70 2e6f 6e65       x = jnp.one
-00011cc0: 7328 2831 362c 2039 2929 0a0a 2020 2020  s((16, 9))..    
-00011cd0: 2020 7072 696e 7428 466f 6f28 292e 7461    print(Foo().ta
-00011ce0: 6275 6c61 7465 286a 6178 2e72 616e 646f  bulate(jax.rando
-00011cf0: 6d2e 5052 4e47 4b65 7928 3029 2c20 7829  m.PRNGKey(0), x)
-00011d00: 290a 0a0a 2020 2020 5468 6973 2067 6976  )...    This giv
-00011d10: 6573 2074 6865 2066 6f6c 6c6f 7769 6e67  es the following
-00011d20: 206f 7574 7075 743a 3a0a 0a20 2020 2020   output::..     
-00011d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d50: 2046 6f6f 2053 756d 6d61 7279 0a20 2020   Foo Summary.   
-00011d60: 2020 20e2 948f e294 81e2 9481 e294 81e2     .............
-00011d70: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011d80: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
-00011d90: e294 81e2 9481 e294 81e2 9481 e294 b3e2  ................
-00011da0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011db0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011dc0: e294 81e2 9481 e294 81e2 9481 e294 b3e2  ................
-00011dd0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011de0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011df0: e294 81e2 9481 e294 81e2 9481 e294 b3e2  ................
-00011e00: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011e10: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011e20: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00011e30: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011e40: 81e2 9493 0a20 2020 2020 20e2 9483 2070  .....      ... p
-00011e50: 6174 6820 2020 20e2 9483 206d 6f64 756c  ath    ... modul
-00011e60: 6520 e294 8320 696e 7075 7473 2020 2020  e ... inputs    
-00011e70: 2020 2020 e294 8320 6f75 7470 7574 7320      ... outputs 
-00011e80: 2020 2020 2020 e294 8320 7061 7261 6d73        ... params
-00011e90: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
-00011ea0: 9483 0a20 2020 2020 20e2 94a1 e294 81e2  ...      .......
-00011eb0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011ec0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
-00011ed0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00011ee0: 9481 e295 87e2 9481 e294 81e2 9481 e294  ................
-00011ef0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011f00: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00011f10: 9481 e295 87e2 9481 e294 81e2 9481 e294  ................
-00011f20: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011f30: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00011f40: 9481 e295 87e2 9481 e294 81e2 9481 e294  ................
-00011f50: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00011f60: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00011f70: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00011f80: 81e2 9481 e294 81e2 94a9 0a20 2020 2020  ...........     
-00011f90: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
-00011fa0: 2046 6f6f 2020 2020 e294 8220 666c 6f61   Foo    ... floa
-00011fb0: 7433 325b 3136 2c39 5d20 e294 8220 666c  t32[16,9] ... fl
-00011fc0: 6f61 7433 325b 3136 2c32 5d20 e294 8220  oat32[16,2] ... 
-00011fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011fe0: 2020 2020 20e2 9482 0a20 2020 2020 20e2       ....      .
-00011ff0: 949c e294 80e2 9480 e294 80e2 9480 e294  ................
-00012000: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012010: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012020: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012030: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012040: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012050: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012060: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012070: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012080: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012090: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000120a0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000120b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000120c0: 80e2 9480 e294 80e2 9480 e294 80e2 94a4  ................
-000120d0: 0a20 2020 2020 20e2 9482 2044 656e 7365  .      ... Dense
-000120e0: 5f30 20e2 9482 2044 656e 7365 2020 e294  _0 ... Dense  ..
-000120f0: 8220 666c 6f61 7433 325b 3136 2c39 5d20  . float32[16,9] 
-00012100: e294 8220 666c 6f61 7433 325b 3136 2c34  ... float32[16,4
-00012110: 5d20 e294 8220 6269 6173 3a20 666c 6f61  ] ... bias: floa
-00012120: 7433 325b 345d 2020 2020 20e2 9482 0a20  t32[4]     .... 
-00012130: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00012140: 20e2 9482 2020 2020 2020 2020 e294 8220   ...        ... 
-00012150: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-00012160: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-00012170: e294 8220 6b65 726e 656c 3a20 666c 6f61  ... kernel: floa
-00012180: 7433 325b 392c 345d 20e2 9482 0a20 2020  t32[9,4] ....   
-00012190: 2020 20e2 9482 2020 2020 2020 2020 20e2     ...         .
-000121a0: 9482 2020 2020 2020 2020 e294 8220 2020  ..        ...   
-000121b0: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-000121c0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-000121d0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-000121e0: 2020 2020 2020 20e2 9482 0a20 2020 2020         ....     
-000121f0: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
-00012200: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00012210: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-00012220: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-00012230: 3430 2028 3136 3020 4229 2020 2020 2020  40 (160 B)      
-00012240: 2020 2020 20e2 9482 0a20 2020 2020 20e2       ....      .
-00012250: 949c e294 80e2 9480 e294 80e2 9480 e294  ................
-00012260: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012270: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012280: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012290: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000122a0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000122b0: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-000122c0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000122d0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000122e0: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-000122f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012300: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012310: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012320: 80e2 9480 e294 80e2 9480 e294 80e2 94a4  ................
-00012330: 0a20 2020 2020 20e2 9482 2044 656e 7365  .      ... Dense
-00012340: 5f31 20e2 9482 2044 656e 7365 2020 e294  _1 ... Dense  ..
-00012350: 8220 666c 6f61 7433 325b 3136 2c34 5d20  . float32[16,4] 
-00012360: e294 8220 666c 6f61 7433 325b 3136 2c32  ... float32[16,2
-00012370: 5d20 e294 8220 6269 6173 3a20 666c 6f61  ] ... bias: floa
-00012380: 7433 325b 325d 2020 2020 20e2 9482 0a20  t32[2]     .... 
-00012390: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-000123a0: 20e2 9482 2020 2020 2020 2020 e294 8220   ...        ... 
-000123b0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-000123c0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-000123d0: e294 8220 6b65 726e 656c 3a20 666c 6f61  ... kernel: floa
-000123e0: 7433 325b 342c 325d 20e2 9482 0a20 2020  t32[4,2] ....   
-000123f0: 2020 20e2 9482 2020 2020 2020 2020 20e2     ...         .
-00012400: 9482 2020 2020 2020 2020 e294 8220 2020  ..        ...   
-00012410: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-00012420: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-00012430: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-00012440: 2020 2020 2020 20e2 9482 0a20 2020 2020         ....     
-00012450: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
-00012460: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00012470: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-00012480: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-00012490: 3130 2028 3430 2042 2920 2020 2020 2020  10 (40 B)       
-000124a0: 2020 2020 20e2 9482 0a20 2020 2020 20e2       ....      .
-000124b0: 949c e294 80e2 9480 e294 80e2 9480 e294  ................
-000124c0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-000124d0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000124e0: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-000124f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012500: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012510: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012520: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012530: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012540: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00012550: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012560: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012570: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012580: 80e2 9480 e294 80e2 9480 e294 80e2 94a4  ................
-00012590: 0a20 2020 2020 20e2 9482 2020 2020 2020  .      ...      
-000125a0: 2020 20e2 9482 2020 2020 2020 2020 e294     ...        ..
-000125b0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-000125c0: e294 8220 2020 2020 2020 2020 546f 7461  ...         Tota
-000125d0: 6c20 e294 8220 3530 2028 3230 3020 4229  l ... 50 (200 B)
-000125e0: 2020 2020 2020 2020 2020 20e2 9482 0a20             .... 
-000125f0: 2020 2020 20e2 9494 e294 80e2 9480 e294       ...........
-00012600: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012610: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
-00012620: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012630: b4e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012640: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012650: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012660: b4e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012670: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012680: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012690: b4e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000126a0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000126b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000126c0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000126d0: e294 80e2 9498 0a0a 2020 2020 2020 2020  ........        
-000126e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000126f0: 2020 2020 546f 7461 6c20 5061 7261 6d65      Total Parame
-00012700: 7465 7273 3a20 3530 2028 3230 3020 4229  ters: 50 (200 B)
-00012710: 0a0a 2020 2020 2a2a 4e6f 7465 2a2a 3a20  ..    **Note**: 
-00012720: 726f 7773 206f 7264 6572 2069 6e20 7468  rows order in th
-00012730: 6520 7461 626c 6520 646f 6573 206e 6f74  e table does not
-00012740: 2072 6570 7265 7365 6e74 2065 7865 6375   represent execu
-00012750: 7469 6f6e 206f 7264 6572 2c0a 2020 2020  tion order,.    
-00012760: 696e 7374 6561 6420 6974 2061 6c69 676e  instead it align
-00012770: 7320 7769 7468 2074 6865 206f 7264 6572  s with the order
-00012780: 206f 6620 6b65 7973 2069 6e20 6076 6172   of keys in `var
-00012790: 6961 626c 6573 6020 7768 6963 6820 6172  iables` which ar
-000127a0: 6520 736f 7274 6564 0a20 2020 2061 6c70  e sorted.    alp
-000127b0: 6861 6265 7469 6361 6c6c 792e 0a0a 2020  habetically...  
-000127c0: 2020 4172 6773 3a0a 2020 2020 2020 726e    Args:.      rn
-000127d0: 6773 3a20 5468 6520 726e 6773 2066 6f72  gs: The rngs for
-000127e0: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
-000127f0: 6c6c 6563 7469 6f6e 7320 6173 2070 6173  llections as pas
-00012800: 7365 6420 746f 2060 4d6f 6475 6c65 2e69  sed to `Module.i
-00012810: 6e69 7460 2e0a 2020 2020 2020 2a61 7267  nit`..      *arg
-00012820: 733a 2054 6865 2061 7267 756d 656e 7473  s: The arguments
-00012830: 2074 6f20 7468 6520 666f 7277 6172 6420   to the forward 
-00012840: 636f 6d70 7574 6174 696f 6e2e 0a20 2020  computation..   
-00012850: 2020 2064 6570 7468 3a20 636f 6e74 726f     depth: contro
-00012860: 6c73 2068 6f77 206d 616e 7920 7375 626d  ls how many subm
-00012870: 6f64 756c 6520 6465 6570 2074 6865 2073  odule deep the s
-00012880: 756d 6d61 7279 2063 616e 2067 6f2e 2042  ummary can go. B
-00012890: 7920 6465 6661 756c 7420 6974 730a 2020  y default its.  
-000128a0: 2020 2020 2020 604e 6f6e 6560 2077 6869        `None` whi
-000128b0: 6368 206d 6561 6e73 206e 6f20 6c69 6d69  ch means no limi
-000128c0: 742e 2049 6620 6120 7375 626d 6f64 756c  t. If a submodul
-000128d0: 6520 6973 206e 6f74 2073 686f 776e 2062  e is not shown b
-000128e0: 6563 6175 7365 206f 6620 7468 650a 2020  ecause of the.  
-000128f0: 2020 2020 2020 6465 7074 6820 6c69 6d69        depth limi
-00012900: 742c 2069 7473 2070 6172 616d 6574 6572  t, its parameter
-00012910: 2063 6f75 6e74 2061 6e64 2062 7974 6573   count and bytes
-00012920: 2077 696c 6c20 6265 2061 6464 6564 2074   will be added t
-00012930: 6f20 7468 6520 726f 7720 6f66 2069 7473  o the row of its
-00012940: 0a20 2020 2020 2020 2066 6972 7374 2073  .        first s
-00012950: 686f 776e 2061 6e63 6573 746f 7220 7375  hown ancestor su
-00012960: 6368 2074 6861 7420 7468 6520 7375 6d20  ch that the sum 
-00012970: 6f66 2061 6c6c 2072 6f77 7320 616c 7761  of all rows alwa
-00012980: 7973 2061 6464 7320 7570 2074 6f20 7468  ys adds up to th
-00012990: 650a 2020 2020 2020 2020 746f 7461 6c20  e.        total 
-000129a0: 6e75 6d62 6572 206f 6620 7061 7261 6d65  number of parame
-000129b0: 7465 7273 206f 6620 7468 6520 4d6f 6475  ters of the Modu
-000129c0: 6c65 2e0a 2020 2020 2020 7368 6f77 5f72  le..      show_r
-000129d0: 6570 6561 7465 643a 2049 6620 6054 7275  epeated: If `Tru
-000129e0: 6560 2c20 7265 7065 6174 6564 2063 616c  e`, repeated cal
-000129f0: 6c73 2074 6f20 7468 6520 7361 6d65 206d  ls to the same m
-00012a00: 6f64 756c 6520 7769 6c6c 2062 6520 7368  odule will be sh
-00012a10: 6f77 6e0a 2020 2020 2020 2020 696e 2074  own.        in t
-00012a20: 6865 2074 6162 6c65 2c20 6f74 6865 7277  he table, otherw
-00012a30: 6973 6520 6f6e 6c79 2074 6865 2066 6972  ise only the fir
-00012a40: 7374 2063 616c 6c20 7769 6c6c 2062 6520  st call will be 
-00012a50: 7368 6f77 6e2e 2044 6566 6175 6c74 2069  shown. Default i
-00012a60: 730a 2020 2020 2020 2020 6046 616c 7365  s.        `False
-00012a70: 602e 0a20 2020 2020 206d 7574 6162 6c65  `..      mutable
-00012a80: 3a20 4361 6e20 6265 2062 6f6f 6c2c 2073  : Can be bool, s
-00012a90: 7472 2c20 6f72 206c 6973 742e 2053 7065  tr, or list. Spe
-00012aa0: 6369 6669 6573 2077 6869 6368 2063 6f6c  cifies which col
-00012ab0: 6c65 6374 696f 6e73 2073 686f 756c 6420  lections should 
-00012ac0: 6265 0a20 2020 2020 2020 2074 7265 6174  be.        treat
-00012ad0: 6564 2061 7320 6d75 7461 626c 653a 2060  ed as mutable: `
-00012ae0: 6062 6f6f 6c60 603a 2061 6c6c 2f6e 6f20  `bool``: all/no 
-00012af0: 636f 6c6c 6563 7469 6f6e 7320 6172 6520  collections are 
-00012b00: 6d75 7461 626c 652e 2060 6073 7472 6060  mutable. ``str``
-00012b10: 3a20 5468 650a 2020 2020 2020 2020 6e61  : The.        na
-00012b20: 6d65 206f 6620 6120 7369 6e67 6c65 206d  me of a single m
-00012b30: 7574 6162 6c65 2063 6f6c 6c65 6374 696f  utable collectio
-00012b40: 6e2e 2060 606c 6973 7460 603a 2041 206c  n. ``list``: A l
-00012b50: 6973 7420 6f66 206e 616d 6573 206f 6620  ist of names of 
-00012b60: 6d75 7461 626c 650a 2020 2020 2020 2020  mutable.        
-00012b70: 636f 6c6c 6563 7469 6f6e 732e 2042 7920  collections. By 
-00012b80: 6465 6661 756c 7420 616c 6c20 636f 6c6c  default all coll
-00012b90: 6563 7469 6f6e 7320 6578 6365 7074 2027  ections except '
-00012ba0: 696e 7465 726d 6564 6961 7465 7327 2061  intermediates' a
-00012bb0: 7265 0a20 2020 2020 2020 206d 7574 6162  re.        mutab
-00012bc0: 6c65 2e0a 2020 2020 2020 636f 6e73 6f6c  le..      consol
-00012bd0: 655f 6b77 6172 6773 3a20 416e 206f 7074  e_kwargs: An opt
-00012be0: 696f 6e61 6c20 6469 6374 696f 6e61 7279  ional dictionary
-00012bf0: 2077 6974 6820 6164 6469 7469 6f6e 616c   with additional
-00012c00: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
-00012c10: 7473 2074 6861 740a 2020 2020 2020 2020  ts that.        
-00012c20: 6172 6520 7061 7373 6564 2074 6f20 6072  are passed to `r
-00012c30: 6963 682e 636f 6e73 6f6c 652e 436f 6e73  ich.console.Cons
-00012c40: 6f6c 6560 2077 6865 6e20 7265 6e64 6572  ole` when render
-00012c50: 696e 6720 7468 6520 7461 626c 652e 2044  ing the table. D
-00012c60: 6566 6175 6c74 2061 7267 756d 656e 7473  efault arguments
-00012c70: 0a20 2020 2020 2020 2061 7265 2060 7b27  .        are `{'
-00012c80: 666f 7263 655f 7465 726d 696e 616c 273a  force_terminal':
-00012c90: 2054 7275 652c 2027 666f 7263 655f 6a75   True, 'force_ju
-00012ca0: 7079 7465 7227 3a20 4661 6c73 657d 602e  pyter': False}`.
-00012cb0: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
-00012cc0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
-00012cd0: 7473 2074 6f20 7061 7373 2074 6f20 7468  ts to pass to th
-00012ce0: 6520 666f 7277 6172 6420 636f 6d70 7574  e forward comput
-00012cf0: 6174 696f 6e2e 0a0a 2020 2020 5265 7475  ation...    Retu
-00012d00: 726e 733a 0a20 2020 2020 2041 2073 7472  rns:.      A str
-00012d10: 696e 6720 7375 6d6d 6172 697a 696e 6720  ing summarizing 
-00012d20: 7468 6520 4d6f 6475 6c65 2e0a 2020 2020  the Module..    
-00012d30: 2222 220a 2020 2020 6672 6f6d 2066 6c61  """.    from fla
-00012d40: 782e 6c69 6e65 6e20 696d 706f 7274 2073  x.linen import s
-00012d50: 756d 6d61 7279 0a0a 2020 2020 7461 6275  ummary..    tabu
-00012d60: 6c61 7465 5f66 6e20 3d20 7375 6d6d 6172  late_fn = summar
-00012d70: 792e 7461 6275 6c61 7465 2873 656c 662c  y.tabulate(self,
-00012d80: 2072 6e67 732c 2064 6570 7468 3d64 6570   rngs, depth=dep
-00012d90: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
-00012da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012db0: 2020 2020 2020 2073 686f 775f 7265 7065         show_repe
-00012dc0: 6174 6564 3d73 686f 775f 7265 7065 6174  ated=show_repeat
-00012dd0: 6564 2c20 6d75 7461 626c 653d 6d75 7461  ed, mutable=muta
-00012de0: 626c 652c 0a20 2020 2020 2020 2020 2020  ble,.           
-00012df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012e00: 2020 2020 2020 2020 636f 6e73 6f6c 655f          console_
-00012e10: 6b77 6172 6773 3d63 6f6e 736f 6c65 5f6b  kwargs=console_k
-00012e20: 7761 7267 7329 0a20 2020 2072 6574 7572  wargs).    retur
-00012e30: 6e20 7461 6275 6c61 7465 5f66 6e28 2a61  n tabulate_fn(*a
-00012e40: 7267 732c 202a 2a6b 7761 7267 7329 0a0a  rgs, **kwargs)..
-00012e50: 0a5f 5061 7265 6e74 5479 7065 203d 2055  ._ParentType = U
-00012e60: 6e69 6f6e 5b54 7970 655b 4d6f 6475 6c65  nion[Type[Module
-00012e70: 5d2c 2054 7970 655b 5363 6f70 655d 2c20  ], Type[Scope], 
-00012e80: 5479 7065 5b5f 5365 6e74 696e 656c 5d2c  Type[_Sentinel],
-00012e90: 204e 6f6e 655d 0a0a 6465 6620 6d65 7267   None]..def merg
-00012ea0: 655f 7061 7261 6d28 6e61 6d65 3a20 7374  e_param(name: st
-00012eb0: 722c 2061 3a20 4f70 7469 6f6e 616c 5b54  r, a: Optional[T
-00012ec0: 5d2c 2062 3a20 4f70 7469 6f6e 616c 5b54  ], b: Optional[T
-00012ed0: 5d29 202d 3e20 543a 0a20 2022 2222 4d65  ]) -> T:.  """Me
-00012ee0: 7267 6573 2063 6f6e 7374 7275 6374 696f  rges constructio
-00012ef0: 6e2d 2061 6e64 2063 616c 6c2d 7469 6d65  n- and call-time
-00012f00: 2061 7267 756d 656e 742e 0a0a 2020 5468   argument...  Th
-00012f10: 6973 2069 7320 6120 7574 696c 6974 7920  is is a utility 
-00012f20: 666f 7220 7375 7070 6f72 7469 6e67 2061  for supporting a
-00012f30: 2070 6174 7465 726e 2077 6865 7265 2061   pattern where a
-00012f40: 204d 6f64 756c 6520 6879 7065 7270 6172   Module hyperpar
-00012f50: 616d 6574 6572 0a20 2063 616e 2062 6520  ameter.  can be 
-00012f60: 7061 7373 6564 2065 6974 6865 7220 746f  passed either to
-00012f70: 2060 605f 5f69 6e69 745f 5f60 6020 6f72   ``__init__`` or
-00012f80: 2060 605f 5f63 616c 6c5f 5f60 602c 2061   ``__call__``, a
-00012f90: 6e64 2074 6865 2076 616c 7565 2074 6861  nd the value tha
-00012fa0: 7420 6973 0a20 206e 6f74 2060 4e6f 6e65  t is.  not `None
-00012fb0: 6020 7769 6c6c 2062 6520 7573 6564 2e0a  ` will be used..
-00012fc0: 0a20 2045 7861 6d70 6c65 3a3a 0a0a 2020  .  Example::..  
-00012fd0: 2020 636c 6173 7320 466f 6f28 6e6e 2e4d    class Foo(nn.M
-00012fe0: 6f64 756c 6529 3a0a 2020 2020 2020 7472  odule):.      tr
-00012ff0: 6169 6e3a 204f 7074 696f 6e61 6c5b 626f  ain: Optional[bo
-00013000: 6f6c 5d20 3d20 4e6f 6e65 0a0a 2020 2020  ol] = None..    
-00013010: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-00013020: 656c 662c 2074 7261 696e 3a20 4f70 7469  elf, train: Opti
-00013030: 6f6e 616c 5b62 6f6f 6c5d 203d 204e 6f6e  onal[bool] = Non
-00013040: 6529 3a0a 2020 2020 2020 2020 7472 6169  e):.        trai
-00013050: 6e20 3d20 6e6e 2e6d 6572 6765 5f70 6172  n = nn.merge_par
-00013060: 616d 2827 7472 6169 6e27 2c20 7365 6c66  am('train', self
-00013070: 2e74 7261 696e 2c20 7472 6169 6e29 0a0a  .train, train)..
-00013080: 2020 416e 2065 7272 6f72 2069 7320 7468    An error is th
-00013090: 726f 776e 2077 6865 6e20 626f 7468 2061  rown when both a
-000130a0: 7267 756d 656e 7473 2061 7265 2060 4e6f  rguments are `No
-000130b0: 6e65 6020 6f72 2062 6f74 6820 7661 6c75  ne` or both valu
-000130c0: 6573 2061 7265 206e 6f74 0a20 2060 4e6f  es are not.  `No
-000130d0: 6e65 602e 0a0a 2020 4172 6773 3a0a 2020  ne`...  Args:.  
-000130e0: 2020 6e61 6d65 3a20 7468 6520 6e61 6d65    name: the name
-000130f0: 206f 6620 7468 6520 7061 7261 6d65 7465   of the paramete
-00013100: 722e 2055 7365 6420 666f 7220 6572 726f  r. Used for erro
-00013110: 7220 6d65 7373 6167 6573 2e0a 2020 2020  r messages..    
-00013120: 613a 206f 7074 696f 6e20 610a 2020 2020  a: option a.    
-00013130: 623a 206f 7074 696f 6e20 620a 2020 5265  b: option b.  Re
-00013140: 7475 726e 733a 0a20 2020 2061 206f 7220  turns:.    a or 
-00013150: 6220 7768 6963 6865 7665 7220 6973 206e  b whichever is n
-00013160: 6f74 2060 4e6f 6e65 602e 0a0a 2020 2222  ot `None`...  ""
-00013170: 220a 2020 6966 2061 2069 7320 4e6f 6e65  ".  if a is None
-00013180: 2061 6e64 2062 2069 7320 4e6f 6e65 3a0a   and b is None:.
-00013190: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-000131a0: 7272 6f72 2866 2750 6172 616d 6574 6572  rror(f'Parameter
-000131b0: 2022 7b6e 616d 657d 2220 6d75 7374 2062   "{name}" must b
-000131c0: 6520 7061 7373 6564 2074 6f20 7468 6520  e passed to the 
-000131d0: 636f 6e73 7472 7563 746f 7220 6f72 2061  constructor or a
-000131e0: 7420 6361 6c6c 2074 696d 652e 2729 0a20  t call time.'). 
-000131f0: 2069 6620 6120 6973 206e 6f74 204e 6f6e   if a is not Non
-00013200: 6520 616e 6420 6220 6973 206e 6f74 204e  e and b is not N
-00013210: 6f6e 653a 0a20 2020 2072 6169 7365 2056  one:.    raise V
-00013220: 616c 7565 4572 726f 7228 6627 5061 7261  alueError(f'Para
-00013230: 6d65 7465 7220 227b 6e61 6d65 7d22 2077  meter "{name}" w
-00013240: 6173 2070 6173 7365 6420 746f 2074 6865  as passed to the
-00013250: 2063 6f6e 7374 7275 6374 6f72 2061 6e64   constructor and
-00013260: 2061 7420 6361 6c6c 2074 696d 652e 270a   at call time.'.
-00013270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013280: 2020 2020 2027 2053 686f 756c 6420 6265       ' Should be
-00013290: 2070 6173 7365 6420 6a75 7374 206f 6e63   passed just onc
-000132a0: 652e 2729 0a20 2069 6620 6120 6973 204e  e.').  if a is N
-000132b0: 6f6e 653a 0a20 2020 2061 7373 6572 7420  one:.    assert 
-000132c0: 6220 6973 206e 6f74 204e 6f6e 650a 2020  b is not None.  
-000132d0: 2020 7265 7475 726e 2062 0a20 2072 6574    return b.  ret
-000132e0: 7572 6e20 610a 0a0a 4074 7261 6365 6261  urn a...@traceba
-000132f0: 636b 5f75 7469 6c2e 6170 695f 626f 756e  ck_util.api_boun
-00013300: 6461 7279 0a64 6566 2061 7070 6c79 2866  dary.def apply(f
-00013310: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
-00013320: 2041 6e79 5d2c 206d 6f64 756c 653a 204d   Any], module: M
-00013330: 6f64 756c 652c 0a20 2020 2020 2020 2020  odule,.         
-00013340: 206d 7574 6162 6c65 3a20 436f 6c6c 6563   mutable: Collec
-00013350: 7469 6f6e 4669 6c74 6572 203d 2046 616c  tionFilter = Fal
-00013360: 7365 2c0a 2020 2020 2020 2020 2020 6361  se,.          ca
-00013370: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-00013380: 7465 733a 2055 6e69 6f6e 5b62 6f6f 6c2c  tes: Union[bool,
-00013390: 2043 616c 6c61 626c 655b 5b4d 6f64 756c   Callable[[Modul
-000133a0: 652c 2073 7472 5d2c 2062 6f6f 6c5d 5d20  e, str], bool]] 
-000133b0: 3d20 4661 6c73 652c 0a20 2020 2020 2020  = False,.       
-000133c0: 2020 2029 202d 3e20 4361 6c6c 6162 6c65     ) -> Callable
-000133d0: 5b2e 2e2e 2c20 416e 795d 3a0a 2020 2222  [..., Any]:.  ""
-000133e0: 2243 7265 6174 6573 2061 6e20 6170 706c  "Creates an appl
-000133f0: 7920 6675 6e63 7469 6f6e 2074 6f20 6361  y function to ca
-00013400: 6c6c 2060 6066 6e60 6020 7769 7468 2061  ll ``fn`` with a
-00013410: 2062 6f75 6e64 206d 6f64 756c 652e 0a0a   bound module...
-00013420: 2020 556e 6c69 6b65 2060 604d 6f64 756c    Unlike ``Modul
-00013430: 652e 6170 706c 7960 6020 7468 6973 2066  e.apply`` this f
-00013440: 756e 6374 696f 6e20 7265 7475 726e 7320  unction returns 
-00013450: 6120 6e65 7720 6675 6e63 7469 6f6e 2077  a new function w
-00013460: 6974 6820 7468 6520 7369 676e 6174 7572  ith the signatur
-00013470: 650a 2020 6060 2876 6172 6961 626c 6573  e.  ``(variables
-00013480: 2c20 2a61 7267 732c 2072 6e67 733d 4e6f  , *args, rngs=No
-00013490: 6e65 2c20 2a2a 6b77 6172 6773 2920 2d3e  ne, **kwargs) ->
-000134a0: 2054 6060 2077 6865 7265 2060 5460 2069   T`` where `T` i
-000134b0: 7320 7468 6520 7265 7475 726e 2074 7970  s the return typ
-000134c0: 650a 2020 6f66 2060 6066 6e60 602e 2049  e.  of ``fn``. I
-000134d0: 6620 6060 6d75 7461 626c 6560 6020 6973  f ``mutable`` is
-000134e0: 206e 6f74 2060 6046 616c 7365 6060 2074   not ``False`` t
-000134f0: 6865 2072 6574 7572 6e20 7479 7065 2069  he return type i
-00013500: 7320 6120 7475 706c 6520 7768 6572 6520  s a tuple where 
-00013510: 7468 650a 2020 7365 636f 6e64 2069 7465  the.  second ite
-00013520: 6d20 6973 2061 2060 6046 726f 7a65 6e44  m is a ``FrozenD
-00013530: 6963 7460 6020 7769 7468 2074 6865 206d  ict`` with the m
-00013540: 7574 6174 6564 2076 6172 6961 626c 6573  utated variables
-00013550: 2e0a 0a20 2054 6865 2061 7070 6c79 2066  ...  The apply f
-00013560: 756e 6374 696f 6e20 7468 6174 2069 7320  unction that is 
-00013570: 7265 7475 726e 6564 2063 616e 2062 6520  returned can be 
-00013580: 6469 7265 6374 6c79 2063 6f6d 706f 7365  directly compose
-00013590: 6420 7769 7468 0a20 204a 4158 2074 7261  d with.  JAX tra
-000135a0: 6e73 666f 726d 6174 696f 6e73 206c 696b  nsformations lik
-000135b0: 6520 6060 6a61 782e 6a69 7460 603a 3a0a  e ``jax.jit``::.
-000135c0: 0a20 2020 2064 6566 2066 2866 6f6f 2c20  .    def f(foo, 
-000135d0: 7829 3a0a 2020 2020 2020 7a20 3d20 666f  x):.      z = fo
-000135e0: 6f2e 656e 636f 6465 2878 290a 2020 2020  o.encode(x).    
-000135f0: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
-00013600: 287a 290a 2020 2020 2020 2320 2e2e 2e0a  (z).      # ....
-00013610: 2020 2020 2020 7265 7475 726e 2079 0a0a        return y..
-00013620: 2020 2020 666f 6f20 3d20 466f 6f28 290a      foo = Foo().
-00013630: 2020 2020 665f 6a69 7474 6564 203d 206a      f_jitted = j
-00013640: 6178 2e6a 6974 286e 6e2e 6170 706c 7928  ax.jit(nn.apply(
-00013650: 662c 2066 6f6f 2929 0a20 2020 2066 5f6a  f, foo)).    f_j
-00013660: 6974 7465 6428 7661 7269 6162 6c65 732c  itted(variables,
-00013670: 2078 290a 0a20 2041 7267 733a 0a20 2020   x)..  Args:.   
-00013680: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
-00013690: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
-000136a0: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
-000136b0: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
-000136c0: 7365 6420 7769 6c6c 0a20 2020 2020 2062  sed will.      b
-000136d0: 6520 616e 206d 6f64 756c 6520 696e 7374  e an module inst
-000136e0: 616e 6365 206f 6620 7468 6520 6060 6d6f  ance of the ``mo
-000136f0: 6475 6c65 6060 2077 6974 6820 7661 7269  dule`` with vari
-00013700: 6162 6c65 7320 616e 6420 524e 4773 2062  ables and RNGs b
-00013710: 6f75 6e64 0a20 2020 2020 2074 6f20 6974  ound.      to it
-00013720: 2e0a 2020 2020 6d6f 6475 6c65 3a20 5468  ..    module: Th
-00013730: 6520 6060 4d6f 6475 6c65 6060 2074 6861  e ``Module`` tha
-00013740: 7420 7769 6c6c 2062 6520 7573 6564 2074  t will be used t
-00013750: 6f20 6269 6e64 2076 6172 6961 626c 6573  o bind variables
-00013760: 2061 6e64 2052 4e47 7320 746f 2e0a 2020   and RNGs to..  
-00013770: 2020 2020 5468 6520 6060 4d6f 6475 6c65      The ``Module
-00013780: 6060 2070 6173 7365 6420 6173 2074 6865  `` passed as the
-00013790: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
-000137a0: 746f 2060 6066 6e60 6020 7769 6c6c 2062  to ``fn`` will b
-000137b0: 6520 6120 636c 6f6e 650a 2020 2020 2020  e a clone.      
-000137c0: 6f66 206d 6f64 756c 652e 0a20 2020 206d  of module..    m
-000137d0: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
-000137e0: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
-000137f0: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
-00013800: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
-00013810: 686f 756c 6420 6265 0a20 2020 2020 2074  hould be.      t
-00013820: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
-00013830: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
-00013840: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
-00013850: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
-00013860: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
-00013870: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
-00013880: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
-00013890: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
-000138a0: 0a20 2020 2020 206c 6973 7420 6f66 206e  .      list of n
-000138b0: 616d 6573 206f 6620 6d75 7461 626c 6520  ames of mutable 
-000138c0: 636f 6c6c 6563 7469 6f6e 732e 0a20 2020  collections..   
-000138d0: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-000138e0: 6469 6174 6573 3a20 4966 2060 5472 7565  diates: If `True
-000138f0: 602c 2063 6170 7475 7265 7320 696e 7465  `, captures inte
-00013900: 726d 6564 6961 7465 2072 6574 7572 6e20  rmediate return 
-00013910: 7661 6c75 6573 0a20 2020 2020 206f 6620  values.      of 
-00013920: 616c 6c20 4d6f 6475 6c65 7320 696e 7369  all Modules insi
-00013930: 6465 2074 6865 2022 696e 7465 726d 6564  de the "intermed
-00013940: 6961 7465 7322 2063 6f6c 6c65 6374 696f  iates" collectio
-00013950: 6e2e 2042 7920 6465 6661 756c 7420 6f6e  n. By default on
-00013960: 6c79 0a20 2020 2020 2074 6865 2072 6574  ly.      the ret
-00013970: 7572 6e20 7661 6c75 6573 206f 6620 616c  urn values of al
-00013980: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
-00013990: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
-000139a0: 2041 2066 756e 6374 696f 6e20 6361 6e0a   A function can.
-000139b0: 2020 2020 2020 6265 2070 6173 7365 6420        be passed 
-000139c0: 746f 2063 6861 6e67 6520 7468 6520 6669  to change the fi
-000139d0: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
-000139e0: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
-000139f0: 6f6e 2074 616b 6573 0a20 2020 2020 2074  on takes.      t
-00013a00: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
-00013a10: 6365 2061 6e64 206d 6574 686f 6420 6e61  ce and method na
-00013a20: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
-00013a30: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
-00013a40: 0a20 2020 2020 2077 6865 7468 6572 2074  .      whether t
-00013a50: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
-00013a60: 7420 6d65 7468 6f64 2069 6e76 6f63 6174  t method invocat
-00013a70: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
-00013a80: 6f72 6564 2e0a 2020 5265 7475 726e 733a  ored..  Returns:
-00013a90: 0a20 2020 2054 6865 2061 7070 6c79 2066  .    The apply f
-00013aa0: 756e 6374 696f 6e20 7772 6170 7069 6e67  unction wrapping
-00013ab0: 2060 6066 6e60 602e 0a20 2022 2222 0a20   ``fn``..  """. 
-00013ac0: 2040 6675 6e63 746f 6f6c 732e 7772 6170   @functools.wrap
-00013ad0: 7328 666e 290a 2020 6465 6620 7363 6f70  s(fn).  def scop
-00013ae0: 655f 666e 2873 636f 7065 2c20 2a61 7267  e_fn(scope, *arg
-00013af0: 732c 202a 2a6b 7761 7267 7329 3a0a 2020  s, **kwargs):.  
-00013b00: 2020 5f63 6f6e 7465 7874 2e63 6170 7475    _context.captu
-00013b10: 7265 5f73 7461 636b 2e61 7070 656e 6428  re_stack.append(
-00013b20: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00013b30: 6961 7465 7329 0a20 2020 2074 7279 3a0a  iates).    try:.
-00013b40: 2020 2020 2020 7265 7475 726e 2066 6e28        return fn(
-00013b50: 6d6f 6475 6c65 2e63 6c6f 6e65 2870 6172  module.clone(par
-00013b60: 656e 743d 7363 6f70 6529 2c20 2a61 7267  ent=scope), *arg
-00013b70: 732c 202a 2a6b 7761 7267 7329 0a20 2020  s, **kwargs).   
-00013b80: 2066 696e 616c 6c79 3a0a 2020 2020 2020   finally:.      
-00013b90: 5f63 6f6e 7465 7874 2e63 6170 7475 7265  _context.capture
-00013ba0: 5f73 7461 636b 2e70 6f70 2829 0a0a 2020  _stack.pop()..  
-00013bb0: 6966 2063 6170 7475 7265 5f69 6e74 6572  if capture_inter
-00013bc0: 6d65 6469 6174 6573 2069 7320 5472 7565  mediates is True
-00013bd0: 3a20 2023 2070 796c 696e 743a 2064 6973  :  # pylint: dis
-00013be0: 6162 6c65 3d67 2d62 6f6f 6c2d 6964 2d63  able=g-bool-id-c
-00013bf0: 6f6d 7061 7269 736f 6e0a 2020 2020 6361  omparison.    ca
-00013c00: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-00013c10: 7465 7320 3d20 6361 7074 7572 655f 6361  tes = capture_ca
-00013c20: 6c6c 5f69 6e74 6572 6d65 6469 6174 6573  ll_intermediates
-00013c30: 0a20 2069 6620 6361 7074 7572 655f 696e  .  if capture_in
-00013c40: 7465 726d 6564 6961 7465 733a 0a20 2020  termediates:.   
-00013c50: 206d 7574 6162 6c65 203d 2075 6e69 6f6e   mutable = union
-00013c60: 5f66 696c 7465 7273 286d 7574 6162 6c65  _filters(mutable
-00013c70: 2c20 2769 6e74 6572 6d65 6469 6174 6573  , 'intermediates
-00013c80: 2729 0a20 2072 6574 7572 6e20 636f 7265  ').  return core
-00013c90: 2e61 7070 6c79 2873 636f 7065 5f66 6e2c  .apply(scope_fn,
-00013ca0: 206d 7574 6162 6c65 3d6d 7574 6162 6c65   mutable=mutable
-00013cb0: 290a 0a0a 4074 7261 6365 6261 636b 5f75  )...@traceback_u
-00013cc0: 7469 6c2e 6170 695f 626f 756e 6461 7279  til.api_boundary
-00013cd0: 0a64 6566 2069 6e69 745f 7769 7468 5f6f  .def init_with_o
-00013ce0: 7574 7075 7428 666e 3a20 4361 6c6c 6162  utput(fn: Callab
-00013cf0: 6c65 5b2e 2e2e 2c20 416e 795d 2c20 6d6f  le[..., Any], mo
-00013d00: 6475 6c65 3a20 4d6f 6475 6c65 2c0a 2020  dule: Module,.  
-00013d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013d20: 2020 206d 7574 6162 6c65 3a20 436f 6c6c     mutable: Coll
-00013d30: 6563 7469 6f6e 4669 6c74 6572 203d 2044  ectionFilter = D
-00013d40: 656e 794c 6973 7428 2769 6e74 6572 6d65  enyList('interme
-00013d50: 6469 6174 6573 2729 2c0a 2020 2020 2020  diates'),.      
-00013d60: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-00013d70: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
-00013d80: 6174 6573 3a20 556e 696f 6e5b 626f 6f6c  ates: Union[bool
-00013d90: 2c20 4361 6c6c 6162 6c65 5b5b 4d6f 6475  , Callable[[Modu
-00013da0: 6c65 2c20 7374 725d 2c20 626f 6f6c 5d5d  le, str], bool]]
-00013db0: 203d 2046 616c 7365 2c0a 2020 2020 2020   = False,.      
-00013dc0: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-00013dd0: 202d 3e20 4361 6c6c 6162 6c65 5b2e 2e2e   -> Callable[...
-00013de0: 2c20 5475 706c 655b 416e 792c 2055 6e69  , Tuple[Any, Uni
-00013df0: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
-00013e00: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
-00013e10: 2041 6e79 5d5d 5d5d 3a0a 2020 2222 2243   Any]]]]:.  """C
-00013e20: 7265 6174 6573 2061 6e20 696e 6974 2066  reates an init f
-00013e30: 756e 6374 696f 6e20 746f 2063 616c 6c20  unction to call 
-00013e40: 6060 666e 6060 2077 6974 6820 6120 626f  ``fn`` with a bo
-00013e50: 756e 6420 6d6f 6475 6c65 2074 6861 7420  und module that 
-00013e60: 616c 736f 2072 6574 7572 6e73 2074 6865  also returns the
-00013e70: 2066 756e 6374 696f 6e20 6f75 7470 7574   function output
-00013e80: 732e 0a0a 2020 556e 6c69 6b65 2060 604d  s...  Unlike ``M
-00013e90: 6f64 756c 652e 696e 6974 5f77 6974 685f  odule.init_with_
-00013ea0: 6f75 7470 7574 6060 2074 6869 7320 6675  output`` this fu
-00013eb0: 6e63 7469 6f6e 2072 6574 7572 6e73 2061  nction returns a
-00013ec0: 206e 6577 2066 756e 6374 696f 6e20 7769   new function wi
-00013ed0: 7468 2074 6865 2073 6967 6e61 7475 7265  th the signature
-00013ee0: 0a20 2060 6028 726e 6773 2c20 2a61 7267  .  ``(rngs, *arg
-00013ef0: 732c 202a 2a6b 7761 7267 7329 202d 3e20  s, **kwargs) -> 
-00013f00: 2854 2c20 7661 7269 6162 6c65 7329 6060  (T, variables)``
-00013f10: 2077 6865 7265 2060 5460 2069 7320 7468   where `T` is th
-00013f20: 6520 7265 7475 726e 2074 7970 6520 6f66  e return type of
-00013f30: 2060 6066 6e60 602e 0a20 2054 6865 2072   ``fn``..  The r
-00013f40: 6e67 7320 6361 6e20 6265 2061 2064 6963  ngs can be a dic
-00013f50: 7420 6f66 2050 524e 474b 6579 7320 6f72  t of PRNGKeys or
-00013f60: 2061 2073 696e 676c 6520 6060 6050 524e   a single ```PRN
-00013f70: 474b 6579 6060 2077 6869 6368 2069 730a  GKey`` which is.
-00013f80: 2020 6571 7569 7661 6c65 6e74 2074 6f20    equivalent to 
-00013f90: 7061 7373 696e 6720 6120 6469 6374 2077  passing a dict w
-00013fa0: 6974 6820 6f6e 6520 5052 4e47 4b65 7920  ith one PRNGKey 
-00013fb0: 7769 7468 2074 6865 206e 616d 6520 2270  with the name "p
-00013fc0: 6172 616d 7322 2e0a 0a20 2054 6865 2069  arams"...  The i
-00013fd0: 6e69 7420 6675 6e63 7469 6f6e 2074 6861  nit function tha
-00013fe0: 7420 6973 2072 6574 7572 6e65 6420 6361  t is returned ca
-00013ff0: 6e20 6265 2064 6972 6563 746c 7920 636f  n be directly co
-00014000: 6d70 6f73 6564 2077 6974 680a 2020 4a41  mposed with.  JA
-00014010: 5820 7472 616e 7366 6f72 6d61 7469 6f6e  X transformation
-00014020: 7320 6c69 6b65 2060 606a 6178 2e6a 6974  s like ``jax.jit
-00014030: 6060 3a3a 0a0a 2020 2020 6465 6620 6628  ``::..    def f(
-00014040: 666f 6f2c 2078 293a 0a20 2020 2020 207a  foo, x):.      z
-00014050: 203d 2066 6f6f 2e65 6e63 6f64 6528 7829   = foo.encode(x)
-00014060: 0a20 2020 2020 2079 203d 2066 6f6f 2e64  .      y = foo.d
-00014070: 6563 6f64 6528 7a29 0a20 2020 2020 2023  ecode(z).      #
-00014080: 202e 2e2e 0a20 2020 2020 2072 6574 7572   ....      retur
-00014090: 6e20 790a 0a20 2020 2066 6f6f 203d 2046  n y..    foo = F
-000140a0: 6f6f 2829 0a20 2020 2066 5f6a 6974 7465  oo().    f_jitte
-000140b0: 6420 3d20 6a61 782e 6a69 7428 6e6e 2e69  d = jax.jit(nn.i
-000140c0: 6e69 745f 7769 7468 5f6f 7574 7075 7428  nit_with_output(
-000140d0: 662c 2066 6f6f 2929 0a20 2020 2079 2c20  f, foo)).    y, 
-000140e0: 7661 7269 6162 6c65 7320 3d20 665f 6a69  variables = f_ji
-000140f0: 7474 6564 2872 6e67 2c20 7829 0a0a 2020  tted(rng, x)..  
-00014100: 4172 6773 3a0a 2020 2020 666e 3a20 5468  Args:.    fn: Th
-00014110: 6520 6675 6e63 7469 6f6e 2074 6861 7420  e function that 
-00014120: 7368 6f75 6c64 2062 6520 6170 706c 6965  should be applie
-00014130: 642e 2054 6865 2066 6972 7374 2061 7267  d. The first arg
-00014140: 756d 656e 7420 7061 7373 6564 2077 696c  ument passed wil
-00014150: 6c0a 2020 2020 2020 6265 2061 6e20 6d6f  l.      be an mo
-00014160: 6475 6c65 2069 6e73 7461 6e63 6520 6f66  dule instance of
-00014170: 2074 6865 2060 606d 6f64 756c 6560 6020   the ``module`` 
-00014180: 7769 7468 2076 6172 6961 626c 6573 2061  with variables a
-00014190: 6e64 2052 4e47 7320 626f 756e 640a 2020  nd RNGs bound.  
-000141a0: 2020 2020 746f 2069 742e 0a20 2020 206d      to it..    m
-000141b0: 6f64 756c 653a 2054 6865 2060 604d 6f64  odule: The ``Mod
-000141c0: 756c 6560 6020 7468 6174 2077 696c 6c20  ule`` that will 
-000141d0: 6265 2075 7365 6420 746f 2062 696e 6420  be used to bind 
-000141e0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
-000141f0: 4773 2074 6f2e 0a20 2020 2020 2054 6865  Gs to..      The
-00014200: 2060 604d 6f64 756c 6560 6020 7061 7373   ``Module`` pass
-00014210: 6564 2061 7320 7468 6520 6669 7273 7420  ed as the first 
-00014220: 6172 6775 6d65 6e74 2074 6f20 6060 666e  argument to ``fn
-00014230: 6060 2077 696c 6c20 6265 2061 2063 6c6f  `` will be a clo
-00014240: 6e65 0a20 2020 2020 206f 6620 6d6f 6475  ne.      of modu
-00014250: 6c65 2e0a 2020 2020 6d75 7461 626c 653a  le..    mutable:
-00014260: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
-00014270: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
-00014280: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
-00014290: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
-000142a0: 650a 2020 2020 2020 7472 6561 7465 6420  e.      treated 
-000142b0: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
-000142c0: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
-000142d0: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
-000142e0: 6162 6c65 2e0a 2020 2020 2020 6060 7374  able..      ``st
-000142f0: 7260 603a 2054 6865 206e 616d 6520 6f66  r``: The name of
-00014300: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
-00014310: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
-00014320: 6c69 7374 6060 3a20 410a 2020 2020 2020  list``: A.      
-00014330: 6c69 7374 206f 6620 6e61 6d65 7320 6f66  list of names of
-00014340: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
-00014350: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
-00014360: 2061 6c6c 2063 6f6c 6c65 6374 696f 6e73   all collections
-00014370: 0a20 2020 2020 2065 7863 6570 7420 2269  .      except "i
-00014380: 6e74 6572 6d65 6469 6174 6573 2220 6172  ntermediates" ar
-00014390: 6520 6d75 7461 626c 652e 0a20 2020 2063  e mutable..    c
-000143a0: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
-000143b0: 6174 6573 3a20 4966 2060 5472 7565 602c  ates: If `True`,
-000143c0: 2063 6170 7475 7265 7320 696e 7465 726d   captures interm
-000143d0: 6564 6961 7465 2072 6574 7572 6e20 7661  ediate return va
-000143e0: 6c75 6573 0a20 2020 2020 206f 6620 616c  lues.      of al
-000143f0: 6c20 4d6f 6475 6c65 7320 696e 7369 6465  l Modules inside
-00014400: 2074 6865 2022 696e 7465 726d 6564 6961   the "intermedia
-00014410: 7465 7322 2063 6f6c 6c65 6374 696f 6e2e  tes" collection.
-00014420: 2042 7920 6465 6661 756c 7420 6f6e 6c79   By default only
-00014430: 0a20 2020 2020 2074 6865 2072 6574 7572  .      the retur
-00014440: 6e20 7661 6c75 6573 206f 6620 616c 6c20  n values of all 
-00014450: 605f 5f63 616c 6c5f 5f60 206d 6574 686f  `__call__` metho
-00014460: 6473 2061 7265 2073 746f 7265 642e 2041  ds are stored. A
-00014470: 2066 756e 6374 696f 6e20 6361 6e0a 2020   function can.  
-00014480: 2020 2020 6265 2070 6173 7365 6420 746f      be passed to
-00014490: 2063 6861 6e67 6520 7468 6520 6669 6c74   change the filt
-000144a0: 6572 2062 6568 6176 696f 722e 2054 6865  er behavior. The
-000144b0: 2066 696c 7465 7220 6675 6e63 7469 6f6e   filter function
-000144c0: 2074 616b 6573 0a20 2020 2020 2074 6865   takes.      the
-000144d0: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
-000144e0: 2061 6e64 206d 6574 686f 6420 6e61 6d65   and method name
-000144f0: 2061 6e64 2072 6574 7572 6e73 2061 2062   and returns a b
-00014500: 6f6f 6c20 696e 6469 6361 7469 6e67 0a20  ool indicating. 
-00014510: 2020 2020 2077 6865 7468 6572 2074 6865       whether the
-00014520: 206f 7574 7075 7420 6f66 2074 6861 7420   output of that 
-00014530: 6d65 7468 6f64 2069 6e76 6f63 6174 696f  method invocatio
-00014540: 6e20 7368 6f75 6c64 2062 6520 7374 6f72  n should be stor
-00014550: 6564 2e0a 2020 5265 7475 726e 733a 0a20  ed..  Returns:. 
-00014560: 2020 2054 6865 2069 6e69 7420 6675 6e63     The init func
-00014570: 7469 6f6e 2077 7261 7070 696e 6720 6060  tion wrapping ``
-00014580: 666e 6060 2e0a 2020 2222 220a 2020 4066  fn``..  """.  @f
-00014590: 756e 6374 6f6f 6c73 2e77 7261 7073 2866  unctools.wraps(f
-000145a0: 6e29 0a20 2064 6566 2073 636f 7065 5f66  n).  def scope_f
-000145b0: 6e28 7363 6f70 652c 202a 6172 6773 2c20  n(scope, *args, 
-000145c0: 2a2a 6b77 6172 6773 293a 0a20 2020 205f  **kwargs):.    _
-000145d0: 636f 6e74 6578 742e 6361 7074 7572 655f  context.capture_
-000145e0: 7374 6163 6b2e 6170 7065 6e64 2863 6170  stack.append(cap
-000145f0: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00014600: 6573 290a 2020 2020 7472 793a 0a20 2020  es).    try:.   
-00014610: 2020 2072 6574 7572 6e20 666e 286d 6f64     return fn(mod
-00014620: 756c 652e 636c 6f6e 6528 7061 7265 6e74  ule.clone(parent
-00014630: 3d73 636f 7065 292c 202a 6172 6773 2c20  =scope), *args, 
-00014640: 2a2a 6b77 6172 6773 290a 2020 2020 6669  **kwargs).    fi
-00014650: 6e61 6c6c 793a 0a20 2020 2020 205f 636f  nally:.      _co
-00014660: 6e74 6578 742e 6361 7074 7572 655f 7374  ntext.capture_st
-00014670: 6163 6b2e 706f 7028 290a 0a20 2069 6620  ack.pop()..  if 
-00014680: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00014690: 6961 7465 7320 6973 2054 7275 653a 2020  iates is True:  
-000146a0: 2320 7079 6c69 6e74 3a20 6469 7361 626c  # pylint: disabl
-000146b0: 653d 672d 626f 6f6c 2d69 642d 636f 6d70  e=g-bool-id-comp
-000146c0: 6172 6973 6f6e 0a20 2020 2063 6170 7475  arison.    captu
-000146d0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
-000146e0: 203d 2063 6170 7475 7265 5f63 616c 6c5f   = capture_call_
-000146f0: 696e 7465 726d 6564 6961 7465 730a 2020  intermediates.  
-00014700: 6966 2063 6170 7475 7265 5f69 6e74 6572  if capture_inter
-00014710: 6d65 6469 6174 6573 3a0a 2020 2020 6d75  mediates:.    mu
-00014720: 7461 626c 6520 3d20 756e 696f 6e5f 6669  table = union_fi
-00014730: 6c74 6572 7328 6d75 7461 626c 652c 2027  lters(mutable, '
-00014740: 696e 7465 726d 6564 6961 7465 7327 290a  intermediates').
-00014750: 2020 7265 7475 726e 2063 6f72 652e 696e    return core.in
-00014760: 6974 2873 636f 7065 5f66 6e2c 206d 7574  it(scope_fn, mut
-00014770: 6162 6c65 3d6d 7574 6162 6c65 290a 0a0a  able=mutable)...
-00014780: 4074 7261 6365 6261 636b 5f75 7469 6c2e  @traceback_util.
-00014790: 6170 695f 626f 756e 6461 7279 0a64 6566  api_boundary.def
-000147a0: 2069 6e69 7428 666e 3a20 4361 6c6c 6162   init(fn: Callab
-000147b0: 6c65 5b2e 2e2e 2c20 416e 795d 2c20 6d6f  le[..., Any], mo
-000147c0: 6475 6c65 3a20 4d6f 6475 6c65 2c0a 2020  dule: Module,.  
-000147d0: 2020 2020 2020 206d 7574 6162 6c65 3a20         mutable: 
-000147e0: 436f 6c6c 6563 7469 6f6e 4669 6c74 6572  CollectionFilter
-000147f0: 203d 2044 656e 794c 6973 7428 2769 6e74   = DenyList('int
-00014800: 6572 6d65 6469 6174 6573 2729 2c0a 2020  ermediates'),.  
-00014810: 2020 2020 2020 2063 6170 7475 7265 5f69         capture_i
-00014820: 6e74 6572 6d65 6469 6174 6573 3a20 556e  ntermediates: Un
-00014830: 696f 6e5b 626f 6f6c 2c20 4361 6c6c 6162  ion[bool, Callab
-00014840: 6c65 5b5b 4d6f 6475 6c65 2c20 7374 725d  le[[Module, str]
-00014850: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
-00014860: 2c0a 2020 2020 2020 2020 2029 202d 3e20  ,.         ) -> 
-00014870: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 556e  Callable[..., Un
-00014880: 696f 6e5b 4672 6f7a 656e 5661 7269 6162  ion[FrozenVariab
-00014890: 6c65 4469 6374 2c20 4469 6374 5b73 7472  leDict, Dict[str
-000148a0: 2c20 416e 795d 5d5d 3a0a 2020 2222 2243  , Any]]]:.  """C
-000148b0: 7265 6174 6573 2061 6e20 696e 6974 2066  reates an init f
-000148c0: 756e 6374 696f 6e20 746f 2063 616c 6c20  unction to call 
-000148d0: 6060 666e 6060 2077 6974 6820 6120 626f  ``fn`` with a bo
-000148e0: 756e 6420 6d6f 6475 6c65 2e0a 0a20 2055  und module...  U
-000148f0: 6e6c 696b 6520 6060 4d6f 6475 6c65 2e69  nlike ``Module.i
-00014900: 6e69 7460 6020 7468 6973 2066 756e 6374  nit`` this funct
-00014910: 696f 6e20 7265 7475 726e 7320 6120 6e65  ion returns a ne
-00014920: 7720 6675 6e63 7469 6f6e 2077 6974 6820  w function with 
-00014930: 7468 6520 7369 676e 6174 7572 650a 2020  the signature.  
-00014940: 6060 2872 6e67 732c 202a 6172 6773 2c20  ``(rngs, *args, 
-00014950: 2a2a 6b77 6172 6773 2920 2d3e 2076 6172  **kwargs) -> var
-00014960: 6961 626c 6573 6060 2e0a 2020 5468 6520  iables``..  The 
-00014970: 726e 6773 2063 616e 2062 6520 6120 6469  rngs can be a di
-00014980: 6374 206f 6620 5052 4e47 4b65 7973 206f  ct of PRNGKeys o
-00014990: 7220 6120 7369 6e67 6c65 2060 6060 5052  r a single ```PR
-000149a0: 4e47 4b65 7960 6020 7768 6963 6820 6973  NGKey`` which is
-000149b0: 0a20 2065 7175 6976 616c 656e 7420 746f  .  equivalent to
-000149c0: 2070 6173 7369 6e67 2061 2064 6963 7420   passing a dict 
-000149d0: 7769 7468 206f 6e65 2050 524e 474b 6579  with one PRNGKey
-000149e0: 2077 6974 6820 7468 6520 6e61 6d65 2022   with the name "
-000149f0: 7061 7261 6d73 222e 0a0a 2020 5468 6520  params"...  The 
-00014a00: 696e 6974 2066 756e 6374 696f 6e20 7468  init function th
-00014a10: 6174 2069 7320 7265 7475 726e 6564 2063  at is returned c
-00014a20: 616e 2062 6520 6469 7265 6374 6c79 2063  an be directly c
-00014a30: 6f6d 706f 7365 6420 7769 7468 0a20 204a  omposed with.  J
-00014a40: 4158 2074 7261 6e73 666f 726d 6174 696f  AX transformatio
-00014a50: 6e73 206c 696b 6520 6060 6a61 782e 6a69  ns like ``jax.ji
-00014a60: 7460 603a 3a0a 0a20 2020 2064 6566 2066  t``::..    def f
-00014a70: 2866 6f6f 2c20 7829 3a0a 2020 2020 2020  (foo, x):.      
-00014a80: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
-00014a90: 290a 2020 2020 2020 7920 3d20 666f 6f2e  ).      y = foo.
-00014aa0: 6465 636f 6465 287a 290a 2020 2020 2020  decode(z).      
-00014ab0: 2320 2e2e 2e0a 2020 2020 2020 7265 7475  # ....      retu
-00014ac0: 726e 2079 0a0a 2020 2020 666f 6f20 3d20  rn y..    foo = 
-00014ad0: 466f 6f28 290a 2020 2020 665f 6a69 7474  Foo().    f_jitt
-00014ae0: 6564 203d 206a 6178 2e6a 6974 286e 6e2e  ed = jax.jit(nn.
-00014af0: 696e 6974 2866 2c20 666f 6f29 290a 2020  init(f, foo)).  
-00014b00: 2020 7661 7269 6162 6c65 7320 3d20 665f    variables = f_
-00014b10: 6a69 7474 6564 2872 6e67 2c20 7829 0a0a  jitted(rng, x)..
-00014b20: 2020 4172 6773 3a0a 2020 2020 666e 3a20    Args:.    fn: 
-00014b30: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
-00014b40: 7420 7368 6f75 6c64 2062 6520 6170 706c  t should be appl
-00014b50: 6965 642e 2054 6865 2066 6972 7374 2061  ied. The first a
-00014b60: 7267 756d 656e 7420 7061 7373 6564 2077  rgument passed w
-00014b70: 696c 6c0a 2020 2020 2020 6265 2061 6e20  ill.      be an 
-00014b80: 6d6f 6475 6c65 2069 6e73 7461 6e63 6520  module instance 
-00014b90: 6f66 2074 6865 2060 606d 6f64 756c 6560  of the ``module`
-00014ba0: 6020 7769 7468 2076 6172 6961 626c 6573  ` with variables
-00014bb0: 2061 6e64 2052 4e47 7320 626f 756e 640a   and RNGs bound.
-00014bc0: 2020 2020 2020 746f 2069 742e 0a20 2020        to it..   
-00014bd0: 206d 6f64 756c 653a 2054 6865 2060 604d   module: The ``M
-00014be0: 6f64 756c 6560 6020 7468 6174 2077 696c  odule`` that wil
-00014bf0: 6c20 6265 2075 7365 6420 746f 2062 696e  l be used to bin
-00014c00: 6420 7661 7269 6162 6c65 7320 616e 6420  d variables and 
-00014c10: 524e 4773 2074 6f2e 0a20 2020 2020 2054  RNGs to..      T
-00014c20: 6865 2060 604d 6f64 756c 6560 6020 7061  he ``Module`` pa
-00014c30: 7373 6564 2061 7320 7468 6520 6669 7273  ssed as the firs
-00014c40: 7420 6172 6775 6d65 6e74 2074 6f20 6060  t argument to ``
-00014c50: 666e 6060 2077 696c 6c20 6265 2061 2063  fn`` will be a c
-00014c60: 6c6f 6e65 0a20 2020 2020 206f 6620 6d6f  lone.      of mo
-00014c70: 6475 6c65 2e0a 2020 2020 6d75 7461 626c  dule..    mutabl
-00014c80: 653a 2043 616e 2062 6520 626f 6f6c 2c20  e: Can be bool, 
-00014c90: 7374 722c 206f 7220 6c69 7374 2e20 5370  str, or list. Sp
-00014ca0: 6563 6966 6965 7320 7768 6963 6820 636f  ecifies which co
-00014cb0: 6c6c 6563 7469 6f6e 7320 7368 6f75 6c64  llections should
-00014cc0: 2062 650a 2020 2020 2020 7472 6561 7465   be.      treate
-00014cd0: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
-00014ce0: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
-00014cf0: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
-00014d00: 7574 6162 6c65 2e0a 2020 2020 2020 6060  utable..      ``
-00014d10: 7374 7260 603a 2054 6865 206e 616d 6520  str``: The name 
-00014d20: 6f66 2061 2073 696e 676c 6520 6d75 7461  of a single muta
-00014d30: 626c 6520 636f 6c6c 6563 7469 6f6e 2e20  ble collection. 
-00014d40: 6060 6c69 7374 6060 3a20 410a 2020 2020  ``list``: A.    
-00014d50: 2020 6c69 7374 206f 6620 6e61 6d65 7320    list of names 
-00014d60: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
-00014d70: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
-00014d80: 6c74 2061 6c6c 2063 6f6c 6c65 6374 696f  lt all collectio
-00014d90: 6e73 0a20 2020 2020 2065 7863 6570 7420  ns.      except 
-00014da0: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
-00014db0: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
-00014dc0: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-00014dd0: 6469 6174 6573 3a20 4966 2060 5472 7565  diates: If `True
-00014de0: 602c 2063 6170 7475 7265 7320 696e 7465  `, captures inte
-00014df0: 726d 6564 6961 7465 2072 6574 7572 6e20  rmediate return 
-00014e00: 7661 6c75 6573 0a20 2020 2020 206f 6620  values.      of 
-00014e10: 616c 6c20 4d6f 6475 6c65 7320 696e 7369  all Modules insi
-00014e20: 6465 2074 6865 2022 696e 7465 726d 6564  de the "intermed
-00014e30: 6961 7465 7322 2063 6f6c 6c65 6374 696f  iates" collectio
-00014e40: 6e2e 2042 7920 6465 6661 756c 7420 6f6e  n. By default on
-00014e50: 6c79 0a20 2020 2020 2074 6865 2072 6574  ly.      the ret
-00014e60: 7572 6e20 7661 6c75 6573 206f 6620 616c  urn values of al
-00014e70: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
-00014e80: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
-00014e90: 2041 2066 756e 6374 696f 6e20 6361 6e0a   A function can.
-00014ea0: 2020 2020 2020 6265 2070 6173 7365 6420        be passed 
-00014eb0: 746f 2063 6861 6e67 6520 7468 6520 6669  to change the fi
-00014ec0: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
-00014ed0: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
-00014ee0: 6f6e 2074 616b 6573 0a20 2020 2020 2074  on takes.      t
-00014ef0: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
-00014f00: 6365 2061 6e64 206d 6574 686f 6420 6e61  ce and method na
-00014f10: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
-00014f20: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
-00014f30: 0a20 2020 2020 2077 6865 7468 6572 2074  .      whether t
-00014f40: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
-00014f50: 7420 6d65 7468 6f64 2069 6e76 6f63 6174  t method invocat
-00014f60: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
-00014f70: 6f72 6564 2e0a 2020 5265 7475 726e 733a  ored..  Returns:
-00014f80: 0a20 2020 2054 6865 2069 6e69 7420 6675  .    The init fu
-00014f90: 6e63 7469 6f6e 2077 7261 7070 696e 6720  nction wrapping 
-00014fa0: 6060 666e 6060 2e0a 2020 2222 220a 2020  ``fn``..  """.  
-00014fb0: 696e 6974 5f66 6e20 3d20 696e 6974 5f77  init_fn = init_w
-00014fc0: 6974 685f 6f75 7470 7574 2866 6e2c 206d  ith_output(fn, m
-00014fd0: 6f64 756c 652c 206d 7574 6162 6c65 2c20  odule, mutable, 
-00014fe0: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00014ff0: 6961 7465 7329 0a20 2040 6675 6e63 746f  iates).  @functo
-00015000: 6f6c 732e 7772 6170 7328 696e 6974 5f66  ols.wraps(init_f
-00015010: 6e29 0a20 2064 6566 2069 6e69 745f 7772  n).  def init_wr
-00015020: 6170 7065 7228 2a61 7267 732c 202a 2a6b  apper(*args, **k
-00015030: 7761 7267 7329 3a0a 2020 2020 7265 7475  wargs):.    retu
-00015040: 726e 2069 6e69 745f 666e 282a 6172 6773  rn init_fn(*args
-00015050: 2c20 2a2a 6b77 6172 6773 295b 315d 0a20  , **kwargs)[1]. 
-00015060: 2072 6574 7572 6e20 696e 6974 5f77 7261   return init_wra
-00015070: 7070 6572 0a                             pper.
+0000a390: 2e0a 2020 2020 2020 5f64 6565 705f 636c  ..      _deep_cl
+0000a3a0: 6f6e 653a 2041 2062 6f6f 6c65 616e 206f  one: A boolean o
+0000a3b0: 7220 6120 7765 616b 2076 616c 7565 2064  r a weak value d
+0000a3c0: 6963 7469 6f6e 6172 7920 746f 2063 6f6e  ictionary to con
+0000a3d0: 7472 6f6c 2064 6565 7020 636c 6f6e 696e  trol deep clonin
+0000a3e0: 670a 2020 2020 2020 2020 6f66 2073 7562  g.        of sub
+0000a3f0: 6d6f 6475 6c65 732e 2049 6620 5472 7565  modules. If True
+0000a400: 2c20 7375 626d 6f64 756c 6573 2077 696c  , submodules wil
+0000a410: 6c20 6265 2063 6c6f 6e65 6420 7265 6375  l be cloned recu
+0000a420: 7273 6976 656c 792e 2049 6620 610a 2020  rsively. If a.  
+0000a430: 2020 2020 2020 7765 616b 2076 616c 7565        weak value
+0000a440: 2064 6963 7469 6f6e 6172 7920 6973 2070   dictionary is p
+0000a450: 6173 7365 642c 2069 7420 7769 6c6c 2062  assed, it will b
+0000a460: 6520 7573 6564 2074 6f20 6361 6368 6520  e used to cache 
+0000a470: 636c 6f6e 6564 0a20 2020 2020 2020 2073  cloned.        s
+0000a480: 7562 6d6f 6475 6c65 732e 2054 6869 7320  ubmodules. This 
+0000a490: 666c 6167 2069 7320 7573 6564 2062 7920  flag is used by 
+0000a4a0: 696e 6974 2f61 7070 6c79 2f62 696e 6420  init/apply/bind 
+0000a4b0: 746f 2061 766f 6964 2073 636f 7065 0a20  to avoid scope. 
+0000a4c0: 2020 2020 2020 206c 6561 6b61 6765 2e0a         leakage..
+0000a4d0: 2020 2020 2020 2a2a 7570 6461 7465 733a        **updates:
+0000a4e0: 2041 7474 7269 6275 7465 2075 7064 6174   Attribute updat
+0000a4f0: 6573 2e0a 2020 2020 5265 7475 726e 733a  es..    Returns:
+0000a500: 0a20 2020 2020 2041 2063 6c6f 6e65 206f  .      A clone o
+0000a510: 6620 7468 6520 7468 6973 204d 6f64 756c  f the this Modul
+0000a520: 6520 7769 7468 2074 6865 2075 7064 6174  e with the updat
+0000a530: 6564 2061 7474 7269 6275 7465 7320 616e  ed attributes an
+0000a540: 6420 7061 7265 6e74 2e0a 2020 2020 2222  d parent..    ""
+0000a550: 220a 2020 2020 6174 7472 7320 3d20 7b66  ".    attrs = {f
+0000a560: 2e6e 616d 653a 2067 6574 6174 7472 2873  .name: getattr(s
+0000a570: 656c 662c 2066 2e6e 616d 6529 2066 6f72  elf, f.name) for
+0000a580: 2066 2069 6e20 6461 7461 636c 6173 7365   f in dataclasse
+0000a590: 732e 6669 656c 6473 2873 656c 6629 2069  s.fields(self) i
+0000a5a0: 6620 662e 696e 6974 7d0a 0a20 2020 2061  f f.init}..    a
+0000a5b0: 7474 7273 2e75 7064 6174 6528 7061 7265  ttrs.update(pare
+0000a5c0: 6e74 3d70 6172 656e 742c 202a 2a75 7064  nt=parent, **upd
+0000a5d0: 6174 6573 290a 0a20 2020 2023 2048 6572  ates)..    # Her
+0000a5e0: 6520 7765 2069 6d70 6c65 6d65 6e74 2064  e we implement d
+0000a5f0: 6565 7020 636c 6f6e 696e 6720 6f66 2073  eep cloning of s
+0000a600: 7562 6d6f 6475 6c65 732c 2074 6869 7320  ubmodules, this 
+0000a610: 6973 206e 6563 6573 7361 7279 2074 6f20  is necessary to 
+0000a620: 6176 6f69 6420 7363 6f70 6520 6c65 616b  avoid scope leak
+0000a630: 6167 650a 2020 2020 2320 6672 6f6d 2065  age.    # from e
+0000a640: 7874 6572 6e61 6c20 7375 626d 6f64 756c  xternal submodul
+0000a650: 6573 2069 6e74 6f20 696e 6974 2f61 7070  es into init/app
+0000a660: 6c79 2f62 696e 6420 7768 696c 6520 7072  ly/bind while pr
+0000a670: 6573 6572 7669 6e67 2073 6861 7269 6e67  eserving sharing
+0000a680: 2d62 792d 7265 6665 7265 6e63 650a 2020  -by-reference.  
+0000a690: 2020 2320 7265 6c61 7469 6f6e 7368 6970    # relationship
+0000a6a0: 7320 6265 7477 6565 6e20 7375 626d 6f64  s between submod
+0000a6b0: 756c 6573 2e0a 2020 2020 6966 205f 6465  ules..    if _de
+0000a6c0: 6570 5f63 6c6f 6e65 2021 3d20 4661 6c73  ep_clone != Fals
+0000a6d0: 653a 0a20 2020 2020 2023 2057 6520 7573  e:.      # We us
+0000a6e0: 6520 6120 7765 616b 2076 616c 7565 2064  e a weak value d
+0000a6f0: 6963 7469 6f6e 6172 7920 746f 2063 6163  ictionary to cac
+0000a700: 6865 2063 6c6f 6e65 6420 7375 626d 6f64  he cloned submod
+0000a710: 756c 6573 2e20 5768 656e 2061 2073 6861  ules. When a sha
+0000a720: 7265 640a 2020 2020 2020 2320 7375 626d  red.      # subm
+0000a730: 6f64 756c 6520 6973 2063 6c6f 6e65 642c  odule is cloned,
+0000a740: 2069 7473 206f 6e6c 7920 636c 6f6e 6564   its only cloned
+0000a750: 206f 6e63 6520 656c 7365 2069 7473 2066   once else its f
+0000a760: 6574 6368 6564 2066 726f 6d20 7468 6520  etched from the 
+0000a770: 6361 6368 652e 0a20 2020 2020 2063 6163  cache..      cac
+0000a780: 6865 203d 2077 6561 6b72 6566 2e57 6561  he = weakref.Wea
+0000a790: 6b56 616c 7565 4469 6374 696f 6e61 7279  kValueDictionary
+0000a7a0: 2829 2069 6620 6973 696e 7374 616e 6365  () if isinstance
+0000a7b0: 285f 6465 6570 5f63 6c6f 6e65 2c20 626f  (_deep_clone, bo
+0000a7c0: 6f6c 2920 656c 7365 205f 6465 6570 5f63  ol) else _deep_c
+0000a7d0: 6c6f 6e65 0a20 2020 2020 2064 6566 2063  lone.      def c
+0000a7e0: 6c6f 6e65 5f66 6e28 6d3a 204d 6f64 756c  lone_fn(m: Modul
+0000a7f0: 6529 202d 3e20 4d6f 6475 6c65 3a0a 2020  e) -> Module:.  
+0000a800: 2020 2020 2020 6966 2068 6173 6174 7472        if hasattr
+0000a810: 286d 2c20 275f 6964 2729 3a0a 2020 2020  (m, '_id'):.    
+0000a820: 2020 2020 2020 6b65 7920 3d20 6d2e 5f69        key = m._i
+0000a830: 640a 2020 2020 2020 2020 2020 6966 206b  d.          if k
+0000a840: 6579 2069 6e20 6361 6368 653a 0a20 2020  ey in cache:.   
+0000a850: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+0000a860: 6361 6368 655b 6b65 795d 0a20 2020 2020  cache[key].     
+0000a870: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+0000a880: 2020 2020 2020 2063 6c6f 6e65 203d 206d         clone = m
+0000a890: 2e63 6c6f 6e65 285f 6465 6570 5f63 6c6f  .clone(_deep_clo
+0000a8a0: 6e65 3d63 6163 6865 290a 2020 2020 2020  ne=cache).      
+0000a8b0: 2020 2020 2020 6361 6368 655b 6b65 795d        cache[key]
+0000a8c0: 203d 2063 6c6f 6e65 0a20 2020 2020 2020   = clone.       
+0000a8d0: 2020 2020 2072 6574 7572 6e20 636c 6f6e       return clon
+0000a8e0: 650a 2020 2020 2020 2020 656c 7365 3a0a  e.        else:.
+0000a8f0: 2020 2020 2020 2020 2020 2320 4966 2074            # If t
+0000a900: 6865 206d 6f64 756c 6520 646f 6573 6e27  he module doesn'
+0000a910: 7420 6861 7665 2061 6e20 5f69 6420 6174  t have an _id at
+0000a920: 7472 6962 7574 6520 6974 2063 6f75 6c64  tribute it could
+0000a930: 2062 6520 6120 6d6f 636b 206f 626a 6563   be a mock objec
+0000a940: 740a 2020 2020 2020 2020 2020 2320 736f  t.          # so
+0000a950: 2077 6520 7265 7475 726e 2069 7420 6173   we return it as
+0000a960: 2069 732e 0a20 2020 2020 2020 2020 2072   is..          r
+0000a970: 6574 7572 6e20 6d0a 0a20 2020 2020 2023  eturn m..      #
+0000a980: 205f 6d61 705f 7375 626d 6f64 756c 6573   _map_submodules
+0000a990: 2077 696c 6c20 6d61 7020 6f76 6572 2061   will map over a
+0000a9a0: 6c6c 2073 7562 6d6f 6475 6c65 7320 696e  ll submodules in
+0000a9b0: 7369 6465 2061 7474 7273 0a20 2020 2020  side attrs.     
+0000a9c0: 2023 2076 616c 7565 2068 6572 6520 6361   # value here ca
+0000a9d0: 6e20 6265 2061 6e79 2070 7974 7265 652c  n be any pytree,
+0000a9e0: 206e 6f6e 2d6d 6f64 756c 6520 7661 6c75   non-module valu
+0000a9f0: 6573 2061 7265 2069 676e 6f72 6564 0a20  es are ignored. 
+0000aa00: 2020 2020 2066 6f72 2066 6965 6c64 5f6e       for field_n
+0000aa10: 616d 652c 2076 616c 7565 2069 6e20 6174  ame, value in at
+0000aa20: 7472 732e 6974 656d 7328 293a 0a20 2020  trs.items():.   
+0000aa30: 2020 2020 2061 7474 7273 5b66 6965 6c64       attrs[field
+0000aa40: 5f6e 616d 655d 203d 205f 6d61 705f 7375  _name] = _map_su
+0000aa50: 626d 6f64 756c 6573 2863 6c6f 6e65 5f66  bmodules(clone_f
+0000aa60: 6e2c 2076 616c 7565 290a 0a20 2020 206d  n, value)..    m
+0000aa70: 6f64 756c 6520 3d20 7365 6c66 2e5f 5f63  odule = self.__c
+0000aa80: 6c61 7373 5f5f 282a 2a61 7474 7273 290a  lass__(**attrs).
+0000aa90: 0a20 2020 2072 6574 7572 6e20 6d6f 6475  .    return modu
+0000aaa0: 6c65 0a0a 2020 6465 6620 7661 7269 6162  le..  def variab
+0000aab0: 6c65 2873 656c 662c 2063 6f6c 3a20 7374  le(self, col: st
+0000aac0: 722c 206e 616d 653a 2073 7472 2c0a 2020  r, name: str,.  
+0000aad0: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
+0000aae0: 745f 666e 3a20 4f70 7469 6f6e 616c 5b43  t_fn: Optional[C
+0000aaf0: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+0000ab00: 5d5d 203d 204e 6f6e 652c 0a20 2020 2020  ]] = None,.     
+0000ab10: 2020 2020 2020 2020 2020 2a69 6e69 745f            *init_
+0000ab20: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
+0000ab30: 2020 2020 2075 6e62 6f78 3a20 626f 6f6c       unbox: bool
+0000ab40: 203d 2054 7275 6529 202d 3e20 5661 7269   = True) -> Vari
+0000ab50: 6162 6c65 3a0a 2020 2020 2222 2244 6563  able:.    """Dec
+0000ab60: 6c61 7265 7320 616e 6420 7265 7475 726e  lares and return
+0000ab70: 7320 6120 7661 7269 6162 6c65 2069 6e20  s a variable in 
+0000ab80: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
+0000ab90: 2020 5365 6520 3a6d 6f64 3a60 666c 6178    See :mod:`flax
+0000aba0: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
+0000abb0: 2066 6f72 206d 6f72 6520 696e 666f 726d   for more inform
+0000abc0: 6174 696f 6e2e 2053 6565 2061 6c73 6f20  ation. See also 
+0000abd0: 3a6d 6574 683a 6070 6172 616d 600a 2020  :meth:`param`.  
+0000abe0: 2020 666f 7220 6120 7368 6f72 7468 616e    for a shorthan
+0000abf0: 6420 7761 7920 746f 2064 6566 696e 6520  d way to define 
+0000ac00: 7265 6164 2d6f 6e6c 7920 7661 7269 6162  read-only variab
+0000ac10: 6c65 7320 696e 2074 6865 2022 7061 7261  les in the "para
+0000ac20: 6d73 220a 2020 2020 636f 6c6c 6563 7469  ms".    collecti
+0000ac30: 6f6e 2e0a 0a20 2020 2043 6f6e 7472 6172  on...    Contrar
+0000ac40: 7920 746f 203a 6d65 7468 3a60 7061 7261  y to :meth:`para
+0000ac50: 6d60 2c20 616c 6c20 6172 6775 6d65 6e74  m`, all argument
+0000ac60: 7320 7061 7373 696e 6720 7573 696e 6720  s passing using 
+0000ac70: 6069 6e69 745f 666e 6020 7368 6f75 6c64  `init_fn` should
+0000ac80: 2062 650a 2020 2020 7061 7373 6564 206f   be.    passed o
+0000ac90: 6e20 6578 706c 6963 6974 6c79 3a3a 0a0a  n explicitly::..
+0000aca0: 2020 2020 2020 6b65 7920 3d20 7365 6c66        key = self
+0000acb0: 2e6d 616b 655f 726e 6728 2773 7461 7473  .make_rng('stats
+0000acc0: 2729 0a20 2020 2020 206d 6561 6e20 3d20  ').      mean = 
+0000acd0: 7365 6c66 2e76 6172 6961 626c 6528 2773  self.variable('s
+0000ace0: 7461 7473 272c 2027 6d65 616e 272c 206c  tats', 'mean', l
+0000acf0: 6563 756e 5f6e 6f72 6d61 6c28 292c 206b  ecun_normal(), k
+0000ad00: 6579 2c20 2832 2c20 3229 290a 0a20 2020  ey, (2, 2))..   
+0000ad10: 2049 6e20 7468 6520 6578 616d 706c 6520   In the example 
+0000ad20: 6162 6f76 652c 2074 6865 2066 756e 6374  above, the funct
+0000ad30: 696f 6e20 606c 6563 756e 5f6e 6f72 6d61  ion `lecun_norma
+0000ad40: 6c60 2065 7870 6563 7473 2074 776f 2061  l` expects two a
+0000ad50: 7267 756d 656e 7473 3a0a 2020 2020 606b  rguments:.    `k
+0000ad60: 6579 6020 616e 6420 6073 6861 7065 602c  ey` and `shape`,
+0000ad70: 2061 6e64 2062 6f74 6820 6861 7665 2074   and both have t
+0000ad80: 6f20 6265 2070 6173 7365 6420 6f6e 2e20  o be passed on. 
+0000ad90: 5468 6520 5052 4e47 2066 6f72 2060 7374  The PRNG for `st
+0000ada0: 6174 7360 2068 6173 0a20 2020 2074 6f20  ats` has.    to 
+0000adb0: 6265 2070 726f 7669 6465 6420 6578 706c  be provided expl
+0000adc0: 6963 6974 6c79 2077 6865 6e20 6361 6c6c  icitly when call
+0000add0: 696e 6720 3a6d 6574 683a 6069 6e69 7460  ing :meth:`init`
+0000ade0: 2061 6e64 203a 6d65 7468 3a60 6170 706c   and :meth:`appl
+0000adf0: 7960 2e0a 0a20 2020 2041 7267 733a 0a20  y`...    Args:. 
+0000ae00: 2020 2020 2063 6f6c 3a20 5468 6520 7661       col: The va
+0000ae10: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
+0000ae20: 6e20 6e61 6d65 2e0a 2020 2020 2020 6e61  n name..      na
+0000ae30: 6d65 3a20 5468 6520 7661 7269 6162 6c65  me: The variable
+0000ae40: 206e 616d 652e 0a20 2020 2020 2069 6e69   name..      ini
+0000ae50: 745f 666e 3a20 5468 6520 6675 6e63 7469  t_fn: The functi
+0000ae60: 6f6e 2074 6861 7420 7769 6c6c 2062 6520  on that will be 
+0000ae70: 6361 6c6c 6564 2074 6f20 636f 6d70 7574  called to comput
+0000ae80: 6520 7468 6520 696e 6974 6961 6c20 7661  e the initial va
+0000ae90: 6c75 650a 2020 2020 2020 2020 6f66 2074  lue.        of t
+0000aea0: 6869 7320 7661 7269 6162 6c65 2e20 5468  his variable. Th
+0000aeb0: 6973 2066 756e 6374 696f 6e20 7769 6c6c  is function will
+0000aec0: 206f 6e6c 7920 6265 2063 616c 6c65 6420   only be called 
+0000aed0: 7468 6520 6669 7273 7420 7469 6d65 0a20  the first time. 
+0000aee0: 2020 2020 2020 2074 6869 7320 7661 7269         this vari
+0000aef0: 6162 6c65 2069 7320 7573 6564 2069 6e20  able is used in 
+0000af00: 7468 6973 206d 6f64 756c 652e 2049 6620  this module. If 
+0000af10: 4e6f 6e65 2c20 7468 6520 7661 7269 6162  None, the variab
+0000af20: 6c65 206d 7573 740a 2020 2020 2020 2020  le must.        
+0000af30: 616c 7265 6164 7920 6265 2069 6e69 7469  already be initi
+0000af40: 616c 697a 6564 206f 7468 6572 7769 7365  alized otherwise
+0000af50: 2061 6e20 6572 726f 7220 6973 2072 6169   an error is rai
+0000af60: 7365 642e 0a20 2020 2020 202a 696e 6974  sed..      *init
+0000af70: 5f61 7267 733a 2054 6865 2061 7267 756d  _args: The argum
+0000af80: 656e 7473 2074 6f20 7061 7373 2074 6f20  ents to pass to 
+0000af90: 696e 6974 5f66 6e2e 0a20 2020 2020 2075  init_fn..      u
+0000afa0: 6e62 6f78 3a20 4966 2054 7275 652c 2060  nbox: If True, `
+0000afb0: 6041 7869 734d 6574 6164 6174 6160 6020  `AxisMetadata`` 
+0000afc0: 696e 7374 616e 6365 7320 6172 6520 7265  instances are re
+0000afd0: 706c 6163 6564 2062 7920 7468 6569 7220  placed by their 
+0000afe0: 756e 626f 7865 640a 2020 2020 2020 2020  unboxed.        
+0000aff0: 7661 6c75 652c 2073 6565 2060 6066 6c61  value, see ``fla
+0000b000: 782e 6e6e 2e6d 6574 612e 756e 626f 7860  x.nn.meta.unbox`
+0000b010: 6020 2864 6566 6175 6c74 3a20 5472 7565  ` (default: True
+0000b020: 292e 0a0a 2020 2020 5265 7475 726e 733a  )...    Returns:
+0000b030: 0a20 2020 2020 2041 203a 636c 6173 733a  .      A :class:
+0000b040: 6066 6c61 782e 636f 7265 2e76 6172 6961  `flax.core.varia
+0000b050: 626c 6573 2e56 6172 6961 626c 6560 2074  bles.Variable` t
+0000b060: 6861 7420 6361 6e20 6265 2072 6561 6420  hat can be read 
+0000b070: 6f72 2073 6574 2076 6961 0a20 2020 2020  or set via.     
+0000b080: 2022 2e76 616c 7565 2220 6174 7472 6962   ".value" attrib
+0000b090: 7574 652e 2054 6872 6f77 7320 616e 2065  ute. Throws an e
+0000b0a0: 7272 6f72 2069 6620 7468 6520 7661 7269  rror if the vari
+0000b0b0: 6162 6c65 2065 7869 7374 7320 616c 7265  able exists alre
+0000b0c0: 6164 792e 0a20 2020 2022 2222 0a20 2020  ady..    """.   
+0000b0d0: 2069 6620 6e6f 7420 7365 6c66 2e5f 696e   if not self._in
+0000b0e0: 6974 6961 6c69 7a61 7469 6f6e 5f61 6c6c  itialization_all
+0000b0f0: 6f77 6564 3a0a 2020 2020 2020 7261 6973  owed:.      rais
+0000b100: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
+0000b110: 2020 2020 2020 2020 2756 6172 6961 626c          'Variabl
+0000b120: 6573 206d 7573 7420 6265 2069 6e69 7469  es must be initi
+0000b130: 616c 697a 6564 2069 6e20 6073 6574 7570  alized in `setup
+0000b140: 2829 6020 6f72 2069 6e20 6120 6d65 7468  ()` or in a meth
+0000b150: 6f64 2027 0a20 2020 2020 2020 2020 2027  od '.          '
+0000b160: 7772 6170 7065 6420 696e 2060 4063 6f6d  wrapped in `@com
+0000b170: 7061 6374 6027 290a 2020 2020 6966 2073  pact`').    if s
+0000b180: 656c 662e 5f6e 616d 655f 7461 6b65 6e28  elf._name_taken(
+0000b190: 6e61 6d65 2c20 636f 6c6c 6563 7469 6f6e  name, collection
+0000b1a0: 3d63 6f6c 293a 0a20 2020 2020 2072 6169  =col):.      rai
+0000b1b0: 7365 2065 7272 6f72 732e 4e61 6d65 496e  se errors.NameIn
+0000b1c0: 5573 6545 7272 6f72 2827 7661 7269 6162  UseError('variab
+0000b1d0: 6c65 272c 206e 616d 652c 2073 656c 662e  le', name, self.
+0000b1e0: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
+0000b1f0: 5f5f 290a 2020 2020 6173 7365 7274 2073  __).    assert s
+0000b200: 656c 662e 7363 6f70 6520 6973 206e 6f74  elf.scope is not
+0000b210: 204e 6f6e 650a 2020 2020 7620 3d20 7365   None.    v = se
+0000b220: 6c66 2e73 636f 7065 2e76 6172 6961 626c  lf.scope.variabl
+0000b230: 6528 636f 6c2c 206e 616d 652c 2069 6e69  e(col, name, ini
+0000b240: 745f 666e 2c20 2a69 6e69 745f 6172 6773  t_fn, *init_args
+0000b250: 2c20 756e 626f 783d 756e 626f 7829 0a20  , unbox=unbox). 
+0000b260: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
+0000b270: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
+0000b280: 636f 6c0a 2020 2020 7265 7475 726e 2076  col.    return v
+0000b290: 0a0a 2020 6465 6620 7061 7261 6d28 7365  ..  def param(se
+0000b2a0: 6c66 2c20 6e61 6d65 3a20 7374 722c 2069  lf, name: str, i
+0000b2b0: 6e69 745f 666e 3a20 4361 6c6c 6162 6c65  nit_fn: Callable
+0000b2c0: 5b2e 2e2e 2c20 545d 2c20 2a69 6e69 745f  [..., T], *init_
+0000b2d0: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
+0000b2e0: 2020 756e 626f 783a 2062 6f6f 6c20 3d20    unbox: bool = 
+0000b2f0: 5472 7565 2920 2d3e 2054 3a0a 2020 2020  True) -> T:.    
+0000b300: 2222 2244 6563 6c61 7265 7320 616e 6420  """Declares and 
+0000b310: 7265 7475 726e 7320 6120 7061 7261 6d65  returns a parame
+0000b320: 7465 7220 696e 2074 6869 7320 4d6f 6475  ter in this Modu
+0000b330: 6c65 2e0a 0a20 2020 2050 6172 616d 6574  le...    Paramet
+0000b340: 6572 7320 6172 6520 7265 6164 2d6f 6e6c  ers are read-onl
+0000b350: 7920 7661 7269 6162 6c65 7320 696e 2074  y variables in t
+0000b360: 6865 2063 6f6c 6c65 6374 696f 6e20 6e61  he collection na
+0000b370: 6d65 6420 2270 6172 616d 7322 2e20 5365  med "params". Se
+0000b380: 650a 2020 2020 3a6d 6f64 3a60 666c 6178  e.    :mod:`flax
+0000b390: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
+0000b3a0: 2066 6f72 206d 6f72 6520 6465 7461 696c   for more detail
+0000b3b0: 7320 6f6e 2076 6172 6961 626c 6573 2e0a  s on variables..
+0000b3c0: 0a20 2020 2054 6865 2066 6972 7374 2061  .    The first a
+0000b3d0: 7267 756d 656e 7420 6f66 2060 696e 6974  rgument of `init
+0000b3e0: 5f66 6e60 2069 7320 6173 7375 6d65 6420  _fn` is assumed 
+0000b3f0: 746f 2062 6520 6120 5052 4e47 206b 6579  to be a PRNG key
+0000b400: 2c20 7768 6963 6820 6973 0a20 2020 2070  , which is.    p
+0000b410: 726f 7669 6465 6420 6175 746f 6d61 7469  rovided automati
+0000b420: 6361 6c6c 7920 616e 6420 646f 6573 206e  cally and does n
+0000b430: 6f74 2068 6176 6520 746f 2062 6520 7061  ot have to be pa
+0000b440: 7373 6564 2075 7369 6e67 2060 696e 6974  ssed using `init
+0000b450: 5f61 7267 7360 3a3a 0a0a 2020 2020 2020  _args`::..      
+0000b460: 6d65 616e 203d 2073 656c 662e 7061 7261  mean = self.para
+0000b470: 6d28 276d 6561 6e27 2c20 6c65 6375 6e5f  m('mean', lecun_
+0000b480: 6e6f 726d 616c 2829 2c20 2832 2c20 3229  normal(), (2, 2)
+0000b490: 290a 0a20 2020 2049 6e20 7468 6520 6578  )..    In the ex
+0000b4a0: 616d 706c 6520 6162 6f76 652c 2074 6865  ample above, the
+0000b4b0: 2066 756e 6374 696f 6e20 606c 6563 756e   function `lecun
+0000b4c0: 5f6e 6f72 6d61 6c60 2065 7870 6563 7473  _normal` expects
+0000b4d0: 2074 776f 2061 7267 756d 656e 7473 3a0a   two arguments:.
+0000b4e0: 2020 2020 606b 6579 6020 616e 6420 6073      `key` and `s
+0000b4f0: 6861 7065 602c 2062 7574 206f 6e6c 7920  hape`, but only 
+0000b500: 6073 6861 7065 6020 6861 7320 746f 2062  `shape` has to b
+0000b510: 6520 7072 6f76 6964 6564 2065 7870 6c69  e provided expli
+0000b520: 6369 746c 793b 2060 6b65 7960 0a20 2020  citly; `key`.   
+0000b530: 2069 7320 7365 7420 6175 746f 6d61 7469   is set automati
+0000b540: 6361 6c6c 7920 7573 696e 6720 7468 6520  cally using the 
+0000b550: 5052 4e47 2066 6f72 2060 7061 7261 6d73  PRNG for `params
+0000b560: 6020 7468 6174 2069 7320 7061 7373 6564  ` that is passed
+0000b570: 2077 6865 6e0a 2020 2020 696e 6974 6961   when.    initia
+0000b580: 6c69 7a69 6e67 2074 6865 206d 6f64 756c  lizing the modul
+0000b590: 6520 7573 696e 6720 3a6d 6574 683a 6069  e using :meth:`i
+0000b5a0: 6e69 7460 2e0a 0a20 2020 2041 7267 733a  nit`...    Args:
+0000b5b0: 0a20 2020 2020 206e 616d 653a 2054 6865  .      name: The
+0000b5c0: 2070 6172 616d 6574 6572 206e 616d 652e   parameter name.
+0000b5d0: 0a20 2020 2020 2069 6e69 745f 666e 3a20  .      init_fn: 
+0000b5e0: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
+0000b5f0: 7420 7769 6c6c 2062 6520 6361 6c6c 6564  t will be called
+0000b600: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
+0000b610: 696e 6974 6961 6c20 7661 6c75 650a 2020  initial value.  
+0000b620: 2020 2020 2020 6f66 2074 6869 7320 7661        of this va
+0000b630: 7269 6162 6c65 2e20 5468 6973 2066 756e  riable. This fun
+0000b640: 6374 696f 6e20 7769 6c6c 206f 6e6c 7920  ction will only 
+0000b650: 6265 2063 616c 6c65 6420 7468 6520 6669  be called the fi
+0000b660: 7273 7420 7469 6d65 0a20 2020 2020 2020  rst time.       
+0000b670: 2074 6869 7320 7061 7261 6d65 7465 7220   this parameter 
+0000b680: 6973 2075 7365 6420 696e 2074 6869 7320  is used in this 
+0000b690: 6d6f 6475 6c65 2e0a 2020 2020 2020 2a69  module..      *i
+0000b6a0: 6e69 745f 6172 6773 3a20 5468 6520 6172  nit_args: The ar
+0000b6b0: 6775 6d65 6e74 7320 746f 2070 6173 7320  guments to pass 
+0000b6c0: 746f 2069 6e69 745f 666e 2e0a 2020 2020  to init_fn..    
+0000b6d0: 2020 756e 626f 783a 2049 6620 5472 7565    unbox: If True
+0000b6e0: 2c20 6060 4178 6973 4d65 7461 6461 7461  , ``AxisMetadata
+0000b6f0: 6060 2069 6e73 7461 6e63 6573 2061 7265  `` instances are
+0000b700: 2072 6570 6c61 6365 6420 6279 2074 6865   replaced by the
+0000b710: 6972 2075 6e62 6f78 6564 0a20 2020 2020  ir unboxed.     
+0000b720: 2020 2076 616c 7565 2c20 7365 6520 6060     value, see ``
+0000b730: 666c 6178 2e6e 6e2e 6d65 7461 2e75 6e62  flax.nn.meta.unb
+0000b740: 6f78 6060 2028 6465 6661 756c 743a 2054  ox`` (default: T
+0000b750: 7275 6529 2e0a 0a20 2020 2052 6574 7572  rue)...    Retur
+0000b760: 6e73 3a0a 2020 2020 2020 5468 6520 7661  ns:.      The va
+0000b770: 6c75 6520 6f66 2074 6865 2069 6e69 7469  lue of the initi
+0000b780: 616c 697a 6564 2070 6172 616d 6574 6572  alized parameter
+0000b790: 2e20 5468 726f 7773 2061 6e20 6572 726f  . Throws an erro
+0000b7a0: 7220 6966 2074 6865 2070 6172 616d 6574  r if the paramet
+0000b7b0: 6572 0a20 2020 2020 2065 7869 7374 7320  er.      exists 
+0000b7c0: 616c 7265 6164 792e 0a20 2020 2022 2222  already..    """
+0000b7d0: 0a20 2020 2069 6620 6e6f 7420 7365 6c66  .    if not self
+0000b7e0: 2e5f 696e 6974 6961 6c69 7a61 7469 6f6e  ._initialization
+0000b7f0: 5f61 6c6c 6f77 6564 3a0a 2020 2020 2020  _allowed:.      
+0000b800: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0000b810: 280a 2020 2020 2020 2020 2020 2750 6172  (.          'Par
+0000b820: 616d 6574 6572 7320 6d75 7374 2062 6520  ameters must be 
+0000b830: 696e 6974 6961 6c69 7a65 6420 696e 2060  initialized in `
+0000b840: 7365 7475 7028 2960 206f 7220 696e 2061  setup()` or in a
+0000b850: 206d 6574 686f 6420 270a 2020 2020 2020   method '.      
+0000b860: 2020 2020 2777 7261 7070 6564 2069 6e20      'wrapped in 
+0000b870: 6040 636f 6d70 6163 7460 2729 0a20 2020  `@compact`').   
+0000b880: 2069 6620 7365 6c66 2e5f 6e61 6d65 5f74   if self._name_t
+0000b890: 616b 656e 286e 616d 652c 2063 6f6c 6c65  aken(name, colle
+0000b8a0: 6374 696f 6e3d 2770 6172 616d 7327 293a  ction='params'):
+0000b8b0: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
+0000b8c0: 6f72 732e 4e61 6d65 496e 5573 6545 7272  ors.NameInUseErr
+0000b8d0: 6f72 2827 7061 7261 6d27 2c20 6e61 6d65  or('param', name
+0000b8e0: 2c20 7365 6c66 2e5f 5f63 6c61 7373 5f5f  , self.__class__
+0000b8f0: 2e5f 5f6e 616d 655f 5f29 0a20 2020 2061  .__name__).    a
+0000b900: 7373 6572 7420 7365 6c66 2e73 636f 7065  ssert self.scope
+0000b910: 2069 7320 6e6f 7420 4e6f 6e65 0a20 2020   is not None.   
+0000b920: 2076 203d 2073 656c 662e 7363 6f70 652e   v = self.scope.
+0000b930: 7061 7261 6d28 6e61 6d65 2c20 696e 6974  param(name, init
+0000b940: 5f66 6e2c 202a 696e 6974 5f61 7267 732c  _fn, *init_args,
+0000b950: 2075 6e62 6f78 3d75 6e62 6f78 290a 2020   unbox=unbox).  
+0000b960: 2020 7365 6c66 2e5f 7374 6174 652e 6368    self._state.ch
+0000b970: 696c 6472 656e 5b6e 616d 655d 203d 2027  ildren[name] = '
+0000b980: 7061 7261 6d73 270a 2020 2020 7265 7475  params'.    retu
+0000b990: 726e 2076 0a0a 2020 6465 6620 6861 735f  rn v..  def has_
+0000b9a0: 7661 7269 6162 6c65 2873 656c 662c 2063  variable(self, c
+0000b9b0: 6f6c 3a20 7374 722c 206e 616d 653a 2073  ol: str, name: s
+0000b9c0: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
+0000b9d0: 2022 2222 4368 6563 6b73 2069 6620 6120   """Checks if a 
+0000b9e0: 7661 7269 6162 6c65 206f 6620 6769 7665  variable of give
+0000b9f0: 6e20 636f 6c6c 6563 7469 6f6e 2061 6e64  n collection and
+0000ba00: 206e 616d 6520 6578 6973 7473 2069 6e20   name exists in 
+0000ba10: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
+0000ba20: 2020 5365 6520 3a6d 6f64 3a60 666c 6178    See :mod:`flax
+0000ba30: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
+0000ba40: 2066 6f72 206d 6f72 6520 6578 706c 616e   for more explan
+0000ba50: 6174 696f 6e20 6f6e 2076 6172 6961 626c  ation on variabl
+0000ba60: 6573 2061 6e64 0a20 2020 2063 6f6c 6c65  es and.    colle
+0000ba70: 6374 696f 6e73 2e0a 0a20 2020 2041 7267  ctions...    Arg
+0000ba80: 733a 0a20 2020 2020 2063 6f6c 3a20 5468  s:.      col: Th
+0000ba90: 6520 7661 7269 6162 6c65 2063 6f6c 6c65  e variable colle
+0000baa0: 6374 696f 6e20 6e61 6d65 2e0a 2020 2020  ction name..    
+0000bab0: 2020 6e61 6d65 3a20 5468 6520 6e61 6d65    name: The name
+0000bac0: 206f 6620 7468 6520 7661 7269 6162 6c65   of the variable
+0000bad0: 2e0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+0000bae0: 2020 2020 2054 7275 6520 6966 2074 6865       True if the
+0000baf0: 2076 6172 6961 626c 6520 6578 6973 7473   variable exists
+0000bb00: 2e0a 2020 2020 2222 220a 2020 2020 6966  ..    """.    if
+0000bb10: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
+0000bb20: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
+0000bb30: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
+0000bb40: 2774 2061 6363 6573 7320 7661 7269 6162  't access variab
+0000bb50: 6c65 7320 6f6e 2075 6e62 6f75 6e64 206d  les on unbound m
+0000bb60: 6f64 756c 6573 2229 0a20 2020 2072 6574  odules").    ret
+0000bb70: 7572 6e20 7365 6c66 2e73 636f 7065 2e68  urn self.scope.h
+0000bb80: 6173 5f76 6172 6961 626c 6528 636f 6c2c  as_variable(col,
+0000bb90: 206e 616d 6529 0a0a 2020 6465 6620 6973   name)..  def is
+0000bba0: 5f6d 7574 6162 6c65 5f63 6f6c 6c65 6374  _mutable_collect
+0000bbb0: 696f 6e28 7365 6c66 2c20 636f 6c3a 2073  ion(self, col: s
+0000bbc0: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
+0000bbd0: 2022 2222 5265 7475 726e 7320 7472 7565   """Returns true
+0000bbe0: 2069 6620 7468 6520 636f 6c6c 6563 7469   if the collecti
+0000bbf0: 6f6e 2060 636f 6c60 2069 7320 6d75 7461  on `col` is muta
+0000bc00: 626c 652e 2222 220a 2020 2020 6966 2073  ble.""".    if s
+0000bc10: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
+0000bc20: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
+0000bc30: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
+0000bc40: 2063 6865 636b 206d 7574 6162 696c 6974   check mutabilit
+0000bc50: 7920 6f6e 2075 6e62 6f75 6e64 206d 6f64  y on unbound mod
+0000bc60: 756c 6573 2229 0a20 2020 2072 6574 7572  ules").    retur
+0000bc70: 6e20 7365 6c66 2e73 636f 7065 2e69 735f  n self.scope.is_
+0000bc80: 6d75 7461 626c 655f 636f 6c6c 6563 7469  mutable_collecti
+0000bc90: 6f6e 2863 6f6c 290a 0a20 2064 6566 2068  on(col)..  def h
+0000bca0: 6173 5f72 6e67 2873 656c 662c 206e 616d  as_rng(self, nam
+0000bcb0: 653a 2073 7472 2920 2d3e 2062 6f6f 6c3a  e: str) -> bool:
+0000bcc0: 0a20 2020 2022 2222 5265 7475 726e 7320  .    """Returns 
+0000bcd0: 7472 7565 2069 6620 6120 5052 4e47 5365  true if a PRNGSe
+0000bce0: 7175 656e 6365 2077 6974 6820 6e61 6d65  quence with name
+0000bcf0: 2060 6e61 6d65 6020 6578 6973 7473 2e22   `name` exists."
+0000bd00: 2222 0a20 2020 2069 6620 7365 6c66 2e73  "".    if self.s
+0000bd10: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
+0000bd20: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+0000bd30: 7272 6f72 2822 4361 6e27 7420 7175 6572  rror("Can't quer
+0000bd40: 7920 666f 7220 524e 4773 206f 6e20 756e  y for RNGs on un
+0000bd50: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
+0000bd60: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+0000bd70: 7363 6f70 652e 6861 735f 726e 6728 6e61  scope.has_rng(na
+0000bd80: 6d65 290a 0a20 2064 6566 206d 616b 655f  me)..  def make_
+0000bd90: 726e 6728 7365 6c66 2c20 6e61 6d65 3a20  rng(self, name: 
+0000bda0: 7374 7229 202d 3e20 4b65 7941 7272 6179  str) -> KeyArray
+0000bdb0: 3a0a 2020 2020 2222 2252 6574 7572 6e73  :.    """Returns
+0000bdc0: 2061 206e 6577 2052 4e47 206b 6579 2066   a new RNG key f
+0000bdd0: 726f 6d20 6120 6769 7665 6e20 524e 4720  rom a given RNG 
+0000bde0: 7365 7175 656e 6365 2066 6f72 2074 6869  sequence for thi
+0000bdf0: 7320 4d6f 6475 6c65 2e0a 0a20 2020 2054  s Module...    T
+0000be00: 6865 206e 6577 2052 4e47 206b 6579 2069  he new RNG key i
+0000be10: 7320 7370 6c69 7420 6672 6f6d 2074 6865  s split from the
+0000be20: 2070 7265 7669 6f75 7320 6f6e 652e 2054   previous one. T
+0000be30: 6875 732c 2065 7665 7279 2063 616c 6c20  hus, every call 
+0000be40: 746f 0a20 2020 2060 6d61 6b65 5f72 6e67  to.    `make_rng
+0000be50: 6020 7265 7475 726e 7320 6120 6e65 7720  ` returns a new 
+0000be60: 524e 4720 6b65 792c 2077 6869 6c65 2073  RNG key, while s
+0000be70: 7469 6c6c 2067 7561 7261 6e74 6565 696e  till guaranteein
+0000be80: 6720 6675 6c6c 0a20 2020 2072 6570 726f  g full.    repro
+0000be90: 6475 6369 6269 6c69 7479 2e0a 0a20 2020  ducibility...   
+0000bea0: 2054 4f44 4f3a 204c 696e 6b20 746f 2046   TODO: Link to F
+0000beb0: 6c61 7820 524e 4720 6465 7369 676e 206e  lax RNG design n
+0000bec0: 6f74 652e 0a0a 2020 2020 4172 6773 3a0a  ote...    Args:.
+0000bed0: 2020 2020 2020 6e61 6d65 3a20 5468 6520        name: The 
+0000bee0: 524e 4720 7365 7175 656e 6365 206e 616d  RNG sequence nam
+0000bef0: 652e 0a20 2020 2052 6574 7572 6e73 3a0a  e..    Returns:.
+0000bf00: 2020 2020 2020 5468 6520 6e65 776c 7920        The newly 
+0000bf10: 6765 6e65 7261 7465 6420 524e 4720 6b65  generated RNG ke
+0000bf20: 792e 0a20 2020 2022 2222 0a20 2020 2069  y..    """.    i
+0000bf30: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
+0000bf40: 4e6f 6e65 3a0a 2020 2020 2020 7261 6973  None:.      rais
+0000bf50: 6520 5661 6c75 6545 7272 6f72 2822 4361  e ValueError("Ca
+0000bf60: 6e27 7420 7573 6520 524e 4773 206f 6e20  n't use RNGs on 
+0000bf70: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
+0000bf80: 290a 2020 2020 7265 7475 726e 2073 656c  ).    return sel
+0000bf90: 662e 7363 6f70 652e 6d61 6b65 5f72 6e67  f.scope.make_rng
+0000bfa0: 286e 616d 6529 0a0a 2020 6465 6620 6973  (name)..  def is
+0000bfb0: 5f69 6e69 7469 616c 697a 696e 6728 7365  _initializing(se
+0000bfc0: 6c66 2920 2d3e 2062 6f6f 6c3a 0a20 2020  lf) -> bool:.   
+0000bfd0: 2022 2222 5265 7475 726e 7320 5472 7565   """Returns True
+0000bfe0: 2069 6620 7275 6e6e 696e 6720 756e 6465   if running unde
+0000bff0: 7220 7365 6c66 2e69 6e69 7428 2e2e 2e29  r self.init(...)
+0000c000: 206f 7220 6e6e 2e69 6e69 7428 2e2e 2e29   or nn.init(...)
+0000c010: 2829 2e0a 0a20 2020 2054 6869 7320 6973  ()...    This is
+0000c020: 2061 2068 656c 7065 7220 6d65 7468 6f64   a helper method
+0000c030: 2074 6f20 6861 6e64 6c65 2074 6865 2063   to handle the c
+0000c040: 6f6d 6d6f 6e20 6361 7365 206f 6620 7369  ommon case of si
+0000c050: 6d70 6c65 2069 6e69 7469 616c 697a 6174  mple initializat
+0000c060: 696f 6e0a 2020 2020 7768 6572 6520 7765  ion.    where we
+0000c070: 2077 6973 6820 746f 2068 6176 6520 7365   wish to have se
+0000c080: 7475 7020 6c6f 6769 6320 6f63 6375 7220  tup logic occur 
+0000c090: 7768 656e 206f 6e6c 7920 6361 6c6c 6564  when only called
+0000c0a0: 2075 6e64 6572 0a20 2020 2060 606d 6f64   under.    ``mod
+0000c0b0: 756c 652e 696e 6974 6060 206f 7220 6060  ule.init`` or ``
+0000c0c0: 6e6e 2e69 6e69 7460 602e 2020 466f 7220  nn.init``.  For 
+0000c0d0: 6d6f 7265 2063 6f6d 706c 6963 6174 6564  more complicated
+0000c0e0: 206d 756c 7469 2d70 6861 7365 0a20 2020   multi-phase.   
+0000c0f0: 2069 6e69 7469 616c 697a 6174 696f 6e20   initialization 
+0000c100: 7363 656e 6172 696f 7320 6974 2069 7320  scenarios it is 
+0000c110: 6265 7474 6572 2074 6f20 7465 7374 2066  better to test f
+0000c120: 6f72 2074 6865 206d 7574 6162 696c 6974  or the mutabilit
+0000c130: 7920 6f66 0a20 2020 2070 6172 7469 6375  y of.    particu
+0000c140: 6c61 7220 7661 7269 6162 6c65 2063 6f6c  lar variable col
+0000c150: 6c65 6374 696f 6e73 206f 7220 666f 7220  lections or for 
+0000c160: 7468 6520 7072 6573 656e 6365 206f 6620  the presence of 
+0000c170: 7061 7274 6963 756c 6172 0a20 2020 2076  particular.    v
+0000c180: 6172 6961 626c 6573 2074 6861 7420 706f  ariables that po
+0000c190: 7465 6e74 6961 6c6c 7920 6e65 6564 2074  tentially need t
+0000c1a0: 6f20 6265 2069 6e69 7469 616c 697a 6564  o be initialized
+0000c1b0: 2e0a 2020 2020 2222 220a 2020 2020 6966  ..    """.    if
+0000c1c0: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
+0000c1d0: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
+0000c1e0: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
+0000c1f0: 2774 2063 6865 636b 2069 6620 7275 6e6e  't check if runn
+0000c200: 696e 6720 756e 6465 7220 696e 6974 2829  ing under init()
+0000c210: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
+0000c220: 6c65 7322 290a 2020 2020 7265 7475 726e  les").    return
+0000c230: 2073 656c 662e 7363 6f70 652e 6765 745f   self.scope.get_
+0000c240: 666c 6167 2827 696e 6974 6961 6c69 7a69  flag('initializi
+0000c250: 6e67 272c 2046 616c 7365 290a 0a20 2064  ng', False)..  d
+0000c260: 6566 205f 6d6f 6475 6c65 5f63 6865 636b  ef _module_check
+0000c270: 7328 7365 6c66 293a 0a20 2020 2022 2222  s(self):.    """
+0000c280: 5275 6e20 7374 616e 6461 7264 2072 756e  Run standard run
+0000c290: 7469 6d65 2063 6865 636b 732e 2222 220a  time checks.""".
+0000c2a0: 0a20 2020 2069 6620 6e6f 7420 6973 696e  .    if not isin
+0000c2b0: 7374 616e 6365 2873 656c 662c 204d 6f64  stance(self, Mod
+0000c2c0: 756c 6529 3a0a 2020 2020 2020 7261 6973  ule):.      rais
+0000c2d0: 6520 6572 726f 7273 2e49 6e76 616c 6964  e errors.Invalid
+0000c2e0: 496e 7374 616e 6365 4d6f 6475 6c65 4572  InstanceModuleEr
+0000c2f0: 726f 7228 290a 0a20 2020 206f 7665 7272  ror()..    overr
+0000c300: 6964 6465 6e5f 706f 7374 5f69 6e69 7420  idden_post_init 
+0000c310: 3d20 7365 6c66 2e5f 5f70 6f73 745f 696e  = self.__post_in
+0000c320: 6974 5f5f 2021 3d20 4d6f 6475 6c65 2e5f  it__ != Module._
+0000c330: 5f70 6f73 745f 696e 6974 5f5f 0a20 2020  _post_init__.   
+0000c340: 2069 6620 6f76 6572 7269 6464 656e 5f70   if overridden_p
+0000c350: 6f73 745f 696e 6974 2061 6e64 206e 6f74  ost_init and not
+0000c360: 2068 6173 6174 7472 2873 656c 662c 2022   hasattr(self, "
+0000c370: 5f69 6422 293a 0a20 2020 2020 2072 6169  _id"):.      rai
+0000c380: 7365 2065 7272 6f72 732e 496e 636f 7272  se errors.Incorr
+0000c390: 6563 7450 6f73 7449 6e69 744f 7665 7272  ectPostInitOverr
+0000c3a0: 6964 6545 7272 6f72 2829 0a0a 2020 4074  ideError()..  @t
+0000c3b0: 7261 6365 6261 636b 5f75 7469 6c2e 6170  raceback_util.ap
+0000c3c0: 695f 626f 756e 6461 7279 0a20 2064 6566  i_boundary.  def
+0000c3d0: 2062 696e 6428 7365 6c66 3a20 4d2c 0a20   bind(self: M,. 
+0000c3e0: 2020 2020 2020 2020 2020 7661 7269 6162            variab
+0000c3f0: 6c65 733a 2056 6172 6961 626c 6544 6963  les: VariableDic
+0000c400: 742c 0a20 2020 2020 2020 2020 2020 2a61  t,.           *a
+0000c410: 7267 732c 0a20 2020 2020 2020 2020 2020  rgs,.           
+0000c420: 726e 6773 3a20 4f70 7469 6f6e 616c 5b52  rngs: Optional[R
+0000c430: 4e47 5365 7175 656e 6365 735d 203d 204e  NGSequences] = N
+0000c440: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
+0000c450: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
+0000c460: 696f 6e46 696c 7465 7220 3d20 4661 6c73  ionFilter = Fals
+0000c470: 6529 202d 3e20 4d3a 0a20 2020 2022 2222  e) -> M:.    """
+0000c480: 4372 6561 7465 7320 616e 2069 6e74 6572  Creates an inter
+0000c490: 6163 7469 7665 204d 6f64 756c 6520 696e  active Module in
+0000c4a0: 7374 616e 6365 2062 7920 6269 6e64 696e  stance by bindin
+0000c4b0: 6720 7661 7269 6162 6c65 7320 616e 6420  g variables and 
+0000c4c0: 524e 4773 2e0a 0a20 2020 2060 6062 696e  RNGs...    ``bin
+0000c4d0: 6460 6020 7072 6f76 6964 6573 2061 6e20  d`` provides an 
+0000c4e0: 2269 6e74 6572 6163 7469 7665 2220 696e  "interactive" in
+0000c4f0: 7374 616e 6365 206f 6620 6120 4d6f 6475  stance of a Modu
+0000c500: 6c65 2064 6972 6563 746c 7920 7769 7468  le directly with
+0000c510: 6f75 740a 2020 2020 7472 616e 7366 6f72  out.    transfor
+0000c520: 6d69 6e67 2061 2066 756e 6374 696f 6e20  ming a function 
+0000c530: 7769 7468 2060 6061 7070 6c79 6060 2e20  with ``apply``. 
+0000c540: 5468 6973 2069 7320 7061 7274 6963 756c  This is particul
+0000c550: 6172 6c79 2075 7365 6675 6c20 666f 720a  arly useful for.
+0000c560: 2020 2020 6465 6275 6767 696e 6720 616e      debugging an
+0000c570: 6420 696e 7465 7261 6374 6976 6520 7573  d interactive us
+0000c580: 6520 6361 7365 7320 6c69 6b65 206e 6f74  e cases like not
+0000c590: 6562 6f6f 6b73 2077 6865 7265 2061 2066  ebooks where a f
+0000c5a0: 756e 6374 696f 6e20 776f 756c 640a 2020  unction would.  
+0000c5b0: 2020 6c69 6d69 7420 7468 6520 6162 696c    limit the abil
+0000c5c0: 6974 7920 746f 2073 706c 6974 2075 7020  ity to split up 
+0000c5d0: 636f 6465 2069 6e74 6f20 6469 6666 6572  code into differ
+0000c5e0: 656e 7420 6365 6c6c 732e 0a0a 2020 2020  ent cells...    
+0000c5f0: 4f6e 6365 2074 6865 2076 6172 6961 626c  Once the variabl
+0000c600: 6573 2028 616e 6420 6f70 7469 6f6e 616c  es (and optional
+0000c610: 6c79 2052 4e47 7329 2061 7265 2062 6f75  ly RNGs) are bou
+0000c620: 6e64 2074 6f20 6120 6060 4d6f 6475 6c65  nd to a ``Module
+0000c630: 6060 2069 740a 2020 2020 6265 636f 6d65  `` it.    become
+0000c640: 7320 6120 7374 6174 6566 756c 206f 626a  s a stateful obj
+0000c650: 6563 742e 204e 6f74 6520 7468 6174 2069  ect. Note that i
+0000c660: 6469 6f6d 6174 6963 204a 4158 2069 7320  diomatic JAX is 
+0000c670: 6675 6e63 7469 6f6e 616c 2061 6e64 0a20  functional and. 
+0000c680: 2020 2074 6865 7265 666f 7265 2061 6e20     therefore an 
+0000c690: 696e 7465 7261 6374 6976 6520 696e 7374  interactive inst
+0000c6a0: 616e 6365 2064 6f65 7320 6e6f 7420 6d69  ance does not mi
+0000c6b0: 7820 7765 6c6c 2077 6974 6820 7661 6e69  x well with vani
+0000c6c0: 6c6c 6120 4a41 5820 4150 4973 2e0a 2020  lla JAX APIs..  
+0000c6d0: 2020 6060 6269 6e64 2829 6060 2073 686f    ``bind()`` sho
+0000c6e0: 756c 6420 6f6e 6c79 2062 6520 7573 6564  uld only be used
+0000c6f0: 2066 6f72 2069 6e74 6572 6163 7469 7665   for interactive
+0000c700: 2065 7870 6572 696d 656e 7461 7469 6f6e   experimentation
+0000c710: 2c20 616e 6420 696e 2061 6c6c 0a20 2020  , and in all.   
+0000c720: 206f 7468 6572 2063 6173 6573 2077 6520   other cases we 
+0000c730: 7374 726f 6e67 6c79 2065 6e63 6f75 7261  strongly encoura
+0000c740: 6765 2075 7365 7273 2074 6f20 7573 6520  ge users to use 
+0000c750: 6060 6170 706c 7928 2960 6020 696e 7374  ``apply()`` inst
+0000c760: 6561 642e 0a0a 2020 2020 4578 616d 706c  ead...    Exampl
+0000c770: 653a 3a0a 0a20 2020 2020 2069 6d70 6f72  e::..      impor
+0000c780: 7420 6a61 780a 2020 2020 2020 696d 706f  t jax.      impo
+0000c790: 7274 206a 6178 2e6e 756d 7079 2061 7320  rt jax.numpy as 
+0000c7a0: 6a6e 700a 2020 2020 2020 696d 706f 7274  jnp.      import
+0000c7b0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+0000c7c0: 6e0a 0a20 2020 2020 2063 6c61 7373 2041  n..      class A
+0000c7d0: 7574 6f45 6e63 6f64 6572 286e 6e2e 4d6f  utoEncoder(nn.Mo
+0000c7e0: 6475 6c65 293a 0a20 2020 2020 2020 2064  dule):.        d
+0000c7f0: 6566 2073 6574 7570 2873 656c 6629 3a0a  ef setup(self):.
+0000c800: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
+0000c810: 6e63 6f64 6572 203d 206e 6e2e 4465 6e73  ncoder = nn.Dens
+0000c820: 6528 3329 0a20 2020 2020 2020 2020 2073  e(3).          s
+0000c830: 656c 662e 6465 636f 6465 7220 3d20 6e6e  elf.decoder = nn
+0000c840: 2e44 656e 7365 2835 290a 0a20 2020 2020  .Dense(5)..     
+0000c850: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0000c860: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
+0000c870: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+0000c880: 6465 636f 6465 7228 7365 6c66 2e65 6e63  decoder(self.enc
+0000c890: 6f64 6572 2878 2929 0a0a 2020 2020 2020  oder(x))..      
+0000c8a0: 7820 3d20 6a6e 702e 6f6e 6573 2828 3136  x = jnp.ones((16
+0000c8b0: 2c20 3929 290a 2020 2020 2020 6165 203d  , 9)).      ae =
+0000c8c0: 2041 7574 6f45 6e63 6f64 6572 2829 0a20   AutoEncoder(). 
+0000c8d0: 2020 2020 2076 6172 6961 626c 6573 203d       variables =
+0000c8e0: 2061 652e 696e 6974 286a 6178 2e72 616e   ae.init(jax.ran
+0000c8f0: 646f 6d2e 5052 4e47 4b65 7928 3029 2c20  dom.PRNGKey(0), 
+0000c900: 7829 0a20 2020 2020 206d 6f64 656c 203d  x).      model =
+0000c910: 2061 652e 6269 6e64 2876 6172 6961 626c   ae.bind(variabl
+0000c920: 6573 290a 2020 2020 2020 7a20 3d20 6d6f  es).      z = mo
+0000c930: 6465 6c2e 656e 636f 6465 7228 7829 0a20  del.encoder(x). 
+0000c940: 2020 2020 2078 5f72 6563 6f6e 7374 7275       x_reconstru
+0000c950: 6374 6564 203d 206d 6f64 656c 2e64 6563  cted = model.dec
+0000c960: 6f64 6572 287a 290a 0a20 2020 2041 7267  oder(z)..    Arg
+0000c970: 733a 0a20 2020 2020 2076 6172 6961 626c  s:.      variabl
+0000c980: 6573 3a20 4120 6469 6374 696f 6e61 7279  es: A dictionary
+0000c990: 2063 6f6e 7461 696e 696e 6720 7661 7269   containing vari
+0000c9a0: 6162 6c65 7320 6b65 7965 6420 6279 2076  ables keyed by v
+0000c9b0: 6172 6961 626c 650a 2020 2020 2020 2020  ariable.        
+0000c9c0: 636f 6c6c 6563 7469 6f6e 732e 2053 6565  collections. See
+0000c9d0: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
+0000c9e0: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
+0000c9f0: 6d6f 7265 2064 6574 6169 6c73 0a20 2020  more details.   
+0000ca00: 2020 2020 2061 626f 7574 2076 6172 6961       about varia
+0000ca10: 626c 6573 2e0a 2020 2020 2020 2a61 7267  bles..      *arg
+0000ca20: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
+0000ca30: 7473 2028 6e6f 7420 7573 6564 292e 0a20  ts (not used).. 
+0000ca40: 2020 2020 2072 6e67 733a 2061 2064 6963       rngs: a dic
+0000ca50: 7420 6f66 2050 524e 474b 6579 7320 746f  t of PRNGKeys to
+0000ca60: 2069 6e69 7469 616c 697a 6520 7468 6520   initialize the 
+0000ca70: 5052 4e47 2073 6571 7565 6e63 6573 2e0a  PRNG sequences..
+0000ca80: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
+0000ca90: 616e 2062 6520 626f 6f6c 2c20 7374 722c  an be bool, str,
+0000caa0: 206f 7220 6c69 7374 2e20 5370 6563 6966   or list. Specif
+0000cab0: 6965 7320 7768 6963 6820 636f 6c6c 6563  ies which collec
+0000cac0: 7469 6f6e 7320 7368 6f75 6c64 2062 650a  tions should be.
+0000cad0: 2020 2020 2020 2020 7472 6561 7465 6420          treated 
+0000cae0: 6173 206d 7574 6162 6c65 3a0a 2020 2020  as mutable:.    
+0000caf0: 2020 2020 2020 6060 626f 6f6c 6060 3a20        ``bool``: 
+0000cb00: 616c 6c2f 6e6f 2063 6f6c 6c65 6374 696f  all/no collectio
+0000cb10: 6e73 2061 7265 206d 7574 6162 6c65 2e0a  ns are mutable..
+0000cb20: 2020 2020 2020 2020 2020 6060 7374 7260            ``str`
+0000cb30: 603a 2054 6865 206e 616d 6520 6f66 2061  `: The name of a
+0000cb40: 2073 696e 676c 6520 6d75 7461 626c 6520   single mutable 
+0000cb50: 636f 6c6c 6563 7469 6f6e 2e0a 2020 2020  collection..    
+0000cb60: 2020 2020 2020 6060 6c69 7374 6060 3a20        ``list``: 
+0000cb70: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
+0000cb80: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
+0000cb90: 6374 696f 6e73 2e0a 0a20 2020 2052 6574  ctions...    Ret
+0000cba0: 7572 6e73 3a0a 2020 2020 2020 4120 636f  urns:.      A co
+0000cbb0: 7079 206f 6620 7468 6973 2069 6e73 7461  py of this insta
+0000cbc0: 6e63 6520 7769 7468 2062 6f75 6e64 2076  nce with bound v
+0000cbd0: 6172 6961 626c 6573 2061 6e64 2052 4e47  ariables and RNG
+0000cbe0: 732e 0a20 2020 2022 2222 0a20 2020 204d  s..    """.    M
+0000cbf0: 6f64 756c 652e 5f6d 6f64 756c 655f 6368  odule._module_ch
+0000cc00: 6563 6b73 2873 656c 6629 0a0a 2020 2020  ecks(self)..    
+0000cc10: 6465 6c20 6172 6773 0a20 2020 2073 636f  del args.    sco
+0000cc20: 7065 203d 2063 6f72 652e 6269 6e64 2876  pe = core.bind(v
+0000cc30: 6172 6961 626c 6573 2c20 726e 6773 3d72  ariables, rngs=r
+0000cc40: 6e67 732c 206d 7574 6162 6c65 3d6d 7574  ngs, mutable=mut
+0000cc50: 6162 6c65 290a 2020 2020 7265 7475 726e  able).    return
+0000cc60: 2073 656c 662e 636c 6f6e 6528 7061 7265   self.clone(pare
+0000cc70: 6e74 3d73 636f 7065 2c20 5f64 6565 705f  nt=scope, _deep_
+0000cc80: 636c 6f6e 653d 5472 7565 290a 0a20 2064  clone=True)..  d
+0000cc90: 6566 2075 6e62 696e 6428 7365 6c66 3a20  ef unbind(self: 
+0000cca0: 4d29 202d 3e20 5475 706c 655b 4d2c 2056  M) -> Tuple[M, V
+0000ccb0: 6172 6961 626c 6544 6963 745d 3a0a 2020  ariableDict]:.  
+0000ccc0: 2020 2222 2252 6574 7572 6e73 2061 6e20    """Returns an 
+0000ccd0: 756e 626f 756e 6420 636f 7079 206f 6620  unbound copy of 
+0000cce0: 6120 4d6f 6475 6c65 2061 6e64 2069 7473  a Module and its
+0000ccf0: 2076 6172 6961 626c 6573 2e0a 0a20 2020   variables...   
+0000cd00: 2060 6075 6e62 696e 6460 6020 6865 6c70   ``unbind`` help
+0000cd10: 7320 6372 6561 7465 2061 2073 7461 7465  s create a state
+0000cd20: 6c65 7373 2076 6572 7369 6f6e 206f 6620  less version of 
+0000cd30: 6120 626f 756e 6420 4d6f 6475 6c65 2e0a  a bound Module..
+0000cd40: 0a20 2020 2041 6e20 6578 616d 706c 6520  .    An example 
+0000cd50: 6f66 2061 2063 6f6d 6d6f 6e20 7573 6520  of a common use 
+0000cd60: 6361 7365 3a20 746f 2065 7874 7261 6374  case: to extract
+0000cd70: 2061 2073 7562 2d4d 6f64 756c 6520 6465   a sub-Module de
+0000cd80: 6669 6e65 6420 696e 7369 6465 0a20 2020  fined inside.   
+0000cd90: 2060 6073 6574 7570 2829 6060 2061 6e64   ``setup()`` and
+0000cda0: 2069 7473 2063 6f72 7265 7370 6f6e 6469   its correspondi
+0000cdb0: 6e67 2076 6172 6961 626c 6573 3a20 3129  ng variables: 1)
+0000cdc0: 2074 656d 706f 7261 7269 6c79 2060 6062   temporarily ``b
+0000cdd0: 696e 6460 6020 7468 6520 7061 7265 6e74  ind`` the parent
+0000cde0: 0a20 2020 204d 6f64 756c 653b 2061 6e64  .    Module; and
+0000cdf0: 2074 6865 6e20 3229 2060 6075 6e62 696e   then 2) ``unbin
+0000ce00: 6460 6020 7468 6520 6465 7369 7265 6420  d`` the desired 
+0000ce10: 7375 622d 4d6f 6475 6c65 2e20 2852 6563  sub-Module. (Rec
+0000ce20: 616c 6c20 7468 6174 2060 6073 6574 7570  all that ``setup
+0000ce30: 2829 6060 0a20 2020 2069 7320 6f6e 6c79  ()``.    is only
+0000ce40: 2063 616c 6c65 6420 7768 656e 2074 6865   called when the
+0000ce50: 204d 6f64 756c 6520 6973 2062 6f75 6e64   Module is bound
+0000ce60: 2e29 3a3a 0a0a 2020 2020 2020 636c 6173  .)::..      clas
+0000ce70: 7320 4175 746f 456e 636f 6465 7228 6e6e  s AutoEncoder(nn
+0000ce80: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
+0000ce90: 2020 6465 6620 7365 7475 7028 7365 6c66    def setup(self
+0000cea0: 293a 0a20 2020 2020 2020 2020 2073 656c  ):.          sel
+0000ceb0: 662e 656e 636f 6465 7220 3d20 456e 636f  f.encoder = Enco
+0000cec0: 6465 7228 290a 2020 2020 2020 2020 2020  der().          
+0000ced0: 7365 6c66 2e64 6563 6f64 6572 203d 2044  self.decoder = D
+0000cee0: 6563 6f64 6572 2829 0a0a 2020 2020 2020  ecoder()..      
+0000cef0: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
+0000cf00: 656c 662c 2078 293a 0a20 2020 2020 2020  elf, x):.       
+0000cf10: 2020 2072 6574 7572 6e20 7365 6c66 2e64     return self.d
+0000cf20: 6563 6f64 6572 2873 656c 662e 656e 636f  ecoder(self.enco
+0000cf30: 6465 7228 7829 290a 0a20 2020 2020 206d  der(x))..      m
+0000cf40: 6f64 756c 6520 3d20 4175 746f 456e 636f  odule = AutoEnco
+0000cf50: 6465 7228 290a 2020 2020 2020 7661 7269  der().      vari
+0000cf60: 6162 6c65 7320 3d20 6d6f 6475 6c65 2e69  ables = module.i
+0000cf70: 6e69 7428 6a61 782e 7261 6e64 6f6d 2e50  nit(jax.random.P
+0000cf80: 524e 474b 6579 2830 292c 206a 6e70 2e6f  RNGKey(0), jnp.o
+0000cf90: 6e65 7328 2831 2c20 3738 3429 2929 0a20  nes((1, 784))). 
+0000cfa0: 2020 2020 202e 2e2e 0a20 2020 2020 2023       ....      #
+0000cfb0: 2045 7874 7261 6374 2074 6865 2045 6e63   Extract the Enc
+0000cfc0: 6f64 6572 2073 7562 2d4d 6f64 756c 6520  oder sub-Module 
+0000cfd0: 616e 6420 6974 7320 7661 7269 6162 6c65  and its variable
+0000cfe0: 730a 2020 2020 2020 656e 636f 6465 722c  s.      encoder,
+0000cff0: 2065 6e63 6f64 6572 5f76 6172 7320 3d20   encoder_vars = 
+0000d000: 6d6f 6475 6c65 2e62 696e 6428 7661 7269  module.bind(vari
+0000d010: 6162 6c65 7329 2e65 6e63 6f64 6572 2e75  ables).encoder.u
+0000d020: 6e62 696e 6428 290a 0a20 2020 2052 6574  nbind()..    Ret
+0000d030: 7572 6e73 3a0a 2020 2020 2020 4120 7475  urns:.      A tu
+0000d040: 706c 6520 7769 7468 2061 6e20 756e 626f  ple with an unbo
+0000d050: 756e 6420 636f 7079 206f 6620 7468 6973  und copy of this
+0000d060: 204d 6f64 756c 6520 616e 6420 6974 7320   Module and its 
+0000d070: 7661 7269 6162 6c65 732e 0a20 2020 2022  variables..    "
+0000d080: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
+0000d090: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
+0000d0a0: 6629 0a0a 2020 2020 6966 2073 656c 662e  f)..    if self.
+0000d0b0: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
+0000d0c0: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
+0000d0d0: 732e 4361 6c6c 556e 6269 6e64 4f6e 556e  s.CallUnbindOnUn
+0000d0e0: 626f 756e 644d 6f64 756c 6545 7272 6f72  boundModuleError
+0000d0f0: 2829 0a0a 2020 2020 7661 7269 6162 6c65  ()..    variable
+0000d100: 7320 3d20 7365 6c66 2e76 6172 6961 626c  s = self.variabl
+0000d110: 6573 0a20 2020 206d 6f64 756c 6520 3d20  es.    module = 
+0000d120: 7365 6c66 2e63 6c6f 6e65 2829 0a20 2020  self.clone().   
+0000d130: 2072 6574 7572 6e20 6d6f 6475 6c65 2c20   return module, 
+0000d140: 7661 7269 6162 6c65 730a 0a20 2040 7472  variables..  @tr
+0000d150: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
+0000d160: 5f62 6f75 6e64 6172 790a 2020 6465 6620  _boundary.  def 
+0000d170: 6170 706c 7928 7365 6c66 2c0a 2020 2020  apply(self,.    
+0000d180: 2020 2020 2020 2020 7661 7269 6162 6c65          variable
+0000d190: 733a 2056 6172 6961 626c 6544 6963 742c  s: VariableDict,
+0000d1a0: 0a20 2020 2020 2020 2020 2020 202a 6172  .            *ar
+0000d1b0: 6773 2c0a 2020 2020 2020 2020 2020 2020  gs,.            
+0000d1c0: 726e 6773 3a20 4f70 7469 6f6e 616c 5b52  rngs: Optional[R
+0000d1d0: 4e47 5365 7175 656e 6365 735d 203d 204e  NGSequences] = N
+0000d1e0: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
+0000d1f0: 206d 6574 686f 643a 2055 6e69 6f6e 5b43   method: Union[C
+0000d200: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+0000d210: 5d2c 2073 7472 2c20 4e6f 6e65 5d20 3d20  ], str, None] = 
+0000d220: 4e6f 6e65 2c0a 2020 2020 2020 2020 2020  None,.          
+0000d230: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+0000d240: 6374 696f 6e46 696c 7465 7220 3d20 4661  ctionFilter = Fa
+0000d250: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
+0000d260: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
+0000d270: 6469 6174 6573 3a20 556e 696f 6e5b 626f  diates: Union[bo
+0000d280: 6f6c 2c20 4361 6c6c 6162 6c65 5b5b 274d  ol, Callable[['M
+0000d290: 6f64 756c 6527 2c20 7374 725d 2c20 626f  odule', str], bo
+0000d2a0: 6f6c 5d5d 203d 2046 616c 7365 2c0a 2020  ol]] = False,.  
+0000d2b0: 2020 2020 2020 2020 2020 2a2a 6b77 6172            **kwar
+0000d2c0: 6773 2920 2d3e 2055 6e69 6f6e 5b41 6e79  gs) -> Union[Any
+0000d2d0: 2c20 5475 706c 655b 416e 792c 2055 6e69  , Tuple[Any, Uni
+0000d2e0: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
+0000d2f0: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
+0000d300: 2041 6e79 5d5d 5d5d 3a0a 2020 2020 2222   Any]]]]:.    ""
+0000d310: 2241 7070 6c69 6573 2061 206d 6f64 756c  "Applies a modul
+0000d320: 6520 6d65 7468 6f64 2074 6f20 7661 7269  e method to vari
+0000d330: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
+0000d340: 7320 6f75 7470 7574 2061 6e64 206d 6f64  s output and mod
+0000d350: 6966 6965 6420 7661 7269 6162 6c65 732e  ified variables.
+0000d360: 0a0a 2020 2020 4e6f 7465 2074 6861 7420  ..    Note that 
+0000d370: 606d 6574 686f 6460 2073 686f 756c 6420  `method` should 
+0000d380: 6265 2073 6574 2069 6620 6f6e 6520 776f  be set if one wo
+0000d390: 756c 6420 6c69 6b65 2074 6f20 6361 6c6c  uld like to call
+0000d3a0: 2060 6170 706c 7960 206f 6e20 610a 2020   `apply` on a.  
+0000d3b0: 2020 6469 6666 6572 656e 7420 636c 6173    different clas
+0000d3c0: 7320 6d65 7468 6f64 2074 6861 6e20 6060  s method than ``
+0000d3d0: 5f5f 6361 6c6c 5f5f 6060 2e20 466f 7220  __call__``. For 
+0000d3e0: 696e 7374 616e 6365 2c20 7375 7070 6f73  instance, suppos
+0000d3f0: 6520 610a 2020 2020 5472 616e 7366 6f72  e a.    Transfor
+0000d400: 6d65 7220 6d6f 6475 6c65 7320 6861 7320  mer modules has 
+0000d410: 6120 6d65 7468 6f64 2063 616c 6c65 6420  a method called 
+0000d420: 6065 6e63 6f64 6560 2c20 7468 656e 2074  `encode`, then t
+0000d430: 6865 2066 6f6c 6c6f 7769 6e67 2063 616c  he following cal
+0000d440: 6c73 0a20 2020 2060 6170 706c 7960 206f  ls.    `apply` o
+0000d450: 6e20 7468 6174 206d 6574 686f 643a 3a0a  n that method::.
+0000d460: 0a20 2020 2020 206d 6f64 656c 203d 2054  .      model = T
+0000d470: 7261 6e73 666f 726d 6572 2829 0a20 2020  ransformer().   
+0000d480: 2020 2065 6e63 6f64 6564 203d 206d 6f64     encoded = mod
+0000d490: 656c 2e61 7070 6c79 287b 2770 6172 616d  el.apply({'param
+0000d4a0: 7327 3a20 7061 7261 6d73 7d2c 2078 2c20  s': params}, x, 
+0000d4b0: 6d65 7468 6f64 3d54 7261 6e73 666f 726d  method=Transform
+0000d4c0: 6572 2e65 6e63 6f64 6529 0a0a 2020 2020  er.encode)..    
+0000d4d0: 4966 2061 2066 756e 6374 696f 6e20 696e  If a function in
+0000d4e0: 7374 616e 6365 2069 7320 7072 6f76 6964  stance is provid
+0000d4f0: 6564 2c20 7468 6520 756e 626f 756e 6420  ed, the unbound 
+0000d500: 6675 6e63 7469 6f6e 2069 7320 7573 6564  function is used
+0000d510: 2e20 466f 720a 2020 2020 696e 7374 616e  . For.    instan
+0000d520: 6365 2c20 7468 6520 6578 616d 706c 6520  ce, the example 
+0000d530: 6265 6c6f 7720 6973 2065 7175 6976 616c  below is equival
+0000d540: 656e 7420 746f 2074 6865 206f 6e65 2061  ent to the one a
+0000d550: 626f 7665 3a3a 0a0a 2020 2020 2020 656e  bove::..      en
+0000d560: 636f 6465 6420 3d20 6d6f 6465 6c2e 6170  coded = model.ap
+0000d570: 706c 7928 7b27 7061 7261 6d73 273a 2070  ply({'params': p
+0000d580: 6172 616d 737d 2c20 782c 206d 6574 686f  arams}, x, metho
+0000d590: 643d 6d6f 6465 6c2e 656e 636f 6465 290a  d=model.encode).
+0000d5a0: 0a20 2020 2059 6f75 2063 616e 2061 6c73  .    You can als
+0000d5b0: 6f20 7061 7373 2061 2073 7472 696e 6720  o pass a string 
+0000d5c0: 746f 2061 2063 616c 6c61 626c 6520 6174  to a callable at
+0000d5d0: 7472 6962 7574 6520 6f66 2074 6865 206d  tribute of the m
+0000d5e0: 6f64 756c 652e 2046 6f72 0a20 2020 2065  odule. For.    e
+0000d5f0: 7861 6d70 6c65 2c20 7468 6520 7072 6576  xample, the prev
+0000d600: 696f 7573 2063 616e 2062 6520 7772 6974  ious can be writ
+0000d610: 7465 6e20 6173 3a3a 0a0a 2020 2020 2020  ten as::..      
+0000d620: 656e 636f 6465 6420 3d20 6d6f 6465 6c2e  encoded = model.
+0000d630: 6170 706c 7928 7b27 7061 7261 6d73 273a  apply({'params':
+0000d640: 2070 6172 616d 737d 2c20 782c 206d 6574   params}, x, met
+0000d650: 686f 643d 2765 6e63 6f64 6527 290a 0a20  hod='encode').. 
+0000d660: 2020 204e 6f74 6520 6060 6d65 7468 6f64     Note ``method
+0000d670: 6060 2063 616e 2061 6c73 6f20 6265 2061  `` can also be a
+0000d680: 2066 756e 6374 696f 6e20 7468 6174 2069   function that i
+0000d690: 7320 6e6f 7420 6465 6669 6e65 6420 696e  s not defined in
+0000d6a0: 0a20 2020 2060 6054 7261 6e73 666f 726d  .    ``Transform
+0000d6b0: 6572 6060 2e20 496e 2074 6861 7420 6361  er``. In that ca
+0000d6c0: 7365 2c20 7468 6520 6675 6e63 7469 6f6e  se, the function
+0000d6d0: 2073 686f 756c 6420 6861 7665 2061 7420   should have at 
+0000d6e0: 6c65 6173 7420 6f6e 650a 2020 2020 6172  least one.    ar
+0000d6f0: 6775 6d65 6e74 2072 6570 7265 7365 6e74  gument represent
+0000d700: 696e 6720 616e 2069 6e73 7461 6e63 6520  ing an instance 
+0000d710: 6f66 2074 6865 204d 6f64 756c 6520 636c  of the Module cl
+0000d720: 6173 733a 3a0a 0a20 2020 2020 2064 6566  ass::..      def
+0000d730: 206f 7468 6572 5f66 6e28 696e 7374 616e   other_fn(instan
+0000d740: 6365 2c20 2e2e 2e29 3a0a 2020 2020 2020  ce, ...):.      
+0000d750: 2020 696e 7374 616e 6365 2e73 6f6d 655f    instance.some_
+0000d760: 6d6f 6475 6c65 5f61 7474 7228 2e2e 2e29  module_attr(...)
+0000d770: 0a20 2020 2020 2020 202e 2e2e 0a0a 2020  .        .....  
+0000d780: 2020 2020 6d6f 6465 6c2e 6170 706c 7928      model.apply(
+0000d790: 7b27 7061 7261 6d73 273a 2070 6172 616d  {'params': param
+0000d7a0: 737d 2c20 782c 206d 6574 686f 643d 6f74  s}, x, method=ot
+0000d7b0: 6865 725f 666e 290a 0a20 2020 2041 7267  her_fn)..    Arg
+0000d7c0: 733a 0a20 2020 2020 2076 6172 6961 626c  s:.      variabl
+0000d7d0: 6573 3a20 4120 6469 6374 696f 6e61 7279  es: A dictionary
+0000d7e0: 2063 6f6e 7461 696e 696e 6720 7661 7269   containing vari
+0000d7f0: 6162 6c65 7320 6b65 7965 6420 6279 2076  ables keyed by v
+0000d800: 6172 6961 626c 650a 2020 2020 2020 2020  ariable.        
+0000d810: 636f 6c6c 6563 7469 6f6e 732e 2053 6565  collections. See
+0000d820: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
+0000d830: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
+0000d840: 6d6f 7265 2064 6574 6169 6c73 0a20 2020  more details.   
+0000d850: 2020 2020 2061 626f 7574 2076 6172 6961       about varia
+0000d860: 626c 6573 2e0a 2020 2020 2020 2a61 7267  bles..      *arg
+0000d870: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
+0000d880: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
+0000d890: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
+0000d8a0: 206d 6574 686f 642e 0a20 2020 2020 2072   method..      r
+0000d8b0: 6e67 733a 2061 2064 6963 7420 6f66 2050  ngs: a dict of P
+0000d8c0: 524e 474b 6579 7320 746f 2069 6e69 7469  RNGKeys to initi
+0000d8d0: 616c 697a 6520 7468 6520 5052 4e47 2073  alize the PRNG s
+0000d8e0: 6571 7565 6e63 6573 2e0a 2020 2020 2020  equences..      
+0000d8f0: 2020 5468 6520 2270 6172 616d 7322 2050    The "params" P
+0000d900: 524e 4720 7365 7175 656e 6365 2069 7320  RNG sequence is 
+0000d910: 7573 6564 2074 6f20 696e 6974 6961 6c69  used to initiali
+0000d920: 7a65 2070 6172 616d 6574 6572 732e 0a20  ze parameters.. 
+0000d930: 2020 2020 206d 6574 686f 643a 2041 2066       method: A f
+0000d940: 756e 6374 696f 6e20 746f 2063 616c 6c20  unction to call 
+0000d950: 6170 706c 7920 6f6e 2e20 5468 6973 2069  apply on. This i
+0000d960: 7320 6765 6e65 7261 6c6c 7920 6120 6675  s generally a fu
+0000d970: 6e63 7469 6f6e 2069 6e20 7468 650a 2020  nction in the.  
+0000d980: 2020 2020 2020 6d6f 6475 6c65 2e20 4966        module. If
+0000d990: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
+0000d9a0: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
+0000d9b0: 4966 206e 6f74 2070 726f 7669 6465 642c  If not provided,
+0000d9c0: 2061 7070 6c69 6573 2074 6865 0a20 2020   applies the.   
+0000d9d0: 2020 2020 2060 605f 5f63 616c 6c5f 5f60       ``__call__`
+0000d9e0: 6020 6d65 7468 6f64 206f 6620 7468 6520  ` method of the 
+0000d9f0: 6d6f 6475 6c65 2e20 4120 7374 7269 6e67  module. A string
+0000da00: 2063 616e 2061 6c73 6f20 6265 2070 726f   can also be pro
+0000da10: 7669 6465 6420 746f 0a20 2020 2020 2020  vided to.       
+0000da20: 2073 7065 6369 6679 2061 206d 6574 686f   specify a metho
+0000da30: 6420 6279 206e 616d 652e 0a20 2020 2020  d by name..     
+0000da40: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
+0000da50: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
+0000da60: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
+0000da70: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
+0000da80: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
+0000da90: 2020 2020 2020 2020 2020 7472 6561 7465            treate
+0000daa0: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
+0000dab0: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
+0000dac0: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
+0000dad0: 7574 6162 6c65 2e0a 2020 2020 2020 2020  utable..        
+0000dae0: 2020 2020 2020 2060 6073 7472 6060 3a20         ``str``: 
+0000daf0: 5468 6520 6e61 6d65 206f 6620 6120 7369  The name of a si
+0000db00: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
+0000db10: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
+0000db20: 603a 2041 0a20 2020 2020 2020 2020 2020  `: A.           
+0000db30: 2020 2020 6c69 7374 206f 6620 6e61 6d65      list of name
+0000db40: 7320 6f66 206d 7574 6162 6c65 2063 6f6c  s of mutable col
+0000db50: 6c65 6374 696f 6e73 2e0a 2020 2020 2020  lections..      
+0000db60: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+0000db70: 6961 7465 733a 2049 6620 6054 7275 6560  iates: If `True`
+0000db80: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
+0000db90: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
+0000dba0: 616c 7565 730a 2020 2020 2020 2020 6f66  alues.        of
+0000dbb0: 2061 6c6c 204d 6f64 756c 6573 2069 6e73   all Modules ins
+0000dbc0: 6964 6520 7468 6520 2269 6e74 6572 6d65  ide the "interme
+0000dbd0: 6469 6174 6573 2220 636f 6c6c 6563 7469  diates" collecti
+0000dbe0: 6f6e 2e20 4279 2064 6566 6175 6c74 206f  on. By default o
+0000dbf0: 6e6c 790a 2020 2020 2020 2020 7468 6520  nly.        the 
+0000dc00: 7265 7475 726e 2076 616c 7565 7320 6f66  return values of
+0000dc10: 2061 6c6c 2060 605f 5f63 616c 6c5f 5f60   all ``__call__`
+0000dc20: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
+0000dc30: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
+0000dc40: 2063 616e 0a20 2020 2020 2020 2062 6520   can.        be 
+0000dc50: 7061 7373 6564 2074 6f20 6368 616e 6765  passed to change
+0000dc60: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
+0000dc70: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
+0000dc80: 2066 756e 6374 696f 6e20 7461 6b65 730a   function takes.
+0000dc90: 2020 2020 2020 2020 7468 6520 4d6f 6475          the Modu
+0000dca0: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
+0000dcb0: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
+0000dcc0: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
+0000dcd0: 6e64 6963 6174 696e 670a 2020 2020 2020  ndicating.      
+0000dce0: 2020 7768 6574 6865 7220 7468 6520 6f75    whether the ou
+0000dcf0: 7470 7574 206f 6620 7468 6174 206d 6574  tput of that met
+0000dd00: 686f 6420 696e 766f 6361 7469 6f6e 2073  hod invocation s
+0000dd10: 686f 756c 6420 6265 2073 746f 7265 642e  hould be stored.
+0000dd20: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
+0000dd30: 204b 6579 776f 7264 2061 7267 756d 656e   Keyword argumen
+0000dd40: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
+0000dd50: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
+0000dd60: 206d 6574 686f 642e 0a20 2020 2052 6574   method..    Ret
+0000dd70: 7572 6e73 3a0a 2020 2020 2020 4966 2060  urns:.      If `
+0000dd80: 606d 7574 6162 6c65 6060 2069 7320 4661  `mutable`` is Fa
+0000dd90: 6c73 652c 2072 6574 7572 6e73 206f 7574  lse, returns out
+0000dda0: 7075 742e 2049 6620 616e 7920 636f 6c6c  put. If any coll
+0000ddb0: 6563 7469 6f6e 7320 6172 650a 2020 2020  ections are.    
+0000ddc0: 2020 6d75 7461 626c 652c 2072 6574 7572    mutable, retur
+0000ddd0: 6e73 2060 6028 6f75 7470 7574 2c20 7661  ns ``(output, va
+0000dde0: 7273 2960 602c 2077 6865 7265 2060 6076  rs)``, where ``v
+0000ddf0: 6172 7360 6020 6172 6520 6973 2061 2064  ars`` are is a d
+0000de00: 6963 740a 2020 2020 2020 6f66 2074 6865  ict.      of the
+0000de10: 206d 6f64 6966 6965 6420 636f 6c6c 6563   modified collec
+0000de20: 7469 6f6e 732e 0a20 2020 2022 2222 0a20  tions..    """. 
+0000de30: 2020 204d 6f64 756c 652e 5f6d 6f64 756c     Module._modul
+0000de40: 655f 6368 6563 6b73 2873 656c 6629 0a0a  e_checks(self)..
+0000de50: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc
+0000de60: 6528 6d65 7468 6f64 2c20 7374 7229 3a0a  e(method, str):.
+0000de70: 2020 2020 2020 6174 7472 6962 7574 655f        attribute_
+0000de80: 6e61 6d65 203d 206d 6574 686f 640a 2020  name = method.  
+0000de90: 2020 2020 6d65 7468 6f64 203d 2067 6574      method = get
+0000dea0: 6174 7472 2873 656c 662c 2061 7474 7269  attr(self, attri
+0000deb0: 6275 7465 5f6e 616d 6529 0a20 2020 2020  bute_name).     
+0000dec0: 2069 6620 6e6f 7420 6361 6c6c 6162 6c65   if not callable
+0000ded0: 286d 6574 686f 6429 3a0a 2020 2020 2020  (method):.      
+0000dee0: 2020 636c 6173 735f 6e61 6d65 203d 2074    class_name = t
+0000def0: 7970 6528 7365 6c66 292e 5f5f 6e61 6d65  ype(self).__name
+0000df00: 5f5f 0a20 2020 2020 2020 2072 6169 7365  __.        raise
+0000df10: 2054 7970 6545 7272 6f72 2866 275c 277b   TypeError(f'\'{
+0000df20: 636c 6173 735f 6e61 6d65 7d2e 7b61 7474  class_name}.{att
+0000df30: 7269 6275 7465 5f6e 616d 657d 5c27 206d  ribute_name}\' m
+0000df40: 7573 7420 6265 2061 2063 616c 6c61 626c  ust be a callabl
+0000df50: 652c 2067 6f74 207b 7479 7065 286d 6574  e, got {type(met
+0000df60: 686f 6429 7d2e 2729 0a20 2020 2065 6c69  hod)}.').    eli
+0000df70: 6620 6d65 7468 6f64 2069 7320 4e6f 6e65  f method is None
+0000df80: 3a0a 2020 2020 2020 6d65 7468 6f64 203d  :.      method =
+0000df90: 2073 656c 662e 5f5f 6361 6c6c 5f5f 0a20   self.__call__. 
+0000dfa0: 2020 206d 6574 686f 6420 3d20 5f67 6574     method = _get
+0000dfb0: 5f75 6e62 6f75 6e64 5f66 6e28 6d65 7468  _unbound_fn(meth
+0000dfc0: 6f64 290a 2020 2020 7265 7475 726e 2061  od).    return a
+0000dfd0: 7070 6c79 280a 2020 2020 2020 2020 6d65  pply(.        me
+0000dfe0: 7468 6f64 2c20 7365 6c66 2c0a 2020 2020  thod, self,.    
+0000dff0: 2020 2020 6d75 7461 626c 653d 6d75 7461      mutable=muta
+0000e000: 626c 652c 0a20 2020 2020 2020 2063 6170  ble,.        cap
+0000e010: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+0000e020: 6573 3d63 6170 7475 7265 5f69 6e74 6572  es=capture_inter
+0000e030: 6d65 6469 6174 6573 2c0a 2020 2020 2928  mediates,.    )(
+0000e040: 7661 7269 6162 6c65 732c 202a 6172 6773  variables, *args
+0000e050: 2c20 2a2a 6b77 6172 6773 2c20 726e 6773  , **kwargs, rngs
+0000e060: 3d72 6e67 7329 0a0a 2020 4074 7261 6365  =rngs)..  @trace
+0000e070: 6261 636b 5f75 7469 6c2e 6170 695f 626f  back_util.api_bo
+0000e080: 756e 6461 7279 0a20 2064 6566 2069 6e69  undary.  def ini
+0000e090: 745f 7769 7468 5f6f 7574 7075 7428 7365  t_with_output(se
+0000e0a0: 6c66 2c0a 2020 2020 2020 2020 2020 2020  lf,.            
+0000e0b0: 2020 2020 2020 2020 2020 2072 6e67 733a             rngs:
+0000e0c0: 2055 6e69 6f6e 5b4b 6579 4172 7261 792c   Union[KeyArray,
+0000e0d0: 2052 4e47 5365 7175 656e 6365 735d 2c0a   RNGSequences],.
+0000e0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e0f0: 2020 2020 2020 202a 6172 6773 2c0a 2020         *args,.  
+0000e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e110: 2020 2020 206d 6574 686f 643a 2055 6e69       method: Uni
+0000e120: 6f6e 5b43 616c 6c61 626c 655b 2e2e 2e2c  on[Callable[...,
+0000e130: 2041 6e79 5d2c 2073 7472 2c20 4e6f 6e65   Any], str, None
+0000e140: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 2020  ] = None,.      
+0000e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e160: 206d 7574 6162 6c65 3a20 436f 6c6c 6563   mutable: Collec
+0000e170: 7469 6f6e 4669 6c74 6572 203d 2044 656e  tionFilter = Den
+0000e180: 794c 6973 7428 2769 6e74 6572 6d65 6469  yList('intermedi
+0000e190: 6174 6573 2729 2c0a 2020 2020 2020 2020  ates'),.        
+0000e1a0: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0000e1b0: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
+0000e1c0: 6174 6573 3a20 556e 696f 6e5b 626f 6f6c  ates: Union[bool
+0000e1d0: 2c20 4361 6c6c 6162 6c65 5b5b 274d 6f64  , Callable[['Mod
+0000e1e0: 756c 6527 2c20 7374 725d 2c20 626f 6f6c  ule', str], bool
+0000e1f0: 5d5d 203d 2046 616c 7365 2c0a 2020 2020  ]] = False,.    
+0000e200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e210: 2020 202a 2a6b 7761 7267 7329 202d 3e20     **kwargs) -> 
+0000e220: 5475 706c 655b 416e 792c 2055 6e69 6f6e  Tuple[Any, Union
+0000e230: 5b46 726f 7a65 6e56 6172 6961 626c 6544  [FrozenVariableD
+0000e240: 6963 742c 2044 6963 745b 7374 722c 2041  ict, Dict[str, A
+0000e250: 6e79 5d5d 5d3a 0a20 2020 2022 2222 496e  ny]]]:.    """In
+0000e260: 6974 6961 6c69 7a65 7320 6120 6d6f 6475  itializes a modu
+0000e270: 6c65 206d 6574 686f 6420 7769 7468 2076  le method with v
+0000e280: 6172 6961 626c 6573 2061 6e64 2072 6574  ariables and ret
+0000e290: 7572 6e73 206f 7574 7075 7420 616e 6420  urns output and 
+0000e2a0: 6d6f 6469 6669 6564 2076 6172 6961 626c  modified variabl
+0000e2b0: 6573 2e0a 0a20 2020 2041 7267 733a 0a20  es...    Args:. 
+0000e2c0: 2020 2020 2072 6e67 733a 2054 6865 2072       rngs: The r
+0000e2d0: 6e67 7320 666f 7220 7468 6520 7661 7269  ngs for the vari
+0000e2e0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+0000e2f0: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
+0000e300: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
+0000e310: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
+0000e320: 7420 6675 6e63 7469 6f6e 2e0a 2020 2020  t function..    
+0000e330: 2020 6d65 7468 6f64 3a20 416e 206f 7074    method: An opt
+0000e340: 696f 6e61 6c20 6d65 7468 6f64 2e20 4966  ional method. If
+0000e350: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
+0000e360: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
+0000e370: 4966 206e 6f74 0a20 2020 2020 2020 2070  If not.        p
+0000e380: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
+0000e390: 2074 6865 2060 605f 5f63 616c 6c5f 5f60   the ``__call__`
+0000e3a0: 6020 6d65 7468 6f64 2e20 4120 7374 7269  ` method. A stri
+0000e3b0: 6e67 2063 616e 2061 6c73 6f20 6265 270a  ng can also be'.
+0000e3c0: 2020 2020 2020 2020 7072 6f76 6964 6564          provided
+0000e3d0: 2074 6f20 7370 6563 6966 7920 6120 6d65   to specify a me
+0000e3e0: 7468 6f64 2062 7920 6e61 6d65 2e0a 2020  thod by name..  
+0000e3f0: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
+0000e400: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
+0000e410: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
+0000e420: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
+0000e430: 6f6e 7320 7368 6f75 6c64 2062 650a 2020  ons should be.  
+0000e440: 2020 2020 2020 7472 6561 7465 6420 6173        treated as
+0000e450: 206d 7574 6162 6c65 3a20 6060 626f 6f6c   mutable: ``bool
+0000e460: 6060 3a20 616c 6c2f 6e6f 2063 6f6c 6c65  ``: all/no colle
+0000e470: 6374 696f 6e73 2061 7265 206d 7574 6162  ctions are mutab
+0000e480: 6c65 2e0a 2020 2020 2020 2020 6060 7374  le..        ``st
+0000e490: 7260 603a 2054 6865 206e 616d 6520 6f66  r``: The name of
+0000e4a0: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
+0000e4b0: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
+0000e4c0: 6c69 7374 6060 3a20 410a 2020 2020 2020  list``: A.      
+0000e4d0: 2020 6c69 7374 206f 6620 6e61 6d65 7320    list of names 
+0000e4e0: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
+0000e4f0: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
+0000e500: 6c74 2061 6c6c 2063 6f6c 6c65 6374 696f  lt all collectio
+0000e510: 6e73 0a20 2020 2020 2020 2065 7863 6570  ns.        excep
+0000e520: 7420 2269 6e74 6572 6d65 6469 6174 6573  t "intermediates
+0000e530: 2220 6172 6520 6d75 7461 626c 652e 0a20  " are mutable.. 
+0000e540: 2020 2020 2063 6170 7475 7265 5f69 6e74       capture_int
+0000e550: 6572 6d65 6469 6174 6573 3a20 4966 2060  ermediates: If `
+0000e560: 5472 7565 602c 2063 6170 7475 7265 7320  True`, captures 
+0000e570: 696e 7465 726d 6564 6961 7465 2072 6574  intermediate ret
+0000e580: 7572 6e20 7661 6c75 6573 0a20 2020 2020  urn values.     
+0000e590: 2020 206f 6620 616c 6c20 4d6f 6475 6c65     of all Module
+0000e5a0: 7320 696e 7369 6465 2074 6865 2022 696e  s inside the "in
+0000e5b0: 7465 726d 6564 6961 7465 7322 2063 6f6c  termediates" col
+0000e5c0: 6c65 6374 696f 6e2e 2042 7920 6465 6661  lection. By defa
+0000e5d0: 756c 7420 6f6e 6c79 0a20 2020 2020 2020  ult only.       
+0000e5e0: 2074 6865 2072 6574 7572 6e20 7661 6c75   the return valu
+0000e5f0: 6573 206f 6620 616c 6c20 6060 5f5f 6361  es of all ``__ca
+0000e600: 6c6c 5f5f 6060 206d 6574 686f 6473 2061  ll__`` methods a
+0000e610: 7265 2073 746f 7265 642e 2041 2066 756e  re stored. A fun
+0000e620: 6374 696f 6e20 6361 6e0a 2020 2020 2020  ction can.      
+0000e630: 2020 6265 2070 6173 7365 6420 746f 2063    be passed to c
+0000e640: 6861 6e67 6520 7468 6520 6669 6c74 6572  hange the filter
+0000e650: 2062 6568 6176 696f 722e 2054 6865 2066   behavior. The f
+0000e660: 696c 7465 7220 6675 6e63 7469 6f6e 2074  ilter function t
+0000e670: 616b 6573 0a20 2020 2020 2020 2074 6865  akes.        the
+0000e680: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
+0000e690: 2061 6e64 206d 6574 686f 6420 6e61 6d65   and method name
+0000e6a0: 2061 6e64 2072 6574 7572 6e73 2061 2062   and returns a b
+0000e6b0: 6f6f 6c20 696e 6469 6361 7469 6e67 0a20  ool indicating. 
+0000e6c0: 2020 2020 2020 2077 6865 7468 6572 2074         whether t
+0000e6d0: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
+0000e6e0: 7420 6d65 7468 6f64 2069 6e76 6f63 6174  t method invocat
+0000e6f0: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
+0000e700: 6f72 6564 2e0a 2020 2020 2020 2a2a 6b77  ored..      **kw
+0000e710: 6172 6773 3a20 4b65 7977 6f72 6420 6172  args: Keyword ar
+0000e720: 6775 6d65 6e74 7320 7061 7373 6564 2074  guments passed t
+0000e730: 6f20 7468 6520 696e 6974 2066 756e 6374  o the init funct
+0000e740: 696f 6e2e 0a20 2020 2052 6574 7572 6e73  ion..    Returns
+0000e750: 3a0a 2020 2020 2020 6028 6f75 7470 7574  :.      `(output
+0000e760: 2c20 7661 7273 2960 602c 2077 6865 7265  , vars)``, where
+0000e770: 2060 6076 6172 7360 6020 6172 6520 6973   ``vars`` are is
+0000e780: 2061 2064 6963 7420 6f66 2074 6865 206d   a dict of the m
+0000e790: 6f64 6966 6965 640a 2020 2020 2020 636f  odified.      co
+0000e7a0: 6c6c 6563 7469 6f6e 732e 0a20 2020 2022  llections..    "
+0000e7b0: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
+0000e7c0: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
+0000e7d0: 6629 0a0a 2020 2020 6966 206e 6f74 2069  f)..    if not i
+0000e7e0: 7369 6e73 7461 6e63 6528 726e 6773 2c20  sinstance(rngs, 
+0000e7f0: 6469 6374 293a 0a20 2020 2020 2069 6620  dict):.      if 
+0000e800: 6e6f 7420 636f 7265 2e73 636f 7065 2e5f  not core.scope._
+0000e810: 6973 5f76 616c 6964 5f72 6e67 2872 6e67  is_valid_rng(rng
+0000e820: 7329 3a0a 2020 2020 2020 2020 7261 6973  s):.        rais
+0000e830: 6520 6572 726f 7273 2e49 6e76 616c 6964  e errors.Invalid
+0000e840: 526e 6745 7272 6f72 280a 2020 2020 2020  RngError(.      
+0000e850: 2020 2020 2020 2752 4e47 7320 7368 6f75        'RNGs shou
+0000e860: 6c64 2062 6520 6f66 2073 6861 7065 2028  ld be of shape (
+0000e870: 322c 2920 6f72 204b 6579 4172 7261 7920  2,) or KeyArray 
+0000e880: 696e 204d 6f64 756c 6520 270a 2020 2020  in Module '.    
+0000e890: 2020 2020 2020 2020 6627 7b73 656c 662e          f'{self.
+0000e8a0: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
+0000e8b0: 5f5f 7d2c 2062 7574 2072 6e67 7320 6172  __}, but rngs ar
+0000e8c0: 653a 207b 726e 6773 7d27 290a 2020 2020  e: {rngs}').    
+0000e8d0: 2020 726e 6773 203d 207b 2770 6172 616d    rngs = {'param
+0000e8e0: 7327 3a20 726e 6773 7d0a 0a20 2020 2069  s': rngs}..    i
+0000e8f0: 6620 6973 696e 7374 616e 6365 286d 6574  f isinstance(met
+0000e900: 686f 642c 2073 7472 293a 0a20 2020 2020  hod, str):.     
+0000e910: 2061 7474 7269 6275 7465 5f6e 616d 6520   attribute_name 
+0000e920: 3d20 6d65 7468 6f64 0a20 2020 2020 206d  = method.      m
+0000e930: 6574 686f 6420 3d20 6765 7461 7474 7228  ethod = getattr(
+0000e940: 7365 6c66 2c20 6174 7472 6962 7574 655f  self, attribute_
+0000e950: 6e61 6d65 290a 2020 2020 2020 6966 206e  name).      if n
+0000e960: 6f74 2063 616c 6c61 626c 6528 6d65 7468  ot callable(meth
+0000e970: 6f64 293a 0a20 2020 2020 2020 2063 6c61  od):.        cla
+0000e980: 7373 5f6e 616d 6520 3d20 7479 7065 2873  ss_name = type(s
+0000e990: 656c 6629 2e5f 5f6e 616d 655f 5f0a 2020  elf).__name__.  
+0000e9a0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
+0000e9b0: 4572 726f 7228 6627 5c27 7b63 6c61 7373  Error(f'\'{class
+0000e9c0: 5f6e 616d 657d 2e7b 6174 7472 6962 7574  _name}.{attribut
+0000e9d0: 655f 6e61 6d65 7d5c 2720 6d75 7374 2062  e_name}\' must b
+0000e9e0: 6520 6120 6361 6c6c 6162 6c65 2c20 676f  e a callable, go
+0000e9f0: 7420 7b74 7970 6528 6d65 7468 6f64 297d  t {type(method)}
+0000ea00: 2e27 290a 2020 2020 656c 6966 206d 6574  .').    elif met
+0000ea10: 686f 6420 6973 204e 6f6e 653a 0a20 2020  hod is None:.   
+0000ea20: 2020 206d 6574 686f 6420 3d20 7365 6c66     method = self
+0000ea30: 2e5f 5f63 616c 6c5f 5f0a 2020 2020 6d65  .__call__.    me
+0000ea40: 7468 6f64 203d 205f 6765 745f 756e 626f  thod = _get_unbo
+0000ea50: 756e 645f 666e 286d 6574 686f 6429 0a20  und_fn(method). 
+0000ea60: 2020 2072 6574 7572 6e20 696e 6974 5f77     return init_w
+0000ea70: 6974 685f 6f75 7470 7574 280a 2020 2020  ith_output(.    
+0000ea80: 2020 2020 6d65 7468 6f64 2c0a 2020 2020      method,.    
+0000ea90: 2020 2020 7365 6c66 2c0a 2020 2020 2020      self,.      
+0000eaa0: 2020 6d75 7461 626c 653d 6d75 7461 626c    mutable=mutabl
+0000eab0: 652c 0a20 2020 2020 2020 2063 6170 7475  e,.        captu
+0000eac0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+0000ead0: 3d63 6170 7475 7265 5f69 6e74 6572 6d65  =capture_interme
+0000eae0: 6469 6174 6573 0a20 2020 2029 2872 6e67  diates.    )(rng
+0000eaf0: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+0000eb00: 6773 290a 0a20 2040 7472 6163 6562 6163  gs)..  @tracebac
+0000eb10: 6b5f 7574 696c 2e61 7069 5f62 6f75 6e64  k_util.api_bound
+0000eb20: 6172 790a 2020 6465 6620 696e 6974 2873  ary.  def init(s
+0000eb30: 656c 662c 0a20 2020 2020 2020 2020 2020  elf,.           
+0000eb40: 726e 6773 3a20 556e 696f 6e5b 4b65 7941  rngs: Union[KeyA
+0000eb50: 7272 6179 2c20 524e 4753 6571 7565 6e63  rray, RNGSequenc
+0000eb60: 6573 5d2c 0a20 2020 2020 2020 2020 2020  es],.           
+0000eb70: 2a61 7267 732c 0a20 2020 2020 2020 2020  *args,.         
+0000eb80: 2020 6d65 7468 6f64 3a20 556e 696f 6e5b    method: Union[
+0000eb90: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
+0000eba0: 795d 2c20 7374 722c 204e 6f6e 655d 203d  y], str, None] =
+0000ebb0: 204e 6f6e 652c 0a20 2020 2020 2020 2020   None,.         
+0000ebc0: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+0000ebd0: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
+0000ebe0: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
+0000ebf0: 6961 7465 7327 292c 0a20 2020 2020 2020  iates'),.       
+0000ec00: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+0000ec10: 726d 6564 6961 7465 733a 2055 6e69 6f6e  rmediates: Union
+0000ec20: 5b62 6f6f 6c2c 2043 616c 6c61 626c 655b  [bool, Callable[
+0000ec30: 5b27 4d6f 6475 6c65 272c 2073 7472 5d2c  ['Module', str],
+0000ec40: 2062 6f6f 6c5d 5d20 3d20 4661 6c73 652c   bool]] = False,
+0000ec50: 0a20 2020 2020 2020 2020 2020 2a2a 6b77  .           **kw
+0000ec60: 6172 6773 2920 2d3e 2055 6e69 6f6e 5b46  args) -> Union[F
+0000ec70: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
+0000ec80: 742c 2044 6963 745b 7374 722c 2041 6e79  t, Dict[str, Any
+0000ec90: 5d5d 3a0a 2020 2020 2222 2249 6e69 7469  ]]:.    """Initi
+0000eca0: 616c 697a 6573 2061 206d 6f64 756c 6520  alizes a module 
+0000ecb0: 6d65 7468 6f64 2077 6974 6820 7661 7269  method with vari
+0000ecc0: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
+0000ecd0: 7320 6d6f 6469 6669 6564 2076 6172 6961  s modified varia
+0000ece0: 626c 6573 2e0a 0a20 2020 2060 6069 6e69  bles...    ``ini
+0000ecf0: 7460 6020 7461 6b65 7320 6173 2066 6972  t`` takes as fir
+0000ed00: 7374 2061 7267 756d 656e 7420 6569 7468  st argument eith
+0000ed10: 6572 2061 2073 696e 676c 6520 6060 5052  er a single ``PR
+0000ed20: 4e47 4b65 7960 602c 206f 7220 6120 6469  NGKey``, or a di
+0000ed30: 6374 696f 6e61 7279 206d 6170 7069 6e67  ctionary mapping
+0000ed40: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
+0000ed50: 7469 6f6e 7320 6e61 6d65 7320 746f 2074  tions names to t
+0000ed60: 6865 6972 2060 6050 524e 474b 6579 7360  heir ``PRNGKeys`
+0000ed70: 602c 2061 6e64 2077 696c 6c20 6361 6c6c  `, and will call
+0000ed80: 2060 606d 6574 686f 6460 6020 2877 6869   ``method`` (whi
+0000ed90: 6368 2069 7320 7468 6520 6d6f 6475 6c65  ch is the module
+0000eda0: 2773 2060 605f 5f63 616c 6c5f 5f60 6020  's ``__call__`` 
+0000edb0: 6675 6e63 7469 6f6e 2062 7920 6465 6661  function by defa
+0000edc0: 756c 7429 2070 6173 7369 6e67 2060 602a  ult) passing ``*
+0000edd0: 6172 6773 6060 2061 6e64 2060 602a 2a6b  args`` and ``**k
+0000ede0: 7761 7267 7360 602c 2061 6e64 2072 6574  wargs``, and ret
+0000edf0: 7572 6e73 0a20 2020 2061 2064 6963 7469  urns.    a dicti
+0000ee00: 6f6e 6172 7920 6f66 2069 6e69 7469 616c  onary of initial
+0000ee10: 697a 6564 2076 6172 6961 626c 6573 2e0a  ized variables..
+0000ee20: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
+0000ee30: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+0000ee40: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+0000ee50: 6e0a 2020 2020 2020 3e3e 3e20 696d 706f  n.      >>> impo
+0000ee60: 7274 206a 6178 2e6e 756d 7079 2061 7320  rt jax.numpy as 
+0000ee70: 6a6e 700a 2020 2020 2020 3e3e 3e20 696d  jnp.      >>> im
+0000ee80: 706f 7274 206a 6178 0a20 2020 2020 202e  port jax.      .
+0000ee90: 2e2e 0a20 2020 2020 203e 3e3e 2063 6c61  ...      >>> cla
+0000eea0: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
+0000eeb0: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
+0000eec0: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
+0000eed0: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
+0000eee0: 6c5f 5f28 7365 6c66 2c20 782c 2074 7261  l__(self, x, tra
+0000eef0: 696e 293a 0a20 2020 2020 202e 2e2e 2020  in):.      ...  
+0000ef00: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
+0000ef10: 3136 2928 7829 0a20 2020 2020 202e 2e2e  16)(x).      ...
+0000ef20: 2020 2020 2078 203d 206e 6e2e 4261 7463       x = nn.Batc
+0000ef30: 684e 6f72 6d28 7573 655f 7275 6e6e 696e  hNorm(use_runnin
+0000ef40: 675f 6176 6572 6167 653d 6e6f 7420 7472  g_average=not tr
+0000ef50: 6169 6e29 2878 290a 2020 2020 2020 2e2e  ain)(x).      ..
+0000ef60: 2e20 2020 2020 7820 3d20 6e6e 2e72 656c  .     x = nn.rel
+0000ef70: 7528 7829 0a20 2020 2020 202e 2e2e 2020  u(x).      ...  
+0000ef80: 2020 2072 6574 7572 6e20 6e6e 2e44 656e     return nn.Den
+0000ef90: 7365 2831 2928 7829 0a20 2020 2020 202e  se(1)(x).      .
+0000efa0: 2e2e 0a20 2020 2020 203e 3e3e 206d 6f64  ...      >>> mod
+0000efb0: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
+0000efc0: 2020 3e3e 3e20 6b65 7920 3d20 6a61 782e    >>> key = jax.
+0000efd0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
+0000efe0: 290a 2020 2020 2020 3e3e 3e20 7661 7269  ).      >>> vari
+0000eff0: 6162 6c65 7320 3d20 6d6f 6475 6c65 2e69  ables = module.i
+0000f000: 6e69 7428 6b65 792c 206a 6e70 2e65 6d70  nit(key, jnp.emp
+0000f010: 7479 2828 312c 2037 2929 2c20 7472 6169  ty((1, 7)), trai
+0000f020: 6e3d 4661 6c73 6529 0a0a 2020 2020 4966  n=False)..    If
+0000f030: 2079 6f75 2070 6173 7320 6120 7369 6e67   you pass a sing
+0000f040: 6c65 2060 6050 524e 474b 6579 6060 2c20  le ``PRNGKey``, 
+0000f050: 466c 6178 2077 696c 6c20 7573 6520 6974  Flax will use it
+0000f060: 2074 6f20 6665 6564 2074 6865 2060 6027   to feed the ``'
+0000f070: 7061 7261 6d73 2760 6020 524e 4720 7374  params'`` RNG st
+0000f080: 7265 616d 2e0a 2020 2020 4966 2079 6f75  ream..    If you
+0000f090: 2077 616e 7420 746f 2075 7365 2061 2064   want to use a d
+0000f0a0: 6966 6665 7265 6e74 2052 4e47 2073 7472  ifferent RNG str
+0000f0b0: 6561 6d20 6f72 206e 6565 6420 746f 2075  eam or need to u
+0000f0c0: 7365 206d 756c 7469 706c 6520 7374 7265  se multiple stre
+0000f0d0: 616d 732c 2079 6f75 206d 7573 7420 7061  ams, you must pa
+0000f0e0: 7373 2061 0a20 2020 2064 6963 7469 6f6e  ss a.    diction
+0000f0f0: 6172 7920 6d61 7070 696e 6720 6561 6368  ary mapping each
+0000f100: 2052 4e47 2073 7472 6561 6d20 6e61 6d65   RNG stream name
+0000f110: 2074 6f20 6974 7320 636f 7272 6573 706f   to its correspo
+0000f120: 6e64 696e 6720 6060 5052 4e47 4b65 7960  nding ``PRNGKey`
+0000f130: 6020 746f 2060 6069 6e69 7460 602e 0a0a  ` to ``init``...
+0000f140: 2020 2020 4578 616d 706c 653a 3a0a 0a20      Example::.. 
+0000f150: 2020 2020 203e 3e3e 2063 6c61 7373 2046       >>> class F
+0000f160: 6f6f 286e 6e2e 4d6f 6475 6c65 293a 0a20  oo(nn.Module):. 
+0000f170: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
+0000f180: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
+0000f190: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0000f1a0: 7365 6c66 2c20 782c 2074 7261 696e 293a  self, x, train):
+0000f1b0: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
+0000f1c0: 203d 206e 6e2e 4465 6e73 6528 3136 2928   = nn.Dense(16)(
+0000f1d0: 7829 0a20 2020 2020 202e 2e2e 2020 2020  x).      ...    
+0000f1e0: 2078 203d 206e 6e2e 4261 7463 684e 6f72   x = nn.BatchNor
+0000f1f0: 6d28 7573 655f 7275 6e6e 696e 675f 6176  m(use_running_av
+0000f200: 6572 6167 653d 6e6f 7420 7472 6169 6e29  erage=not train)
+0000f210: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
+0000f220: 2020 7820 3d20 6e6e 2e72 656c 7528 7829    x = nn.relu(x)
+0000f230: 0a20 2020 2020 202e 2e2e 0a20 2020 2020  .      ....     
+0000f240: 202e 2e2e 2020 2020 2023 2041 6464 2067   ...     # Add g
+0000f250: 6175 7373 6961 6e20 6e6f 6973 650a 2020  aussian noise.  
+0000f260: 2020 2020 2e2e 2e20 2020 2020 6e6f 6973      ...     nois
+0000f270: 655f 6b65 7920 3d20 7365 6c66 2e6d 616b  e_key = self.mak
+0000f280: 655f 726e 6728 276e 6f69 7365 2729 0a20  e_rng('noise'). 
+0000f290: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
+0000f2a0: 2078 202b 206a 6178 2e72 616e 646f 6d2e   x + jax.random.
+0000f2b0: 6e6f 726d 616c 286e 6f69 7365 5f6b 6579  normal(noise_key
+0000f2c0: 2c20 782e 7368 6170 6529 0a20 2020 2020  , x.shape).     
+0000f2d0: 202e 2e2e 0a20 2020 2020 202e 2e2e 2020   ....      ...  
+0000f2e0: 2020 2072 6574 7572 6e20 6e6e 2e44 656e     return nn.Den
+0000f2f0: 7365 2831 2928 7829 0a20 2020 2020 202e  se(1)(x).      .
+0000f300: 2e2e 0a20 2020 2020 203e 3e3e 206d 6f64  ...      >>> mod
+0000f310: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
+0000f320: 2020 3e3e 3e20 726e 6773 203d 207b 2770    >>> rngs = {'p
+0000f330: 6172 616d 7327 3a20 6a61 782e 7261 6e64  arams': jax.rand
+0000f340: 6f6d 2e50 524e 474b 6579 2830 292c 2027  om.PRNGKey(0), '
+0000f350: 6e6f 6973 6527 3a20 6a61 782e 7261 6e64  noise': jax.rand
+0000f360: 6f6d 2e50 524e 474b 6579 2831 297d 0a20  om.PRNGKey(1)}. 
+0000f370: 2020 2020 203e 3e3e 2076 6172 6961 626c       >>> variabl
+0000f380: 6573 203d 206d 6f64 756c 652e 696e 6974  es = module.init
+0000f390: 2872 6e67 732c 206a 6e70 2e65 6d70 7479  (rngs, jnp.empty
+0000f3a0: 2828 312c 2037 2929 2c20 7472 6169 6e3d  ((1, 7)), train=
+0000f3b0: 4661 6c73 6529 0a0a 2020 2020 4a69 7474  False)..    Jitt
+0000f3c0: 696e 6720 6069 6e69 7460 2069 6e69 7469  ing `init` initi
+0000f3d0: 616c 697a 6573 2061 206d 6f64 656c 206c  alizes a model l
+0000f3e0: 617a 696c 7920 7573 696e 6720 6f6e 6c79  azily using only
+0000f3f0: 2074 6865 2073 6861 7065 7320 6f66 2074   the shapes of t
+0000f400: 6865 0a20 2020 2070 726f 7669 6465 6420  he.    provided 
+0000f410: 6172 6775 6d65 6e74 732c 2061 6e64 2061  arguments, and a
+0000f420: 766f 6964 7320 636f 6d70 7574 696e 6720  voids computing 
+0000f430: 7468 6520 666f 7277 6172 6420 7061 7373  the forward pass
+0000f440: 2077 6974 6820 6163 7475 616c 0a20 2020   with actual.   
+0000f450: 2076 616c 7565 732e 2045 7861 6d70 6c65   values. Example
+0000f460: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
+0000f470: 6475 6c65 203d 206e 6e2e 4465 6e73 6528  dule = nn.Dense(
+0000f480: 3129 0a20 2020 2020 203e 3e3e 2069 6e69  1).      >>> ini
+0000f490: 745f 6a69 7420 3d20 6a61 782e 6a69 7428  t_jit = jax.jit(
+0000f4a0: 6d6f 6475 6c65 2e69 6e69 7429 0a20 2020  module.init).   
+0000f4b0: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
+0000f4c0: 203d 2069 6e69 745f 6a69 7428 6a61 782e   = init_jit(jax.
+0000f4d0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
+0000f4e0: 292c 206a 6e70 2e65 6d70 7479 2828 312c  ), jnp.empty((1,
+0000f4f0: 2037 2929 290a 0a20 2020 2060 6069 6e69   7)))..    ``ini
+0000f500: 7460 6020 6973 2061 206c 6967 6874 2077  t`` is a light w
+0000f510: 7261 7070 6572 206f 7665 7220 6060 6170  rapper over ``ap
+0000f520: 706c 7960 602c 2073 6f20 6f74 6865 7220  ply``, so other 
+0000f530: 6060 6170 706c 7960 6020 6172 6775 6d65  ``apply`` argume
+0000f540: 6e74 7320 6c69 6b65 0a20 2020 2060 606d  nts like.    ``m
+0000f550: 6574 686f 6460 602c 2060 606d 7574 6162  ethod``, ``mutab
+0000f560: 6c65 6060 2c20 616e 6420 6060 6361 7074  le``, and ``capt
+0000f570: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
+0000f580: 7360 6020 6172 6520 616c 736f 2061 7661  s`` are also ava
+0000f590: 696c 6162 6c65 2e0a 0a20 2020 2041 7267  ilable...    Arg
+0000f5a0: 733a 0a20 2020 2020 2072 6e67 733a 2054  s:.      rngs: T
+0000f5b0: 6865 2072 6e67 7320 666f 7220 7468 6520  he rngs for the 
+0000f5c0: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
+0000f5d0: 696f 6e73 2e0a 2020 2020 2020 2a61 7267  ions..      *arg
+0000f5e0: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
+0000f5f0: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
+0000f600: 2069 6e69 7420 6675 6e63 7469 6f6e 2e0a   init function..
+0000f610: 2020 2020 2020 6d65 7468 6f64 3a20 416e        method: An
+0000f620: 206f 7074 696f 6e61 6c20 6d65 7468 6f64   optional method
+0000f630: 2e20 4966 2070 726f 7669 6465 642c 2061  . If provided, a
+0000f640: 7070 6c69 6573 2074 6869 7320 6d65 7468  pplies this meth
+0000f650: 6f64 2e20 4966 206e 6f74 0a20 2020 2020  od. If not.     
+0000f660: 2020 2070 726f 7669 6465 642c 2061 7070     provided, app
+0000f670: 6c69 6573 2074 6865 2060 605f 5f63 616c  lies the ``__cal
+0000f680: 6c5f 5f60 6020 6d65 7468 6f64 2e20 4120  l__`` method. A 
+0000f690: 7374 7269 6e67 2063 616e 2061 6c73 6f20  string can also 
+0000f6a0: 6265 0a20 2020 2020 2020 2070 726f 7669  be.        provi
+0000f6b0: 6465 6420 746f 2073 7065 6369 6679 2061  ded to specify a
+0000f6c0: 206d 6574 686f 6420 6279 206e 616d 652e   method by name.
+0000f6d0: 0a20 2020 2020 206d 7574 6162 6c65 3a20  .      mutable: 
+0000f6e0: 4361 6e20 6265 2062 6f6f 6c2c 2073 7472  Can be bool, str
+0000f6f0: 2c20 6f72 206c 6973 742e 2053 7065 6369  , or list. Speci
+0000f700: 6669 6573 2077 6869 6368 2063 6f6c 6c65  fies which colle
+0000f710: 6374 696f 6e73 2073 686f 756c 6420 6265  ctions should be
+0000f720: 0a20 2020 2020 2020 2074 7265 6174 6564  .        treated
+0000f730: 2061 7320 6d75 7461 626c 653a 2060 6062   as mutable: ``b
+0000f740: 6f6f 6c60 603a 2061 6c6c 2f6e 6f20 636f  ool``: all/no co
+0000f750: 6c6c 6563 7469 6f6e 7320 6172 6520 6d75  llections are mu
+0000f760: 7461 626c 652e 0a20 2020 2020 2020 2060  table..        `
+0000f770: 6073 7472 6060 3a20 5468 6520 6e61 6d65  `str``: The name
+0000f780: 206f 6620 6120 7369 6e67 6c65 206d 7574   of a single mut
+0000f790: 6162 6c65 2063 6f6c 6c65 6374 696f 6e2e  able collection.
+0000f7a0: 2060 606c 6973 7460 603a 2041 0a20 2020   ``list``: A.   
+0000f7b0: 2020 2020 206c 6973 7420 6f66 206e 616d       list of nam
+0000f7c0: 6573 206f 6620 6d75 7461 626c 6520 636f  es of mutable co
+0000f7d0: 6c6c 6563 7469 6f6e 732e 2042 7920 6465  llections. By de
+0000f7e0: 6661 756c 7420 616c 6c20 636f 6c6c 6563  fault all collec
+0000f7f0: 7469 6f6e 730a 2020 2020 2020 2020 6578  tions.        ex
+0000f800: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
+0000f810: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
+0000f820: 2e0a 2020 2020 2020 6361 7074 7572 655f  ..      capture_
+0000f830: 696e 7465 726d 6564 6961 7465 733a 2049  intermediates: I
+0000f840: 6620 6054 7275 6560 2c20 6361 7074 7572  f `True`, captur
+0000f850: 6573 2069 6e74 6572 6d65 6469 6174 6520  es intermediate 
+0000f860: 7265 7475 726e 2076 616c 7565 730a 2020  return values.  
+0000f870: 2020 2020 2020 6f66 2061 6c6c 204d 6f64        of all Mod
+0000f880: 756c 6573 2069 6e73 6964 6520 7468 6520  ules inside the 
+0000f890: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
+0000f8a0: 636f 6c6c 6563 7469 6f6e 2e20 4279 2064  collection. By d
+0000f8b0: 6566 6175 6c74 206f 6e6c 790a 2020 2020  efault only.    
+0000f8c0: 2020 2020 7468 6520 7265 7475 726e 2076      the return v
+0000f8d0: 616c 7565 7320 6f66 2061 6c6c 2060 605f  alues of all ``_
+0000f8e0: 5f63 616c 6c5f 5f60 6020 6d65 7468 6f64  _call__`` method
+0000f8f0: 7320 6172 6520 7374 6f72 6564 2e20 4120  s are stored. A 
+0000f900: 6675 6e63 7469 6f6e 2063 616e 0a20 2020  function can.   
+0000f910: 2020 2020 2062 6520 7061 7373 6564 2074       be passed t
+0000f920: 6f20 6368 616e 6765 2074 6865 2066 696c  o change the fil
+0000f930: 7465 7220 6265 6861 7669 6f72 2e20 5468  ter behavior. Th
+0000f940: 6520 6669 6c74 6572 2066 756e 6374 696f  e filter functio
+0000f950: 6e20 7461 6b65 730a 2020 2020 2020 2020  n takes.        
+0000f960: 7468 6520 4d6f 6475 6c65 2069 6e73 7461  the Module insta
+0000f970: 6e63 6520 616e 6420 6d65 7468 6f64 206e  nce and method n
+0000f980: 616d 6520 616e 6420 7265 7475 726e 7320  ame and returns 
+0000f990: 6120 626f 6f6c 2069 6e64 6963 6174 696e  a bool indicatin
+0000f9a0: 670a 2020 2020 2020 2020 7768 6574 6865  g.        whethe
+0000f9b0: 7220 7468 6520 6f75 7470 7574 206f 6620  r the output of 
+0000f9c0: 7468 6174 206d 6574 686f 6420 696e 766f  that method invo
+0000f9d0: 6361 7469 6f6e 2073 686f 756c 6420 6265  cation should be
+0000f9e0: 2073 746f 7265 642e 0a20 2020 2020 202a   stored..      *
+0000f9f0: 2a6b 7761 7267 733a 204b 6579 776f 7264  *kwargs: Keyword
+0000fa00: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
+0000fa10: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
+0000fa20: 6e63 7469 6f6e 2e0a 2020 2020 5265 7475  nction..    Retu
+0000fa30: 726e 733a 0a20 2020 2020 2054 6865 2069  rns:.      The i
+0000fa40: 6e69 7469 616c 697a 6564 2076 6172 6961  nitialized varia
+0000fa50: 626c 6520 6469 6374 2e0a 2020 2020 2222  ble dict..    ""
+0000fa60: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
+0000fa70: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
+0000fa80: 290a 0a20 2020 205f 2c20 765f 6f75 7420  )..    _, v_out 
+0000fa90: 3d20 7365 6c66 2e69 6e69 745f 7769 7468  = self.init_with
+0000faa0: 5f6f 7574 7075 7428 0a20 2020 2020 2020  _output(.       
+0000fab0: 2072 6e67 732c 0a20 2020 2020 2020 202a   rngs,.        *
+0000fac0: 6172 6773 2c0a 2020 2020 2020 2020 6d65  args,.        me
+0000fad0: 7468 6f64 3d6d 6574 686f 642c 0a20 2020  thod=method,.   
+0000fae0: 2020 2020 206d 7574 6162 6c65 3d6d 7574       mutable=mut
+0000faf0: 6162 6c65 2c0a 2020 2020 2020 2020 6361  able,.        ca
+0000fb00: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+0000fb10: 7465 733d 6361 7074 7572 655f 696e 7465  tes=capture_inte
+0000fb20: 726d 6564 6961 7465 732c 0a20 2020 2020  rmediates,.     
+0000fb30: 2020 202a 2a6b 7761 7267 7329 0a20 2020     **kwargs).   
+0000fb40: 2072 6574 7572 6e20 765f 6f75 740a 0a20   return v_out.. 
+0000fb50: 2040 7472 6163 6562 6163 6b5f 7574 696c   @traceback_util
+0000fb60: 2e61 7069 5f62 6f75 6e64 6172 790a 2020  .api_boundary.  
+0000fb70: 6465 6620 6c61 7a79 5f69 6e69 7428 7365  def lazy_init(se
+0000fb80: 6c66 2c0a 2020 2020 2020 2020 2020 2072  lf,.           r
+0000fb90: 6e67 733a 2055 6e69 6f6e 5b4b 6579 4172  ngs: Union[KeyAr
+0000fba0: 7261 792c 2052 4e47 5365 7175 656e 6365  ray, RNGSequence
+0000fbb0: 735d 2c0a 2020 2020 2020 2020 2020 202a  s],.           *
+0000fbc0: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
+0000fbd0: 206d 6574 686f 643a 204f 7074 696f 6e61   method: Optiona
+0000fbe0: 6c5b 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  l[Callable[..., 
+0000fbf0: 416e 795d 5d20 3d20 4e6f 6e65 2c0a 2020  Any]] = None,.  
+0000fc00: 2020 2020 2020 2020 206d 7574 6162 6c65           mutable
+0000fc10: 3a20 436f 6c6c 6563 7469 6f6e 4669 6c74  : CollectionFilt
+0000fc20: 6572 203d 2044 656e 794c 6973 7428 2769  er = DenyList('i
+0000fc30: 6e74 6572 6d65 6469 6174 6573 2729 2c0a  ntermediates'),.
+0000fc40: 2020 2020 2020 2020 2020 202a 2a6b 7761             **kwa
+0000fc50: 7267 7329 202d 3e20 4672 6f7a 656e 5661  rgs) -> FrozenVa
+0000fc60: 7269 6162 6c65 4469 6374 3a0a 2020 2020  riableDict:.    
+0000fc70: 2222 2249 6e69 7469 616c 697a 6573 2061  """Initializes a
+0000fc80: 206d 6f64 756c 6520 7769 7468 6f75 7420   module without 
+0000fc90: 636f 6d70 7574 696e 6720 6f6e 2061 6e20  computing on an 
+0000fca0: 6163 7475 616c 2069 6e70 7574 2e0a 0a20  actual input... 
+0000fcb0: 2020 206c 617a 795f 696e 6974 2077 696c     lazy_init wil
+0000fcc0: 6c20 696e 6974 6961 6c69 7a65 2074 6865  l initialize the
+0000fcd0: 2076 6172 6961 626c 6573 2077 6974 686f   variables witho
+0000fce0: 7574 2064 6f69 6e67 2075 6e6e 6563 6573  ut doing unneces
+0000fcf0: 7361 7279 2063 6f6d 7075 7465 2e0a 2020  sary compute..  
+0000fd00: 2020 5468 6520 696e 7075 7420 6461 7461    The input data
+0000fd10: 2073 686f 756c 6420 6265 2070 6173 7365   should be passe
+0000fd20: 6420 6173 2061 2060 606a 6178 2e53 6861  d as a ``jax.Sha
+0000fd30: 7065 4474 7970 6553 7472 7563 7460 6020  peDtypeStruct`` 
+0000fd40: 7768 6963 6820 7370 6563 6966 6965 730a  which specifies.
+0000fd50: 2020 2020 7468 6520 7368 6170 6520 616e      the shape an
+0000fd60: 6420 6474 7970 6520 6f66 2074 6865 2069  d dtype of the i
+0000fd70: 6e70 7574 2062 7574 206e 6f20 636f 6e63  nput but no conc
+0000fd80: 7265 7465 2064 6174 612e 0a0a 2020 2020  rete data...    
+0000fd90: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
+0000fda0: 206d 6f64 656c 203d 206e 6e2e 4465 6e73   model = nn.Dens
+0000fdb0: 6528 6665 6174 7572 6573 3d32 3536 290a  e(features=256).
+0000fdc0: 2020 2020 2020 7661 7269 6162 6c65 7320        variables 
+0000fdd0: 3d20 6d6f 6465 6c2e 6c61 7a79 5f69 6e69  = model.lazy_ini
+0000fde0: 7428 726e 672c 206a 6178 2e53 6861 7065  t(rng, jax.Shape
+0000fdf0: 4474 7970 6553 7472 7563 7428 2831 2c20  DtypeStruct((1, 
+0000fe00: 3132 3829 2c20 6a6e 702e 666c 6f61 7433  128), jnp.float3
+0000fe10: 3229 290a 0a20 2020 2054 6865 2061 7267  2))..    The arg
+0000fe20: 7320 616e 6420 6b77 6172 6773 2061 7267  s and kwargs arg
+0000fe30: 7320 7061 7373 6564 2074 6f20 6060 6c61  s passed to ``la
+0000fe40: 7a79 5f69 6e69 7460 6020 6361 6e20 6265  zy_init`` can be
+0000fe50: 2061 206d 6978 206f 660a 2020 2020 636f   a mix of.    co
+0000fe60: 6e63 7265 7465 2028 6a61 7820 6172 7261  ncrete (jax arra
+0000fe70: 7973 2c20 7363 616c 6172 732c 2062 6f6f  ys, scalars, boo
+0000fe80: 6c73 2920 616e 6420 6162 7374 7261 6374  ls) and abstract
+0000fe90: 2028 5368 6170 6544 7479 7065 5374 7275   (ShapeDtypeStru
+0000fea0: 6374 2920 7661 6c75 6573 2e0a 2020 2020  ct) values..    
+0000feb0: 436f 6e63 7265 7465 2076 616c 7565 7320  Concrete values 
+0000fec0: 6172 6520 6f6e 6c79 206e 6563 6573 7361  are only necessa
+0000fed0: 7279 2066 6f72 2061 7267 756d 656e 7473  ry for arguments
+0000fee0: 2074 6861 7420 6166 6665 6374 0a20 2020   that affect.   
+0000fef0: 2074 6865 2069 6e69 7469 616c 697a 6174   the initializat
+0000ff00: 696f 6e20 6f66 2076 6172 6961 626c 6573  ion of variables
+0000ff10: 2e20 466f 7220 6578 616d 706c 652c 2074  . For example, t
+0000ff20: 6865 206d 6f64 656c 206d 6967 6874 2065  he model might e
+0000ff30: 7870 6563 740a 2020 2020 6120 6b65 7977  xpect.    a keyw
+0000ff40: 6f72 6420 6172 6720 7468 6174 2065 6e61  ord arg that ena
+0000ff50: 626c 6573 2f64 6973 6162 6c65 7320 6120  bles/disables a 
+0000ff60: 7375 6270 6172 7420 6f66 2074 6865 206d  subpart of the m
+0000ff70: 6f64 656c 2e0a 2020 2020 496e 2074 6869  odel..    In thi
+0000ff80: 7320 6361 7365 2c20 616e 2065 7870 6c69  s case, an expli
+0000ff90: 6369 7420 7661 6c75 6520 2854 7275 652f  cit value (True/
+0000ffa0: 466c 6173 6529 2073 686f 756c 6420 6265  Flase) should be
+0000ffb0: 2070 6173 7365 6420 6f74 6865 7277 6973   passed otherwis
+0000ffc0: 650a 2020 2020 6060 6c61 7a79 5f69 6e69  e.    ``lazy_ini
+0000ffd0: 7460 6020 6361 6e6e 6f74 2069 6e66 6572  t`` cannot infer
+0000ffe0: 2077 6869 6368 2076 6172 6961 626c 6573   which variables
+0000fff0: 2073 686f 756c 6420 6265 2069 6e69 7469   should be initi
+00010000: 616c 697a 6564 2e0a 0a20 2020 2041 7267  alized...    Arg
+00010010: 733a 0a20 2020 2020 2072 6e67 733a 2054  s:.      rngs: T
+00010020: 6865 2072 6e67 7320 666f 7220 7468 6520  he rngs for the 
+00010030: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
+00010040: 696f 6e73 2e0a 2020 2020 2020 2a61 7267  ions..      *arg
+00010050: 733a 2061 7267 756d 656e 7473 2070 6173  s: arguments pas
+00010060: 7365 6420 746f 2074 6865 2069 6e69 7420  sed to the init 
+00010070: 6675 6e63 7469 6f6e 2e0a 2020 2020 2020  function..      
+00010080: 6d65 7468 6f64 3a20 416e 206f 7074 696f  method: An optio
+00010090: 6e61 6c20 6d65 7468 6f64 2e20 4966 2070  nal method. If p
+000100a0: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
+000100b0: 2074 6869 7320 6d65 7468 6f64 2e20 4966   this method. If
+000100c0: 206e 6f74 0a20 2020 2020 2020 2070 726f   not.        pro
+000100d0: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
+000100e0: 6865 2060 605f 5f63 616c 6c5f 5f60 6020  he ``__call__`` 
+000100f0: 6d65 7468 6f64 2e0a 2020 2020 2020 6d75  method..      mu
+00010100: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+00010110: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+00010120: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+00010130: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+00010140: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
+00010150: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
+00010160: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
+00010170: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
+00010180: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
+00010190: 2020 2020 2020 6060 7374 7260 603a 2054        ``str``: T
+000101a0: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
+000101b0: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+000101c0: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+000101d0: 3a20 410a 2020 2020 2020 2020 6c69 7374  : A.        list
+000101e0: 206f 6620 6e61 6d65 7320 6f66 206d 7574   of names of mut
+000101f0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+00010200: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
+00010210: 2063 6f6c 6c65 6374 696f 6e73 0a20 2020   collections.   
+00010220: 2020 2020 2065 7863 6570 7420 2269 6e74       except "int
+00010230: 6572 6d65 6469 6174 6573 2220 6172 6520  ermediates" are 
+00010240: 6d75 7461 626c 652e 0a20 2020 2020 202a  mutable..      *
+00010250: 2a6b 7761 7267 733a 204b 6579 776f 7264  *kwargs: Keyword
+00010260: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
+00010270: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
+00010280: 6e63 7469 6f6e 2e0a 2020 2020 5265 7475  nction..    Retu
+00010290: 726e 733a 0a20 2020 2020 2054 6865 2069  rns:.      The i
+000102a0: 6e69 7469 616c 697a 6564 2076 6172 6961  nitialized varia
+000102b0: 626c 6520 6469 6374 2e0a 2020 2020 2222  ble dict..    ""
+000102c0: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
+000102d0: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
+000102e0: 290a 2020 2020 6465 6620 6c61 7a79 5f77  ).    def lazy_w
+000102f0: 7261 7070 6572 2872 6e67 732c 202a 6172  rapper(rngs, *ar
+00010300: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
+00010310: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00010320: 2e69 6e69 7428 726e 6773 2c20 2a61 7267  .init(rngs, *arg
+00010330: 732c 206d 6574 686f 643d 6d65 7468 6f64  s, method=method
+00010340: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
+00010350: 652c 202a 2a6b 7761 7267 7329 0a20 2020  e, **kwargs).   
+00010360: 2072 6574 7572 6e20 7061 7274 6961 6c5f   return partial_
+00010370: 6576 616c 2e6c 617a 795f 696e 6974 286c  eval.lazy_init(l
+00010380: 617a 795f 7772 6170 7065 7229 2872 6e67  azy_wrapper)(rng
+00010390: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+000103a0: 6773 290a 0a20 2040 7072 6f70 6572 7479  gs)..  @property
+000103b0: 0a20 2064 6566 2076 6172 6961 626c 6573  .  def variables
+000103c0: 2873 656c 6629 202d 3e20 5661 7269 6162  (self) -> Variab
+000103d0: 6c65 4469 6374 3a0a 2020 2020 2222 2252  leDict:.    """R
+000103e0: 6574 7572 6e73 2074 6865 2076 6172 6961  eturns the varia
+000103f0: 626c 6573 2069 6e20 7468 6973 206d 6f64  bles in this mod
+00010400: 756c 652e 2222 220a 2020 2020 6966 2073  ule.""".    if s
+00010410: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
+00010420: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
+00010430: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
+00010440: 2061 6363 6573 7320 7661 7269 6162 6c65   access variable
+00010450: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
+00010460: 756c 6573 2229 0a20 2020 2072 6574 7572  ules").    retur
+00010470: 6e20 7365 6c66 2e73 636f 7065 2e76 6172  n self.scope.var
+00010480: 6961 626c 6573 2829 0a0a 2020 6465 6620  iables()..  def 
+00010490: 6765 745f 7661 7269 6162 6c65 2873 656c  get_variable(sel
+000104a0: 662c 2063 6f6c 3a20 7374 722c 206e 616d  f, col: str, nam
+000104b0: 653a 2073 7472 2c20 6465 6661 756c 743a  e: str, default:
+000104c0: 204f 7074 696f 6e61 6c5b 545d 203d 204e   Optional[T] = N
+000104d0: 6f6e 6529 202d 3e20 543a 0a20 2020 2022  one) -> T:.    "
+000104e0: 2222 5265 7472 6965 7665 7320 7468 6520  ""Retrieves the 
+000104f0: 7661 6c75 6520 6f66 2061 2056 6172 6961  value of a Varia
+00010500: 626c 652e 0a0a 2020 2020 4172 6773 3a0a  ble...    Args:.
+00010510: 2020 2020 2020 636f 6c3a 2074 6865 2076        col: the v
+00010520: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
+00010530: 6f6e 2e0a 2020 2020 2020 6e61 6d65 3a20  on..      name: 
+00010540: 7468 6520 6e61 6d65 206f 6620 7468 6520  the name of the 
+00010550: 7661 7269 6162 6c65 2e0a 2020 2020 2020  variable..      
+00010560: 6465 6661 756c 743a 2074 6865 2064 6566  default: the def
+00010570: 6175 6c74 2076 616c 7565 2074 6f20 7265  ault value to re
+00010580: 7475 726e 2069 6620 7468 6520 7661 7269  turn if the vari
+00010590: 6162 6c65 2064 6f65 7320 6e6f 7420 6578  able does not ex
+000105a0: 6973 7420 696e 0a20 2020 2020 2020 2074  ist in.        t
+000105b0: 6869 7320 7363 6f70 652e 0a0a 2020 2020  his scope...    
+000105c0: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
+000105d0: 6865 2076 616c 7565 206f 6620 7468 6520  he value of the 
+000105e0: 696e 7075 7420 7661 7269 6162 6c65 2c20  input variable, 
+000105f0: 6f66 2074 6865 2064 6566 6175 6c74 2076  of the default v
+00010600: 616c 7565 2069 6620 7468 6520 7661 7269  alue if the vari
+00010610: 6162 6c65 0a20 2020 2020 2064 6f65 736e  able.      doesn
+00010620: 2774 2065 7869 7374 2069 6e20 7468 6973  't exist in this
+00010630: 2073 636f 7065 2e0a 2020 2020 2222 220a   scope..    """.
+00010640: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
+00010650: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
+00010660: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00010670: 7228 2243 616e 2774 2061 6363 6573 7320  r("Can't access 
+00010680: 7661 7269 6162 6c65 7320 6f6e 2075 6e62  variables on unb
+00010690: 6f75 6e64 206d 6f64 756c 6573 2229 0a20  ound modules"). 
+000106a0: 2020 2072 6574 7572 6e20 7365 6c66 2e73     return self.s
+000106b0: 636f 7065 2e67 6574 5f76 6172 6961 626c  cope.get_variabl
+000106c0: 6528 636f 6c2c 206e 616d 652c 2064 6566  e(col, name, def
+000106d0: 6175 6c74 290a 0a20 2064 6566 2070 7574  ault)..  def put
+000106e0: 5f76 6172 6961 626c 6528 7365 6c66 2c20  _variable(self, 
+000106f0: 636f 6c3a 2073 7472 2c20 6e61 6d65 3a20  col: str, name: 
+00010700: 7374 722c 2076 616c 7565 3a20 416e 7929  str, value: Any)
+00010710: 3a0a 2020 2020 2222 2255 7064 6174 6573  :.    """Updates
+00010720: 2074 6865 2076 616c 7565 206f 6620 7468   the value of th
+00010730: 6520 6769 7665 6e20 7661 7269 6162 6c65  e given variable
+00010740: 2069 6620 6974 2069 7320 6d75 7461 626c   if it is mutabl
+00010750: 652c 206f 7220 616e 2065 7272 6f72 206f  e, or an error o
+00010760: 7468 6572 7769 7365 2e0a 0a20 2020 2041  therwise...    A
+00010770: 7267 733a 0a20 2020 2020 2063 6f6c 3a20  rgs:.      col: 
+00010780: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
+00010790: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
+000107a0: 616d 653a 2074 6865 206e 616d 6520 6f66  ame: the name of
+000107b0: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
+000107c0: 2020 2020 2076 616c 7565 3a20 7468 6520       value: the 
+000107d0: 6e65 7720 7661 6c75 6520 6f66 2074 6865  new value of the
+000107e0: 2076 6172 6961 626c 652e 0a20 2020 2022   variable..    "
+000107f0: 2222 0a20 2020 2069 6620 7365 6c66 2e73  "".    if self.s
+00010800: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
+00010810: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00010820: 7272 6f72 2822 4361 6e27 7420 6163 6365  rror("Can't acce
+00010830: 7373 2076 6172 6961 626c 6573 206f 6e20  ss variables on 
+00010840: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
+00010850: 290a 2020 2020 7365 6c66 2e73 636f 7065  ).    self.scope
+00010860: 2e70 7574 5f76 6172 6961 626c 6528 636f  .put_variable(co
+00010870: 6c2c 206e 616d 652c 2076 616c 7565 290a  l, name, value).
+00010880: 0a20 2040 6f76 6572 6c6f 6164 0a20 2064  .  @overload.  d
+00010890: 6566 2073 6f77 2873 656c 662c 2063 6f6c  ef sow(self, col
+000108a0: 3a20 7374 722c 206e 616d 653a 2073 7472  : str, name: str
+000108b0: 2c20 7661 6c75 653a 2041 6e79 2920 2d3e  , value: Any) ->
+000108c0: 2062 6f6f 6c3a 0a20 2020 202e 2e2e 0a0a   bool:.    .....
+000108d0: 2020 406f 7665 726c 6f61 640a 2020 6465    @overload.  de
+000108e0: 6620 736f 7728 7365 6c66 2c20 636f 6c3a  f sow(self, col:
+000108f0: 2073 7472 2c20 6e61 6d65 3a20 7374 722c   str, name: str,
+00010900: 2076 616c 7565 3a20 542c 0a20 2020 2020   value: T,.     
+00010910: 2020 2020 2072 6564 7563 655f 666e 3a20       reduce_fn: 
+00010920: 4361 6c6c 6162 6c65 5b5b 4b2c 2054 5d2c  Callable[[K, T],
+00010930: 204b 5d20 3d20 7475 706c 655f 7265 6475   K] = tuple_redu
+00010940: 6365 2c0a 2020 2020 2020 2020 2020 696e  ce,.          in
+00010950: 6974 5f66 6e3a 2043 616c 6c61 626c 655b  it_fn: Callable[
+00010960: 5b5d 2c20 4b5d 203d 2074 7570 6c65 5f69  [], K] = tuple_i
+00010970: 6e69 7429 202d 3e20 626f 6f6c 3a20 2320  nit) -> bool: # 
+00010980: 7479 7065 3a20 6967 6e6f 7265 0a20 2020  type: ignore.   
+00010990: 202e 2e2e 0a0a 2020 6465 6620 736f 7728   .....  def sow(
+000109a0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
+000109b0: 6e61 6d65 3a20 7374 722c 2076 616c 7565  name: str, value
+000109c0: 3a20 542c 0a20 2020 2020 2020 2020 2072  : T,.          r
+000109d0: 6564 7563 655f 666e 3a20 4361 6c6c 6162  educe_fn: Callab
+000109e0: 6c65 5b5b 4b2c 2054 5d2c 204b 5d20 3d20  le[[K, T], K] = 
+000109f0: 7475 706c 655f 7265 6475 6365 2c0a 2020  tuple_reduce,.  
+00010a00: 2020 2020 2020 2020 696e 6974 5f66 6e3a          init_fn:
+00010a10: 2043 616c 6c61 626c 655b 5b5d 2c20 4b5d   Callable[[], K]
+00010a20: 203d 2074 7570 6c65 5f69 6e69 7429 202d   = tuple_init) -
+00010a30: 3e20 626f 6f6c 3a20 2320 7479 7065 3a20  > bool: # type: 
+00010a40: 6967 6e6f 7265 0a20 2020 2022 2222 5374  ignore.    """St
+00010a50: 6f72 6573 2061 2076 616c 7565 2069 6e20  ores a value in 
+00010a60: 6120 636f 6c6c 6563 7469 6f6e 2e0a 0a20  a collection... 
+00010a70: 2020 2043 6f6c 6c65 6374 696f 6e73 2063     Collections c
+00010a80: 616e 2062 6520 7573 6564 2074 6f20 636f  an be used to co
+00010a90: 6c6c 6563 7420 696e 7465 726d 6564 6961  llect intermedia
+00010aa0: 7465 2076 616c 7565 7320 7769 7468 6f75  te values withou
+00010ab0: 740a 2020 2020 7468 6520 6f76 6572 6865  t.    the overhe
+00010ac0: 6164 206f 6620 6578 706c 6963 6974 6c79  ad of explicitly
+00010ad0: 2070 6173 7369 6e67 2061 2063 6f6e 7461   passing a conta
+00010ae0: 696e 6572 2074 6872 6f75 6768 2065 6163  iner through eac
+00010af0: 6820 4d6f 6475 6c65 2063 616c 6c2e 0a0a  h Module call...
+00010b00: 2020 2020 4966 2074 6865 2074 6172 6765      If the targe
+00010b10: 7420 636f 6c6c 6563 7469 6f6e 2069 7320  t collection is 
+00010b20: 6e6f 7420 6d75 7461 626c 6520 6073 6f77  not mutable `sow
+00010b30: 6020 6265 6861 7665 7320 6c69 6b65 2061  ` behaves like a
+00010b40: 206e 6f2d 6f70 0a20 2020 2061 6e64 2072   no-op.    and r
+00010b50: 6574 7572 6e73 2060 4661 6c73 6560 2e0a  eturns `False`..
+00010b60: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
+00010b70: 2020 2020 2020 696d 706f 7274 206a 6178        import jax
+00010b80: 0a20 2020 2020 2069 6d70 6f72 7420 6a61  .      import ja
+00010b90: 782e 6e75 6d70 7920 6173 206a 6e70 0a20  x.numpy as jnp. 
+00010ba0: 2020 2020 2069 6d70 6f72 7420 666c 6178       import flax
+00010bb0: 2e6c 696e 656e 2061 7320 6e6e 0a0a 2020  .linen as nn..  
+00010bc0: 2020 2020 636c 6173 7320 466f 6f28 6e6e      class Foo(nn
+00010bd0: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
+00010be0: 2020 406e 6e2e 636f 6d70 6163 740a 2020    @nn.compact.  
+00010bf0: 2020 2020 2020 6465 6620 5f5f 6361 6c6c        def __call
+00010c00: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+00010c10: 2020 2020 2020 2068 203d 206e 6e2e 4465         h = nn.De
+00010c20: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
+00010c30: 2020 2020 7365 6c66 2e73 6f77 2827 696e      self.sow('in
+00010c40: 7465 726d 6564 6961 7465 7327 2c20 2768  termediates', 'h
+00010c50: 272c 2068 290a 2020 2020 2020 2020 2020  ', h).          
+00010c60: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
+00010c70: 3229 2868 290a 0a20 2020 2020 2078 203d  2)(h)..      x =
+00010c80: 206a 6e70 2e6f 6e65 7328 2831 362c 2039   jnp.ones((16, 9
+00010c90: 2929 0a20 2020 2020 206d 6f64 656c 203d  )).      model =
+00010ca0: 2046 6f6f 2829 0a20 2020 2020 2076 6172   Foo().      var
+00010cb0: 6961 626c 6573 203d 206d 6f64 656c 2e69  iables = model.i
+00010cc0: 6e69 7428 6a61 782e 7261 6e64 6f6d 2e50  nit(jax.random.P
+00010cd0: 524e 474b 6579 2830 292c 2078 290a 2020  RNGKey(0), x).  
+00010ce0: 2020 2020 792c 2073 7461 7465 203d 206d      y, state = m
+00010cf0: 6f64 656c 2e61 7070 6c79 2876 6172 6961  odel.apply(varia
+00010d00: 626c 6573 2c20 782c 206d 7574 6162 6c65  bles, x, mutable
+00010d10: 3d5b 2769 6e74 6572 6d65 6469 6174 6573  =['intermediates
+00010d20: 275d 290a 2020 2020 2020 7072 696e 7428  ']).      print(
+00010d30: 7374 6174 655b 2769 6e74 6572 6d65 6469  state['intermedi
+00010d40: 6174 6573 275d 2920 2023 207b 2768 273a  ates'])  # {'h':
+00010d50: 2028 2e2e 2e2c 297d 0a0a 2020 2020 4279   (...,)}..    By
+00010d60: 2064 6566 6175 6c74 2074 6865 2076 616c   default the val
+00010d70: 7565 7320 6172 6520 7374 6f72 6564 2069  ues are stored i
+00010d80: 6e20 6120 7475 706c 6520 616e 6420 6561  n a tuple and ea
+00010d90: 6368 2073 746f 7265 6420 7661 6c75 650a  ch stored value.
+00010da0: 2020 2020 6973 2061 7070 656e 6465 6420      is appended 
+00010db0: 6174 2074 6865 2065 6e64 2e20 5468 6973  at the end. This
+00010dc0: 2077 6179 2061 6c6c 2069 6e74 6572 6d65   way all interme
+00010dd0: 6469 6174 6573 2063 616e 2062 6520 7472  diates can be tr
+00010de0: 6163 6b65 6420 7768 656e 0a20 2020 2074  acked when.    t
+00010df0: 6865 2073 616d 6520 6d6f 6475 6c65 2069  he same module i
+00010e00: 7320 6361 6c6c 6564 206d 756c 7469 706c  s called multipl
+00010e10: 6520 7469 6d65 732e 2041 6c74 6572 6e61  e times. Alterna
+00010e20: 7469 7665 6c79 2c20 6120 6375 7374 6f6d  tively, a custom
+00010e30: 0a20 2020 2069 6e69 742f 7265 6475 6365  .    init/reduce
+00010e40: 2066 756e 6374 696f 6e20 6361 6e20 6265   function can be
+00010e50: 2070 6173 7365 643a 3a0a 0a20 2020 2020   passed::..     
+00010e60: 2063 6c61 7373 2046 6f6f 3228 6e6e 2e4d   class Foo2(nn.M
+00010e70: 6f64 756c 6529 3a0a 2020 2020 2020 2020  odule):.        
+00010e80: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
+00010e90: 2020 2020 6465 6620 5f5f 6361 6c6c 5f5f      def __call__
+00010ea0: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
+00010eb0: 2020 2020 2069 6e69 745f 666e 203d 206c       init_fn = l
+00010ec0: 616d 6264 613a 2030 0a20 2020 2020 2020  ambda: 0.       
+00010ed0: 2020 2072 6564 7563 655f 666e 203d 206c     reduce_fn = l
+00010ee0: 616d 6264 6120 612c 2062 3a20 6120 2b20  ambda a, b: a + 
+00010ef0: 620a 2020 2020 2020 2020 2020 7365 6c66  b.          self
+00010f00: 2e73 6f77 2827 696e 7465 726d 6564 6961  .sow('intermedia
+00010f10: 7465 7327 2c20 2768 272c 2078 2c0a 2020  tes', 'h', x,.  
+00010f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010f30: 2069 6e69 745f 666e 3d69 6e69 745f 666e   init_fn=init_fn
+00010f40: 2c20 7265 6475 6365 5f66 6e3d 7265 6475  , reduce_fn=redu
+00010f50: 6365 5f66 6e29 0a20 2020 2020 2020 2020  ce_fn).         
+00010f60: 2073 656c 662e 736f 7728 2769 6e74 6572   self.sow('inter
+00010f70: 6d65 6469 6174 6573 272c 2027 6827 2c20  mediates', 'h', 
+00010f80: 7820 2a20 322c 0a20 2020 2020 2020 2020  x * 2,.         
+00010f90: 2020 2020 2020 2020 2020 696e 6974 5f66            init_f
+00010fa0: 6e3d 696e 6974 5f66 6e2c 2072 6564 7563  n=init_fn, reduc
+00010fb0: 655f 666e 3d72 6564 7563 655f 666e 290a  e_fn=reduce_fn).
+00010fc0: 2020 2020 2020 2020 2020 7265 7475 726e            return
+00010fd0: 2078 0a0a 2020 2020 2020 6d6f 6465 6c20   x..      model 
+00010fe0: 3d20 466f 6f32 2829 0a20 2020 2020 2076  = Foo2().      v
+00010ff0: 6172 6961 626c 6573 203d 206d 6f64 656c  ariables = model
+00011000: 2e69 6e69 7428 6a61 782e 7261 6e64 6f6d  .init(jax.random
+00011010: 2e50 524e 474b 6579 2830 292c 2078 290a  .PRNGKey(0), x).
+00011020: 2020 2020 2020 792c 2073 7461 7465 203d        y, state =
+00011030: 206d 6f64 656c 2e61 7070 6c79 2876 6172   model.apply(var
+00011040: 6961 626c 6573 2c20 6a6e 702e 6f6e 6573  iables, jnp.ones
+00011050: 2828 312c 2031 2929 2c20 6d75 7461 626c  ((1, 1)), mutabl
+00011060: 653d 5b27 696e 7465 726d 6564 6961 7465  e=['intermediate
+00011070: 7327 5d29 0a20 2020 2020 2070 7269 6e74  s']).      print
+00011080: 2873 7461 7465 5b27 696e 7465 726d 6564  (state['intermed
+00011090: 6961 7465 7327 5d29 2020 2320 3d3d 3e20  iates'])  # ==> 
+000110a0: 7b27 6827 3a20 5b5b 332e 5d5d 7d0a 0a20  {'h': [[3.]]}.. 
+000110b0: 2020 2041 7267 733a 0a20 2020 2020 2063     Args:.      c
+000110c0: 6f6c 3a20 5468 6520 6e61 6d65 206f 6620  ol: The name of 
+000110d0: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
+000110e0: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
+000110f0: 616d 653a 2054 6865 206e 616d 6520 6f66  ame: The name of
+00011100: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
+00011110: 2020 2020 2076 616c 7565 3a20 5468 6520       value: The 
+00011120: 7661 6c75 6520 6f66 2074 6865 2076 6172  value of the var
+00011130: 6961 626c 652e 0a20 2020 2020 2072 6564  iable..      red
+00011140: 7563 655f 666e 3a20 5468 6520 6675 6e63  uce_fn: The func
+00011150: 7469 6f6e 2075 7365 6420 746f 2063 6f6d  tion used to com
+00011160: 6269 6e65 2074 6865 2065 7869 7374 696e  bine the existin
+00011170: 6720 7661 6c75 6520 7769 7468 0a20 2020  g value with.   
+00011180: 2020 2020 2074 6865 206e 6577 2076 616c       the new val
+00011190: 7565 2e20 5468 6520 6465 6661 756c 7420  ue. The default 
+000111a0: 6973 2074 6f20 6170 7065 6e64 2074 6865  is to append the
+000111b0: 2076 616c 7565 2074 6f20 6120 7475 706c   value to a tupl
+000111c0: 652e 0a20 2020 2020 2069 6e69 745f 666e  e..      init_fn
+000111d0: 3a20 466f 7220 7468 6520 6669 7273 7420  : For the first 
+000111e0: 7661 6c75 6520 7374 6f72 6564 2c20 6072  value stored, `r
+000111f0: 6564 7563 655f 666e 6020 7769 6c6c 2062  educe_fn` will b
+00011200: 6520 7061 7373 6564 0a20 2020 2020 2020  e passed.       
+00011210: 2074 6865 2072 6573 756c 7420 6f66 2060   the result of `
+00011220: 696e 6974 5f66 6e60 2074 6f67 6574 6865  init_fn` togethe
+00011230: 7220 7769 7468 2074 6865 2076 616c 7565  r with the value
+00011240: 2074 6f20 6265 2073 746f 7265 642e 0a20   to be stored.. 
+00011250: 2020 2020 2020 2054 6865 2064 6566 6175         The defau
+00011260: 6c74 2069 7320 616e 2065 6d70 7479 2074  lt is an empty t
+00011270: 7570 6c65 2e0a 0a20 2020 2052 6574 7572  uple...    Retur
+00011280: 6e73 3a0a 2020 2020 2020 6054 7275 6560  ns:.      `True`
+00011290: 2069 6620 7468 6520 7661 6c75 6520 6861   if the value ha
+000112a0: 7320 6265 656e 2073 746f 7265 6420 7375  s been stored su
+000112b0: 6363 6573 7366 756c 6c79 2c20 6046 616c  ccessfully, `Fal
+000112c0: 7365 6020 6f74 6865 7277 6973 652e 0a20  se` otherwise.. 
+000112d0: 2020 2022 2222 0a20 2020 2069 6620 7365     """.    if se
+000112e0: 6c66 2e73 636f 7065 2069 7320 4e6f 6e65  lf.scope is None
+000112f0: 3a0a 2020 2020 2020 7261 6973 6520 5661  :.      raise Va
+00011300: 6c75 6545 7272 6f72 2822 4361 6e27 7420  lueError("Can't 
+00011310: 7374 6f72 6520 7661 7269 6162 6c65 7320  store variables 
+00011320: 6f6e 2075 6e62 6f75 6e64 206d 6f64 756c  on unbound modul
+00011330: 6573 2229 0a20 2020 2069 6620 6e6f 7420  es").    if not 
+00011340: 7365 6c66 2e73 636f 7065 2e69 735f 6d75  self.scope.is_mu
+00011350: 7461 626c 655f 636f 6c6c 6563 7469 6f6e  table_collection
+00011360: 2863 6f6c 293a 0a20 2020 2020 2072 6574  (col):.      ret
+00011370: 7572 6e20 4661 6c73 650a 2020 2020 6966  urn False.    if
+00011380: 2073 656c 662e 7363 6f70 652e 6861 735f   self.scope.has_
+00011390: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
+000113a0: 6d65 293a 0a20 2020 2020 2078 7320 3d20  me):.      xs = 
+000113b0: 7365 6c66 2e73 636f 7065 2e67 6574 5f76  self.scope.get_v
+000113c0: 6172 6961 626c 6528 636f 6c2c 206e 616d  ariable(col, nam
+000113d0: 6529 0a20 2020 2065 6c73 653a 0a20 2020  e).    else:.   
+000113e0: 2020 2073 656c 662e 7363 6f70 652e 7265     self.scope.re
+000113f0: 7365 7276 6528 6e61 6d65 2c20 636f 6c29  serve(name, col)
+00011400: 0a20 2020 2020 2073 656c 662e 5f73 7461  .      self._sta
+00011410: 7465 2e63 6869 6c64 7265 6e5b 6e61 6d65  te.children[name
+00011420: 5d20 3d20 636f 6c0a 2020 2020 2020 7873  ] = col.      xs
+00011430: 203d 2069 6e69 745f 666e 2829 0a20 2020   = init_fn().   
+00011440: 2078 7320 3d20 7265 6475 6365 5f66 6e28   xs = reduce_fn(
+00011450: 7873 2c20 7661 6c75 6529 0a20 2020 2073  xs, value).    s
+00011460: 656c 662e 7363 6f70 652e 7075 745f 7661  elf.scope.put_va
+00011470: 7269 6162 6c65 2863 6f6c 2c20 6e61 6d65  riable(col, name
+00011480: 2c20 7873 290a 2020 2020 7265 7475 726e  , xs).    return
+00011490: 2054 7275 650a 0a20 2064 6566 2070 6572   True..  def per
+000114a0: 7475 7262 2873 656c 662c 206e 616d 653a  turb(self, name:
+000114b0: 2073 7472 2c20 7661 6c75 653a 2054 2c20   str, value: T, 
+000114c0: 636f 6c6c 6563 7469 6f6e 3a20 7374 7220  collection: str 
+000114d0: 3d20 2770 6572 7475 7262 6174 696f 6e73  = 'perturbations
+000114e0: 2729 202d 3e20 543a 0a20 2020 2022 2222  ') -> T:.    """
+000114f0: 4164 6420 616e 207a 6572 6f2d 7661 6c75  Add an zero-valu
+00011500: 6520 7661 7269 6162 6c65 2028 2770 6572  e variable ('per
+00011510: 7475 7262 6174 696f 6e27 2920 746f 2074  turbation') to t
+00011520: 6865 2069 6e74 6572 6d65 6469 6174 6520  he intermediate 
+00011530: 7661 6c75 652e 0a0a 2020 2020 5468 6520  value...    The 
+00011540: 6772 6164 6965 6e74 206f 6620 6076 616c  gradient of `val
+00011550: 7565 6020 776f 756c 6420 6265 2074 6865  ue` would be the
+00011560: 2073 616d 6520 6173 2074 6865 2067 7261   same as the gra
+00011570: 6469 656e 7420 6f66 2074 6869 730a 2020  dient of this.  
+00011580: 2020 7065 7274 7572 6261 7469 6f6e 2076    perturbation v
+00011590: 6172 6961 626c 652e 2054 6865 7265 666f  ariable. Therefo
+000115a0: 7265 2c20 6966 2079 6f75 2064 6566 696e  re, if you defin
+000115b0: 6520 796f 7572 206c 6f73 7320 6675 6e63  e your loss func
+000115c0: 7469 6f6e 2077 6974 680a 2020 2020 626f  tion with.    bo
+000115d0: 7468 2070 6172 616d 7320 616e 6420 7065  th params and pe
+000115e0: 7274 7572 6261 7469 6f6e 7320 6173 2073  rturbations as s
+000115f0: 7461 6e64 616c 6f6e 6520 6172 6775 6d65  tandalone argume
+00011600: 6e74 732c 2079 6f75 2063 616e 2067 6574  nts, you can get
+00011610: 2074 6865 0a20 2020 2069 6e74 6572 6d65   the.    interme
+00011620: 6469 6174 6520 6772 6164 6965 6e74 7320  diate gradients 
+00011630: 6f66 2060 7661 6c75 6560 2062 7920 7275  of `value` by ru
+00011640: 6e6e 696e 6720 606a 6178 2e67 7261 6460  nning `jax.grad`
+00011650: 206f 6e20 7468 6520 7065 7274 7572 6261   on the perturba
+00011660: 7469 6f6e 0a20 2020 2061 7267 756d 656e  tion.    argumen
+00011670: 742e 0a0a 2020 2020 4e6f 7465 3a20 7468  t...    Note: th
+00011680: 6973 2069 7320 616e 2065 7870 6572 696d  is is an experim
+00011690: 656e 7461 6c20 4150 4920 616e 6420 6d61  ental API and ma
+000116a0: 7920 6265 2074 7765 616b 6564 206c 6174  y be tweaked lat
+000116b0: 6572 2066 6f72 2062 6574 7465 720a 2020  er for better.  
+000116c0: 2020 7065 7266 6f72 6d61 6e63 6520 616e    performance an
+000116d0: 6420 7573 6162 696c 6974 792e 0a20 2020  d usability..   
+000116e0: 2041 7420 6974 7320 6375 7272 656e 7420   At its current 
+000116f0: 7374 6167 652c 2069 7420 6372 6561 7465  stage, it create
+00011700: 7320 6578 7472 6120 6475 6d6d 7920 7661  s extra dummy va
+00011710: 7269 6162 6c65 7320 7468 6174 206f 6363  riables that occ
+00011720: 7570 6965 7320 6578 7472 610a 2020 2020  upies extra.    
+00011730: 6d65 6d6f 7279 2073 7061 6365 2e20 5573  memory space. Us
+00011740: 6520 6974 206f 6e6c 7920 746f 2064 6562  e it only to deb
+00011750: 7567 2067 7261 6469 656e 7473 2069 6e20  ug gradients in 
+00011760: 7472 6169 6e69 6e67 2e0a 0a20 2020 2045  training...    E
+00011770: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
+00011780: 696d 706f 7274 206a 6178 0a20 2020 2020  import jax.     
+00011790: 2069 6d70 6f72 7420 6a61 782e 6e75 6d70   import jax.nump
+000117a0: 7920 6173 206a 6e70 0a20 2020 2020 2069  y as jnp.      i
+000117b0: 6d70 6f72 7420 666c 6178 2e6c 696e 656e  mport flax.linen
+000117c0: 2061 7320 6e6e 0a0a 2020 2020 2020 636c   as nn..      cl
+000117d0: 6173 7320 466f 6f28 6e6e 2e4d 6f64 756c  ass Foo(nn.Modul
+000117e0: 6529 3a0a 2020 2020 2020 2020 2020 406e  e):.          @n
+000117f0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
+00011800: 2020 2020 6465 6620 5f5f 6361 6c6c 5f5f      def __call__
+00011810: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
+00011820: 2020 2020 2020 2020 2078 203d 206e 6e2e           x = nn.
+00011830: 4465 6e73 6528 3329 2878 290a 2020 2020  Dense(3)(x).    
+00011840: 2020 2020 2020 2020 2020 7820 3d20 7365            x = se
+00011850: 6c66 2e70 6572 7475 7262 2827 6465 6e73  lf.perturb('dens
+00011860: 6533 272c 2078 290a 2020 2020 2020 2020  e3', x).        
+00011870: 2020 2020 2020 7265 7475 726e 206e 6e2e        return nn.
+00011880: 4465 6e73 6528 3229 2878 290a 0a20 2020  Dense(2)(x)..   
+00011890: 2020 2064 6566 206c 6f73 7328 7061 7261     def loss(para
+000118a0: 6d73 2c20 7065 7274 7572 6261 7469 6f6e  ms, perturbation
+000118b0: 732c 2069 6e70 7574 732c 2074 6172 6765  s, inputs, targe
+000118c0: 7473 293a 0a20 2020 2020 2020 2076 6172  ts):.        var
+000118d0: 6961 626c 6573 203d 207b 2770 6172 616d  iables = {'param
+000118e0: 7327 3a20 7061 7261 6d73 2c20 2770 6572  s': params, 'per
+000118f0: 7475 7262 6174 696f 6e73 273a 2070 6572  turbations': per
+00011900: 7475 7262 6174 696f 6e73 7d0a 2020 2020  turbations}.    
+00011910: 2020 2020 7072 6564 7320 3d20 6d6f 6465      preds = mode
+00011920: 6c2e 6170 706c 7928 7661 7269 6162 6c65  l.apply(variable
+00011930: 732c 2069 6e70 7574 7329 0a20 2020 2020  s, inputs).     
+00011940: 2020 2072 6574 7572 6e20 6a6e 702e 7371     return jnp.sq
+00011950: 7561 7265 2870 7265 6473 202d 2074 6172  uare(preds - tar
+00011960: 6765 7473 292e 6d65 616e 2829 0a0a 2020  gets).mean()..  
+00011970: 2020 2020 7820 3d20 6a6e 702e 6f6e 6573      x = jnp.ones
+00011980: 2828 322c 2039 2929 0a20 2020 2020 2079  ((2, 9)).      y
+00011990: 203d 206a 6e70 2e6f 6e65 7328 2832 2c20   = jnp.ones((2, 
+000119a0: 3229 290a 2020 2020 2020 6d6f 6465 6c20  2)).      model 
+000119b0: 3d20 466f 6f28 290a 2020 2020 2020 7661  = Foo().      va
+000119c0: 7269 6162 6c65 7320 3d20 6d6f 6465 6c2e  riables = model.
+000119d0: 696e 6974 286a 6178 2e72 616e 646f 6d2e  init(jax.random.
+000119e0: 5052 4e47 4b65 7928 3029 2c20 7829 0a20  PRNGKey(0), x). 
+000119f0: 2020 2020 2069 6e74 6d5f 6772 6164 7320       intm_grads 
+00011a00: 3d20 6a61 782e 6772 6164 286c 6f73 732c  = jax.grad(loss,
+00011a10: 2061 7267 6e75 6d73 3d31 2928 7661 7269   argnums=1)(vari
+00011a20: 6162 6c65 735b 2770 6172 616d 7327 5d2c  ables['params'],
+00011a30: 2076 6172 6961 626c 6573 5b27 7065 7274   variables['pert
+00011a40: 7572 6261 7469 6f6e 7327 5d2c 2078 2c20  urbations'], x, 
+00011a50: 7929 0a20 2020 2020 2070 7269 6e74 2869  y).      print(i
+00011a60: 6e74 6d5f 6772 6164 735b 2764 656e 7365  ntm_grads['dense
+00011a70: 3327 5d29 2023 203d 3d3e 205b 5b2d 312e  3']) # ==> [[-1.
+00011a80: 3435 3639 3234 2020 202d 302e 3434 3333  456924   -0.4433
+00011a90: 3235 3337 2020 302e 3032 3432 3238 3437  2537  0.02422847
+00011aa0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+00011ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011ac0: 2020 2020 2320 2020 2020 205b 2d31 2e34      #      [-1.4
+00011ad0: 3536 3932 3420 2020 2d30 2e34 3433 3332  56924   -0.44332
+00011ae0: 3533 3720 2030 2e30 3234 3232 3834 375d  537  0.02422847]
+00011af0: 5d0a 0a20 2020 2049 6620 7065 7274 7572  ]..    If pertur
+00011b00: 6261 7469 6f6e 7320 6172 6520 6e6f 7420  bations are not 
+00011b10: 7061 7373 6564 2074 6f20 6061 7070 6c79  passed to `apply
+00011b20: 602c 2060 7065 7274 7572 6260 2062 6568  `, `perturb` beh
+00011b30: 6176 6573 206c 696b 6520 6120 6e6f 2d6f  aves like a no-o
+00011b40: 700a 2020 2020 736f 2079 6f75 2063 616e  p.    so you can
+00011b50: 2065 6173 696c 7920 6469 7361 626c 6520   easily disable 
+00011b60: 7468 6520 6265 6861 7669 6f72 2077 6865  the behavior whe
+00011b70: 6e20 6e6f 7420 6e65 6564 6564 3a3a 0a0a  n not needed::..
+00011b80: 2020 2020 2020 6d6f 6465 6c2e 6170 706c        model.appl
+00011b90: 7928 7b27 7061 7261 6d73 273a 2070 6172  y({'params': par
+00011ba0: 616d 732c 2027 7065 7274 7572 6261 7469  ams, 'perturbati
+00011bb0: 6f6e 7327 3a20 7065 7274 7572 6261 7469  ons': perturbati
+00011bc0: 6f6e 737d 2c20 7829 2023 2077 6f72 6b73  ons}, x) # works
+00011bd0: 2061 7320 6578 7065 6374 6564 0a20 2020   as expected.   
+00011be0: 2020 206d 6f64 656c 2e61 7070 6c79 287b     model.apply({
+00011bf0: 2770 6172 616d 7327 3a20 7061 7261 6d73  'params': params
+00011c00: 7d2c 2078 2920 2320 6265 6861 7665 7320  }, x) # behaves 
+00011c10: 6c69 6b65 2061 206e 6f2d 6f70 0a0a 2020  like a no-op..  
+00011c20: 2020 2222 220a 2020 2020 6465 6620 5f72    """.    def _r
+00011c30: 6f6f 745f 6861 735f 636f 6c6c 6563 7469  oot_has_collecti
+00011c40: 6f6e 2829 3a0a 2020 2020 2020 2222 2252  on():.      """R
+00011c50: 6574 7572 6e73 2054 7275 6520 6966 2074  eturns True if t
+00011c60: 6865 2072 6f6f 7420 7363 6f70 6520 6861  he root scope ha
+00011c70: 7320 7468 6520 636f 6c6c 6563 7469 6f6e  s the collection
+00011c80: 2e22 2222 0a20 2020 2020 2061 7373 6572  .""".      asser
+00011c90: 7420 7365 6c66 2e73 636f 7065 2069 7320  t self.scope is 
+00011ca0: 6e6f 7420 4e6f 6e65 0a20 2020 2020 2072  not None.      r
+00011cb0: 6574 7572 6e20 636f 6c6c 6563 7469 6f6e  eturn collection
+00011cc0: 2069 6e20 7365 6c66 2e73 636f 7065 2e72   in self.scope.r
+00011cd0: 6f6f 742e 5f76 6172 6961 626c 6573 0a20  oot._variables. 
+00011ce0: 2020 2023 2077 6520 7769 6c6c 206f 6e6c     # we will onl
+00011cf0: 7920 6164 6420 7468 6520 7065 7274 7572  y add the pertur
+00011d00: 6261 7469 6f6e 2076 6172 6961 626c 6520  bation variable 
+00011d10: 6966 2074 6865 2063 6f6c 6c65 6374 696f  if the collectio
+00011d20: 6e20 6973 206d 7574 6162 6c65 0a20 2020  n is mutable.   
+00011d30: 2023 2028 652e 672e 2064 7572 696e 6720   # (e.g. during 
+00011d40: 6069 6e69 7460 2920 6f72 2069 6620 7468  `init`) or if th
+00011d50: 6520 636f 6c6c 6563 7469 6f6e 2077 6173  e collection was
+00011d60: 2070 6173 7365 6420 746f 2060 6170 706c   passed to `appl
+00011d70: 7960 2028 636f 6e74 6169 6e65 6420 696e  y` (contained in
+00011d80: 0a20 2020 2023 2074 6865 2072 6f6f 7420  .    # the root 
+00011d90: 7363 6f70 6529 2e0a 2020 2020 6966 2073  scope)..    if s
+00011da0: 656c 662e 6973 5f6d 7574 6162 6c65 5f63  elf.is_mutable_c
+00011db0: 6f6c 6c65 6374 696f 6e28 636f 6c6c 6563  ollection(collec
+00011dc0: 7469 6f6e 2920 6f72 205f 726f 6f74 5f68  tion) or _root_h
+00011dd0: 6173 5f63 6f6c 6c65 6374 696f 6e28 293a  as_collection():
+00011de0: 0a20 2020 2020 2076 616c 7565 202b 3d20  .      value += 
+00011df0: 7365 6c66 2e76 6172 6961 626c 6528 636f  self.variable(co
+00011e00: 6c6c 6563 7469 6f6e 2c20 6e61 6d65 2c20  llection, name, 
+00011e10: 6c61 6d62 6461 3a20 6a6e 702e 7a65 726f  lambda: jnp.zero
+00011e20: 735f 6c69 6b65 2876 616c 7565 2929 2e76  s_like(value)).v
+00011e30: 616c 7565 2023 2074 7970 653a 2069 676e  alue # type: ign
+00011e40: 6f72 650a 2020 2020 7265 7475 726e 2076  ore.    return v
+00011e50: 616c 7565 0a0a 2020 6465 6620 7461 6275  alue..  def tabu
+00011e60: 6c61 7465 280a 2020 2020 7365 6c66 2c0a  late(.    self,.
+00011e70: 2020 2020 726e 6773 3a20 556e 696f 6e5b      rngs: Union[
+00011e80: 4b65 7941 7272 6179 2c20 524e 4753 6571  KeyArray, RNGSeq
+00011e90: 7565 6e63 6573 5d2c 0a20 2020 202a 6172  uences],.    *ar
+00011ea0: 6773 2c0a 2020 2020 6465 7074 683a 204f  gs,.    depth: O
+00011eb0: 7074 696f 6e61 6c5b 696e 745d 203d 204e  ptional[int] = N
+00011ec0: 6f6e 652c 0a20 2020 2073 686f 775f 7265  one,.    show_re
+00011ed0: 7065 6174 6564 3a20 626f 6f6c 203d 2046  peated: bool = F
+00011ee0: 616c 7365 2c0a 2020 2020 6d75 7461 626c  alse,.    mutabl
+00011ef0: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
+00011f00: 7465 7220 3d20 5472 7565 2c0a 2020 2020  ter = True,.    
+00011f10: 636f 6e73 6f6c 655f 6b77 6172 6773 3a20  console_kwargs: 
+00011f20: 4f70 7469 6f6e 616c 5b4d 6170 7069 6e67  Optional[Mapping
+00011f30: 5b73 7472 2c20 416e 795d 5d20 3d20 4e6f  [str, Any]] = No
+00011f40: 6e65 2c0a 2020 2020 2a2a 6b77 6172 6773  ne,.    **kwargs
+00011f50: 2920 2d3e 2073 7472 3a0a 2020 2020 2222  ) -> str:.    ""
+00011f60: 2243 7265 6174 6573 2061 2073 756d 6d61  "Creates a summa
+00011f70: 7279 206f 6620 7468 6520 4d6f 6475 6c65  ry of the Module
+00011f80: 2072 6570 7265 7365 6e74 6564 2061 7320   represented as 
+00011f90: 6120 7461 626c 652e 0a0a 2020 2020 5468  a table...    Th
+00011fa0: 6973 206d 6574 686f 6420 6861 7320 7468  is method has th
+00011fb0: 6520 7361 6d65 2073 6967 6e61 7475 7265  e same signature
+00011fc0: 2061 6e64 2069 6e74 6572 6e61 6c6c 7920   and internally 
+00011fd0: 6361 6c6c 7320 604d 6f64 756c 652e 696e  calls `Module.in
+00011fe0: 6974 602c 0a20 2020 2062 7574 2069 6e73  it`,.    but ins
+00011ff0: 7465 6164 206f 6620 7265 7475 726e 696e  tead of returnin
+00012000: 6720 7468 6520 7661 7269 6162 6c65 732c  g the variables,
+00012010: 2069 7420 7265 7475 726e 7320 7468 6520   it returns the 
+00012020: 7374 7269 6e67 2073 756d 6d61 7269 7a69  string summarizi
+00012030: 6e67 0a20 2020 2074 6865 204d 6f64 756c  ng.    the Modul
+00012040: 6520 696e 2061 2074 6162 6c65 2e20 6074  e in a table. `t
+00012050: 6162 756c 6174 6560 2075 7365 7320 606a  abulate` uses `j
+00012060: 6178 2e65 7661 6c5f 7368 6170 6560 2074  ax.eval_shape` t
+00012070: 6f20 7275 6e20 7468 6520 666f 7277 6172  o run the forwar
+00012080: 640a 2020 2020 636f 6d70 7574 6174 696f  d.    computatio
+00012090: 6e20 7769 7468 6f75 7420 636f 6e73 756d  n without consum
+000120a0: 696e 6720 616e 7920 464c 4f50 7320 6f72  ing any FLOPs or
+000120b0: 2061 6c6c 6f63 6174 696e 6720 6d65 6d6f   allocating memo
+000120c0: 7279 2e0a 0a20 2020 2041 6464 6974 696f  ry...    Additio
+000120d0: 6e61 6c20 6172 6775 6d65 6e74 7320 6361  nal arguments ca
+000120e0: 6e20 6265 2070 6173 7365 6420 696e 746f  n be passed into
+000120f0: 2074 6865 2060 636f 6e73 6f6c 655f 6b77   the `console_kw
+00012100: 6172 6773 6020 6172 6775 6d65 6e74 2c20  args` argument, 
+00012110: 666f 7220 6578 616d 706c 652c 0a20 2020  for example,.   
+00012120: 2060 7b27 7769 6474 6827 3a20 3132 307d   `{'width': 120}
+00012130: 602e 2046 6f72 2061 2066 756c 6c20 6c69  `. For a full li
+00012140: 7374 206f 6620 6063 6f6e 736f 6c65 5f6b  st of `console_k
+00012150: 7761 7267 7360 2061 7267 756d 656e 7473  wargs` arguments
+00012160: 2c20 7365 653a 0a20 2020 2068 7474 7073  , see:.    https
+00012170: 3a2f 2f72 6963 682e 7265 6164 7468 6564  ://rich.readthed
+00012180: 6f63 732e 696f 2f65 6e2f 7374 6162 6c65  ocs.io/en/stable
+00012190: 2f72 6566 6572 656e 6365 2f63 6f6e 736f  /reference/conso
+000121a0: 6c65 2e68 746d 6c23 7269 6368 2e63 6f6e  le.html#rich.con
+000121b0: 736f 6c65 2e43 6f6e 736f 6c65 0a0a 2020  sole.Console..  
+000121c0: 2020 4578 616d 706c 653a 3a0a 0a20 2020    Example::..   
+000121d0: 2020 2069 6d70 6f72 7420 6a61 780a 2020     import jax.  
+000121e0: 2020 2020 696d 706f 7274 206a 6178 2e6e      import jax.n
+000121f0: 756d 7079 2061 7320 6a6e 700a 2020 2020  umpy as jnp.    
+00012200: 2020 696d 706f 7274 2066 6c61 782e 6c69    import flax.li
+00012210: 6e65 6e20 6173 206e 6e0a 0a20 2020 2020  nen as nn..     
+00012220: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
+00012230: 6475 6c65 293a 0a20 2020 2020 2020 2020  dule):.         
+00012240: 2040 6e6e 2e63 6f6d 7061 6374 0a20 2020   @nn.compact.   
+00012250: 2020 2020 2020 2064 6566 205f 5f63 616c         def __cal
+00012260: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
+00012270: 2020 2020 2020 2020 2020 2020 6820 3d20              h = 
+00012280: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
+00012290: 2020 2020 2020 2020 2020 2020 2072 6574               ret
+000122a0: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
+000122b0: 6829 0a0a 2020 2020 2020 7820 3d20 6a6e  h)..      x = jn
+000122c0: 702e 6f6e 6573 2828 3136 2c20 3929 290a  p.ones((16, 9)).
+000122d0: 0a20 2020 2020 2070 7269 6e74 2846 6f6f  .      print(Foo
+000122e0: 2829 2e74 6162 756c 6174 6528 6a61 782e  ().tabulate(jax.
+000122f0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
+00012300: 292c 2078 2929 0a0a 0a20 2020 2054 6869  ), x))...    Thi
+00012310: 7320 6769 7665 7320 7468 6520 666f 6c6c  s gives the foll
+00012320: 6f77 696e 6720 6f75 7470 7574 3a3a 0a0a  owing output::..
+00012330: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012350: 2020 2020 2020 466f 6f20 5375 6d6d 6172        Foo Summar
+00012360: 790a 2020 2020 2020 e294 8fe2 9481 e294  y.      ........
+00012370: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012380: e294 81e2 9481 e294 b3e2 9481 e294 81e2  ................
+00012390: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000123a0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+000123b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000123c0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000123d0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+000123e0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000123f0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012400: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+00012410: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012420: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012430: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012440: e294 81e2 9481 e294 930a 2020 2020 2020  ..........      
+00012450: e294 8320 7061 7468 2020 2020 e294 8320  ... path    ... 
+00012460: 6d6f 6475 6c65 20e2 9483 2069 6e70 7574  module ... input
+00012470: 7320 2020 2020 2020 20e2 9483 206f 7574  s        ... out
+00012480: 7075 7473 2020 2020 2020 20e2 9483 2070  puts       ... p
+00012490: 6172 616d 7320 2020 2020 2020 2020 2020  arams           
+000124a0: 2020 2020 e294 830a 2020 2020 2020 e294      ....      ..
+000124b0: a1e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000124c0: e294 81e2 9481 e294 81e2 9481 e295 87e2  ................
+000124d0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000124e0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+000124f0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012500: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012510: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+00012520: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012530: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012540: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+00012550: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012560: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012570: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012580: e294 81e2 9481 e294 81e2 9481 e294 a90a  ................
+00012590: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+000125a0: 2020 e294 8220 466f 6f20 2020 20e2 9482    ... Foo    ...
+000125b0: 2066 6c6f 6174 3332 5b31 362c 395d 20e2   float32[16,9] .
+000125c0: 9482 2066 6c6f 6174 3332 5b31 362c 325d  .. float32[16,2]
+000125d0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+000125e0: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+000125f0: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+00012600: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012610: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+00012620: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012630: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012640: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012650: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012660: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012670: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012680: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012690: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000126a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000126b0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+000126c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000126d0: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+000126e0: 4465 6e73 655f 3020 e294 8220 4465 6e73  Dense_0 ... Dens
+000126f0: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
+00012700: 362c 395d 20e2 9482 2066 6c6f 6174 3332  6,9] ... float32
+00012710: 5b31 362c 345d 20e2 9482 2062 6961 733a  [16,4] ... bias:
+00012720: 2066 6c6f 6174 3332 5b34 5d20 2020 2020   float32[4]     
+00012730: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
+00012740: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012750: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00012760: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012770: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
+00012780: 2066 6c6f 6174 3332 5b39 2c34 5d20 e294   float32[9,4] ..
+00012790: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
+000127a0: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
+000127b0: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+000127c0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+000127d0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+000127e0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+000127f0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012800: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
+00012810: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00012820: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012830: 20e2 9482 2034 3020 2831 3630 2042 2920   ... 40 (160 B) 
+00012840: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+00012850: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+00012860: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012870: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+00012880: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012890: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000128a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000128b0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+000128c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000128d0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000128e0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+000128f0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012900: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012910: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012920: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012930: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+00012940: 4465 6e73 655f 3120 e294 8220 4465 6e73  Dense_1 ... Dens
+00012950: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
+00012960: 362c 345d 20e2 9482 2066 6c6f 6174 3332  6,4] ... float32
+00012970: 5b31 362c 325d 20e2 9482 2062 6961 733a  [16,2] ... bias:
+00012980: 2066 6c6f 6174 3332 5b32 5d20 2020 2020   float32[2]     
+00012990: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
+000129a0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+000129b0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+000129c0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+000129d0: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
+000129e0: 2066 6c6f 6174 3332 5b34 2c32 5d20 e294   float32[4,2] ..
+000129f0: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
+00012a00: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
+00012a10: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012a20: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00012a30: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012a40: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+00012a50: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012a60: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
+00012a70: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00012a80: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012a90: 20e2 9482 2031 3020 2834 3020 4229 2020   ... 10 (40 B)  
+00012aa0: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+00012ab0: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+00012ac0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012ad0: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+00012ae0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012af0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012b10: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012b20: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012b40: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012b50: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b60: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012b70: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012b80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b90: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+00012ba0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00012bb0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012bc0: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00012bd0: 2054 6f74 616c 20e2 9482 2035 3020 2832   Total ... 50 (2
+00012be0: 3030 2042 2920 2020 2020 2020 2020 2020  00 B)           
+00012bf0: e294 820a 2020 2020 2020 e294 94e2 9480  ....      ......
+00012c00: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012c10: 9480 e294 80e2 9480 e294 b4e2 9480 e294  ................
+00012c20: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012c30: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012c40: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012c50: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012c60: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012c70: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012c80: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012c90: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012ca0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012cb0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012cc0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012cd0: 9480 e294 80e2 9480 e294 980a 0a20 2020  .............   
+00012ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012cf0: 2020 2020 2020 2020 2054 6f74 616c 2050           Total P
+00012d00: 6172 616d 6574 6572 733a 2035 3020 2832  arameters: 50 (2
+00012d10: 3030 2042 290a 0a20 2020 202a 2a4e 6f74  00 B)..    **Not
+00012d20: 652a 2a3a 2072 6f77 7320 6f72 6465 7220  e**: rows order 
+00012d30: 696e 2074 6865 2074 6162 6c65 2064 6f65  in the table doe
+00012d40: 7320 6e6f 7420 7265 7072 6573 656e 7420  s not represent 
+00012d50: 6578 6563 7574 696f 6e20 6f72 6465 722c  execution order,
+00012d60: 0a20 2020 2069 6e73 7465 6164 2069 7420  .    instead it 
+00012d70: 616c 6967 6e73 2077 6974 6820 7468 6520  aligns with the 
+00012d80: 6f72 6465 7220 6f66 206b 6579 7320 696e  order of keys in
+00012d90: 2060 7661 7269 6162 6c65 7360 2077 6869   `variables` whi
+00012da0: 6368 2061 7265 2073 6f72 7465 640a 2020  ch are sorted.  
+00012db0: 2020 616c 7068 6162 6574 6963 616c 6c79    alphabetically
+00012dc0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
+00012dd0: 2020 2072 6e67 733a 2054 6865 2072 6e67     rngs: The rng
+00012de0: 7320 666f 7220 7468 6520 7661 7269 6162  s for the variab
+00012df0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2061  le collections a
+00012e00: 7320 7061 7373 6564 2074 6f20 604d 6f64  s passed to `Mod
+00012e10: 756c 652e 696e 6974 602e 0a20 2020 2020  ule.init`..     
+00012e20: 202a 6172 6773 3a20 5468 6520 6172 6775   *args: The argu
+00012e30: 6d65 6e74 7320 746f 2074 6865 2066 6f72  ments to the for
+00012e40: 7761 7264 2063 6f6d 7075 7461 7469 6f6e  ward computation
+00012e50: 2e0a 2020 2020 2020 6465 7074 683a 2063  ..      depth: c
+00012e60: 6f6e 7472 6f6c 7320 686f 7720 6d61 6e79  ontrols how many
+00012e70: 2073 7562 6d6f 6475 6c65 2064 6565 7020   submodule deep 
+00012e80: 7468 6520 7375 6d6d 6172 7920 6361 6e20  the summary can 
+00012e90: 676f 2e20 4279 2064 6566 6175 6c74 2069  go. By default i
+00012ea0: 7473 0a20 2020 2020 2020 2060 4e6f 6e65  ts.        `None
+00012eb0: 6020 7768 6963 6820 6d65 616e 7320 6e6f  ` which means no
+00012ec0: 206c 696d 6974 2e20 4966 2061 2073 7562   limit. If a sub
+00012ed0: 6d6f 6475 6c65 2069 7320 6e6f 7420 7368  module is not sh
+00012ee0: 6f77 6e20 6265 6361 7573 6520 6f66 2074  own because of t
+00012ef0: 6865 0a20 2020 2020 2020 2064 6570 7468  he.        depth
+00012f00: 206c 696d 6974 2c20 6974 7320 7061 7261   limit, its para
+00012f10: 6d65 7465 7220 636f 756e 7420 616e 6420  meter count and 
+00012f20: 6279 7465 7320 7769 6c6c 2062 6520 6164  bytes will be ad
+00012f30: 6465 6420 746f 2074 6865 2072 6f77 206f  ded to the row o
+00012f40: 6620 6974 730a 2020 2020 2020 2020 6669  f its.        fi
+00012f50: 7273 7420 7368 6f77 6e20 616e 6365 7374  rst shown ancest
+00012f60: 6f72 2073 7563 6820 7468 6174 2074 6865  or such that the
+00012f70: 2073 756d 206f 6620 616c 6c20 726f 7773   sum of all rows
+00012f80: 2061 6c77 6179 7320 6164 6473 2075 7020   always adds up 
+00012f90: 746f 2074 6865 0a20 2020 2020 2020 2074  to the.        t
+00012fa0: 6f74 616c 206e 756d 6265 7220 6f66 2070  otal number of p
+00012fb0: 6172 616d 6574 6572 7320 6f66 2074 6865  arameters of the
+00012fc0: 204d 6f64 756c 652e 0a20 2020 2020 2073   Module..      s
+00012fd0: 686f 775f 7265 7065 6174 6564 3a20 4966  how_repeated: If
+00012fe0: 2060 5472 7565 602c 2072 6570 6561 7465   `True`, repeate
+00012ff0: 6420 6361 6c6c 7320 746f 2074 6865 2073  d calls to the s
+00013000: 616d 6520 6d6f 6475 6c65 2077 696c 6c20  ame module will 
+00013010: 6265 2073 686f 776e 0a20 2020 2020 2020  be shown.       
+00013020: 2069 6e20 7468 6520 7461 626c 652c 206f   in the table, o
+00013030: 7468 6572 7769 7365 206f 6e6c 7920 7468  therwise only th
+00013040: 6520 6669 7273 7420 6361 6c6c 2077 696c  e first call wil
+00013050: 6c20 6265 2073 686f 776e 2e20 4465 6661  l be shown. Defa
+00013060: 756c 7420 6973 0a20 2020 2020 2020 2060  ult is.        `
+00013070: 4661 6c73 6560 2e0a 2020 2020 2020 6d75  False`..      mu
+00013080: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+00013090: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+000130a0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+000130b0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+000130c0: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
+000130d0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
+000130e0: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
+000130f0: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
+00013100: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
+00013110: 7374 7260 603a 2054 6865 0a20 2020 2020  str``: The.     
+00013120: 2020 206e 616d 6520 6f66 2061 2073 696e     name of a sin
+00013130: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+00013140: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+00013150: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
+00013160: 7320 6f66 206d 7574 6162 6c65 0a20 2020  s of mutable.   
+00013170: 2020 2020 2063 6f6c 6c65 6374 696f 6e73       collections
+00013180: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
+00013190: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
+000131a0: 6570 7420 2769 6e74 6572 6d65 6469 6174  ept 'intermediat
+000131b0: 6573 2720 6172 650a 2020 2020 2020 2020  es' are.        
+000131c0: 6d75 7461 626c 652e 0a20 2020 2020 2063  mutable..      c
+000131d0: 6f6e 736f 6c65 5f6b 7761 7267 733a 2041  onsole_kwargs: A
+000131e0: 6e20 6f70 7469 6f6e 616c 2064 6963 7469  n optional dicti
+000131f0: 6f6e 6172 7920 7769 7468 2061 6464 6974  onary with addit
+00013200: 696f 6e61 6c20 6b65 7977 6f72 6420 6172  ional keyword ar
+00013210: 6775 6d65 6e74 7320 7468 6174 0a20 2020  guments that.   
+00013220: 2020 2020 2061 7265 2070 6173 7365 6420       are passed 
+00013230: 746f 2060 7269 6368 2e63 6f6e 736f 6c65  to `rich.console
+00013240: 2e43 6f6e 736f 6c65 6020 7768 656e 2072  .Console` when r
+00013250: 656e 6465 7269 6e67 2074 6865 2074 6162  endering the tab
+00013260: 6c65 2e20 4465 6661 756c 7420 6172 6775  le. Default argu
+00013270: 6d65 6e74 730a 2020 2020 2020 2020 6172  ments.        ar
+00013280: 6520 607b 2766 6f72 6365 5f74 6572 6d69  e `{'force_termi
+00013290: 6e61 6c27 3a20 5472 7565 2c20 2766 6f72  nal': True, 'for
+000132a0: 6365 5f6a 7570 7974 6572 273a 2046 616c  ce_jupyter': Fal
+000132b0: 7365 7d60 2e0a 2020 2020 2020 2a2a 6b77  se}`..      **kw
+000132c0: 6172 6773 3a20 6b65 7977 6f72 6420 6172  args: keyword ar
+000132d0: 6775 6d65 6e74 7320 746f 2070 6173 7320  guments to pass 
+000132e0: 746f 2074 6865 2066 6f72 7761 7264 2063  to the forward c
+000132f0: 6f6d 7075 7461 7469 6f6e 2e0a 0a20 2020  omputation...   
+00013300: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
+00013310: 4120 7374 7269 6e67 2073 756d 6d61 7269  A string summari
+00013320: 7a69 6e67 2074 6865 204d 6f64 756c 652e  zing the Module.
+00013330: 0a20 2020 2022 2222 0a20 2020 2066 726f  .    """.    fro
+00013340: 6d20 666c 6178 2e6c 696e 656e 2069 6d70  m flax.linen imp
+00013350: 6f72 7420 7375 6d6d 6172 790a 0a20 2020  ort summary..   
+00013360: 2074 6162 756c 6174 655f 666e 203d 2073   tabulate_fn = s
+00013370: 756d 6d61 7279 2e74 6162 756c 6174 6528  ummary.tabulate(
+00013380: 7365 6c66 2c20 726e 6773 2c20 6465 7074  self, rngs, dept
+00013390: 683d 6465 7074 682c 0a20 2020 2020 2020  h=depth,.       
+000133a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000133b0: 2020 2020 2020 2020 2020 2020 7368 6f77              show
+000133c0: 5f72 6570 6561 7465 643d 7368 6f77 5f72  _repeated=show_r
+000133d0: 6570 6561 7465 642c 206d 7574 6162 6c65  epeated, mutable
+000133e0: 3d6d 7574 6162 6c65 2c0a 2020 2020 2020  =mutable,.      
+000133f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013400: 2020 2020 2020 2020 2020 2020 2063 6f6e               con
+00013410: 736f 6c65 5f6b 7761 7267 733d 636f 6e73  sole_kwargs=cons
+00013420: 6f6c 655f 6b77 6172 6773 290a 2020 2020  ole_kwargs).    
+00013430: 7265 7475 726e 2074 6162 756c 6174 655f  return tabulate_
+00013440: 666e 282a 6172 6773 2c20 2a2a 6b77 6172  fn(*args, **kwar
+00013450: 6773 290a 0a0a 5f50 6172 656e 7454 7970  gs)..._ParentTyp
+00013460: 6520 3d20 556e 696f 6e5b 5479 7065 5b4d  e = Union[Type[M
+00013470: 6f64 756c 655d 2c20 5479 7065 5b53 636f  odule], Type[Sco
+00013480: 7065 5d2c 2054 7970 655b 5f53 656e 7469  pe], Type[_Senti
+00013490: 6e65 6c5d 2c20 4e6f 6e65 5d0a 0a64 6566  nel], None]..def
+000134a0: 206d 6572 6765 5f70 6172 616d 286e 616d   merge_param(nam
+000134b0: 653a 2073 7472 2c20 613a 204f 7074 696f  e: str, a: Optio
+000134c0: 6e61 6c5b 545d 2c20 623a 204f 7074 696f  nal[T], b: Optio
+000134d0: 6e61 6c5b 545d 2920 2d3e 2054 3a0a 2020  nal[T]) -> T:.  
+000134e0: 2222 224d 6572 6765 7320 636f 6e73 7472  """Merges constr
+000134f0: 7563 7469 6f6e 2d20 616e 6420 6361 6c6c  uction- and call
+00013500: 2d74 696d 6520 6172 6775 6d65 6e74 2e0a  -time argument..
+00013510: 0a20 2054 6869 7320 6973 2061 2075 7469  .  This is a uti
+00013520: 6c69 7479 2066 6f72 2073 7570 706f 7274  lity for support
+00013530: 696e 6720 6120 7061 7474 6572 6e20 7768  ing a pattern wh
+00013540: 6572 6520 6120 4d6f 6475 6c65 2068 7970  ere a Module hyp
+00013550: 6572 7061 7261 6d65 7465 720a 2020 6361  erparameter.  ca
+00013560: 6e20 6265 2070 6173 7365 6420 6569 7468  n be passed eith
+00013570: 6572 2074 6f20 6060 5f5f 696e 6974 5f5f  er to ``__init__
+00013580: 6060 206f 7220 6060 5f5f 6361 6c6c 5f5f  `` or ``__call__
+00013590: 6060 2c20 616e 6420 7468 6520 7661 6c75  ``, and the valu
+000135a0: 6520 7468 6174 2069 730a 2020 6e6f 7420  e that is.  not 
+000135b0: 604e 6f6e 6560 2077 696c 6c20 6265 2075  `None` will be u
+000135c0: 7365 642e 0a0a 2020 4578 616d 706c 653a  sed...  Example:
+000135d0: 3a0a 0a20 2020 2063 6c61 7373 2046 6f6f  :..    class Foo
+000135e0: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
+000135f0: 2020 2074 7261 696e 3a20 4f70 7469 6f6e     train: Option
+00013600: 616c 5b62 6f6f 6c5d 203d 204e 6f6e 650a  al[bool] = None.
+00013610: 0a20 2020 2020 2064 6566 205f 5f63 616c  .      def __cal
+00013620: 6c5f 5f28 7365 6c66 2c20 7472 6169 6e3a  l__(self, train:
+00013630: 204f 7074 696f 6e61 6c5b 626f 6f6c 5d20   Optional[bool] 
+00013640: 3d20 4e6f 6e65 293a 0a20 2020 2020 2020  = None):.       
+00013650: 2074 7261 696e 203d 206e 6e2e 6d65 7267   train = nn.merg
+00013660: 655f 7061 7261 6d28 2774 7261 696e 272c  e_param('train',
+00013670: 2073 656c 662e 7472 6169 6e2c 2074 7261   self.train, tra
+00013680: 696e 290a 0a20 2041 6e20 6572 726f 7220  in)..  An error 
+00013690: 6973 2074 6872 6f77 6e20 7768 656e 2062  is thrown when b
+000136a0: 6f74 6820 6172 6775 6d65 6e74 7320 6172  oth arguments ar
+000136b0: 6520 604e 6f6e 6560 206f 7220 626f 7468  e `None` or both
+000136c0: 2076 616c 7565 7320 6172 6520 6e6f 740a   values are not.
+000136d0: 2020 604e 6f6e 6560 2e0a 0a20 2041 7267    `None`...  Arg
+000136e0: 733a 0a20 2020 206e 616d 653a 2074 6865  s:.    name: the
+000136f0: 206e 616d 6520 6f66 2074 6865 2070 6172   name of the par
+00013700: 616d 6574 6572 2e20 5573 6564 2066 6f72  ameter. Used for
+00013710: 2065 7272 6f72 206d 6573 7361 6765 732e   error messages.
+00013720: 0a20 2020 2061 3a20 6f70 7469 6f6e 2061  .    a: option a
+00013730: 0a20 2020 2062 3a20 6f70 7469 6f6e 2062  .    b: option b
+00013740: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
+00013750: 6120 6f72 2062 2077 6869 6368 6576 6572  a or b whichever
+00013760: 2069 7320 6e6f 7420 604e 6f6e 6560 2e0a   is not `None`..
+00013770: 0a20 2022 2222 0a20 2069 6620 6120 6973  .  """.  if a is
+00013780: 204e 6f6e 6520 616e 6420 6220 6973 204e   None and b is N
+00013790: 6f6e 653a 0a20 2020 2072 6169 7365 2056  one:.    raise V
+000137a0: 616c 7565 4572 726f 7228 6627 5061 7261  alueError(f'Para
+000137b0: 6d65 7465 7220 227b 6e61 6d65 7d22 206d  meter "{name}" m
+000137c0: 7573 7420 6265 2070 6173 7365 6420 746f  ust be passed to
+000137d0: 2074 6865 2063 6f6e 7374 7275 6374 6f72   the constructor
+000137e0: 206f 7220 6174 2063 616c 6c20 7469 6d65   or at call time
+000137f0: 2e27 290a 2020 6966 2061 2069 7320 6e6f  .').  if a is no
+00013800: 7420 4e6f 6e65 2061 6e64 2062 2069 7320  t None and b is 
+00013810: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 7261  not None:.    ra
+00013820: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
+00013830: 2750 6172 616d 6574 6572 2022 7b6e 616d  'Parameter "{nam
+00013840: 657d 2220 7761 7320 7061 7373 6564 2074  e}" was passed t
+00013850: 6f20 7468 6520 636f 6e73 7472 7563 746f  o the constructo
+00013860: 7220 616e 6420 6174 2063 616c 6c20 7469  r and at call ti
+00013870: 6d65 2e27 0a20 2020 2020 2020 2020 2020  me.'.           
+00013880: 2020 2020 2020 2020 2020 2720 5368 6f75            ' Shou
+00013890: 6c64 2062 6520 7061 7373 6564 206a 7573  ld be passed jus
+000138a0: 7420 6f6e 6365 2e27 290a 2020 6966 2061  t once.').  if a
+000138b0: 2069 7320 4e6f 6e65 3a0a 2020 2020 6173   is None:.    as
+000138c0: 7365 7274 2062 2069 7320 6e6f 7420 4e6f  sert b is not No
+000138d0: 6e65 0a20 2020 2072 6574 7572 6e20 620a  ne.    return b.
+000138e0: 2020 7265 7475 726e 2061 0a0a 0a40 7472    return a...@tr
+000138f0: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
+00013900: 5f62 6f75 6e64 6172 790a 6465 6620 6170  _boundary.def ap
+00013910: 706c 7928 666e 3a20 4361 6c6c 6162 6c65  ply(fn: Callable
+00013920: 5b2e 2e2e 2c20 416e 795d 2c20 6d6f 6475  [..., Any], modu
+00013930: 6c65 3a20 4d6f 6475 6c65 2c0a 2020 2020  le: Module,.    
+00013940: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
+00013950: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
+00013960: 3d20 4661 6c73 652c 0a20 2020 2020 2020  = False,.       
+00013970: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
+00013980: 6d65 6469 6174 6573 3a20 556e 696f 6e5b  mediates: Union[
+00013990: 626f 6f6c 2c20 4361 6c6c 6162 6c65 5b5b  bool, Callable[[
+000139a0: 4d6f 6475 6c65 2c20 7374 725d 2c20 626f  Module, str], bo
+000139b0: 6f6c 5d5d 203d 2046 616c 7365 2c0a 2020  ol]] = False,.  
+000139c0: 2020 2020 2020 2020 2920 2d3e 2043 616c          ) -> Cal
+000139d0: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d3a  lable[..., Any]:
+000139e0: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
+000139f0: 2061 7070 6c79 2066 756e 6374 696f 6e20   apply function 
+00013a00: 746f 2063 616c 6c20 6060 666e 6060 2077  to call ``fn`` w
+00013a10: 6974 6820 6120 626f 756e 6420 6d6f 6475  ith a bound modu
+00013a20: 6c65 2e0a 0a20 2055 6e6c 696b 6520 6060  le...  Unlike ``
+00013a30: 4d6f 6475 6c65 2e61 7070 6c79 6060 2074  Module.apply`` t
+00013a40: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
+00013a50: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
+00013a60: 696f 6e20 7769 7468 2074 6865 2073 6967  ion with the sig
+00013a70: 6e61 7475 7265 0a20 2060 6028 7661 7269  nature.  ``(vari
+00013a80: 6162 6c65 732c 202a 6172 6773 2c20 726e  ables, *args, rn
+00013a90: 6773 3d4e 6f6e 652c 202a 2a6b 7761 7267  gs=None, **kwarg
+00013aa0: 7329 202d 3e20 5460 6020 7768 6572 6520  s) -> T`` where 
+00013ab0: 6054 6020 6973 2074 6865 2072 6574 7572  `T` is the retur
+00013ac0: 6e20 7479 7065 0a20 206f 6620 6060 666e  n type.  of ``fn
+00013ad0: 6060 2e20 4966 2060 606d 7574 6162 6c65  ``. If ``mutable
+00013ae0: 6060 2069 7320 6e6f 7420 6060 4661 6c73  `` is not ``Fals
+00013af0: 6560 6020 7468 6520 7265 7475 726e 2074  e`` the return t
+00013b00: 7970 6520 6973 2061 2074 7570 6c65 2077  ype is a tuple w
+00013b10: 6865 7265 2074 6865 0a20 2073 6563 6f6e  here the.  secon
+00013b20: 6420 6974 656d 2069 7320 6120 6060 4672  d item is a ``Fr
+00013b30: 6f7a 656e 4469 6374 6060 2077 6974 6820  ozenDict`` with 
+00013b40: 7468 6520 6d75 7461 7465 6420 7661 7269  the mutated vari
+00013b50: 6162 6c65 732e 0a0a 2020 5468 6520 6170  ables...  The ap
+00013b60: 706c 7920 6675 6e63 7469 6f6e 2074 6861  ply function tha
+00013b70: 7420 6973 2072 6574 7572 6e65 6420 6361  t is returned ca
+00013b80: 6e20 6265 2064 6972 6563 746c 7920 636f  n be directly co
+00013b90: 6d70 6f73 6564 2077 6974 680a 2020 4a41  mposed with.  JA
+00013ba0: 5820 7472 616e 7366 6f72 6d61 7469 6f6e  X transformation
+00013bb0: 7320 6c69 6b65 2060 606a 6178 2e6a 6974  s like ``jax.jit
+00013bc0: 6060 3a3a 0a0a 2020 2020 6465 6620 6628  ``::..    def f(
+00013bd0: 666f 6f2c 2078 293a 0a20 2020 2020 207a  foo, x):.      z
+00013be0: 203d 2066 6f6f 2e65 6e63 6f64 6528 7829   = foo.encode(x)
+00013bf0: 0a20 2020 2020 2079 203d 2066 6f6f 2e64  .      y = foo.d
+00013c00: 6563 6f64 6528 7a29 0a20 2020 2020 2023  ecode(z).      #
+00013c10: 202e 2e2e 0a20 2020 2020 2072 6574 7572   ....      retur
+00013c20: 6e20 790a 0a20 2020 2066 6f6f 203d 2046  n y..    foo = F
+00013c30: 6f6f 2829 0a20 2020 2066 5f6a 6974 7465  oo().    f_jitte
+00013c40: 6420 3d20 6a61 782e 6a69 7428 6e6e 2e61  d = jax.jit(nn.a
+00013c50: 7070 6c79 2866 2c20 666f 6f29 290a 2020  pply(f, foo)).  
+00013c60: 2020 665f 6a69 7474 6564 2876 6172 6961    f_jitted(varia
+00013c70: 626c 6573 2c20 7829 0a0a 2020 4172 6773  bles, x)..  Args
+00013c80: 3a0a 2020 2020 666e 3a20 5468 6520 6675  :.    fn: The fu
+00013c90: 6e63 7469 6f6e 2074 6861 7420 7368 6f75  nction that shou
+00013ca0: 6c64 2062 6520 6170 706c 6965 642e 2054  ld be applied. T
+00013cb0: 6865 2066 6972 7374 2061 7267 756d 656e  he first argumen
+00013cc0: 7420 7061 7373 6564 2077 696c 6c0a 2020  t passed will.  
+00013cd0: 2020 2020 6265 2061 6e20 6d6f 6475 6c65      be an module
+00013ce0: 2069 6e73 7461 6e63 6520 6f66 2074 6865   instance of the
+00013cf0: 2060 606d 6f64 756c 6560 6020 7769 7468   ``module`` with
+00013d00: 2076 6172 6961 626c 6573 2061 6e64 2052   variables and R
+00013d10: 4e47 7320 626f 756e 640a 2020 2020 2020  NGs bound.      
+00013d20: 746f 2069 742e 0a20 2020 206d 6f64 756c  to it..    modul
+00013d30: 653a 2054 6865 2060 604d 6f64 756c 6560  e: The ``Module`
+00013d40: 6020 7468 6174 2077 696c 6c20 6265 2075  ` that will be u
+00013d50: 7365 6420 746f 2062 696e 6420 7661 7269  sed to bind vari
+00013d60: 6162 6c65 7320 616e 6420 524e 4773 2074  ables and RNGs t
+00013d70: 6f2e 0a20 2020 2020 2054 6865 2060 604d  o..      The ``M
+00013d80: 6f64 756c 6560 6020 7061 7373 6564 2061  odule`` passed a
+00013d90: 7320 7468 6520 6669 7273 7420 6172 6775  s the first argu
+00013da0: 6d65 6e74 2074 6f20 6060 666e 6060 2077  ment to ``fn`` w
+00013db0: 696c 6c20 6265 2061 2063 6c6f 6e65 0a20  ill be a clone. 
+00013dc0: 2020 2020 206f 6620 6d6f 6475 6c65 2e0a       of module..
+00013dd0: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
+00013de0: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
+00013df0: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
+00013e00: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
+00013e10: 6f6e 7320 7368 6f75 6c64 2062 650a 2020  ons should be.  
+00013e20: 2020 2020 7472 6561 7465 6420 6173 206d      treated as m
+00013e30: 7574 6162 6c65 3a20 6060 626f 6f6c 6060  utable: ``bool``
+00013e40: 3a20 616c 6c2f 6e6f 2063 6f6c 6c65 6374  : all/no collect
+00013e50: 696f 6e73 2061 7265 206d 7574 6162 6c65  ions are mutable
+00013e60: 2e0a 2020 2020 2020 6060 7374 7260 603a  ..      ``str``:
+00013e70: 2054 6865 206e 616d 6520 6f66 2061 2073   The name of a s
+00013e80: 696e 676c 6520 6d75 7461 626c 6520 636f  ingle mutable co
+00013e90: 6c6c 6563 7469 6f6e 2e20 6060 6c69 7374  llection. ``list
+00013ea0: 6060 3a20 410a 2020 2020 2020 6c69 7374  ``: A.      list
+00013eb0: 206f 6620 6e61 6d65 7320 6f66 206d 7574   of names of mut
+00013ec0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+00013ed0: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
+00013ee0: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
+00013ef0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
+00013f00: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+00013f10: 7475 726e 2076 616c 7565 730a 2020 2020  turn values.    
+00013f20: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
+00013f30: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+00013f40: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+00013f50: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+00013f60: 6c74 206f 6e6c 790a 2020 2020 2020 7468  lt only.      th
+00013f70: 6520 7265 7475 726e 2076 616c 7565 7320  e return values 
+00013f80: 6f66 2061 6c6c 2060 5f5f 6361 6c6c 5f5f  of all `__call__
+00013f90: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
+00013fa0: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
+00013fb0: 2063 616e 0a20 2020 2020 2062 6520 7061   can.      be pa
+00013fc0: 7373 6564 2074 6f20 6368 616e 6765 2074  ssed to change t
+00013fd0: 6865 2066 696c 7465 7220 6265 6861 7669  he filter behavi
+00013fe0: 6f72 2e20 5468 6520 6669 6c74 6572 2066  or. The filter f
+00013ff0: 756e 6374 696f 6e20 7461 6b65 730a 2020  unction takes.  
+00014000: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
+00014010: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
+00014020: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
+00014030: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
+00014040: 6174 696e 670a 2020 2020 2020 7768 6574  ating.      whet
+00014050: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
+00014060: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
+00014070: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
+00014080: 6265 2073 746f 7265 642e 0a20 2052 6574  be stored..  Ret
+00014090: 7572 6e73 3a0a 2020 2020 5468 6520 6170  urns:.    The ap
+000140a0: 706c 7920 6675 6e63 7469 6f6e 2077 7261  ply function wra
+000140b0: 7070 696e 6720 6060 666e 6060 2e0a 2020  pping ``fn``..  
+000140c0: 2222 220a 2020 4066 756e 6374 6f6f 6c73  """.  @functools
+000140d0: 2e77 7261 7073 2866 6e29 0a20 2064 6566  .wraps(fn).  def
+000140e0: 2073 636f 7065 5f66 6e28 7363 6f70 652c   scope_fn(scope,
+000140f0: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
+00014100: 293a 0a20 2020 205f 636f 6e74 6578 742e  ):.    _context.
+00014110: 6361 7074 7572 655f 7374 6163 6b2e 6170  capture_stack.ap
+00014120: 7065 6e64 2863 6170 7475 7265 5f69 6e74  pend(capture_int
+00014130: 6572 6d65 6469 6174 6573 290a 2020 2020  ermediates).    
+00014140: 7472 793a 0a20 2020 2020 2072 6574 7572  try:.      retur
+00014150: 6e20 666e 286d 6f64 756c 652e 636c 6f6e  n fn(module.clon
+00014160: 6528 7061 7265 6e74 3d73 636f 7065 2c20  e(parent=scope, 
+00014170: 5f64 6565 705f 636c 6f6e 653d 5472 7565  _deep_clone=True
+00014180: 292c 202a 6172 6773 2c20 2a2a 6b77 6172  ), *args, **kwar
+00014190: 6773 290a 2020 2020 6669 6e61 6c6c 793a  gs).    finally:
+000141a0: 0a20 2020 2020 205f 636f 6e74 6578 742e  .      _context.
+000141b0: 6361 7074 7572 655f 7374 6163 6b2e 706f  capture_stack.po
+000141c0: 7028 290a 0a20 2069 6620 6361 7074 7572  p()..  if captur
+000141d0: 655f 696e 7465 726d 6564 6961 7465 7320  e_intermediates 
+000141e0: 6973 2054 7275 653a 2020 2320 7079 6c69  is True:  # pyli
+000141f0: 6e74 3a20 6469 7361 626c 653d 672d 626f  nt: disable=g-bo
+00014200: 6f6c 2d69 642d 636f 6d70 6172 6973 6f6e  ol-id-comparison
+00014210: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
+00014220: 6572 6d65 6469 6174 6573 203d 2063 6170  ermediates = cap
+00014230: 7475 7265 5f63 616c 6c5f 696e 7465 726d  ture_call_interm
+00014240: 6564 6961 7465 730a 2020 6966 2063 6170  ediates.  if cap
+00014250: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+00014260: 6573 3a0a 2020 2020 6d75 7461 626c 6520  es:.    mutable 
+00014270: 3d20 756e 696f 6e5f 6669 6c74 6572 7328  = union_filters(
+00014280: 6d75 7461 626c 652c 2027 696e 7465 726d  mutable, 'interm
+00014290: 6564 6961 7465 7327 290a 2020 7265 7475  ediates').  retu
+000142a0: 726e 2063 6f72 652e 6170 706c 7928 7363  rn core.apply(sc
+000142b0: 6f70 655f 666e 2c20 6d75 7461 626c 653d  ope_fn, mutable=
+000142c0: 6d75 7461 626c 6529 0a0a 0a40 7472 6163  mutable)...@trac
+000142d0: 6562 6163 6b5f 7574 696c 2e61 7069 5f62  eback_util.api_b
+000142e0: 6f75 6e64 6172 790a 6465 6620 696e 6974  oundary.def init
+000142f0: 5f77 6974 685f 6f75 7470 7574 2866 6e3a  _with_output(fn:
+00014300: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
+00014310: 6e79 5d2c 206d 6f64 756c 653a 204d 6f64  ny], module: Mod
+00014320: 756c 652c 0a20 2020 2020 2020 2020 2020  ule,.           
+00014330: 2020 2020 2020 2020 2020 6d75 7461 626c            mutabl
+00014340: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
+00014350: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
+00014360: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
+00014370: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00014380: 2020 2020 2020 6361 7074 7572 655f 696e        capture_in
+00014390: 7465 726d 6564 6961 7465 733a 2055 6e69  termediates: Uni
+000143a0: 6f6e 5b62 6f6f 6c2c 2043 616c 6c61 626c  on[bool, Callabl
+000143b0: 655b 5b4d 6f64 756c 652c 2073 7472 5d2c  e[[Module, str],
+000143c0: 2062 6f6f 6c5d 5d20 3d20 4661 6c73 652c   bool]] = False,
+000143d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000143e0: 2020 2020 2020 2920 2d3e 2043 616c 6c61        ) -> Calla
+000143f0: 626c 655b 2e2e 2e2c 2054 7570 6c65 5b41  ble[..., Tuple[A
+00014400: 6e79 2c20 556e 696f 6e5b 4672 6f7a 656e  ny, Union[Frozen
+00014410: 5661 7269 6162 6c65 4469 6374 2c20 4469  VariableDict, Di
+00014420: 6374 5b73 7472 2c20 416e 795d 5d5d 5d3a  ct[str, Any]]]]:
+00014430: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
+00014440: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
+00014450: 6f20 6361 6c6c 2060 6066 6e60 6020 7769  o call ``fn`` wi
+00014460: 7468 2061 2062 6f75 6e64 206d 6f64 756c  th a bound modul
+00014470: 6520 7468 6174 2061 6c73 6f20 7265 7475  e that also retu
+00014480: 726e 7320 7468 6520 6675 6e63 7469 6f6e  rns the function
+00014490: 206f 7574 7075 7473 2e0a 0a20 2055 6e6c   outputs...  Unl
+000144a0: 696b 6520 6060 4d6f 6475 6c65 2e69 6e69  ike ``Module.ini
+000144b0: 745f 7769 7468 5f6f 7574 7075 7460 6020  t_with_output`` 
+000144c0: 7468 6973 2066 756e 6374 696f 6e20 7265  this function re
+000144d0: 7475 726e 7320 6120 6e65 7720 6675 6e63  turns a new func
+000144e0: 7469 6f6e 2077 6974 6820 7468 6520 7369  tion with the si
+000144f0: 676e 6174 7572 650a 2020 6060 2872 6e67  gnature.  ``(rng
+00014500: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+00014510: 6773 2920 2d3e 2028 542c 2076 6172 6961  gs) -> (T, varia
+00014520: 626c 6573 2960 6020 7768 6572 6520 6054  bles)`` where `T
+00014530: 6020 6973 2074 6865 2072 6574 7572 6e20  ` is the return 
+00014540: 7479 7065 206f 6620 6060 666e 6060 2e0a  type of ``fn``..
+00014550: 2020 5468 6520 726e 6773 2063 616e 2062    The rngs can b
+00014560: 6520 6120 6469 6374 206f 6620 5052 4e47  e a dict of PRNG
+00014570: 4b65 7973 206f 7220 6120 7369 6e67 6c65  Keys or a single
+00014580: 2060 6060 5052 4e47 4b65 7960 6020 7768   ```PRNGKey`` wh
+00014590: 6963 6820 6973 0a20 2065 7175 6976 616c  ich is.  equival
+000145a0: 656e 7420 746f 2070 6173 7369 6e67 2061  ent to passing a
+000145b0: 2064 6963 7420 7769 7468 206f 6e65 2050   dict with one P
+000145c0: 524e 474b 6579 2077 6974 6820 7468 6520  RNGKey with the 
+000145d0: 6e61 6d65 2022 7061 7261 6d73 222e 0a0a  name "params"...
+000145e0: 2020 5468 6520 696e 6974 2066 756e 6374    The init funct
+000145f0: 696f 6e20 7468 6174 2069 7320 7265 7475  ion that is retu
+00014600: 726e 6564 2063 616e 2062 6520 6469 7265  rned can be dire
+00014610: 6374 6c79 2063 6f6d 706f 7365 6420 7769  ctly composed wi
+00014620: 7468 0a20 204a 4158 2074 7261 6e73 666f  th.  JAX transfo
+00014630: 726d 6174 696f 6e73 206c 696b 6520 6060  rmations like ``
+00014640: 6a61 782e 6a69 7460 603a 3a0a 0a20 2020  jax.jit``::..   
+00014650: 2064 6566 2066 2866 6f6f 2c20 7829 3a0a   def f(foo, x):.
+00014660: 2020 2020 2020 7a20 3d20 666f 6f2e 656e        z = foo.en
+00014670: 636f 6465 2878 290a 2020 2020 2020 7920  code(x).      y 
+00014680: 3d20 666f 6f2e 6465 636f 6465 287a 290a  = foo.decode(z).
+00014690: 2020 2020 2020 2320 2e2e 2e0a 2020 2020        # ....    
+000146a0: 2020 7265 7475 726e 2079 0a0a 2020 2020    return y..    
+000146b0: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
+000146c0: 665f 6a69 7474 6564 203d 206a 6178 2e6a  f_jitted = jax.j
+000146d0: 6974 286e 6e2e 696e 6974 5f77 6974 685f  it(nn.init_with_
+000146e0: 6f75 7470 7574 2866 2c20 666f 6f29 290a  output(f, foo)).
+000146f0: 2020 2020 792c 2076 6172 6961 626c 6573      y, variables
+00014700: 203d 2066 5f6a 6974 7465 6428 726e 672c   = f_jitted(rng,
+00014710: 2078 290a 0a20 2041 7267 733a 0a20 2020   x)..  Args:.   
+00014720: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
+00014730: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
+00014740: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
+00014750: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
+00014760: 7365 6420 7769 6c6c 0a20 2020 2020 2062  sed will.      b
+00014770: 6520 616e 206d 6f64 756c 6520 696e 7374  e an module inst
+00014780: 616e 6365 206f 6620 7468 6520 6060 6d6f  ance of the ``mo
+00014790: 6475 6c65 6060 2077 6974 6820 7661 7269  dule`` with vari
+000147a0: 6162 6c65 7320 616e 6420 524e 4773 2062  ables and RNGs b
+000147b0: 6f75 6e64 0a20 2020 2020 2074 6f20 6974  ound.      to it
+000147c0: 2e0a 2020 2020 6d6f 6475 6c65 3a20 5468  ..    module: Th
+000147d0: 6520 6060 4d6f 6475 6c65 6060 2074 6861  e ``Module`` tha
+000147e0: 7420 7769 6c6c 2062 6520 7573 6564 2074  t will be used t
+000147f0: 6f20 6269 6e64 2076 6172 6961 626c 6573  o bind variables
+00014800: 2061 6e64 2052 4e47 7320 746f 2e0a 2020   and RNGs to..  
+00014810: 2020 2020 5468 6520 6060 4d6f 6475 6c65      The ``Module
+00014820: 6060 2070 6173 7365 6420 6173 2074 6865  `` passed as the
+00014830: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
+00014840: 746f 2060 6066 6e60 6020 7769 6c6c 2062  to ``fn`` will b
+00014850: 6520 6120 636c 6f6e 650a 2020 2020 2020  e a clone.      
+00014860: 6f66 206d 6f64 756c 652e 0a20 2020 206d  of module..    m
+00014870: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
+00014880: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
+00014890: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
+000148a0: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
+000148b0: 686f 756c 6420 6265 0a20 2020 2020 2074  hould be.      t
+000148c0: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
+000148d0: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
+000148e0: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
+000148f0: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
+00014900: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
+00014910: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
+00014920: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+00014930: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
+00014940: 0a20 2020 2020 206c 6973 7420 6f66 206e  .      list of n
+00014950: 616d 6573 206f 6620 6d75 7461 626c 6520  ames of mutable 
+00014960: 636f 6c6c 6563 7469 6f6e 732e 2042 7920  collections. By 
+00014970: 6465 6661 756c 7420 616c 6c20 636f 6c6c  default all coll
+00014980: 6563 7469 6f6e 730a 2020 2020 2020 6578  ections.      ex
+00014990: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
+000149a0: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
+000149b0: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
+000149c0: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
+000149d0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
+000149e0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+000149f0: 7475 726e 2076 616c 7565 730a 2020 2020  turn values.    
+00014a00: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
+00014a10: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+00014a20: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+00014a30: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+00014a40: 6c74 206f 6e6c 790a 2020 2020 2020 7468  lt only.      th
+00014a50: 6520 7265 7475 726e 2076 616c 7565 7320  e return values 
+00014a60: 6f66 2061 6c6c 2060 5f5f 6361 6c6c 5f5f  of all `__call__
+00014a70: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
+00014a80: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
+00014a90: 2063 616e 0a20 2020 2020 2062 6520 7061   can.      be pa
+00014aa0: 7373 6564 2074 6f20 6368 616e 6765 2074  ssed to change t
+00014ab0: 6865 2066 696c 7465 7220 6265 6861 7669  he filter behavi
+00014ac0: 6f72 2e20 5468 6520 6669 6c74 6572 2066  or. The filter f
+00014ad0: 756e 6374 696f 6e20 7461 6b65 730a 2020  unction takes.  
+00014ae0: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
+00014af0: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
+00014b00: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
+00014b10: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
+00014b20: 6174 696e 670a 2020 2020 2020 7768 6574  ating.      whet
+00014b30: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
+00014b40: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
+00014b50: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
+00014b60: 6265 2073 746f 7265 642e 0a20 2052 6574  be stored..  Ret
+00014b70: 7572 6e73 3a0a 2020 2020 5468 6520 696e  urns:.    The in
+00014b80: 6974 2066 756e 6374 696f 6e20 7772 6170  it function wrap
+00014b90: 7069 6e67 2060 6066 6e60 602e 0a20 2022  ping ``fn``..  "
+00014ba0: 2222 0a20 2040 6675 6e63 746f 6f6c 732e  "".  @functools.
+00014bb0: 7772 6170 7328 666e 290a 2020 6465 6620  wraps(fn).  def 
+00014bc0: 7363 6f70 655f 666e 2873 636f 7065 2c20  scope_fn(scope, 
+00014bd0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+00014be0: 3a0a 2020 2020 5f63 6f6e 7465 7874 2e63  :.    _context.c
+00014bf0: 6170 7475 7265 5f73 7461 636b 2e61 7070  apture_stack.app
+00014c00: 656e 6428 6361 7074 7572 655f 696e 7465  end(capture_inte
+00014c10: 726d 6564 6961 7465 7329 0a20 2020 2074  rmediates).    t
+00014c20: 7279 3a0a 2020 2020 2020 7265 7475 726e  ry:.      return
+00014c30: 2066 6e28 6d6f 6475 6c65 2e63 6c6f 6e65   fn(module.clone
+00014c40: 2870 6172 656e 743d 7363 6f70 652c 205f  (parent=scope, _
+00014c50: 6465 6570 5f63 6c6f 6e65 3d54 7275 6529  deep_clone=True)
+00014c60: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
+00014c70: 7329 0a20 2020 2066 696e 616c 6c79 3a0a  s).    finally:.
+00014c80: 2020 2020 2020 5f63 6f6e 7465 7874 2e63        _context.c
+00014c90: 6170 7475 7265 5f73 7461 636b 2e70 6f70  apture_stack.pop
+00014ca0: 2829 0a0a 2020 6966 2063 6170 7475 7265  ()..  if capture
+00014cb0: 5f69 6e74 6572 6d65 6469 6174 6573 2069  _intermediates i
+00014cc0: 7320 5472 7565 3a20 2023 2070 796c 696e  s True:  # pylin
+00014cd0: 743a 2064 6973 6162 6c65 3d67 2d62 6f6f  t: disable=g-boo
+00014ce0: 6c2d 6964 2d63 6f6d 7061 7269 736f 6e0a  l-id-comparison.
+00014cf0: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+00014d00: 726d 6564 6961 7465 7320 3d20 6361 7074  rmediates = capt
+00014d10: 7572 655f 6361 6c6c 5f69 6e74 6572 6d65  ure_call_interme
+00014d20: 6469 6174 6573 0a20 2069 6620 6361 7074  diates.  if capt
+00014d30: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
+00014d40: 733a 0a20 2020 206d 7574 6162 6c65 203d  s:.    mutable =
+00014d50: 2075 6e69 6f6e 5f66 696c 7465 7273 286d   union_filters(m
+00014d60: 7574 6162 6c65 2c20 2769 6e74 6572 6d65  utable, 'interme
+00014d70: 6469 6174 6573 2729 0a20 2072 6574 7572  diates').  retur
+00014d80: 6e20 636f 7265 2e69 6e69 7428 7363 6f70  n core.init(scop
+00014d90: 655f 666e 2c20 6d75 7461 626c 653d 6d75  e_fn, mutable=mu
+00014da0: 7461 626c 6529 0a0a 0a40 7472 6163 6562  table)...@traceb
+00014db0: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
+00014dc0: 6e64 6172 790a 6465 6620 696e 6974 2866  ndary.def init(f
+00014dd0: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
+00014de0: 2041 6e79 5d2c 206d 6f64 756c 653a 204d   Any], module: M
+00014df0: 6f64 756c 652c 0a20 2020 2020 2020 2020  odule,.         
+00014e00: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
+00014e10: 696f 6e46 696c 7465 7220 3d20 4465 6e79  ionFilter = Deny
+00014e20: 4c69 7374 2827 696e 7465 726d 6564 6961  List('intermedia
+00014e30: 7465 7327 292c 0a20 2020 2020 2020 2020  tes'),.         
+00014e40: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+00014e50: 6961 7465 733a 2055 6e69 6f6e 5b62 6f6f  iates: Union[boo
+00014e60: 6c2c 2043 616c 6c61 626c 655b 5b4d 6f64  l, Callable[[Mod
+00014e70: 756c 652c 2073 7472 5d2c 2062 6f6f 6c5d  ule, str], bool]
+00014e80: 5d20 3d20 4661 6c73 652c 0a20 2020 2020  ] = False,.     
+00014e90: 2020 2020 2920 2d3e 2043 616c 6c61 626c      ) -> Callabl
+00014ea0: 655b 2e2e 2e2c 2055 6e69 6f6e 5b46 726f  e[..., Union[Fro
+00014eb0: 7a65 6e56 6172 6961 626c 6544 6963 742c  zenVariableDict,
+00014ec0: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
+00014ed0: 5d3a 0a20 2022 2222 4372 6561 7465 7320  ]:.  """Creates 
+00014ee0: 616e 2069 6e69 7420 6675 6e63 7469 6f6e  an init function
+00014ef0: 2074 6f20 6361 6c6c 2060 6066 6e60 6020   to call ``fn`` 
+00014f00: 7769 7468 2061 2062 6f75 6e64 206d 6f64  with a bound mod
+00014f10: 756c 652e 0a0a 2020 556e 6c69 6b65 2060  ule...  Unlike `
+00014f20: 604d 6f64 756c 652e 696e 6974 6060 2074  `Module.init`` t
+00014f30: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
+00014f40: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
+00014f50: 696f 6e20 7769 7468 2074 6865 2073 6967  ion with the sig
+00014f60: 6e61 7475 7265 0a20 2060 6028 726e 6773  nature.  ``(rngs
+00014f70: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
+00014f80: 7329 202d 3e20 7661 7269 6162 6c65 7360  s) -> variables`
+00014f90: 602e 0a20 2054 6865 2072 6e67 7320 6361  `..  The rngs ca
+00014fa0: 6e20 6265 2061 2064 6963 7420 6f66 2050  n be a dict of P
+00014fb0: 524e 474b 6579 7320 6f72 2061 2073 696e  RNGKeys or a sin
+00014fc0: 676c 6520 6060 6050 524e 474b 6579 6060  gle ```PRNGKey``
+00014fd0: 2077 6869 6368 2069 730a 2020 6571 7569   which is.  equi
+00014fe0: 7661 6c65 6e74 2074 6f20 7061 7373 696e  valent to passin
+00014ff0: 6720 6120 6469 6374 2077 6974 6820 6f6e  g a dict with on
+00015000: 6520 5052 4e47 4b65 7920 7769 7468 2074  e PRNGKey with t
+00015010: 6865 206e 616d 6520 2270 6172 616d 7322  he name "params"
+00015020: 2e0a 0a20 2054 6865 2069 6e69 7420 6675  ...  The init fu
+00015030: 6e63 7469 6f6e 2074 6861 7420 6973 2072  nction that is r
+00015040: 6574 7572 6e65 6420 6361 6e20 6265 2064  eturned can be d
+00015050: 6972 6563 746c 7920 636f 6d70 6f73 6564  irectly composed
+00015060: 2077 6974 680a 2020 4a41 5820 7472 616e   with.  JAX tran
+00015070: 7366 6f72 6d61 7469 6f6e 7320 6c69 6b65  sformations like
+00015080: 2060 606a 6178 2e6a 6974 6060 3a3a 0a0a   ``jax.jit``::..
+00015090: 2020 2020 6465 6620 6628 666f 6f2c 2078      def f(foo, x
+000150a0: 293a 0a20 2020 2020 207a 203d 2066 6f6f  ):.      z = foo
+000150b0: 2e65 6e63 6f64 6528 7829 0a20 2020 2020  .encode(x).     
+000150c0: 2079 203d 2066 6f6f 2e64 6563 6f64 6528   y = foo.decode(
+000150d0: 7a29 0a20 2020 2020 2023 202e 2e2e 0a20  z).      # .... 
+000150e0: 2020 2020 2072 6574 7572 6e20 790a 0a20       return y.. 
+000150f0: 2020 2066 6f6f 203d 2046 6f6f 2829 0a20     foo = Foo(). 
+00015100: 2020 2066 5f6a 6974 7465 6420 3d20 6a61     f_jitted = ja
+00015110: 782e 6a69 7428 6e6e 2e69 6e69 7428 662c  x.jit(nn.init(f,
+00015120: 2066 6f6f 2929 0a20 2020 2076 6172 6961   foo)).    varia
+00015130: 626c 6573 203d 2066 5f6a 6974 7465 6428  bles = f_jitted(
+00015140: 726e 672c 2078 290a 0a20 2041 7267 733a  rng, x)..  Args:
+00015150: 0a20 2020 2066 6e3a 2054 6865 2066 756e  .    fn: The fun
+00015160: 6374 696f 6e20 7468 6174 2073 686f 756c  ction that shoul
+00015170: 6420 6265 2061 7070 6c69 6564 2e20 5468  d be applied. Th
+00015180: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
+00015190: 2070 6173 7365 6420 7769 6c6c 0a20 2020   passed will.   
+000151a0: 2020 2062 6520 616e 206d 6f64 756c 6520     be an module 
+000151b0: 696e 7374 616e 6365 206f 6620 7468 6520  instance of the 
+000151c0: 6060 6d6f 6475 6c65 6060 2077 6974 6820  ``module`` with 
+000151d0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
+000151e0: 4773 2062 6f75 6e64 0a20 2020 2020 2074  Gs bound.      t
+000151f0: 6f20 6974 2e0a 2020 2020 6d6f 6475 6c65  o it..    module
+00015200: 3a20 5468 6520 6060 4d6f 6475 6c65 6060  : The ``Module``
+00015210: 2074 6861 7420 7769 6c6c 2062 6520 7573   that will be us
+00015220: 6564 2074 6f20 6269 6e64 2076 6172 6961  ed to bind varia
+00015230: 626c 6573 2061 6e64 2052 4e47 7320 746f  bles and RNGs to
+00015240: 2e0a 2020 2020 2020 5468 6520 6060 4d6f  ..      The ``Mo
+00015250: 6475 6c65 6060 2070 6173 7365 6420 6173  dule`` passed as
+00015260: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
+00015270: 656e 7420 746f 2060 6066 6e60 6020 7769  ent to ``fn`` wi
+00015280: 6c6c 2062 6520 6120 636c 6f6e 650a 2020  ll be a clone.  
+00015290: 2020 2020 6f66 206d 6f64 756c 652e 0a20      of module.. 
+000152a0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
+000152b0: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
+000152c0: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
+000152d0: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
+000152e0: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
+000152f0: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
+00015300: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
+00015310: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
+00015320: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
+00015330: 0a20 2020 2020 2060 6073 7472 6060 3a20  .      ``str``: 
+00015340: 5468 6520 6e61 6d65 206f 6620 6120 7369  The name of a si
+00015350: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
+00015360: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
+00015370: 603a 2041 0a20 2020 2020 206c 6973 7420  `: A.      list 
+00015380: 6f66 206e 616d 6573 206f 6620 6d75 7461  of names of muta
+00015390: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
+000153a0: 2042 7920 6465 6661 756c 7420 616c 6c20   By default all 
+000153b0: 636f 6c6c 6563 7469 6f6e 730a 2020 2020  collections.    
+000153c0: 2020 6578 6365 7074 2022 696e 7465 726d    except "interm
+000153d0: 6564 6961 7465 7322 2061 7265 206d 7574  ediates" are mut
+000153e0: 6162 6c65 2e0a 2020 2020 6361 7074 7572  able..    captur
+000153f0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
+00015400: 2049 6620 6054 7275 6560 2c20 6361 7074   If `True`, capt
+00015410: 7572 6573 2069 6e74 6572 6d65 6469 6174  ures intermediat
+00015420: 6520 7265 7475 726e 2076 616c 7565 730a  e return values.
+00015430: 2020 2020 2020 6f66 2061 6c6c 204d 6f64        of all Mod
+00015440: 756c 6573 2069 6e73 6964 6520 7468 6520  ules inside the 
+00015450: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
+00015460: 636f 6c6c 6563 7469 6f6e 2e20 4279 2064  collection. By d
+00015470: 6566 6175 6c74 206f 6e6c 790a 2020 2020  efault only.    
+00015480: 2020 7468 6520 7265 7475 726e 2076 616c    the return val
+00015490: 7565 7320 6f66 2061 6c6c 2060 5f5f 6361  ues of all `__ca
+000154a0: 6c6c 5f5f 6020 6d65 7468 6f64 7320 6172  ll__` methods ar
+000154b0: 6520 7374 6f72 6564 2e20 4120 6675 6e63  e stored. A func
+000154c0: 7469 6f6e 2063 616e 0a20 2020 2020 2062  tion can.      b
+000154d0: 6520 7061 7373 6564 2074 6f20 6368 616e  e passed to chan
+000154e0: 6765 2074 6865 2066 696c 7465 7220 6265  ge the filter be
+000154f0: 6861 7669 6f72 2e20 5468 6520 6669 6c74  havior. The filt
+00015500: 6572 2066 756e 6374 696f 6e20 7461 6b65  er function take
+00015510: 730a 2020 2020 2020 7468 6520 4d6f 6475  s.      the Modu
+00015520: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
+00015530: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
+00015540: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
+00015550: 6e64 6963 6174 696e 670a 2020 2020 2020  ndicating.      
+00015560: 7768 6574 6865 7220 7468 6520 6f75 7470  whether the outp
+00015570: 7574 206f 6620 7468 6174 206d 6574 686f  ut of that metho
+00015580: 6420 696e 766f 6361 7469 6f6e 2073 686f  d invocation sho
+00015590: 756c 6420 6265 2073 746f 7265 642e 0a20  uld be stored.. 
+000155a0: 2052 6574 7572 6e73 3a0a 2020 2020 5468   Returns:.    Th
+000155b0: 6520 696e 6974 2066 756e 6374 696f 6e20  e init function 
+000155c0: 7772 6170 7069 6e67 2060 6066 6e60 602e  wrapping ``fn``.
+000155d0: 0a20 2022 2222 0a20 2069 6e69 745f 666e  .  """.  init_fn
+000155e0: 203d 2069 6e69 745f 7769 7468 5f6f 7574   = init_with_out
+000155f0: 7075 7428 666e 2c20 6d6f 6475 6c65 2c20  put(fn, module, 
+00015600: 6d75 7461 626c 652c 2063 6170 7475 7265  mutable, capture
+00015610: 5f69 6e74 6572 6d65 6469 6174 6573 290a  _intermediates).
+00015620: 2020 4066 756e 6374 6f6f 6c73 2e77 7261    @functools.wra
+00015630: 7073 2869 6e69 745f 666e 290a 2020 6465  ps(init_fn).  de
+00015640: 6620 696e 6974 5f77 7261 7070 6572 282a  f init_wrapper(*
+00015650: 6172 6773 2c20 2a2a 6b77 6172 6773 293a  args, **kwargs):
+00015660: 0a20 2020 2072 6574 7572 6e20 696e 6974  .    return init
+00015670: 5f66 6e28 2a61 7267 732c 202a 2a6b 7761  _fn(*args, **kwa
+00015680: 7267 7329 5b31 5d0a 2020 7265 7475 726e  rgs)[1].  return
+00015690: 2069 6e69 745f 7772 6170 7065 720a        init_wrapper.
```

### Comparing `flax-0.6.9/flax/linen/normalization.py` & `flax-0.7.0/flax/linen/normalization.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,17 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Normalization modules for Flax."""
 
+import functools
 from typing import (Any, Callable, Iterable, Optional, Tuple, Union)
 from flax.linen.dtypes import canonicalize_dtype
-
 from flax.linen.module import Module, compact, merge_param  # pylint: disable=g-multiple-import
 from jax import lax
 from jax.nn import initializers
 import jax.numpy as jnp
 
 
 PRNGKey = Any
@@ -86,20 +86,21 @@
   mean2 = jnp.mean(_abs_sq(x), axes)
   if use_mean:
     mean = jnp.mean(x, axes)
   else:
     mean = jnp.zeros(mean2.shape, dtype=dtype)
 
   if axis_name is not None:
-    concatenated_mean = jnp.concatenate([mean, mean2])
-    mean, mean2 = jnp.split(
-        lax.pmean(
-            concatenated_mean,
-            axis_name=axis_name,
-            axis_index_groups=axis_index_groups), 2)
+    pmean = functools.partial(
+        lax.pmean, axis_name=axis_name, axis_index_groups=axis_index_groups
+    )
+    if use_mean:
+      mean, mean2 = jnp.split(pmean(jnp.concatenate([mean, mean2])), 2)
+    else:
+      mean2 = pmean(mean2)
   # mean2 - _abs_sq(mean) is not guaranteed to be non-negative due
   # to floating point round-off errors.
   var = jnp.maximum(0., mean2 - _abs_sq(mean))
   return mean, var
 
 
 def _normalize(mdl: Module, x: Array, mean: Array, var: Array,
@@ -129,24 +130,22 @@
     scale_init: Initialization function for the scaling function.
 
   Returns:
     The normalized input.
   """
   reduction_axes = _canonicalize_axes(x.ndim, reduction_axes)
   feature_axes = _canonicalize_axes(x.ndim, feature_axes)
-  stats_shape = list(x.shape)
-  for axis in reduction_axes:
-    stats_shape[axis] = 1
-  mean = mean.reshape(stats_shape)
-  var = var.reshape(stats_shape)
   feature_shape = [1] * x.ndim
   reduced_feature_shape = []
   for ax in feature_axes:
     feature_shape[ax] = x.shape[ax]
     reduced_feature_shape.append(x.shape[ax])
+
+  mean = jnp.expand_dims(mean, reduction_axes)
+  var = jnp.expand_dims(var, reduction_axes)
   y = x - mean
   mul = lax.rsqrt(var + epsilon)
   args = [x]
   if use_scale:
     scale = mdl.param('scale', scale_init, reduced_feature_shape,
                       param_dtype).reshape(feature_shape)
     mul *= scale
@@ -493,23 +492,18 @@
     if num_groups <= 0 or channels % num_groups != 0:
       raise ValueError('Number of groups ({}) does not divide the number'
                        ' of channels ({}).'.format(num_groups, channels))
 
     group_size = x.shape[-1] // num_groups
     group_shape = x.shape[:-1] + (num_groups, group_size)
 
-    def broadcast_stat(stat):
-      stat = jnp.broadcast_to(stat[..., None],
-                              (x.shape[0], num_groups, group_size))
-      return stat.reshape((x.shape[0], num_groups * group_size))
-
     mean, var = _compute_stats(
         x.reshape(group_shape), reduction_axes, self.dtype, self.axis_name,
         self.axis_index_groups)
-    mean = broadcast_stat(mean)
-    var = broadcast_stat(var)
+    mean = jnp.repeat(mean, group_size, axis=-1)
+    var = jnp.repeat(var, group_size, axis=-1)
 
     return _normalize(
         self, x, mean, var, reduction_axes[:-1], feature_axes,
         self.dtype, self.param_dtype, self.epsilon,
         self.use_bias, self.use_scale,
         self.bias_init, self.scale_init)
```

### Comparing `flax-0.6.9/flax/linen/partitioning.py` & `flax-0.7.0/flax/linen/partitioning.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/pooling.py` & `flax-0.7.0/flax/linen/pooling.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/recurrent.py` & `flax-0.7.0/flax/linen/recurrent.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,69 +11,104 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Recurrent neural network modules.
 
 THe RNNCell modules can be scanned using lifted transforms. For more information
-see: https://flax.readthedocs.io/en/latest/advanced_topics/lift.html.
+see: https://flax.readthedocs.io/en/latest/developer_notes/lift.html.
 """
 
+from abc import ABCMeta
 from functools import partial   # pylint: disable=g-importing-member
 from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple, Union, TypeVar, cast
 from typing_extensions import Protocol
 from absl import logging
 
 from flax.linen.activation import sigmoid
 from flax.linen.activation import tanh
 from flax.linen.dtypes import promote_dtype
 from flax.linen import initializers
 from flax.linen.linear import Conv
 from flax.linen.linear import default_kernel_init
 from flax.linen.linear import Dense
 from flax.linen.linear import PrecisionLike
-from flax.linen.module import compact
+from flax.linen.module import compact, nowrap
 from flax.linen.module import Module
 from jax import numpy as jnp
 from jax import random
 import numpy as np
 from flax.core import lift
 from flax.core.frozen_dict import FrozenDict
 from flax.linen import transforms
 import jax
 
 A = TypeVar('A')
-PRNGKey = Any
+PRNGKey = jax.random.KeyArray
 Shape = Tuple[int, ...]
 Dtype = Any  # this could be a real type?
 Array = jax.Array
 Carry = Any
 CarryHistory = Any
 Output = Any
 
+class _Never:
+  pass
+
+NEVER = _Never()
+
+LEGACY_UPDATE_MESSAGE = (
+  "The RNNCellBase API has changed, "
+  "the error you are experiencing might be caused by this change. Please "
+  "update your code to the new API, for more information on how to do this "
+  "please check out the RNNCellBase migration guide: "
+  "https://flax.readthedocs.io/en/latest/guides/rnncell_upgrade_guide.html"
+)
+
+class RNNCellCompatibilityMeta(ABCMeta):
+  """Metaclass for RNNCell compatibility."""
+
+  def __call__(self, *args: Any, **kwds: Any) -> Any:
+    try:
+      return super().__call__(*args, **kwds)
+    except TypeError as e:
+      msg = e.args[0]
+      raise TypeError(f'{msg} \n\n {LEGACY_UPDATE_MESSAGE}') from e
+
+def deprecation_method_decorator(f):
+  def wrapper(*args, **kwargs):
+    if len(args) < 1 or not isinstance(args[0], RNNCellBase):
+      raise TypeError(LEGACY_UPDATE_MESSAGE)
+    return f(*args, **kwargs)
+  return wrapper
 
 class RNNCellBase(Module):
   """RNN cell base class."""
 
-  @staticmethod
-  def initialize_carry(rng, batch_dims, size, init_fn=initializers.zeros_init()):
+  @nowrap
+  def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Carry:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
-      batch_dims: a tuple providing the shape of the batch dimensions.
-      size: the size or number of features of the memory.
-      init_fn: initializer function for the carry.
+      input_shape: a tuple providing the shape of the input to the cell.
+
     Returns:
       An initialized carry for the given RNN cell.
     """
     raise NotImplementedError
 
 
-class LSTMCell(RNNCellBase):
+  @property
+  def num_feature_axes(self) -> int:
+    """Returns the number of feature axes of the RNN cell."""
+    raise NotImplementedError
+
+
+class LSTMCell(RNNCellBase, metaclass=RNNCellCompatibilityMeta):
   r"""LSTM cell.
 
   The mathematical definition of the cell is as follows
 
   .. math::
       \begin{array}{ll}
       i = \sigma(W_{ii} x + W_{hi} h + b_{hi}) \\
@@ -84,32 +119,35 @@
       h' = o * \tanh(c') \\
       \end{array}
 
   where x is the input, h is the output of the previous time step, and c is
   the memory.
 
   Attributes:
+    features: number of output features.
     gate_fn: activation function used for gates (default: sigmoid)
     activation_fn: activation function used for output and memory update
       (default: tanh).
     kernel_init: initializer function for the kernels that transform
       the input (default: lecun_normal).
     recurrent_kernel_init: initializer function for the kernels that transform
       the hidden state (default: initializers.orthogonal()).
     bias_init: initializer for the bias parameters (default: initializers.zeros_init())
     dtype: the dtype of the computation (default: infer from inputs and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
   """
+  features: int
   gate_fn: Callable[..., Any] = sigmoid
   activation_fn: Callable[..., Any] = tanh
-  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.orthogonal()
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  kernel_init: initializers.Initializer = default_kernel_init
+  recurrent_kernel_init: initializers.Initializer = initializers.orthogonal()
+  bias_init: initializers.Initializer = initializers.zeros_init()
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
+  carry_init: initializers.Initializer = initializers.zeros_init()
 
   @compact
   def __call__(self, carry, inputs):
     r"""A long short-term memory (LSTM) cell.
 
     Args:
       carry: the hidden state of the LSTM cell,
@@ -140,29 +178,36 @@
     f = self.gate_fn(dense_i(name='if')(inputs) + dense_h(name='hf')(h))
     g = self.activation_fn(dense_i(name='ig')(inputs) + dense_h(name='hg')(h))
     o = self.gate_fn(dense_i(name='io')(inputs) + dense_h(name='ho')(h))
     new_c = f * c + i * g
     new_h = o * self.activation_fn(new_c)
     return (new_c, new_h), new_h
 
-  @staticmethod
-  def initialize_carry(rng, batch_dims, size, init_fn=initializers.zeros_init()):
+  @nowrap
+  @deprecation_method_decorator
+  def initialize_carry(
+      self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Tuple[Array, Array]:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
-      batch_dims: a tuple providing the shape of the batch dimensions.
-      size: the size or number of features of the memory.
-      init_fn: initializer function for the carry.
+      input_shape: a tuple providing the shape of the input to the cell.
     Returns:
       An initialized carry for the given RNN cell.
     """
+    batch_dims = input_shape[:-1]
     key1, key2 = random.split(rng)
-    mem_shape = batch_dims + (size,)
-    return init_fn(key1, mem_shape), init_fn(key2, mem_shape)
+    mem_shape = batch_dims + (self.features,)
+    c = self.carry_init(key1, mem_shape, self.param_dtype)
+    h = self.carry_init(key2, mem_shape, self.param_dtype)
+    return (c, h)
+
+  @property
+  def num_feature_axes(self) -> int:
+    return 1
 
 
 class DenseParams(Module):
   """Dummy module for creating parameters matching `flax.linen.Dense`."""
 
   features: int
   use_bias: bool = True
@@ -179,15 +224,15 @@
     if self.use_bias:
       b = self.param('bias', self.bias_init, (self.features,), self.param_dtype)
     else:
       b = None
     return k, b
 
 
-class OptimizedLSTMCell(RNNCellBase):
+class OptimizedLSTMCell(RNNCellBase, metaclass=RNNCellCompatibilityMeta):
   r"""More efficient LSTM Cell that concatenates state components before matmul.
 
   The parameters are compatible with `LSTMCell`. Note that this cell is often
   faster than `LSTMCell` as long as the hidden size is roughly <= 2048 units.
 
   The mathematical definition of the cell is the same as `LSTMCell` and as
   follows
@@ -214,21 +259,23 @@
       the input (default: lecun_normal).
     recurrent_kernel_init: initializer function for the kernels that transform
       the hidden state (default: initializers.orthogonal()).
     bias_init: initializer for the bias parameters (default: initializers.zeros_init()).
     dtype: the dtype of the computation (default: infer from inputs and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
   """
+  features: int
   gate_fn: Callable[..., Any] = sigmoid
   activation_fn: Callable[..., Any] = tanh
-  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.orthogonal()
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  kernel_init: initializers.Initializer = default_kernel_init
+  recurrent_kernel_init: initializers.Initializer = initializers.orthogonal()
+  bias_init: initializers.Initializer = initializers.zeros_init()
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
+  carry_init: initializers.Initializer = initializers.zeros_init()
 
   @compact
   def __call__(self, carry: Tuple[Array, Array],
                inputs: Array) -> Tuple[Tuple[Array, Array], Array]:
     r"""An optimized long short-term memory (LSTM) cell.
 
     Args:
@@ -294,33 +341,39 @@
     g = self.activation_fn(dense_h['g'] + dense_i['g'])
     o = self.gate_fn(dense_h['o'] + dense_i['o'])
 
     new_c = f * c + i * g
     new_h = o * self.activation_fn(new_c)
     return (new_c, new_h), new_h
 
-  @staticmethod
-  def initialize_carry(rng, batch_dims, size, init_fn=initializers.zeros_init()):
+  @nowrap
+  @deprecation_method_decorator
+  def initialize_carry(
+      self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Tuple[Array, Array]:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
-      batch_dims: a tuple providing the shape of the batch dimensions.
-      size: the size or number of features of the memory.
-      init_fn: initializer function for the carry.
+      input_shape: a tuple providing the shape of the input to the cell.
 
     Returns:
       An initialized carry for the given RNN cell.
     """
+    batch_dims = input_shape[:-1]
     key1, key2 = random.split(rng)
-    mem_shape = batch_dims + (size,)
-    return init_fn(key1, mem_shape), init_fn(key2, mem_shape)
-
+    mem_shape = batch_dims + (self.features,)
+    c = self.carry_init(key1, mem_shape, self.param_dtype)
+    h = self.carry_init(key2, mem_shape, self.param_dtype)
+    return c, h
+
+  @property
+  def num_feature_axes(self) -> int:
+    return 1
 
-class GRUCell(RNNCellBase):
+class GRUCell(RNNCellBase, metaclass=RNNCellCompatibilityMeta):
   r"""GRU cell.
 
   The mathematical definition of the cell is as follows
 
   .. math::
 
       \begin{array}{ll}
@@ -340,23 +393,23 @@
       the input (default: lecun_normal).
     recurrent_kernel_init: initializer function for the kernels that transform
       the hidden state (default: initializers.orthogonal()).
     bias_init: initializer for the bias parameters (default: initializers.zeros_init())
     dtype: the dtype of the computation (default: None).
     param_dtype: the dtype passed to parameter initializers (default: float32).
   """
+  features: int
   gate_fn: Callable[..., Any] = sigmoid
   activation_fn: Callable[..., Any] = tanh
-  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
-      default_kernel_init)
-  recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
-      initializers.orthogonal())
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  kernel_init: initializers.Initializer = default_kernel_init
+  recurrent_kernel_init: initializers.Initializer = initializers.orthogonal()
+  bias_init: initializers.Initializer = initializers.zeros_init()
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
+  carry_init: initializers.Initializer = initializers.zeros_init()
 
   @compact
   def __call__(self, carry, inputs):
     """Gated recurrent unit (GRU) cell.
 
     Args:
       carry: the hidden state of the GRU cell,
@@ -388,28 +441,33 @@
     z = self.gate_fn(dense_i(name='iz')(inputs) + dense_h(name='hz')(h))
     # add bias because the linear transformations aren't directly summed.
     n = self.activation_fn(dense_i(name='in')(inputs) +
                            r * dense_h(name='hn', use_bias=True)(h))
     new_h = (1. - z) * n + z * h
     return new_h, new_h
 
-  @staticmethod
-  def initialize_carry(rng, batch_dims, size, init_fn=initializers.zeros_init()):
+  @nowrap
+  @deprecation_method_decorator
+  def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]):
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
-      batch_dims: a tuple providing the shape of the batch dimensions.
-      size: the size or number of features of the memory.
-      init_fn: initializer function for the carry.
+      input_shape: a tuple providing the shape of the input to the cell.
+
     Returns:
       An initialized carry for the given RNN cell.
     """
-    mem_shape = batch_dims + (size,)
-    return init_fn(rng, mem_shape)
+    batch_dims = input_shape[:-1]
+    mem_shape = batch_dims + (self.features,)
+    return self.carry_init(rng, mem_shape, self.param_dtype)
+
+  @property
+  def num_feature_axes(self) -> int:
+    return 1
 
 
 class ConvLSTMCell(RNNCellBase):
   r"""A convolutional LSTM cell.
 
   The implementation is based on xingjian2015convolutional.
   Given x_t and the previous state (h_{t-1}, c_{t-1})
@@ -452,14 +510,15 @@
   features: int
   kernel_size: Sequence[int]
   strides: Optional[Sequence[int]] = None
   padding: Union[str, Sequence[Tuple[int, int]]] = 'SAME'
   use_bias: bool = True
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
+  carry_init: initializers.Initializer = initializers.zeros_init()
 
   @compact
   def __call__(self, carry, inputs):
     """Constructs a convolutional LSTM.
 
     Args:
       carry: the hidden state of the Conv2DLSTM cell,
@@ -493,119 +552,118 @@
     i, g, f, o = jnp.split(gates, indices_or_sections=4, axis=-1)
 
     f = sigmoid(f + 1)
     new_c = f * c + sigmoid(i) * jnp.tanh(g)
     new_h = sigmoid(o) * jnp.tanh(new_c)
     return (new_c, new_h), new_h
 
-  @staticmethod
-  def initialize_carry(rng, batch_dims, size, init_fn=initializers.zeros_init()):
+  @nowrap
+  @deprecation_method_decorator
+  def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]):
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
-      batch_dims: a tuple providing the shape of the batch dimensions.
-      size: the input_shape + (features,).
-      init_fn: initializer function for the carry.
+      input_shape: a tuple providing the shape of the input to the cell.
+
     Returns:
       An initialized carry for the given RNN cell.
     """
+    # (*batch_dims, *signal_dims, features)
+    signal_dims = input_shape[-self.num_feature_axes:-1]
+    batch_dims = input_shape[:-self.num_feature_axes]
     key1, key2 = random.split(rng)
-    mem_shape = batch_dims + size
-    return init_fn(key1, mem_shape), init_fn(key2, mem_shape)
+    mem_shape = batch_dims + signal_dims + (self.features,)
+    c = self.carry_init(key1, mem_shape, self.param_dtype)
+    h = self.carry_init(key2, mem_shape, self.param_dtype)
+    return c, h
+
+  @property
+  def num_feature_axes(self) -> int:
+    return len(self.kernel_size) + 1
 
 class RNN(Module):
   """The ``RNN`` module takes any :class:`RNNCellBase` instance and applies it over a sequence
   using :func:`flax.linen.scan`.
 
   Example::
 
     >>> import jax.numpy as jnp
     >>> import jax
     >>> import flax.linen as nn
     ...
     >>> x = jnp.ones((10, 50, 32)) # (batch, time, features)
-    >>> lstm = nn.RNN(nn.LSTMCell(), cell_size=64)
+    >>> lstm = nn.RNN(nn.LSTMCell(64))
     >>> variables = lstm.init(jax.random.PRNGKey(0), x)
     >>> y = lstm.apply(variables, x)
     >>> y.shape # (batch, time, cell_size)
     (10, 50, 64)
 
   As shown above, RNN uses the ``cell_size`` argument to set the ``size`` argument for the cell's
   ``initialize_carry`` method, in practice this is typically the number of hidden units you want
   for the cell. However, this may vary depending on the cell you are using, for example the
   :class:`ConvLSTMCell` requires a ``size`` argument of the form
   ``(kernel_height, kernel_width, features)``::
 
     >>> x = jnp.ones((10, 50, 32, 32, 3)) # (batch, time, height, width, features)
-    >>> conv_lstm = nn.RNN(nn.ConvLSTMCell(64, kernel_size=(3, 3)), cell_size=(32, 32, 64))
+    >>> conv_lstm = nn.RNN(nn.ConvLSTMCell(64, kernel_size=(3, 3)))
     >>> y, variables = conv_lstm.init_with_output(jax.random.PRNGKey(0), x)
     >>> y.shape # (batch, time, height, width, features)
     (10, 50, 32, 32, 64)
 
   By default RNN expect the time dimension after the batch dimension (``(*batch, time, *features)``),
   if you set ``time_major=True`` RNN will instead expect the time dimesion to be at the beginning
   (``(time, *batch, *features)``)::
 
     >>> x = jnp.ones((50, 10, 32)) # (time, batch, features)
-    >>> lstm = nn.RNN(nn.LSTMCell(), cell_size=64, time_major=True)
+    >>> lstm = nn.RNN(nn.LSTMCell(64), time_major=True)
     >>> variables = lstm.init(jax.random.PRNGKey(0), x)
     >>> y = lstm.apply(variables, x)
     >>> y.shape # (time, batch, cell_size)
     (50, 10, 64)
 
   The output is an array of shape ``(*batch, time, *cell_size)`` by default (typically), however
   if you set ``return_carry=True`` it will instead return a tuple of the final carry and the output::
 
     >>> x = jnp.ones((10, 50, 32)) # (batch, time, features)
-    >>> lstm = nn.RNN(nn.LSTMCell(), cell_size=64, return_carry=True)
+    >>> lstm = nn.RNN(nn.LSTMCell(64), return_carry=True)
     >>> variables = lstm.init(jax.random.PRNGKey(0), x)
     >>> carry, y = lstm.apply(variables, x)
     >>> jax.tree_map(jnp.shape, carry) # ((batch, cell_size), (batch, cell_size))
     ((10, 64), (10, 64))
     >>> y.shape # (batch, time, cell_size)
     (10, 50, 64)
 
-  To support variable length sequences, you can pass a ``segmentation_mask`` which is an integer
-  array of shape ``(*batch, time)``, where a 1 indicates the element is part of the sequence and a 0 indicates
-  a padding element. Sequences must be padded to the right, i.e. all elements of a sequence must be
-  contiguous and padded elements must be to the right of the sequence. For example::
-
-    >>> # 3 sequences with max length 5
-    >>> segmentation_mask = jnp.array([
-    ...   [1, 1, 1, 0, 0], # length 3
-    ...   [1, 1, 0, 0, 0], # length 2
-    ...   [1, 1, 1, 1, 1], # length 5
-    ... ])
-
-  We use this integer mask format because its compatible with sequence packing which might get
-  implemented in the future. The output elements corresponding to padding elements are NOT
-  zeroed out. If ``return_carry`` is set to ``True`` the carry will be the state of the last
-  valid element of each sequence.
+  To support variable length sequences, you can pass a ``seq_lengths`` which is an integer
+  array of shape ``(*batch)`` where each element is the length of the sequence in the batch.
+  For example::
+
+    >>> seq_lengths = jnp.array([3, 2, 5])
+
+  The output elements corresponding to padding elements are NOT zeroed out. If ``return_carry``
+  is set to ``True`` the carry will be the state of the last valid element of each sequence.
 
   RNN also accepts some of the arguments of :func:`flax.linen.scan`, by default they are set to
   work with cells like :class:`LSTMCell` and :class:`GRUCell` but they can be overriden as needed.
   Overriding default values to scan looks like this::
 
     >>> lstm = nn.RNN(
-    ...   nn.LSTMCell(), cell_size=64,
+    ...   nn.LSTMCell(64),
     ...   unroll=1, variable_axes={}, variable_broadcast='params',
     ...   variable_carry=False, split_rngs={'params': False})
 
   Attributes:
     cell: an instance of :class:`RNNCellBase`.
-    cell_size: the size of the cell as requested by :meth:`RNNCellBase.initialize_carry`,
-      it can be an integer or a tuple of integers.
     time_major: if ``time_major=False`` (default) it will expect inputs with shape
       ``(*batch, time, *features)``, else it will expect inputs with shape ``(time, *batch, *features)``.
     return_carry: if ``return_carry=False`` (default) only the output sequence is returned,
       else it will return a tuple of the final carry and the output sequence.
     reverse: if ``reverse=False`` (default) the sequence is processed from left to right and
       returned in the original order, else it will be processed from right to left, and
-      returned in reverse order. If ``segmentation_mask`` is passed, padding will always remain
+      returned in reverse order. If ``seq_lengths`` is passed, padding will always remain
       at the end of the sequence.
     keep_order: if ``keep_order=True``, when ``reverse=True``
       the output will be reversed back to the original order after processing, this is
       useful to align sequences in bidirectional RNNs. If ``keep_order=False`` (default),
       the output will remain in the order specified by ``reverse``.
     unroll: how many scan iterations to unroll within a single iteration of a loop,
       defaults to 1. This argument will be passed to `nn.scan`.
@@ -620,32 +678,39 @@
       and will be preserved when the scan finishes. This argument is forwarded to
       `nn.scan`.
     split_rngs: a mapping from PRNGSequenceFilter to bool specifying whether a collection's
       PRNG key should be split such that its values are different at each step, or replicated
       such that its values remain the same at each step. This argument is forwarded to `nn.scan`.
   """
   cell: RNNCellBase
-  cell_size: Union[int, Tuple[int, ...]]
+  cell_size: Any = NEVER
   time_major: bool = False
   return_carry: bool = False
   reverse: bool = False
   keep_order: bool = False
   unroll: int = 1
   variable_axes: Mapping[lift.CollectionFilter,lift.InOutScanAxis] = FrozenDict()
   variable_broadcast: lift.CollectionFilter = 'params'
   variable_carry: lift.CollectionFilter = False
   split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict({'params': False})
 
+  def __post_init__(self) -> None:
+    if self.cell_size is not NEVER:
+      raise TypeError(
+        f'The `cell_size` argument is no longer available`. ' + LEGACY_UPDATE_MESSAGE
+      )
+    return super().__post_init__()
+
   def __call__(
     self,
     inputs: jax.Array,
     *,
     initial_carry: Optional[Carry] = None,
     init_key: Optional[random.KeyArray] = None,
-    segmentation_mask: Optional[Array] = None,
+    seq_lengths: Optional[Array] = None,
     return_carry: Optional[bool] = None,
     time_major: Optional[bool] = None,
     reverse: Optional[bool] = None,
     keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     """
     Applies the RNN to the inputs.
@@ -656,23 +721,25 @@
     Arguments:
       inputs: the input sequence.
       initial_carry: the initial carry, if not provided it will be initialized
         using the cell's :meth:`RNNCellBase.initialize_carry` method.
       init_key: a PRNG key used to initialize the carry, if not provided
         ``jax.random.PRNGKey(0)`` will be used. Most cells will ignore this
         argument.
-      segmentation_mask: an integer array of shape ``(*batch, time)`` indicating
-        which elements are part of the sequence and which are padding elements.
+      seq_lengths: an optional integer array of shape ``(*batch)`` indicating
+        the length of each sequence, elements whose index in the time dimension
+        is greater than the corresponding length will be considered padding and
+        will be ignored.
       return_carry: if ``return_carry=False`` (default) only the output sequence is returned,
         else it will return a tuple of the final carry and the output sequence.
       time_major: if ``time_major=False`` (default) it will expect inputs with shape
         ``(*batch, time, *features)``, else it will expect inputs with shape ``(time, *batch, *features)``.
       reverse: overrides the ``reverse`` attribute, if ``reverse=False`` (default) the sequence is
         processed from left to right and returned in the original order, else it will be processed
-        from right to left, and returned in reverse order. If ``segmentation_mask`` is passed,
+        from right to left, and returned in reverse order. If ``seq_lengths`` is passed,
         padding will always remain at the end of the sequence.
       keep_order: overrides the ``keep_order`` attribute, if ``keep_order=True``, when ``reverse=True``
         the output will be reversed back to the original order after processing, this is
         useful to align sequences in bidirectional RNNs. If ``keep_order=False`` (default),
         the output will remain in the order specified by ``reverse``.
     Returns:
       if ``return_carry=False`` (default) only the output sequence is returned,
@@ -686,103 +753,105 @@
     if reverse is None:
       reverse = self.reverse
     if keep_order is None:
       keep_order = self.keep_order
 
     # Infer the number of batch dimensions from the input shape.
     # Cells like ConvLSTM have additional spatial dimensions.
-    num_features_dims = 1 if isinstance(self.cell_size, int) else len(self.cell_size)
-    time_axis = 0 if time_major else inputs.ndim - num_features_dims - 1
+    time_axis = 0 if time_major else inputs.ndim - (self.cell.num_feature_axes + 1)
+
+    # make time_axis positive
+    if time_axis < 0:
+      time_axis += inputs.ndim
+
     if time_major:
-      batch_dims = inputs.shape[1:-num_features_dims]
+      # we add +1 because we moved the time axis to the front
+      batch_dims = inputs.shape[1:-self.cell.num_feature_axes]
     else:
       batch_dims = inputs.shape[:time_axis]
 
     # maybe reverse the sequence
     if reverse:
       inputs = jax.tree_map(
         lambda x: flip_sequences(
-          x, segmentation_mask, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
+          x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
         inputs)
 
     carry: Carry
     if initial_carry is None:
       if init_key is None:
         init_key = random.PRNGKey(0)
-      carry = self.cell.initialize_carry(
-        init_key, batch_dims=batch_dims, size=self.cell_size)
+
+      input_shape = inputs.shape[:time_axis] + inputs.shape[time_axis + 1:]
+      carry = self.cell.initialize_carry(init_key, input_shape)
     else:
       carry = initial_carry
 
     def scan_fn(
       cell: RNNCellBase, carry: Carry, x: Array
     ) -> Union[Tuple[Carry, Array], Tuple[Carry, Tuple[Carry, Array]]]:
       carry, y = cell(carry, x)
       # When we have a segmentation mask we return the carry as an output
       # so that we can select the last carry for each sequence later.
       # This uses more memory but is faster than using jnp.where at each
       # iteration. As a small optimization do this when we really need it.
-      if segmentation_mask is not None and return_carry:
+      if seq_lengths is not None and return_carry:
         return carry, (carry, y)
       else:
         return carry, y
 
     scan = transforms.scan(
       scan_fn,
       in_axes=time_axis,
-      out_axes=time_axis if segmentation_mask is None else (0, time_axis),
+      out_axes=time_axis if seq_lengths is None else (0, time_axis),
       unroll=self.unroll,
       variable_axes=self.variable_axes,
       variable_broadcast=self.variable_broadcast,
       variable_carry=self.variable_carry,
       split_rngs=self.split_rngs,
     )
 
     scan_output = scan(self.cell, carry, inputs)
 
     # Next we select the final carry. If a segmentation mask was provided and
     # return_carry is True we slice the carry history and select the last valid
     # carry for each sequence. Otherwise we just use the last carry.
-    if segmentation_mask is not None and return_carry:
+    if seq_lengths is not None and return_carry:
       _, (carries, outputs) = scan_output
-      # segmentation_mask[None] expands the shape of the mask to match the
+      # seq_lengths[None] expands the shape of the mask to match the
       # number of dimensions of the carry.
-      carry = _select_last(carries, segmentation_mask[None], axis=0)
+      carry = _select_last_carry(carries, seq_lengths)
     else:
       carry, outputs = scan_output
 
     if reverse and keep_order:
       outputs = jax.tree_map(
         lambda x: flip_sequences(
-          x, segmentation_mask, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
+          x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
         outputs)
 
     if return_carry:
       return carry, outputs
     else:
       return outputs
 
-def _select_last(sequence: A, segmentation_mask: jnp.ndarray, axis: int) -> A:
-  last_idx = segmentation_mask.sum(axis=-1) - 1
+def _select_last_carry(sequence: A, seq_lengths: jnp.ndarray) -> A:
+  last_idx = seq_lengths - 1
 
   def _slice_array(x: jnp.ndarray):
-    _last_idx = _expand_dims_like(last_idx, target=x)
-    x = jnp.take_along_axis(x, _last_idx, axis=axis)
-    return x.squeeze(axis=axis)
+    return x[last_idx, jnp.arange(x.shape[1])]
 
   return jax.tree_map(_slice_array, sequence)
 
 def _expand_dims_like(x, target):
   """Expands the shape of `x` to match `target`'s shape by adding singleton dimensions."""
   return x.reshape(list(x.shape) + [1] * (target.ndim - x.ndim))
 
-# TODO: Make flip_sequences a method of RNN and generalize it to work with
-# multiple batch dimensions.
 def flip_sequences(
-  inputs: Array, segmentation_mask: Optional[Array], num_batch_dims: int, time_major: bool
+  inputs: Array, seq_lengths: Optional[Array], num_batch_dims: int, time_major: bool
 ) -> Array:
   """Flips a sequence of inputs along the time axis.
 
   This function can be used to prepare inputs for the reverse direction of a
   bidirectional LSTM. It solves the issue that, when naively flipping multiple
   padded sequences stored in a matrix, the first elements would be padding
   values for those sequences that were padded. This function keeps the padding
@@ -806,27 +875,28 @@
   Returns:
     An ndarray with the flipped inputs.
   """
   # Compute the indices to put the inputs in flipped order as per above example.
   time_axis = 0 if time_major else num_batch_dims
   max_steps = inputs.shape[time_axis]
 
-  if segmentation_mask is None:
+  if seq_lengths is None:
     # reverse inputs and return
     inputs = jnp.flip(inputs, axis=time_axis)
     return inputs
 
-  lengths = jnp.sum(segmentation_mask, axis=time_axis, keepdims=True) # [*batch, 1]
+  seq_lengths = jnp.expand_dims(seq_lengths, axis=time_axis)
+
   # create indexes
   idxs = jnp.arange(max_steps - 1, -1, -1) # [max_steps]
   if time_major:
     idxs = jnp.reshape(idxs, [max_steps] + [1] * num_batch_dims)
   else:
     idxs = jnp.reshape(idxs, [1] * num_batch_dims + [max_steps]) # [1, ..., max_steps]
-  idxs = (idxs + lengths) % max_steps # [*batch, max_steps]
+  idxs = (idxs + seq_lengths) % max_steps # [*batch, max_steps]
   idxs = _expand_dims_like(idxs, target=inputs) # [*batch, max_steps, *features]
   # Select the inputs in flipped order.
   outputs = jnp.take_along_axis(inputs, idxs, axis=time_axis)
 
   return outputs
 
 
@@ -837,15 +907,15 @@
 class RNNBase(Protocol):
   def __call__(
     self,
     inputs: jax.Array,
     *,
     initial_carry: Optional[Carry] = None,
     init_key: Optional[random.KeyArray] = None,
-    segmentation_mask: Optional[Array] = None,
+    seq_lengths: Optional[Array] = None,
     return_carry: Optional[bool] = None,
     time_major: Optional[bool] = None,
     reverse: Optional[bool] = None,
     keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     ...
 
@@ -859,15 +929,15 @@
 
   def __call__(
     self,
     inputs: jax.Array,
     *,
     initial_carry: Optional[Carry] = None,
     init_key: Optional[random.KeyArray] = None,
-    segmentation_mask: Optional[Array] = None,
+    seq_lengths: Optional[Array] = None,
     return_carry: Optional[bool] = None,
     time_major: Optional[bool] = None,
     reverse: Optional[bool] = None,
     keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     if time_major is None:
       time_major = self.time_major
@@ -886,22 +956,22 @@
     if self.forward_rnn is self.backward_rnn:
       logging.warning(("forward_rnn and backward_rnn is the same object, so "
       "they will share parameters."))
 
     # Encode in the forward direction.
     carry_forward, outputs_forward = self.forward_rnn(
       inputs, initial_carry=initial_carry_forward, init_key=key_forward,
-      segmentation_mask=segmentation_mask, return_carry=True,
+      seq_lengths=seq_lengths, return_carry=True,
       time_major=time_major, reverse=False)
 
     carry_backward, outputs_backward = self.backward_rnn(
       inputs, initial_carry=initial_carry_backward, init_key=key_backward,
-      segmentation_mask=segmentation_mask, return_carry=True,
+      seq_lengths=seq_lengths, return_carry=True,
       time_major=time_major, reverse=True, keep_order=True)
 
     carry = (carry_forward, carry_backward)
     outputs = jax.tree_map(self.merge_fn, outputs_forward, outputs_backward)
 
     if return_carry:
       return carry, outputs
     else:
-      return outputs
+      return outputs
```

### Comparing `flax-0.6.9/flax/linen/spmd.py` & `flax-0.7.0/flax/linen/spmd.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/stochastic.py` & `flax-0.7.0/flax/linen/stochastic.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,23 +10,27 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Stochastic modules."""
 
-from typing import Optional, Sequence
+from typing import Optional, Sequence, Union
+
+import jax
 
 from flax.linen.module import compact
 from flax.linen.module import merge_param
 from flax.linen.module import Module
 from jax import lax
 from jax import random
 import jax.numpy as jnp
 
+KeyArray = Union[jax.Array, jax.random.KeyArray]
+
 
 class Dropout(Module):
   """Create a dropout layer.
 
     Note: When using :meth:`Module.apply() <flax.linen.Module.apply>`, make sure
     to include an RNG seed named `'dropout'`. For example::
 
@@ -42,22 +46,26 @@
   """
   rate: float
   broadcast_dims: Sequence[int] = ()
   deterministic: Optional[bool] = None
   rng_collection: str = 'dropout'
 
   @compact
-  def __call__(self, inputs, deterministic: Optional[bool] = None):
+  def __call__(
+    self, inputs, deterministic: Optional[bool] = None, rng: Optional[KeyArray] = None
+  ):
     """Applies a random dropout mask to the input.
 
     Args:
       inputs: the inputs that should be randomly masked.
       deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
         masked, whereas if true, no mask is applied and the inputs are returned
         as is.
+      rng: an optional PRNGKey used as the random key, if not specified, one
+        will be generated using ``make_rng`` with the ``rng_collection`` name.
 
     Returns:
       The masked inputs reweighted to preserve mean.
     """
     deterministic = merge_param(
         'deterministic', self.deterministic, deterministic)
 
@@ -65,14 +73,15 @@
       return inputs
 
     # Prevent gradient NaNs in 1.0 edge-case.
     if self.rate == 1.0:
       return jnp.zeros_like(inputs)
 
     keep_prob = 1. - self.rate
-    rng = self.make_rng(self.rng_collection)
+    if rng is None:
+      rng = self.make_rng(self.rng_collection)
     broadcast_shape = list(inputs.shape)
     for dim in self.broadcast_dims:
       broadcast_shape[dim] = 1
     mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)
     mask = jnp.broadcast_to(mask, inputs.shape)
     return lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs))
```

### Comparing `flax-0.6.9/flax/linen/summary.py` & `flax-0.7.0/flax/linen/summary.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/linen/transforms.py` & `flax-0.7.0/flax/linen/transforms.py`

 * *Files 0% similar despite different names*

```diff
@@ -768,22 +768,22 @@
     >>> import jax.numpy as jnp
     ...
     >>> class LSTM(nn.Module):
     ...   features: int
     ...
     ...   @nn.compact
     ...   def __call__(self, x):
-    ...     batch_size = x.shape[0]
-    ...     ScanLSTMCell = nn.scan(
+    ...     ScanLSTM = nn.scan(
     ...       nn.LSTMCell, variable_broadcast="params",
     ...       split_rngs={"params": False}, in_axes=1, out_axes=1)
     ...
-    ...     carry = nn.LSTMCell.initialize_carry(
-    ...       jax.random.PRNGKey(0), (batch_size,), self.features)
-    ...     carry, x = ScanLSTMCell()(carry, x)
+    ...     lstm = ScanLSTM(self.features)
+    ...     input_shape =  x[:, 0].shape
+    ...     carry = lstm.initialize_carry(jax.random.PRNGKey(0), input_shape)
+    ...     carry, x = lstm(carry, x)
     ...     return x
     ...
     >>> x = jnp.ones((4, 12, 7))
     >>> module = LSTM(features=32)
     >>> y, variables = module.init_with_output(jax.random.PRNGKey(0), x)
 
   Note that when providing a function to ``nn.scan``, the scanning happens over
@@ -791,26 +791,26 @@
   The previous example could also be written using the functional form as::
 
     >>> class LSTM(nn.Module):
     ...   features: int
     ...
     ...   @nn.compact
     ...   def __call__(self, x):
-    ...     batch_size = x.shape[0]
     ...
-    ...     cell = nn.LSTMCell()
+    ...     cell = nn.LSTMCell(self.features)
     ...     def body_fn(cell, carry, x):
     ...       carry, y = cell(carry, x)
     ...       return carry, y
     ...     scan = nn.scan(
     ...       body_fn, variable_broadcast="params",
     ...       split_rngs={"params": False}, in_axes=1, out_axes=1)
     ...
-    ...     carry = nn.LSTMCell.initialize_carry(
-    ...       jax.random.PRNGKey(0), (batch_size,), self.features)
+    ...     input_shape =  x[:, 0].shape
+    ...     carry = cell.initialize_carry(
+    ...       jax.random.PRNGKey(0), input_shape)
     ...     carry, x = scan(cell, carry, x)
     ...     return x
     ...
     >>> module = LSTM(features=32)
     >>> variables = module.init(jax.random.PRNGKey(0), jnp.ones((4, 12, 7)))
 
   You can also use ``scan`` to reduce the compilation time of your JAX program
@@ -858,15 +858,15 @@
       If split is False the PRNGs will be the same across iterations.
     in_axes: Specifies the axis to scan over for the arguments. Should be a
       prefix tree of the arguments. Use `flax.core.broadcast` to feed an entire
       input to each iteration of the scan body.
     out_axes: Specifies the axis to scan over for the return value. Should be a
       prefix tree of the return value.
     length: Specifies the number of loop iterations. This only needs to be
-      specified if it cannot be derivied from the scan arguments.
+      specified if it cannot be derived from the scan arguments.
     reverse: If true, scan from end to start in reverse order.
     unroll: how many scan iterations to unroll within a single
       iteration of a loop (default: 1).
     data_transform: optional function to transform raw functional-core variable
       and rng groups inside lifted scan body_fn, intended for inline SPMD
       annotations.
     metadata_params: arguments dict passed to AxisMetadata instances in the
@@ -946,15 +946,15 @@
     mutable: If True, the mapped variable collections will be mutable.
     rngs: PRNGSequences added to the transformed scope (default: all).
     variables: Additional Variable collections added to the transformed scope.
       Besides those specified by `target` (default: all).
     methods: If `target` is a `Module`, the methods of `Module` to map variables
       for.
   Returns:
-    a wrapped version of ``target`` that will map the specificied collections.
+    a wrapped version of ``target`` that will map the specified collections.
   """
 
   return lift_transform(
       lift.map_variables, target,
       mapped_collections,
       trans_in_fn, trans_out_fn,
       init, mutable,
@@ -1474,8 +1474,8 @@
     target = map_variables(
         target,
         col_name,
         trans_in_fn=remove_fn(axis),
         trans_out_fn=add_fn(axis),
         mutable=True,
     )
-  return target
+  return target
```

### Comparing `flax-0.6.9/flax/metrics/__init__.py` & `flax-0.7.0/flax/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/metrics/tensorboard.py` & `flax-0.7.0/flax/metrics/tensorboard.py`

 * *Files 0% similar despite different names*

```diff
@@ -195,15 +195,15 @@
     """
     if not isinstance(textdata, (str, bytes)):
       raise ValueError('`textdata` should be of the type `str` or `bytes`.')
     with self._as_default(self._event_writer):
       tf.summary.text(name=tag, data=tf.constant(textdata), step=step)
 
   def write(self, tag, tensor, step, metadata=None):
-    """Saves a arbitrary tensor summary.
+    """Saves an arbitrary tensor summary.
 
     Useful when working with custom plugins or constructing a summary directly.
 
     Args:
       tag: str: label for this data
       tensor: ndarray: tensor data to save.
       step: int: training step
```

### Comparing `flax-0.6.9/flax/serialization.py` & `flax-0.7.0/flax/serialization.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Serialization utilities for Jax.
 
-All Flax classes that carry state (e.g. ,Optimizer) can be turned into a
+All Flax classes that carry state (e.g., Optimizer) can be turned into a
 state dict of numpy arrays for easy serialization.
 """
 import enum
 from typing import Any, Dict, List
 import threading
 from contextlib import contextmanager
 
@@ -375,15 +375,15 @@
   Low-level function that only supports python trees with array leaves,
   for custom objects use `to_bytes`.  It splits arrays above MAX_CHUNK_SIZE into
   multiple chunks.
 
   Args:
     pytree: python tree of dict, list, tuple with python primitives
       and array leaves.
-    in_place: boolean specifyng if pytree should be modified in place.
+    in_place: boolean specifying if pytree should be modified in place.
 
   Returns:
     msgpack-encoded bytes of pytree.
   """
   if not in_place:
     pytree = jax.tree_util.tree_map(lambda x: x, pytree)
   pytree = _np_convert_in_place(pytree)
```

### Comparing `flax-0.6.9/flax/struct.py` & `flax-0.7.0/flax/struct.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/testing/__init__.py` & `flax-0.7.0/flax/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/testing/benchmark.py` & `flax-0.7.0/flax/testing/benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/traceback_util.py` & `flax-0.7.0/flax/traceback_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/training/__init__.py` & `flax-0.7.0/flax/training/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/training/checkpoints.py` & `flax-0.7.0/flax/training/checkpoints.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,44 +37,45 @@
 from flax import traverse_util
 from flax.training import orbax_utils
 import jax
 from jax import monitoring
 from jax import process_index
 from jax import tree_util as jtu
 from jax.experimental.multihost_utils import sync_global_devices
+import numpy as np
 import orbax.checkpoint as orbax
 
 _READ_CHECKPOINT_EVENT: str = '/jax/checkpoint/read/durations_sec'
 _WRITE_CHECKPOINT_EVENT: str = '/jax/checkpoint/write/durations_sec'
 _IMPORT_GDAM_SUCCESSFUL = False
 try:
-  from jax.experimental.gda_serialization.serialization import get_tensorstore_spec
-  from jax.experimental.gda_serialization.serialization import GlobalAsyncCheckpointManager
+  from jax.experimental.array_serialization.serialization import get_tensorstore_spec
+  from jax.experimental.array_serialization.serialization import GlobalAsyncCheckpointManager
   _IMPORT_GDAM_SUCCESSFUL = True
 except ImportError:
   logging.warning('GlobalAsyncCheckpointManager is not imported correctly. '
                   'Checkpointing of GlobalDeviceArrays will not be available.'
                   'To use the feature, install tensorstore.')
 
 
 # Single-group reg-exps for int or float numerical substrings.
 # captures sign:
 SIGNED_FLOAT_RE = re.compile(
     r'([-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
 # does not capture sign:
 UNSIGNED_FLOAT_RE = re.compile(
     r'[-+]?((?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
-# Module name folowed by number.
+# Module name followed by number.
 MODULE_NUM_RE = re.compile(r'(.*)_\d+$')
 # Alternative schemes handled by `gfile`, e.g. on Google Cloud Storage (GCS).
 SCHEME_RE = re.compile('^(?P<scheme>[a-z][a-z0-9.+-]+://)?(?P<path>.*)', re.I)
 
 # Multiprocess arrays (GlobalDeviceArray, or JAX array with multiprocess
 # sharding) is across processes and will be stored in directories with this
-# postfix, seperated from the non-distributed data (e.g. the larger pytree)
+# postfix, separated from the non-distributed data (e.g. the larger pytree)
 MP_ARRAY_POSTFIX = '_gda'
 # Occurrences of multiprocess arrays in the target pytree will be
 # replaced by this string placeholder.
 MP_ARRAY_PH = '//GDAPlaceholder:'
 
 # Add a copy-success file to a distributed array directory to indicate the
 # array save is complete.
@@ -87,14 +88,21 @@
 PyTree = Any
 
 # TODO(flax-dev): Remove this once flax is using the latest jax release
 # containing jax.Array attribute.
 MultiprocessArrayType = Any
 
 
+def _is_multiprocess_array(value: Any) -> bool:
+  """Use GlobalAsyncCheckpointManager to save the array if it's only partially available on this host."""
+  if isinstance(value, jax.Array):
+    return not value.is_fully_addressable
+  return False
+
+
 def _checkpoint_path(ckpt_dir: str,
                      step: Union[int, float, str],
                      prefix: str = 'checkpoint_') -> str:
   return os.path.join(ckpt_dir, f'{prefix}{step}')
 
 
 def _checkpoint_path_step(path: str) -> Optional[float]:
@@ -135,37 +143,39 @@
       logging.warning(
           'The previous async save_checkpoint has not finished yet. Waiting '
           'for it to complete before the next save.'
       )
       self.save_future.result()
 
   def save_async(self, task: Callable[[], Any]):
-    """Run a task async. The future will be tracked as self.save_future.
+    """Run a task async.
+
+    The future will be tracked as self.save_future.
 
     Args:
-      task: The callable to be executed asynchrously.
+      task: The callable to be executed asynchronously.
     """
     self.wait_previous_save()
     self.save_future = self.executor.submit(task) # type: ignore
 
 
 def _split_mp_arrays(
     target: Dict[str, Any]
 ) -> Tuple[Dict[str, Any], List[Tuple[MultiprocessArrayType, str]]]:
   """Split out the multiprocess arrays from the target pytree to save."""
   # When target is a single leaf instead of a pytree dict.
   if not isinstance(target, (core.FrozenDict, dict)):
-    if orbax_utils.is_multiprocess_array(target):
+    if _is_multiprocess_array(target):
       return MP_ARRAY_PH, [(target, '')]
     return target, []
   # Traverse the target and handle distributed arrays.
   flattened = traverse_util.flatten_dict(target, keep_empty_nodes=True)
   mpa_targets = []
   for key, value in flattened.items():
-    if orbax_utils.is_multiprocess_array(value):
+    if _is_multiprocess_array(value):
       subpath = '/'.join(key)
       mpa_targets.append((value, subpath))
       flattened[key] = MP_ARRAY_PH + subpath
   target = traverse_util.unflatten_dict(flattened)
   return target, mpa_targets
 
 
@@ -253,38 +263,43 @@
 
     # Restore the arrays.
     ts_specs = [get_tensorstore_spec(path) for _, _, path in target_mpas]
     return gda_manager.deserialize(shardings, ts_specs)
 
   # When target is a single leaf instead of a pytree dict.
   if not isinstance(state_dict, (core.FrozenDict, dict)):
-    if orbax_utils.is_multiprocess_array(target) and isinstance(
+    if _is_multiprocess_array(target) and isinstance(
         state_dict, str) and state_dict.startswith(MP_ARRAY_PH):
       _check_mpa_errors()
       return _safe_deserialize([((), target, ckpt_path + MP_ARRAY_POSTFIX)],
                                gda_manager)[0]
     return state_dict
 
   # Go through the restored checkpoint pytree for all MPAs
   flattened = traverse_util.flatten_dict(state_dict, keep_empty_nodes=True)
+  target_flattened = {}
   if target:
     target_flattened = traverse_util.flatten_dict(
         serialization.to_state_dict(target), keep_empty_nodes=True)
   # A list of (state_dict_key, target_array, array_file_path) for every array
   # to be restored
   target_mpas = []
   for key, value in flattened.items():
     if isinstance(value, str) and value.startswith(MP_ARRAY_PH):
       _check_mpa_errors()
       if not target or (key not in target_flattened) or (
-          not orbax_utils.is_multiprocess_array(target_flattened[key])):
+          not _is_multiprocess_array(target_flattened[key])):
         if allow_partial:
           logging.warning(
-              'Multiprocess array %s could not be restored because a valid array is not found in target at the corresponding location. Proceed to restore other arrays because allow_partial_restoration=True',
-              key)
+              'Multiprocess array %s could not be restored because a valid'
+              ' array is not found in target at the corresponding location.'
+              ' Proceed to restore other arrays because'
+              ' allow_partial_restoration=True',
+              key,
+          )
         else:
           raise errors.MPARestoreTargetRequiredError(ckpt_path, step, key)
       else:
         mpa_path = os.path.join(ckpt_path + MP_ARRAY_POSTFIX,
                                 value[len(MP_ARRAY_PH):])
         target_mpas.append((key, target_flattened[key], mpa_path))
 
@@ -563,15 +578,17 @@
 
     # Make sure any previous work is done before making file changes.
     if orbax_checkpointer and isinstance(orbax_checkpointer,
                                          orbax.AsyncCheckpointer):
       orbax_checkpointer.wait_until_finished()
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())
+      orbax_checkpointer = orbax.Checkpointer(
+          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
         orbax_checkpointer._handler, orbax.ArrayCheckpointHandler  # pylint: disable=protected-access
     ):
       raise ValueError(
           'Orbax backend only accept pytree as save target. To save singular'
           ' objects like numbers or Numpy arrays, checkout'
@@ -691,15 +708,17 @@
     # Make sure any previous work is done before making file changes.
     if orbax_checkpointer and isinstance(orbax_checkpointer,
                                          orbax.AsyncCheckpointer):
       orbax_checkpointer.wait_until_finished()
 
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())
+      orbax_checkpointer = orbax.Checkpointer(
+          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
         orbax_checkpointer._handler, orbax.ArrayCheckpointHandler  # pylint: disable=protected-access
     ):
       raise ValueError(
           'Orbax backend only accept pytree as save target. To save singular'
           ' objects like numbers or Numpy arrays, checkout'
@@ -920,29 +939,22 @@
 
   # Restore the checkpoint with Orbax if needed.
   is_orbax = io.exists(os.path.join(ckpt_path, ORBAX_CKPT_FILENAME))
   ckpt_type = 'orbax' if is_orbax else 'legacy Flax'
   logging.info(f'Restoring {ckpt_type} checkpoint from {ckpt_path}')
   if is_orbax:
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())
-
-    def make_restore_args(x):
-      if orbax_utils.is_multiprocess_array(x):
-        return orbax.ArrayRestoreArgs(
-            restore_type=jax.Array,
-            sharding=x.sharding,
-        )
-      return orbax.RestoreArgs()
-
+      orbax_checkpointer = orbax.Checkpointer(
+          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      )
 
     restore_kwargs = {}
     if target is not None:
-      restore_kwargs['restore_args'] = jax.tree_util.tree_map(
-          make_restore_args, target
+      restore_kwargs['restore_args'] = orbax_utils.restore_args_from_target(
+          target
       )
     if orbax_transforms is not None:
       restore_kwargs['transforms'] = orbax_transforms
     restored = orbax_checkpointer.restore(
         ckpt_path, item=target, **restore_kwargs)
     restored = serialization.to_state_dict(restored)
     if target is not None:
```

### Comparing `flax-0.6.9/flax/training/common_utils.py` & `flax-0.7.0/flax/training/common_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -37,15 +37,15 @@
   return jax.tree_util.tree_map(
       lambda x: x.reshape((local_device_count, -1) + x.shape[1:]), xs)
 
 
 def shard_prng_key(prng_key):
   """Helper to shard (aka split) a PRNGKey for use with pmap'd functions.
 
-  PRNG keys can used at train time to drive stochastic modules
+  PRNG keys can be used at train time to drive stochastic modules
   e.g. Dropout. We would like a different PRNG key for each local
   device so that we end up with different random numbers on each one,
   hence we split our PRNG key.
 
   Args:
     prng_key: JAX PRNGKey
   Returns:
```

### Comparing `flax-0.6.9/flax/training/dynamic_scale.py` & `flax-0.7.0/flax/training/dynamic_scale.py`

 * *Files 7% similar despite different names*

```diff
@@ -62,32 +62,37 @@
     compute_grad = jax.jit(lambda ds, p: ds.value_and_grad(loss_fn)(p))
     for _ in range(100):
       dyn_scale, is_fin, loss, grad = compute_grad(dyn_scale, p)
       p += jnp.where(is_fin, 0.01 * grad, 0.)
       print(loss)
 
   Jax currently cannot execute conditionals efficiently on GPUs therefore we
-  selectifly ignore the gradient update using `jax.numpy.where` in case of
+  selectively ignore the gradient update using `jax.numpy.where` in case of
   non-finite gradients.
 
   Attributes:
     growth_factor: how much to grow the scalar after a period of finite
       gradients (default: 2.).
     backoff_factor: how much to shrink the scalar after a non-finite gradient
       (default: 0.5).
     growth_interval: after how many steps of finite gradients the scale should
       be increased (default: 2000).
     fin_steps: indicates how many gradient steps in a row have been finite.
     scale: the current scale by which the loss is multiplied.
+    minimum_scale: the minimum value that the scale can take (default: the
+      smallest positive number representable in floating point).
   """
   growth_factor: float = struct.field(pytree_node=False, default=2.0)
   backoff_factor: float = struct.field(pytree_node=False, default=0.5)
   growth_interval: int = struct.field(pytree_node=False, default=2000)
   fin_steps: Array = 0
   scale: Array = 65536.0
+  minimum_scale: Optional[float] = struct.field(
+      pytree_node=False, default=jnp.finfo(jnp.float32).tiny
+  )
 
   def value_and_grad(self, fun: Callable[..., Any],
                      argnums: Union[int, Sequence[int]] = 0,
                      has_aux: bool = False,
                      axis_name: Optional[str] = None,
                      ) -> Callable[..., DynamicScaleResult]:
     """Wrapper around `jax.value_and_grad`.
@@ -133,13 +138,15 @@
 
       grow = self.fin_steps == self.growth_interval
       fin_scale = jnp.where(
           grow & finite,
           jnp.minimum(self.scale * self.growth_factor, jnp.finfo(jnp.float32).max),
           self.scale)
       inf_scale = self.scale * self.backoff_factor
+      if self.minimum_scale is not None:
+        inf_scale = jnp.maximum(inf_scale, self.minimum_scale)
       new_scale = jnp.where(finite, fin_scale, inf_scale)
       new_fin_steps = jnp.where(grow | (~finite), 0, self.fin_steps + 1)
 
       new_self = self.replace(fin_steps=new_fin_steps, scale=new_scale)
       return DynamicScaleResult(new_self, finite, aux, grad)
     return grad_fn_wrapper
```

### Comparing `flax-0.6.9/flax/training/early_stopping.py` & `flax-0.7.0/flax/training/early_stopping.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/training/lr_schedule.py` & `flax-0.7.0/flax/training/lr_schedule.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/training/orbax_utils.py` & `flax-0.7.0/flax/training/orbax_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,30 +15,31 @@
 """Utils for Orbax Checkpointing, available even after Flax Checkpointing is deprecated."""
 
 from typing import Any, Optional
 import warnings
 
 import jax
 from jax.sharding import Mesh
+import numpy as np
 from orbax import checkpoint as orbax
 
 
 PyTree = Any
 
 
-def is_multiprocess_array(value: Any) -> bool:
-  """Use GlobalAsyncCheckpointManager to save the array if it's only partially available on this host."""
+def is_multi_device_array(value: Any) -> bool:
+  """Instruct Orbax to save this array with Tensorstore instead of msgpack."""
   if isinstance(value, jax.Array):
-    return not value.is_fully_addressable
+    return not value.is_fully_replicated
   return False
 
 
 def save_args_from_target(target: Any) -> Any:
   return jax.tree_util.tree_map(
-      lambda x: orbax.SaveArgs(aggregate=not is_multiprocess_array(x)), target
+      lambda x: orbax.SaveArgs(aggregate=not is_multi_device_array(x)), target
   )
 
 
 def restore_args_from_target(target: Any, mesh: Optional[Mesh] = None) -> Any:
   """Creates Orbax `restore_args` given a target Pytree.
 
   Args:
@@ -48,23 +49,25 @@
     mesh: DEPRECATED ARG. Please simply use your mesh to create the arrays
       in your `target`, no need to pass it here.
 
   Returns:
     A Pytree of Orbax `RestoreArgs` or `ArrayRestoreArgs`
   """
   def find_sharding(x):
-    if is_multiprocess_array(x):
+    if is_multi_device_array(x):
       return x.sharding
     return None
 
   # Simpler case: no multihost arrays
   if not any(
-      jax.tree_util.tree_flatten(jax.tree_map(is_multiprocess_array, target))[0]
+      jax.tree_util.tree_flatten(jax.tree_map(is_multi_device_array, target))[0]
   ):
-    return jax.tree_util.tree_map(lambda x: orbax.RestoreArgs(), target)
+    return jax.tree_util.tree_map(
+        lambda x: orbax.RestoreArgs(restore_type=np.ndarray), target
+    )
 
   # Multihost arrays: find sharding from the given target
   sharding_tree = jax.tree_util.tree_map(find_sharding, target)
   if mesh is not None:
     warnings.warn(
         (
             'restore_args_from_target(): `mesh` arg is deprecated. Simply'
```

### Comparing `flax-0.6.9/flax/training/prefetch_iterator.py` & `flax-0.7.0/flax/training/prefetch_iterator.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/training/train_state.py` & `flax-0.7.0/flax/training/train_state.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/flax/traverse_util.py` & `flax-0.7.0/flax/traverse_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -194,15 +194,15 @@
   """Base class for all traversals."""
 
   def __new__(cls, *args, **kwargs):
     # Must override __new__ instead of __init__ since this is an ABC
     warnings.warn(
         '`flax.traverse_util.Traversal` will be deprecated. If you are using '
         'it for `flax.optim`, use `optax` instead. Refer to the update guide '
-        'https://flax.readthedocs.io/en/latest/advanced_topics/optax_update_guide.html '
+        'https://flax.readthedocs.io/en/latest/guides/optax_update_guide.html '
         'for detailed instructions.', DeprecationWarning)
     return super().__new__(cls)
 
   @abc.abstractmethod
   def update(self, fn, inputs):
     """Update the focused items.
```

### Comparing `flax-0.6.9/flax/version.py` & `flax-0.7.0/flax/version.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,9 +9,9 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Current Flax version at head on Github."""
-__version__ = "0.6.9"
+__version__ = "0.7.0"
```

### Comparing `flax-0.6.9/flax.egg-info/PKG-INFO` & `flax-0.7.0/flax.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.6.9
+Version: 0.7.0
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -18,15 +18,15 @@
 
 <div align="center">
 <img src="https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png" alt="logo"></img>
 </div>
 
 # Flax: A neural network library and ecosystem for JAX designed for flexibility
 
-![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/github/google/flax)](https://codecov.io/github/google/flax)
+![Build](https://github.com/google/flax/workflows/Build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/gh/google/flax)](https://codecov.io/gh/google/flax)
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
@@ -211,15 +211,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.6.9},
+  version = {0.7.0},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.6.9/flax.egg-info/SOURCES.txt` & `flax-0.7.0/flax.egg-info/SOURCES.txt`

 * *Files 5% similar despite different names*

```diff
@@ -32,46 +32,46 @@
 docs/examples_core_examples.rst
 docs/examples_google_research_examples.rst
 docs/examples_repositories_that_use_flax.rst
 docs/flax.png
 docs/getting_started.ipynb
 docs/getting_started.md
 docs/glossary.rst
-docs/howtos.rst
 docs/index.rst
-docs/installation.md
 docs/mission.md
 docs/overview.md
 docs/philosophy.md
 docs/requirements.txt
 docs/_ext/codediff.py
 docs/_ext/codediff_test.py
+docs/_ext/flax_module.py
 docs/_static/css/flax_theme.css
 docs/_templates/autosummary/flax_module.rst
-docs/advanced_topics/arguments.md
-docs/advanced_topics/contributing.md
-docs/advanced_topics/convert_pytorch_to_flax.rst
-docs/advanced_topics/index.rst
-docs/advanced_topics/lift.md
-docs/advanced_topics/linen_design_principles.rst
-docs/advanced_topics/linen_upgrade_guide.rst
-docs/advanced_topics/module_lifecycle.rst
-docs/advanced_topics/optax_update_guide.rst
-docs/advanced_topics/philosophy.md
 docs/api_reference/flax.config.rst
 docs/api_reference/flax.core.frozen_dict.rst
 docs/api_reference/flax.errors.rst
 docs/api_reference/flax.jax_utils.rst
-docs/api_reference/flax.linen.rst
 docs/api_reference/flax.serialization.rst
 docs/api_reference/flax.struct.rst
 docs/api_reference/flax.traceback_util.rst
 docs/api_reference/flax.training.rst
 docs/api_reference/flax.traverse_util.rst
 docs/api_reference/index.rst
+docs/api_reference/flax.linen/activation_functions.rst
+docs/api_reference/flax.linen/decorators.rst
+docs/api_reference/flax.linen/index.rst
+docs/api_reference/flax.linen/init_apply.rst
+docs/api_reference/flax.linen/initializers.rst
+docs/api_reference/flax.linen/inspection.rst
+docs/api_reference/flax.linen/layers.rst
+docs/api_reference/flax.linen/module.rst
+docs/api_reference/flax.linen/profiling.rst
+docs/api_reference/flax.linen/spmd.rst
+docs/api_reference/flax.linen/transformations.rst
+docs/api_reference/flax.linen/variable.rst
 docs/developer_notes/index.rst
 docs/developer_notes/lift.md
 docs/developer_notes/module_lifecycle.rst
 docs/flip/0000-template.md
 docs/flip/1009-optimizer-api.md
 docs/flip/1777-default-dtype.md
 docs/flip/2434-general-metadata.md
@@ -84,14 +84,15 @@
 docs/guides/ensembling.rst
 docs/guides/extracting_intermediates.rst
 docs/guides/flax_basics.ipynb
 docs/guides/flax_basics.md
 docs/guides/flax_on_pjit.ipynb
 docs/guides/flax_on_pjit.md
 docs/guides/full_eval.rst
+docs/guides/haiku_migration_guide.rst
 docs/guides/index.rst
 docs/guides/index_converting_and_upgrading.rst
 docs/guides/index_data_preprocessing.rst
 docs/guides/index_flax_fundamentals.rst
 docs/guides/index_model_inspection.rst
 docs/guides/index_parallel_training.rst
 docs/guides/index_training_techniques.rst
@@ -99,14 +100,16 @@
 docs/guides/jax_for_the_impatient.md
 docs/guides/linen_upgrade_guide.rst
 docs/guides/lr_schedule.rst
 docs/guides/model_surgery.ipynb
 docs/guides/model_surgery.md
 docs/guides/optax_update_guide.rst
 docs/guides/orbax_upgrade_guide.rst
+docs/guides/regular_dict_upgrade_guide.rst
+docs/guides/rnncell_upgrade_guide.rst
 docs/guides/setup_or_nncompact.rst
 docs/guides/state_params.rst
 docs/guides/transfer_learning.ipynb
 docs/guides/transfer_learning.md
 docs/guides/use_checkpointing.ipynb
 docs/guides/use_checkpointing.md
 docs/notebooks/flax_sharp_bits.ipynb
@@ -221,14 +224,17 @@
 examples/sst2/sst2.ipynb
 examples/sst2/train.py
 examples/sst2/train_test.py
 examples/sst2/vocab.txt
 examples/sst2/vocabulary.py
 examples/sst2/configs/default.py
 examples/vae/README.md
+examples/vae/input_pipeline.py
+examples/vae/main.py
+examples/vae/models.py
 examples/vae/reconstruction.png
 examples/vae/requirements.txt
 examples/vae/sample.png
 examples/vae/train.py
 examples/vae/utils.py
 examples/vae/results/.gitignore
 examples/wmt/README.md
@@ -276,15 +282,14 @@
 flax/core/nn/normalization.py
 flax/core/nn/stochastic.py
 flax/linen/README.md
 flax/linen/__init__.py
 flax/linen/activation.py
 flax/linen/attention.py
 flax/linen/combinators.py
-flax/linen/dotgetter.py
 flax/linen/dtypes.py
 flax/linen/initializers.py
 flax/linen/kw_only_dataclasses.py
 flax/linen/linear.py
 flax/linen/module.py
 flax/linen/normalization.py
 flax/linen/partitioning.py
@@ -337,15 +342,14 @@
 tests/core/design/core_dense_test.py
 tests/core/design/core_flow_test.py
 tests/core/design/core_resnet_test.py
 tests/core/design/core_scan_test.py
 tests/core/design/core_tied_autoencoder_test.py
 tests/core/design/core_vmap_test.py
 tests/core/design/core_weight_std_test.py
-tests/linen/dotgetter_test.py
 tests/linen/initializers_test.py
 tests/linen/kw_only_dataclasses_test.py
 tests/linen/linen_activation_test.py
 tests/linen/linen_attention_test.py
 tests/linen/linen_combinators_test.py
 tests/linen/linen_dtypes_test.py
 tests/linen/linen_linear_test.py
```

### Comparing `flax-0.6.9/images/flax_logo.png` & `flax-0.7.0/images/flax_logo.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/images/flax_logo.svg` & `flax-0.7.0/images/flax_logo.svg`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/images/flax_logo_250px.png` & `flax-0.7.0/images/flax_logo_250px.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/images/flax_logo_500px.png` & `flax-0.7.0/images/flax_logo_500px.png`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/pylintrc` & `flax-0.7.0/pylintrc`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/pyproject.toml` & `flax-0.7.0/pyproject.toml`

 * *Files 12% similar despite different names*

```diff
@@ -32,18 +32,17 @@
 
 [project.optional-dependencies]
 all = [
     "matplotlib",  # only needed for tensorboard export
 ]
 testing = [
     # Last version does not have the ROMs we test on pre-packaged
-    "atari-py==0.2.5",
     "clu",  # All examples.
     "einops",
-    "gym==0.18.3",
+    "gymnasium[atari, accept-rom-license]",
     "jaxlib",
     "jraph>=0.0.6dev0",
     "ml-collections",
     "mypy",
     "opencv-python",
     "pytest",
     "pytest-cov",
@@ -127,15 +126,21 @@
     "ignore:jax.experimental.maps.Mesh is deprecated. Use jax.sharding.Mesh.*:DeprecationWarning",
     # Deprecated legacy checkpoint - just want to keep the tests running for a while
     "ignore:Flax Checkpointing will soon be deprecated in favor of Orbax.*:DeprecationWarning",
     # Some Tensorflow IO error on 3/27/2023
     "ignore:file system plugins are not loaded.*:UserWarning",
     "ignore:unable to load libtensorflow_io_plugins.so.*:UserWarning",
     # Remove this after next Optax release after 3/27/2023
-    "ignore:jax.numpy.DeviceArray is deprecated. Use jax.Array.*:DeprecationWarning"
+    "ignore:jax.numpy.DeviceArray is deprecated. Use jax.Array.*:DeprecationWarning",
+    # DeprecationWarnings for pkg_resources
+    "ignore:.*pkg_resources is deprecated as an API.*:DeprecationWarning",
+    # DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
+    "ignore:.*Deprecated call to `pkg_resources.declare_namespace.*:DeprecationWarning",
+    # DeprecationWarning: module 'sre_constants' is deprecated
+    "ignore:.*module 'sre_constants' is deprecated.*:DeprecationWarning",
 ]
 
 [tool.coverage.report]
 exclude_lines = [
     "@abc.abstractmethod",
     "raise NotImplementedError",
 ]
```

### Comparing `flax-0.6.9/tests/checkpoints_test.py` & `flax-0.7.0/tests/checkpoints_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/colab_tpu_jax_version.ipynb` & `flax-0.7.0/tests/colab_tpu_jax_version.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/core_frozen_dict_test.py` & `flax-0.7.0/tests/core/core_frozen_dict_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/core_lift_test.py` & `flax-0.7.0/tests/core/core_lift_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import operator
 from flax import errors
 from flax.core import Scope, init, apply, lift, nn, FrozenDict, unfreeze, copy
-from flax.configurations import use_regular_dict
+from flax.configurations import temp_flip_flag
 
 import jax
 from jax import random
 from jax import numpy as jnp
 
 import numpy as np
 
@@ -112,18 +112,20 @@
     x = jnp.ones((3,))
     _, params = init(f)(random.PRNGKey(0), x)
     y_t = apply(f)(params, x)
     np.testing.assert_allclose(y_t, jnp.ones_like(x))
 
   def test_while_loop(self):
     def f(scope, x):
+      key_zero = random.PRNGKey(0)
+      key_zero = jnp.broadcast_to(key_zero, (2, *key_zero.shape))
       scope.param('inc', lambda _: 1)
       scope.put_variable('state', 'acc', 0)
-      scope.put_variable('state', 'rng_params', jnp.zeros((2, 2), jnp.uint32))
-      scope.put_variable('state', 'rng_loop', jnp.zeros((2, 2), jnp.uint32))
+      scope.put_variable('state', 'rng_params', key_zero)
+      scope.put_variable('state', 'rng_loop', key_zero)
 
       def cond_fn(scope, c):
         acc = scope.get_variable('state', 'acc')
         return acc < x
       def body_fn(scope, c):
         i = scope.get_variable('state', 'acc')
         p_rng = scope.make_rng('params')
@@ -131,15 +133,15 @@
         scope.put_variable('state', 'rng_params', scope.get_variable('state', 'rng_params').at[i].set(p_rng))
         scope.put_variable('state', 'rng_loop', scope.get_variable('state', 'rng_loop').at[i].set(l_rng))
         inc = scope.get_variable('params', 'inc')
         scope.put_variable('state', 'acc', i + inc)
         return c + 2
       return lift.while_loop(cond_fn, body_fn, scope, 0, carry_variables='state', split_rngs={'params': False, 'loop': True})
     x = 2
-    c, vars = apply(f, mutable=True)({}, x, rngs={'params': random.PRNGKey(0), 'loop': random.PRNGKey(1)})
+    c, vars = apply(f, mutable=True)({}, x, rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)})
     self.assertEqual(vars['state']['acc'], x)
     self.assertEqual(c, 2 * x)
     np.testing.assert_array_equal(vars['state']['rng_params'][0], vars['state']['rng_params'][1])
     np.testing.assert_array_compare(operator.__ne__, vars['state']['rng_loop'][0], vars['state']['rng_loop'][1])
 
   def test_cond(self):
     def f(scope, x, pred):
@@ -158,15 +160,15 @@
     x = jnp.ones((1, 3))
     y1, vars = init(f)(random.PRNGKey(0), x, True)
     self.assertEqual(vars['state'], {'true_count': 1, 'false_count': 0})
     y2, vars = apply(f, mutable="state")(vars, x, False)
     self.assertEqual(vars['state'], {'true_count': 1, 'false_count': 1})
     np.testing.assert_allclose(y1, -y2)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_switch(self):
     def f(scope, x, index):
       scope.variable('state', 'a_count', lambda: 0)
       scope.variable('state', 'b_count', lambda: 0)
       scope.variable('state', 'c_count', lambda: 0)
 
       def a_fn(scope, x):
```

### Comparing `flax-0.6.9/tests/core/core_meta_test.py` & `flax-0.7.0/tests/core/core_meta_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/core_scope_test.py` & `flax-0.7.0/tests/core/core_scope_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import unittest
 from flax import errors
 from flax.core import Scope, scope, freeze, lazy_init, init, apply, nn
 from flax.core.scope import LazyRng
+from flax.configurations import temp_flip_flag
 
 import jax
 from jax import config as jax_config
 from jax import random
 from jax import numpy as jnp
 
 import numpy as np
@@ -231,9 +232,15 @@
       # kernel is initialized with x so params are now dependent on the input
       k = scope.param("kernel", lambda _: x)
       return x * k
     init_fn = lazy_init(f)
     with self.assertRaises(errors.LazyInitError):
       init_fn(random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32))
 
+  @temp_flip_flag('fix_rng_separator', True)
+  def test_fold_in_static_seperator(self):
+    x = LazyRng(random.PRNGKey(0), ("ab", "c"))
+    y = LazyRng(random.PRNGKey(0), ("a", "bc"))
+    self.assertFalse(np.all(x.as_jax_rng() == y.as_jax_rng()))
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.6.9/tests/core/design/core_attention_test.py` & `flax-0.7.0/tests/core/design/core_attention_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_auto_encoder_test.py` & `flax-0.7.0/tests/core/design/core_auto_encoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_big_resnets_test.py` & `flax-0.7.0/tests/core/design/core_big_resnets_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_custom_vjp_test.py` & `flax-0.7.0/tests/core/design/core_custom_vjp_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_dense_test.py` & `flax-0.7.0/tests/core/design/core_dense_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_flow_test.py` & `flax-0.7.0/tests/core/design/core_flow_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_resnet_test.py` & `flax-0.7.0/tests/core/design/core_resnet_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_scan_test.py` & `flax-0.7.0/tests/core/design/core_scan_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_tied_autoencoder_test.py` & `flax-0.7.0/tests/core/design/core_tied_autoencoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_vmap_test.py` & `flax-0.7.0/tests/core/design/core_vmap_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/core/design/core_weight_std_test.py` & `flax-0.7.0/tests/core/design/core_weight_std_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/download_dataset_metadata.sh` & `flax-0.7.0/tests/download_dataset_metadata.sh`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/early_stopping_test.py` & `flax-0.7.0/tests/early_stopping_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/import_test.ipynb` & `flax-0.7.0/tests/import_test.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/io_test.py` & `flax-0.7.0/tests/io_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/jax_utils_test.py` & `flax-0.7.0/tests/jax_utils_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/initializers_test.py` & `flax-0.7.0/tests/linen/initializers_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/kw_only_dataclasses_test.py` & `flax-0.7.0/tests/linen/kw_only_dataclasses_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/linen_activation_test.py` & `flax-0.7.0/tests/linen/linen_activation_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/linen_attention_test.py` & `flax-0.7.0/tests/linen/linen_attention_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 from absl.testing import absltest
 from absl.testing import parameterized
 
 from flax import linen as nn
 from flax import jax_utils
 from flax.core import pop
-from flax.configurations import use_regular_dict
+from flax.configurations import temp_flip_flag
 
 import jax
 from jax import lax
 from jax import random
 from jax.nn import initializers
 import jax.numpy as jnp
 
@@ -89,25 +89,66 @@
         deterministic=False,
     )
     rng1, rng2 = random.split(rng)
     rngs = {'params': rng1, 'dropout': rng2}
     y, _ = sa_module.init_with_output(rngs, x, x)
     self.assertEqual(y.shape, x.shape)
 
+  def test_multihead_self_attention_w_dropout_disabled(self):
+    rng = random.PRNGKey(0)
+    x = jnp.ones((4, 2, 3, 5))
+    sa_module0 = nn.MultiHeadDotProductAttention(
+        num_heads=8,
+        qkv_features=16,
+        kernel_init=initializers.ones,
+        bias_init=initializers.zeros,
+        dropout_rate=0.0,
+        deterministic=True,
+    )
+    rng1, rng2, rng3, rng4 = random.split(rng, 4)
+    rngs1 = {'params': rng1, 'dropout': rng2}
+    rngs2 = {'params': rng3, 'dropout': rng4}
+    y1, vs = sa_module0.init_with_output(rngs1, x, x)
+    y2, _ = sa_module0.init_with_output(rngs2, x, x)
+    np.testing.assert_allclose(y1, y2)
+    y3 = sa_module0.apply(vs, x, x, rngs=rngs1)
+    y4 = sa_module0.apply(vs, x, x, rngs=rngs2)
+    np.testing.assert_allclose(y3, y4)
+    sa_module1 = nn.MultiHeadDotProductAttention(
+        num_heads=8,
+        qkv_features=16,
+        kernel_init=initializers.ones,
+        bias_init=initializers.zeros,
+        dropout_rate=0.0,
+    )
+    y5 = sa_module1.apply(vs, x, x, deterministic=True, rngs=rngs1)
+    y6 = sa_module1.apply(vs, x, x, deterministic=True, rngs=rngs2)
+    np.testing.assert_allclose(y5, y6)
+    sa_module2 = nn.MultiHeadDotProductAttention(
+        num_heads=8,
+        qkv_features=16,
+        kernel_init=initializers.ones,
+        bias_init=initializers.zeros,
+        dropout_rate=0.5,
+    )
+    y7 = sa_module2.apply(vs, x, x, deterministic=True, rngs=rngs1)
+    y8 = sa_module2.apply(vs, x, x, deterministic=True, rngs=rngs2)
+    np.testing.assert_allclose(y7, y8)
+
   def test_causal_mask_1d(self):
     """Tests autoregresive masking for 1d attention."""
     x = jnp.ones((3, 16))  # (bs1, length)
     mask_1d = nn.attention.make_causal_mask(x)
     ts = np.arange(16)
     mask_1d_simple = (ts[:, None] >= ts[None, :])[None, None, :, :]
     mask_1d_simple = jnp.broadcast_to(mask_1d_simple, (3, 1, 16, 16))
     np.testing.assert_allclose(mask_1d, mask_1d_simple,)
 
   @parameterized.parameters([((5,), (1,)), ((6, 5), (2,))])
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_decoding(self, spatial_shape, attn_dims):
     bs = 2
     num_heads = 3
     num_features = 4
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     inputs = random.normal(
```

### Comparing `flax-0.6.9/tests/linen/linen_combinators_test.py` & `flax-0.7.0/tests/linen/linen_combinators_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/linen_dtypes_test.py` & `flax-0.7.0/tests/linen/linen_dtypes_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/linen_linear_test.py` & `flax-0.7.0/tests/linen/linen_linear_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/linen_meta_test.py` & `flax-0.7.0/tests/linen/linen_meta_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -156,19 +156,23 @@
                     'kernel': PartitionSpec(None, 'model', 'data'),
                 },
             },
         },
     })
     x_spec = PartitionSpec('data', 'model')
     f = lambda x: jax.sharding.NamedSharding(mesh, x)
+    if jax.config.jax_enable_custom_prng:
+      key_spec = PartitionSpec()
+    else:
+      key_spec = PartitionSpec(None)
     init_fn = jax.jit(model.init,
-                      in_shardings=jax.tree_map(f, (PartitionSpec(None), x_spec)),
+                      in_shardings=jax.tree_map(f, (key_spec, x_spec)),
                       out_shardings=jax.tree_map(f, spec))
     variables = init_fn(random.PRNGKey(0), x)
     apply_fn = jax.jit(model.apply,
-                      in_shardings=jax.tree_map(f, (spec, x_spec)),
-                      out_shardings=jax.tree_map(f, x_spec))
+                       in_shardings=jax.tree_map(f, (spec, x_spec)),
+                       out_shardings=jax.tree_map(f, x_spec))
     y = apply_fn(variables, x)
     self.assertEqual(y.shape, (8, 128))
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.6.9/tests/linen/linen_module_test.py` & `flax-0.7.0/tests/linen/linen_module_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 from absl.testing import absltest
 from flax import config
 from flax import errors
 from flax import linen as nn
 from flax import struct
 from flax.core import Scope, freeze, FrozenDict, tracers
 from flax.linen import compact
-from flax.configurations import use_regular_dict
+from flax.configurations import temp_flip_flag
 import jax
 from jax import random
 from jax.nn import initializers
 import jax.numpy as jnp
 import numpy as np
 from unittest.mock import patch
 
@@ -456,21 +456,16 @@
 
       def __call__(self, x):
         return x + self.bias
 
     x = jnp.array([1.])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
 
-    if config.flax_relaxed_naming:
-      with self.assertRaises(errors.NameInUseError):
-        unused_y = Dummy(x.shape, parent=scope)(x)
-    else:
-      msg = 'Duplicate use of scope name: "bias"'
-      with self.assertRaisesWithLiteralMatch(ValueError, msg):
-        unused_y = Dummy(x.shape, parent=scope)(x)
+    with self.assertRaises(errors.NameInUseError):
+      unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_submodule_var_collision_with_submodule(self):
     rngkey = jax.random.PRNGKey(0)
 
     class Dummy(nn.Module):
       xshape: Tuple[int, ...]
 
@@ -517,52 +512,14 @@
 
       @compact
       def __call__(self):
         pass
 
     Foo({'a': ()}).apply({})
 
-  @absltest.skipIf(config.flax_relaxed_naming, "relaxed naming")
-  def test_attr_param_name_collision(self):
-    rngkey = jax.random.PRNGKey(0)
-
-    class Dummy(nn.Module):
-      bias: bool
-
-      def setup(self):
-        self.bias = self.param('bias', initializers.ones, (3, 3))
-
-      def __call__(self, x):
-        return x + self.bias
-
-    x = jnp.array([1.])
-    scope = Scope({}, {'params': rngkey}, mutable=['params'])
-    msg = 'Could not create param "bias" in Module Dummy: Name in use'
-    with self.assertRaisesRegex(errors.NameInUseError, msg):
-      unused_y = Dummy(True, parent=scope)(x)
-
-  @absltest.skipIf(config.flax_relaxed_naming, "relaxed naming")
-  def test_attr_submodule_name_collision(self):
-    rngkey = jax.random.PRNGKey(0)
-
-    class Dummy(nn.Module):
-      bias: bool
-
-      def setup(self):
-        self.bias = DummyModule(name='bias')
-
-      def __call__(self, x):
-        return self.bias(x)
-
-    x = jnp.array([1.])
-    scope = Scope({}, {'params': rngkey}, mutable=['params'])
-    msg = 'Could not create submodule "bias" in Module Dummy: Name in use'
-    with self.assertRaisesRegex(errors.NameInUseError, msg):
-      unused_y = Dummy(True, parent=scope)(x)
-
   def test_only_one_compact_method(self):
     msg = 'Only one method per class can be @compact'
     with self.assertRaisesRegex(errors.MultipleMethodsCompactError, msg):
 
       class MultipleCompactMethods(nn.Module):
 
         @compact
@@ -1137,15 +1094,15 @@
 
       def test(self):
         pass
 
     A().test()
     self.assertFalse(setup_called)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_module_pass_as_attr(self):
 
     class A(nn.Module):
 
       def setup(self):
         self.b = B(nn.Dense(2))
 
@@ -1168,15 +1125,15 @@
                     'kernel': (1, 2),
                 }
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_module_pass_in_closure(self):
     a = nn.Dense(2)
 
     class B(nn.Module):
 
       def setup(self):
         self.foo = a
@@ -1193,15 +1150,15 @@
                 'kernel': (1, 2),
             }
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
     self.assertIsNone(a.name)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption(self):
 
     class Encoder(nn.Module):
       n_layers: int
       ch: int
 
       def setup(self):
@@ -1249,15 +1206,15 @@
                     'kernel': (4, 8),
                 },
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_pytree(self):
 
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, c, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
@@ -1293,15 +1250,15 @@
     }
     self.assertTrue(
         jax.tree_util.tree_all(
             jax.tree_util.tree_map(
                 lambda x, y: np.testing.assert_allclose(x, y, atol=1e-7),
                 counters, ref_counters)))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_sharing(self):
     dense = functools.partial(nn.Dense, use_bias=False)
 
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, x):
@@ -1344,15 +1301,15 @@
                     'kernel': (2, 2),
                 },
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_named_submodule_adoption(self):
     dense = functools.partial(nn.Dense, use_bias=False)
 
     class A(nn.Module):
 
       def setup(self):
         self.dense = dense(4)
@@ -1399,15 +1356,15 @@
               'proj': {
                   'kernel': (4, 6),
               },
           },
       }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_pytree_adoption_sharing(self):
 
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
@@ -1775,15 +1732,15 @@
     class B(nn.Module):
 
       @nn.compact
       def __call__(self):
         a = A()
         x0 = a()
         x1 = a()
-        return jnp.alltrue(x0 == x1)
+        return jnp.all(x0 == x1)
 
     k = random.PRNGKey(0)
     rng_equals = B().apply({}, rngs={'dropout': k})
     self.assertFalse(rng_equals)
 
   def test_module_get_put_has_variable(self):
 
@@ -1996,14 +1953,188 @@
         return self.prop
 
     foo = Foo()
     with self.assertRaisesRegex(errors.DescriptorAttributeError,
                                 'Trying to access a property that'):
       foo.apply({})
 
+  def test_nested_external_modules(self):
+    class Baz(nn.Module):
+      a: int
+
+      def setup(self):
+        self.b = self.param('b', lambda k: 2)
+
+      def __call__(self, x):
+        return x + self.a * self.b
+
+    class Bar(nn.Module):
+      baz: Baz
+
+      def __call__(self, x):
+        return self.baz(x)
+
+    class Foo(nn.Module):
+      def setup(self):
+        self.bar = Bar(baz=Baz(a=1))
+
+      def __call__(self, x):
+        return self.bar.baz(x)
+
+    module = Foo()
+    y, variables = module.init_with_output(jax.random.PRNGKey(0), 1)
+    self.assertEqual(y, 3)
+
+  def test_getattribute_triggers_setup(self):
+    class B(nn.Module):
+      def setup(self):
+        self.p1 = self.param('p1', lambda k: jnp.ones((2,)))
+      def fn1(self, x):
+        return self.p1 + x
+    class A(nn.Module):
+      b: nn.Module
+      def __call__(self, x):
+        return self.b.fn1(x)
+    a = A(b=B())
+    k = random.PRNGKey(0)
+    x = jnp.zeros((2,))
+    vs = nn.init(lambda a,x: a(x), a)(k, x)
+    y = nn.apply(lambda a,x: a.b.fn1(x), a)(vs, x)
+    np.testing.assert_array_equal(y, jnp.ones((2,)))
+
+  def test_nested_sequential_in_call(self):
+    class Foo(nn.Module):
+      def setup(self):
+        self.seq = nn.Sequential([nn.Dense(10) for i in range(10)])
+
+      def __call__(self, x):
+        # try calling only the first layer
+        return self.seq.layers[0](x)
+
+    module = Foo()
+    variables = module.init(jax.random.PRNGKey(0), jnp.ones((1, 10)))
+
+  def test_setup_called_bounded_submodules(self):
+    module = nn.Sequential([
+      nn.Sequential([
+        nn.Dense(2),
+        nn.relu,
+        nn.Dense(2),
+      ]),
+      nn.relu,
+      nn.Dense(2),
+    ])
+    x = jnp.ones((1, 3))
+    variables = module.init(jax.random.PRNGKey(0), x)
+    bound_module = module.bind(variables)
+
+    self.assertIsNotNone(bound_module.layers[0].layers[0].scope)
+    self.assertIsNotNone(bound_module.layers[0].layers[2].scope)
+    self.assertIsNotNone(bound_module.layers[2].scope)
+
+  def test_call_bounded_toplevel_mutable(self):
+    class Bar(nn.Module):
+      a: int
+
+      def setup(self):
+        self.b = self.param('b', lambda k: 1)
+
+      def __call__(self, x):
+        return x + self.a * self.b
+
+    class Foo(nn.Module):
+      bars: Sequence[Bar]
+
+      def __call__(self, x):
+        for bar in self.bars:
+          x = bar(x)
+        return x
+
+
+    module = Foo(bars=[])
+    module.bars = [Bar(a=1)]
+
+    variables = module.init(jax.random.PRNGKey(0), jnp.ones(()))
+    bound_module = module.bind(variables)
+
+    bar1 = bound_module.bars[0]
+    self.assertIsNotNone(bar1.scope)
+
+  def test_nested_init(self):
+    class Baz(nn.Module):
+      a: int
+
+      def setup(self):
+        self.b = self.param('b', lambda k: jnp.ones(()))
+
+      def __call__(self, x):
+        return x + self.a * self.b
+
+    class Bar(nn.Module):
+      baz: Baz
+
+      def setup(self):
+        a = 1
+
+      def __call__(self, x):
+        return self.baz(x)
+
+    class Foo(nn.Module):
+
+      def setup(self):
+        self.bar: Bar = Bar(baz=Baz(a=1))
+
+      def __call__(self, x):
+        # y = self.bar(x)
+        y, bar_vars = self.bar.init_with_output(jax.random.PRNGKey(0), x)
+        return y, bar_vars
+
+    # create foo
+    module = Foo()
+
+    # run foo
+    (y, bar_vars), variables = module.init_with_output(
+      jax.random.PRNGKey(0), jnp.ones(()))
+
+    self.assertIn('params', bar_vars)
+
+  def test_nested_shared(self):
+    class Shared(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        return nn.Dense(1)(x)
+
+    class Unshared(nn.Module):
+      shared: nn.Module
+      def __call__(self, x):
+        return self.shared(x)
+
+    class Super(nn.Module):
+      a: nn.Module
+      b: nn.Module
+      def run_a(self, x):
+        return self.a(x)
+      def run_b(self, x):
+        return self.b(x)
+      def __call__(self, x):
+        return self.a(x) + self.b(x)
+
+
+    sh = Shared()
+    a = Unshared(shared=sh)
+    b = Unshared(shared=sh)
+    module = Super(a=a, b=b)
+
+    rng = jax.random.PRNGKey(0)
+    params = module.init(rng, jnp.ones(1))["params"]
+
+    module.apply({"params": params}, jnp.ones(1))  # works as expected
+    module.apply({"params": params}, jnp.ones(1), method="run_a")  # works as expected
+    module.apply({"params": params}, jnp.ones(1), method="run_b")  # ScopeParamNotFoundError: Could not find parameter named "kernel" in scope "/b/shared/Dense_0"
+
   def test_repr(self):
 
     class Base1(nn.Module):
       a: int
 
     class Base2(nn.Module):
       b: str
@@ -2014,14 +2145,37 @@
     module = Foo(a=1, b='ok', c=3.0)
     str_rep = repr(module)
 
     self.assertIn('a = 1', str_rep)
     self.assertIn("b = 'ok'", str_rep)
     self.assertIn('c = 3.0', str_rep)
 
+  def test_repr_should_not_cause_setup(self):
+    class MLP(nn.Module):
+
+      @nn.compact
+      def __call__(self, x):
+        x = nn.Dense(1)(x)
+        return repr(self)
+
+    class Foo(nn.Module):
+      a: float
+      b: MLP
+
+    scope = Scope({})
+    module = Foo(parent=scope, a=1, b=MLP(parent=scope))
+    str_rep = repr(module)
+    self.assertIn('a = 1', str_rep)
+
+    self.assertEqual(module._state.setup_called, nn.module.SetupState.NEW)
+    # repr() on a module should not cause inadvertent setup of submodules
+    # i.e. module.b._state.setup_called should remain nn.module.SetupState.NEW
+    # and not nn.module.SetupState.DONE
+    self.assertEqual(module.b._state.setup_called, nn.module.SetupState.NEW)
+
   def test_kw_only(self):
     def create_kw_layers():
       class BaseLayer(nn.Module, kw_only=True):
         base_multiplier: Optional[int] = -1
 
       class ChildLayer(BaseLayer):
         child_multiplier: int  # Don't want to have to set a default argument!
@@ -2224,48 +2378,48 @@
     class Foo(nn.Module):
       dummy = 0
       @nn.compact
       def __call__(self, x):
         p = self.param('dummy', nn.initializers.zeros, x.shape)
         return x + p
 
-    with set_config('flax_relaxed_naming', True):
-      foo = Foo(name='foo')
-      k = random.PRNGKey(0)
-      x = jnp.zeros((1,))
-      vs = foo.init(k, x)
-
-    with set_config('flax_relaxed_naming', False):
-      foo = Foo(name='foo')
-      k = random.PRNGKey(0)
-      x = jnp.zeros((1,))
-      with self.assertRaises(errors.NameInUseError):
-        vs = foo.init(k, x)
+    foo = Foo(name='foo')
+    k = random.PRNGKey(0)
+    x = jnp.zeros((1,))
+    vs = foo.init(k, x)
 
   def test_relaxed_intercollection_conflict(self):
 
     class Foo(nn.Module):
       @nn.compact
       def __call__(self, x):
         v1 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
         v2 = self.variable('col2', 'v', lambda x: jnp.zeros(x), x.shape)
         return x + v1.value + v2.value
 
-    with set_config('flax_relaxed_naming', True):
-      foo = Foo(name='foo')
-      k = random.PRNGKey(0)
-      x = jnp.zeros((1,))
-      vs = foo.init(k, x)
+    foo = Foo(name='foo')
+    k = random.PRNGKey(0)
+    x = jnp.zeros((1,))
+    vs = foo.init(k, x)
 
-    with set_config('flax_relaxed_naming', False):
-      foo = Foo(name='foo')
-      k = random.PRNGKey(0)
-      x = jnp.zeros((1,))
-      with self.assertRaises(errors.NameInUseError):
-        vs = foo.init(k, x)
+  def test_relaxed_intercollection_conflict_set(self):
+
+    class Foo(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        v1 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
+        v2 = self.variable('col2', 'v', lambda x: jnp.zeros(x), x.shape)
+        v3 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
+        return x + v1.value + v2.value + v3.value
+
+    foo = Foo(name='foo')
+    k = random.PRNGKey(0)
+    x = jnp.zeros((1,))
+    with self.assertRaises(errors.NameInUseError):
+      vs = foo.init(k, x)
 
 
 class FrozenDictTests(absltest.TestCase):
 
   def test_frozendict_flag(self):
 
     with set_config('flax_return_frozendict', True):
```

### Comparing `flax-0.6.9/tests/linen/linen_recurrent_test.py` & `flax-0.7.0/tests/linen/linen_recurrent_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Recurrent tests."""
 
 
-from absl.testing import absltest
+from absl.testing import absltest, parameterized
 import jax
 import jax.numpy as jnp
 import numpy as np
 from flax import errors
 from flax import linen as nn
 import pytest
 import einops
@@ -32,15 +32,15 @@
 class RNNTest(absltest.TestCase):
   def test_rnn_basic_forward(self):
     batch_size = 10
     seq_len = 40
     channels_in = 5
     channels_out = 15
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out))
@@ -53,15 +53,15 @@
 
   def test_rnn_multiple_batch_dims(self):
     batch_dims = (10, 11)
     seq_len = 40
     channels_in = 5
     channels_out = 15
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((*batch_dims, seq_len, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (*batch_dims, seq_len, channels_out))
@@ -74,15 +74,15 @@
 
   def test_rnn_unroll(self):
     batch_size = 10
     seq_len = 40
     channels_in = 5
     channels_out = 15
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, unroll=10, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), unroll=10, return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out))
@@ -95,15 +95,15 @@
 
   def test_rnn_time_major(self):
     seq_len = 40
     batch_size = 10
     channels_in = 5
     channels_out = 15
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, time_major=True, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), time_major=True, return_carry=True)
 
     xs = jnp.ones((seq_len, batch_size, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
 
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs)
 
@@ -126,15 +126,14 @@
     kernel_size = (3, 3)
     image_size = (32, 32)
     channels_in = 5
     channels_out = 15
 
     rnn = nn.RNN(
       nn.ConvLSTMCell(channels_out, kernel_size),
-      cell_size=(*image_size, channels_out),
     )
 
     xs = jnp.ones((batch_size, seq_len, *image_size, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
 
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs, return_carry=True)
@@ -155,21 +154,21 @@
 
   def test_numerical_equivalence(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
-    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), channels_out)
+    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     cell_params = variables['params']['cell']
 
     for i in range(seq_len):
       cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-5)
 
     np.testing.assert_allclose(cell_carry, carry, rtol=1e-5)
@@ -178,25 +177,22 @@
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     key = jax.random.PRNGKey(0)
     seq_lengths = jax.random.randint(key, (batch_size,), minval=1, maxval=seq_len + 1)
-    segmentation_mask = einops.repeat(
-      jnp.arange(seq_len), 'time -> batch time', batch=batch_size)
-    segmentation_mask = (segmentation_mask < seq_lengths[:, None]).astype(jnp.int32)
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
-    (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs, segmentation_mask=segmentation_mask)
+    (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs, seq_lengths=seq_lengths)
 
-    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), channels_out)
+    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     cell_params = variables['params']['cell']
     carries = []
 
     for i in range(seq_len):
       cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-5)
       carries.append(cell_carry)
@@ -208,52 +204,52 @@
 
   def test_numerical_equivalence_single_batch(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), (1,), channels_out)
+      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
 
       for i in range(seq_len):
         cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, i, :][None])
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-6)
 
       carry_i = jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry)
       np.testing.assert_allclose(cell_carry, carry_i, rtol=1e-6)
 
   def test_numerical_equivalence_single_batch_nn_scan(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    cell = nn.LSTMCell()
-    rnn = nn.scan(nn.LSTMCell, in_axes=1, out_axes=1,
+    cell: nn.LSTMCell = nn.LSTMCell(channels_out)
+    rnn: nn.LSTMCell = nn.scan(nn.LSTMCell, in_axes=1, out_axes=1,
                    variable_broadcast='params',
-                   split_rngs={'params': False})()
+                   split_rngs={'params': False})(channels_out)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
-    carry = rnn.initialize_carry(jax.random.PRNGKey(0), (batch_size,), channels_out)
+    carry = rnn.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), carry, xs)
 
     cell_params = variables['params']
 
     for batch_idx in range(batch_size):
-      cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), (1,), channels_out)
+      cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
 
       for i in range(seq_len):
         cell_carry, y = cell.apply({'params': cell_params}, cell_carry, xs[batch_idx:batch_idx+1, i, :])
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-5)
 
       carry_i = jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry)
       np.testing.assert_allclose(cell_carry, carry_i, rtol=1e-5)
@@ -261,163 +257,151 @@
   def test_numerical_equivalence_single_batch_jax_scan(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     xs = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, seq_len, channels_in))
-    cell = nn.LSTMCell()
-    carry = cell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), channels_out)
+    cell: nn.LSTMCell = nn.LSTMCell(channels_out)
+    carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     variables = cell.init(jax.random.PRNGKey(0), carry, xs[:, 0])
     cell_params = variables['params']
 
     def scan_fn(carry, x):
       return cell.apply({'params': cell_params}, carry, x)
 
     ys: jnp.ndarray
     carry, ys = jax.lax.scan(scan_fn, carry, xs.swapaxes(0, 1))
     ys = ys.swapaxes(0, 1)
 
-    cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), channels_out)
+    cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
 
     for i in range(seq_len):
       cell_carry, y = cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-4)
 
     np.testing.assert_allclose(cell_carry, carry, rtol=1e-4)
 
   def test_reverse(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True, reverse=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True, reverse=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), (1,), channels_out)
+      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
 
       for i in range(seq_len):
         cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, seq_len - i - 1, :][None])
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-5)
 
       np.testing.assert_allclose(
         cell_carry, jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry), rtol=1e-5)
 
   def test_reverse_but_keep_order(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    rnn = nn.RNN(nn.LSTMCell(), channels_out, return_carry=True, reverse=True, keep_order=True)
+    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True, reverse=True, keep_order=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), (1,), channels_out)
+      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
 
       for i in range(seq_len):
         cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, seq_len - i - 1, :][None])
         np.testing.assert_allclose(y[0], ys[batch_idx, seq_len - i - 1, :], rtol=1e-5)
 
       np.testing.assert_allclose(
         cell_carry, jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry), rtol=1e-5)
 
   def test_flip_sequence(self):
     x = jnp.arange(2 * 5).reshape((2, 5))
-    segmentation_mask = jnp.array([[1, 1, 1, 1, 0], [1, 1, 0, 0, 0]])
+    seq_lengths = jnp.array([4, 2])
 
-    flipped = flip_sequences(x, segmentation_mask, num_batch_dims=1, time_major=False)
+    flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=False)
 
     self.assertEqual(flipped.shape, (2, 5))
     np.testing.assert_allclose(flipped[0, :4], [3, 2, 1, 0])
     np.testing.assert_allclose(flipped[1, :2], [6, 5])
 
   def test_flip_sequence_more_feature_dims(self):
     x = jnp.arange(2 * 5 * 3).reshape((2, 5, 3))
-    segmentation_mask = jnp.array([[1, 1, 1, 1, 0], [1, 1, 0, 0, 0]])
+    seq_lengths = jnp.array([4, 2])
 
-    flipped = flip_sequences(x, segmentation_mask, num_batch_dims=1, time_major=False)
+    flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=False)
 
     self.assertEqual(flipped.shape, (2, 5, 3))
     np.testing.assert_allclose(flipped[0, :4], x[0, :4][::-1])
     np.testing.assert_allclose(flipped[1, :2], x[1, :2][::-1])
 
   def test_flip_sequence_time_major(self):
     x = jnp.arange(2 * 5).reshape((5, 2))
-    segmentation_mask = jnp.array([
-      [1, 1],
-      [1, 1],
-      [1, 0],
-      [1, 0],
-      [0, 0],
-    ])
+    seq_lengths = jnp.array([4, 2])
 
-    flipped = flip_sequences(x, segmentation_mask, num_batch_dims=1, time_major=True)
+    flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=True)
 
     self.assertEqual(flipped.shape, (5, 2))
     np.testing.assert_allclose(flipped[:4, 0], x[:4, 0][::-1])
     np.testing.assert_allclose(flipped[:2, 1], x[:2, 1][::-1])
 
   def test_flip_sequence_time_major_more_feature_dims(self):
     x = jnp.arange(2 * 5 * 3).reshape((5, 2, 3))
-    segmentation_mask = jnp.array([
-      [1, 1],
-      [1, 1],
-      [1, 0],
-      [1, 0],
-      [0, 0],
-    ])
+    seq_lengths = jnp.array([4, 2])
 
-    flipped = flip_sequences(x, segmentation_mask, num_batch_dims=1, time_major=True)
+    flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=True)
 
     self.assertEqual(flipped.shape, (5, 2, 3))
     np.testing.assert_allclose(flipped[:4, 0], x[:4, 0][::-1])
     np.testing.assert_allclose(flipped[:2, 1], x[:2, 1][::-1])
 
 class BidirectionalTest(absltest.TestCase):
 
   def test_bidirectional(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(), channels_out),
-      nn.RNN(nn.LSTMCell(), channels_out)
+      nn.RNN(nn.LSTMCell(channels_out)),
+      nn.RNN(nn.LSTMCell(channels_out))
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out * 2))
 
   def test_shared_cell(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    cell = nn.LSTMCell()
+    cell = nn.LSTMCell(channels_out)
     bdirectional = nn.Bidirectional(
-      nn.RNN(cell, channels_out),
-      nn.RNN(cell, channels_out)
+      nn.RNN(cell),
+      nn.RNN(cell)
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out * 2))
@@ -425,16 +409,16 @@
   def test_custom_merge_fn(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(), channels_out),
-      nn.RNN(nn.LSTMCell(), channels_out),
+      nn.RNN(nn.LSTMCell(channels_out)),
+      nn.RNN(nn.LSTMCell(channels_out)),
       merge_fn=lambda x, y: x + y
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
@@ -443,16 +427,16 @@
   def test_return_carry(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(), channels_out),
-      nn.RNN(nn.LSTMCell(), channels_out),
+      nn.RNN(nn.LSTMCell(channels_out)),
+      nn.RNN(nn.LSTMCell(channels_out)),
       return_carry=True
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
     carry_forward, carry_backward = carry
@@ -462,7 +446,43 @@
       jax.tree_map(jnp.shape, carry_forward),
       ((batch_size, channels_out), (batch_size, channels_out))
     )
     self.assertEqual(
       jax.tree_map(jnp.shape, carry_backward),
       ((batch_size, channels_out), (batch_size, channels_out))
     )
+
+class TestRecurrentDeprecation(parameterized.TestCase):
+
+  @parameterized.product(
+    cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
+  )
+  def test_constructor(self, cell_type):
+
+    with self.assertRaisesRegex(
+      TypeError,
+      "The RNNCellBase API has changed"
+    ):
+      cell_type()
+
+  @parameterized.product(
+    cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
+  )
+  def test_initialize_carry(self, cell_type):
+    key = jax.random.PRNGKey(0)
+    with self.assertRaisesRegex(
+      TypeError,
+      "The RNNCellBase API has changed"
+    ):
+      cell_type.initialize_carry(key, (2,), 3)
+
+  def test_rnn(self):
+    cell = nn.LSTMCell(3)
+    with self.assertRaisesRegex(
+      TypeError,
+      "The RNNCellBase API has changed"
+    ):
+      nn.RNN(cell, cell_size=8)
+
+
+if __name__ == '__main__':
+  absltest.main()
```

### Comparing `flax-0.6.9/tests/linen/linen_test.py` & `flax-0.7.0/tests/linen/linen_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -294,16 +294,16 @@
                          jnp.ones((100, 100)),
                          deterministic=False,
                          rngs={'dropout': key})
         nonzero_counts += np.sum(y > 0.0)
       all_counts = np.prod((100, 100, n_trials))
       frac = np.sum(nonzero_counts) / all_counts
       keep_rate = 1.0 - rate
-      # just check within 3 sigma.
-      delta = 3 * np.sqrt(rate * keep_rate) / np.sqrt(all_counts)
+      # just check within 4 sigma.
+      delta = 4 * np.sqrt(rate * keep_rate) / np.sqrt(all_counts)
       self.assertTrue(keep_rate - delta < frac < keep_rate + delta)
 
   def test_dropout_rate_limits(self):
     rng = random.PRNGKey(0)
     key1, key2, key3 = random.split(rng, 3)
     inputs = jnp.ones((20, 20))
     d0 = nn.Dropout(rate=0.0)
@@ -319,26 +319,41 @@
     # ensure gradient of rate==1.0 case is non-NaN
     fn = lambda x, k: d1.apply({}, x,
                                rngs={'dropout': k},
                                deterministic=False)
     res = jax.grad(lambda x, k: jnp.sum(fn(x, k)))(inputs, key3)
     self.assertFalse(np.isnan(res).any())
 
+  def test_dropout_manual_rng(self):
+    class Foo(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        key = self.make_rng('dropout')
+        x1 = nn.Dropout(rate=0.5, deterministic=False)(x, rng=key)
+        x2 = nn.Dropout(rate=0.5, deterministic=False)(x, rng=key)
+        return x1, x2
+
+    module = Foo()
+    x1, x2 = module.apply(
+      {}, jnp.ones((20, 20)), rngs={'dropout': random.PRNGKey(0)})
+
+    np.testing.assert_array_equal(x1, x2)
+
 
 # TODO(flax-dev): add integration tests for RNN cells
 class RecurrentTest(absltest.TestCase):
 
   def test_lstm(self):
+    lstm = nn.LSTMCell(features=4)
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 3))
-    c0, h0 = nn.LSTMCell.initialize_carry(rng, (2,), 4)
+    c0, h0 = lstm.initialize_carry(rng, x.shape)
     self.assertEqual(c0.shape, (2, 4))
     self.assertEqual(h0.shape, (2, 4))
-    lstm = nn.LSTMCell()
     (carry, y), initial_params = lstm.init_with_output(key2, (c0, h0), x)
     self.assertEqual(carry[0].shape, (2, 4))
     self.assertEqual(carry[1].shape, (2, 4))
     np.testing.assert_allclose(y, carry[1])
     param_shapes = jax.tree_util.tree_map(np.shape, initial_params['params'])
     self.assertEqual(param_shapes, {
         'ii': {'kernel': (3, 4)},
@@ -348,82 +363,82 @@
         'hi': {'kernel': (4, 4), 'bias': (4,)},
         'hf': {'kernel': (4, 4), 'bias': (4,)},
         'hg': {'kernel': (4, 4), 'bias': (4,)},
         'ho': {'kernel': (4, 4), 'bias': (4,)},
     })
 
   def test_gru(self):
+    gru = nn.GRUCell(features=4)
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 3))
-    carry0 = nn.GRUCell.initialize_carry(rng, (2,), 4)
+    carry0 = gru.initialize_carry(rng, x.shape)
     self.assertEqual(carry0.shape, (2, 4))
-    gru = nn.GRUCell()
     (carry, y), initial_params = gru.init_with_output(key2, carry0, x)
     self.assertEqual(carry.shape, (2, 4))
     np.testing.assert_allclose(y, carry)
     param_shapes = jax.tree_util.tree_map(np.shape, initial_params['params'])
     self.assertEqual(param_shapes, {
         'ir': {'kernel': (3, 4), 'bias': (4,)},
         'iz': {'kernel': (3, 4), 'bias': (4,)},
         'in': {'kernel': (3, 4), 'bias': (4,)},
         'hr': {'kernel': (4, 4)},
         'hz': {'kernel': (4, 4)},
         'hn': {'kernel': (4, 4), 'bias': (4,)},
     })
 
   def test_complex_input_gru(self):
+    gru = nn.GRUCell(features=4)
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 3), dtype=jnp.complex64)
-    carry0 = nn.GRUCell.initialize_carry(rng, (2,), 4)
+    carry0 = gru.initialize_carry(rng, x.shape)
     self.assertEqual(carry0.shape, (2, 4))
-    gru = nn.GRUCell()
     (carry, y), _ = gru.init_with_output(key2, carry0, x)
     self.assertEqual(carry.dtype, jnp.complex64)
     self.assertEqual(y.dtype, jnp.complex64)
 
   def test_convlstm(self):
+    lstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 4, 4, 3))
-    c0, h0 = nn.ConvLSTMCell.initialize_carry(rng, (2,), (4, 4, 6))
+    c0, h0 = lstm.initialize_carry(rng, x.shape)
     self.assertEqual(c0.shape, (2, 4, 4, 6))
     self.assertEqual(h0.shape, (2, 4, 4, 6))
-    lstm = nn.ConvLSTMCell(features=6, kernel_size=(3, 3))
     (carry, y), initial_params = lstm.init_with_output(key2, (c0, h0), x)
     self.assertEqual(carry[0].shape, (2, 4, 4, 6))
     self.assertEqual(carry[1].shape, (2, 4, 4, 6))
     np.testing.assert_allclose(y, carry[1])
     param_shapes = jax.tree_util.tree_map(np.shape, initial_params['params'])
     self.assertEqual(param_shapes, {
         'hh': {'bias': (6*4,), 'kernel': (3, 3, 6, 6*4)},
         'ih': {'bias': (6*4,), 'kernel': (3, 3, 3, 6*4)},
     })
 
   def test_optimized_lstm_cell_matches_regular(self):
 
     # Create regular LSTMCell.
+    lstm = nn.LSTMCell(features=4)
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 3))
-    c0, h0 = nn.LSTMCell.initialize_carry(rng, (2,), 4)
+    c0, h0 = lstm.initialize_carry(rng, x.shape)
     self.assertEqual(c0.shape, (2, 4))
     self.assertEqual(h0.shape, (2, 4))
-    lstm = nn.LSTMCell()
     (_, y), lstm_params = lstm.init_with_output(key2, (c0, h0), x)
 
     # Create OptimizedLSTMCell.
+    lstm_opt = nn.OptimizedLSTMCell(features=4)
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     x = random.normal(key1, (2, 3))
-    c0, h0 = nn.OptimizedLSTMCell.initialize_carry(rng, (2,), 4)
+    c0, h0 = lstm_opt.initialize_carry(rng, x.shape)
     self.assertEqual(c0.shape, (2, 4))
     self.assertEqual(h0.shape, (2, 4))
-    lstm_opt = nn.OptimizedLSTMCell()
     (_, y_opt), lstm_opt_params = lstm_opt.init_with_output(key2, (c0, h0), x)
 
     np.testing.assert_allclose(y, y_opt, rtol=1e-6)
     check_eq(lstm_params, lstm_opt_params)
 
 
 class IdsTest(absltest.TestCase):
@@ -437,8 +452,8 @@
     id1c = copy.copy(id1)
     id1dc = copy.deepcopy(id1)
     self.assertNotEqual(hash(id1), hash(id1c))
     self.assertNotEqual(hash(id1), hash(id1dc))
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.6.9/tests/linen/linen_transforms_test.py` & `flax-0.7.0/tests/linen/linen_transforms_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 from jax import random
 import jax.numpy as jnp
 import numpy as np
 from flax import config
 from flax import errors
 from flax import linen as nn
 from flax.core import freeze, copy
-from flax.configurations import use_regular_dict
+from flax.configurations import temp_flip_flag
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 # pylint: disable=attribute-defined-outside-init,unused-variable,g-wrong-blank-lines,g-bare-generic
 
 def tree_equals(x, y):
@@ -274,73 +274,71 @@
 
     normal_model = MlpBn()
     vmap_model = vmap(MlpBn)(axis_name='batch')
     init_variables = normal_model.init(key2, x)
     y1 = normal_model.apply(init_variables, x2.reshape((-1, 4)), mutable=['batch_stats'])[0]
     y1 = y1.reshape((5, 4, 3))
     y2 = vmap_model.apply(init_variables, x2, mutable=['batch_stats'])[0]
-    np.testing.assert_allclose(y1, y2, atol=1e-6)
+    np.testing.assert_allclose(y1, y2, atol=1e-5)
 
   def test_scan(self):
     class SimpleScan(nn.Module):
+      features: int
       @nn.compact
       def __call__(self, c, xs):
         LSTM = nn.scan(nn.LSTMCell,
                        variable_broadcast='params',
                        split_rngs={'params': False})
-        return LSTM(name="lstm_cell")(c, xs)
+        return LSTM(self.features, name="lstm_cell")(c, xs)
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     xs = random.uniform(key1, (5, 3, 2))
     dummy_rng = random.PRNGKey(0)
-    init_carry = nn.LSTMCell.initialize_carry(dummy_rng,
-                                              xs.shape[1:-1],
-                                              xs.shape[-1])
-    model = SimpleScan()
+    init_carry = nn.LSTMCell(2).initialize_carry(dummy_rng, xs[0].shape)
+    model = SimpleScan(2)
     init_variables = model.init(key2, init_carry, xs)
     # simulate scan in python for comparison:
     c = init_carry
     ys = []
     lstmcell_variables = freeze({'params': init_variables['params']['lstm_cell']})
     for i in range(xs.shape[0]):
-      c, y = nn.LSTMCell().apply(lstmcell_variables, c, xs[i])
+      c, y = nn.LSTMCell(2).apply(lstmcell_variables, c, xs[i])
       ys.append(y[None, ...])
     y1 = jnp.vstack(ys)
 
     c2, y2 = model.apply(init_variables, init_carry, xs)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
     np.testing.assert_allclose(c[0], c2[0], atol=1e-7)
     np.testing.assert_allclose(c[1], c2[1], atol=1e-7)
 
   def test_scan_decorated(self):
     class SimpleScan(nn.Module):
+      features: int
       @partial(nn.scan,
                variable_broadcast='params',
                in_axes=(nn.broadcast, 0),
                split_rngs={'params': False})
       @nn.compact
       def __call__(self, c, b, xs):
         assert b.shape == (4,)
-        return nn.LSTMCell(name="lstm_cell")(c, xs)
+        return nn.LSTMCell(self.features, name="lstm_cell")(c, xs)
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     xs = random.uniform(key1, (4, 3, 2))
     b = jnp.ones((4,))
     dummy_rng = random.PRNGKey(0)
-    init_carry = nn.LSTMCell.initialize_carry(dummy_rng,
-                                              xs.shape[1:-1],
-                                              xs.shape[-1])
-    model = SimpleScan()
+    init_carry = nn.LSTMCell(2).initialize_carry(dummy_rng, xs[0].shape)
+    model = SimpleScan(2)
     init_variables = model.init(key2, init_carry, b, xs)
     # simulate scan in python for comparison:
     c = init_carry
     ys = []
     lstmcell_variables = freeze({'params': init_variables['params']['lstm_cell']})
     for i in range(xs.shape[0]):
-      c, y = nn.LSTMCell().apply(lstmcell_variables, c, xs[i])
+      c, y = nn.LSTMCell(2).apply(lstmcell_variables, c, xs[i])
       ys.append(y[None, ...])
     y1 = jnp.vstack(ys)
 
     c2, y2 = model.apply(init_variables, init_carry, b, xs)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
     np.testing.assert_allclose(c[0], c2[0], atol=1e-7)
     np.testing.assert_allclose(c[1], c2[1], atol=1e-7)
@@ -738,17 +736,19 @@
         y2 = self.a.bar(x)
         return y1, y2
 
     key = random.PRNGKey(0)
     x = jnp.ones((2,))
     (y1, y2), _ = B().init_with_output(key, x)
     np.testing.assert_array_equal(y1, y2)
-    # cntr == 3 due to 1 call by _validate_setup
-    # and two further "real" calls.
-    self.assertEqual(cntr, 3)
+    # cntr == 4 due to:
+    # 1 call by _validate_setup
+    # 1 call for the setup() outside transform boundary
+    # and two further "real" calls in transform boundaries
+    self.assertEqual(cntr, 4)
 
   def test_toplevel_submodule_adoption_transform(self):
     class A(nn.Module):
       @nn.compact
       def __call__(self, x):
         return nn.Dense(3)(x)
     class B(nn.Module):
@@ -803,15 +803,15 @@
     Ctrafo = nn.vmap(Csimple,
                      variable_axes={'params': 0},
                      split_rngs={'params': True})
 
     y3 = Ctrafo(a2, b).apply(p2, x)
     np.testing.assert_allclose(y1, y3, atol=1e-7)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_pytree_transform(self):
     class A(nn.Module):
       @nn.compact
       def __call__(self, c, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
         counter.value += 1
         x = nn.Dense(1)(x)
@@ -848,15 +848,15 @@
       }
     self.assertTrue(jax.tree_util.tree_all(
         jax.tree_util.tree_map(
             lambda x, y: np.testing.assert_allclose(x, y, atol=1e-7),
             cntrs, ref_cntrs)
         ))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_partially_applied_module_constructor_transform(self):
     k = random.PRNGKey(0)
     x = jnp.ones((3,4,4))
     dense = partial(nn.Dense, use_bias=False)
     vmap_dense = nn.vmap(
         dense,
         variable_axes={'params':0},
@@ -866,15 +866,15 @@
     ref_var_shapes = {
         'params': {
             'kernel': (3, 4, 4),
         },
     }
     self.assertTrue(tree_equals(init_vars_shapes, ref_var_shapes))
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_partial_module_method(self):
     k = random.PRNGKey(0)
     x = jnp.ones((3,4,4))
     class Foo(nn.Module):
 
       @nn.compact
       def inner(self, x):
@@ -1290,21 +1290,16 @@
       @nn.jit
       def __call__(self, x):
         return x
 
     k = random.PRNGKey(0)
     x = jnp.array([1.])
 
-    if config.flax_relaxed_naming:
-      with self.assertRaises(errors.NameInUseError):
-        y = Test().init(k, x)
-    else:
-      msg = 'Duplicate use of scope name: "sub"'
-      with self.assertRaisesWithLiteralMatch(ValueError, msg):
-        y = Test().init(k, x)
+    with self.assertRaises(errors.NameInUseError):
+      y = Test().init(k, x)
 
   def test_transform_with_setup_and_methods_on_submodule_pytrees(self):
     class Foo(nn.Module):
       def setup(self):
         self.inners = [nn.Dense(2), nn.Dense(2)]
       def helper(self, x, ms):
         return ms[0](x) + ms[1](x)
@@ -1461,18 +1456,20 @@
     y1 = Foo().apply(vs, x)
     np.testing.assert_array_equal(y0, y1)
 
   def test_while_loop(self):
     class Foo(nn.Module):
       @nn.compact
       def __call__(self, x):
+        key_zero = random.PRNGKey(0)
+        key_zero = jnp.broadcast_to(key_zero, (2, *key_zero.shape))
         self.param('inc', lambda _: 1)
         self.put_variable('state', 'acc', 0)
-        self.put_variable('state', 'rng_params', jnp.zeros((2, 2), jnp.uint32))
-        self.put_variable('state', 'rng_loop', jnp.zeros((2, 2), jnp.uint32))
+        self.put_variable('state', 'rng_params', key_zero)
+        self.put_variable('state', 'rng_loop', key_zero)
 
         def cond_fn(mdl, c):
           acc = mdl.get_variable('state', 'acc')
           return acc < x
         def body_fn(mdl, c):
           i = mdl.get_variable('state', 'acc')
           p_rng = mdl.make_rng('params')
@@ -1483,15 +1480,15 @@
           mdl.put_variable('state', 'acc', i + inc)
           return c
         return nn.while_loop(
             cond_fn, body_fn, self, (),
             carry_variables='state', split_rngs={'params': False, 'loop': True})
     x = 2
     mdl = Foo()
-    _, vars = mdl.apply({}, x, mutable=True, rngs={'params': random.PRNGKey(0), 'loop': random.PRNGKey(1)})
+    _, vars = mdl.apply({}, x, mutable=True, rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)})
     self.assertEqual(vars['state']['acc'], x)
     np.testing.assert_array_equal(vars['state']['rng_params'][0], vars['state']['rng_params'][1])
     np.testing.assert_array_compare(operator.__ne__, vars['state']['rng_loop'][0], vars['state']['rng_loop'][1])
 
   def test_cond(self):
     class Foo(nn.Module):
       @nn.compact
@@ -1504,15 +1501,15 @@
 
         def false_fn(mdl, x):
           mdl.variable('state', 'false_count').value += 1
           return -nn.Dense(2, name='dense')(x)
 
         return nn.cond(pred, true_fn, false_fn, self, x)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_switch(self):
     class Foo(nn.Module):
       @nn.compact
       def __call__(self, x, pred):
         self.variable('state', 'a_count', lambda: 0)
         self.variable('state', 'b_count', lambda: 0)
         self.variable('state', 'c_count', lambda: 0)
@@ -1539,15 +1536,15 @@
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 0})
     np.testing.assert_allclose(y1, -y2)
     y3, updates = foo.apply(vars, x, 2, mutable="state")
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 1})
     np.testing.assert_allclose(y1, y3)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_switch_multihead(self):
     class Foo(nn.Module):
       def setup(self) -> None:
         self.heads = [
           nn.Sequential([nn.Dense(10), nn.Dense(7), nn.Dense(5)]),
           nn.Sequential([nn.Dense(11), nn.Dense(5)]),
           nn.Dense(5),
@@ -1672,9 +1669,38 @@
     inner_expect = jax.tree_map(jnp.shape,
         freeze({'params': {'dense': {'kernel':
             nn.Partitioned(jnp.ones((4, 4)), names=('foo', 'bar'))}}}))
     self.assertEqual(jax.tree_map(jnp.shape, vs), outer_expect)
     self.assertEqual(jax.tree_map(jnp.shape, vars_copy), inner_expect)
 
 
+  def test_outer_setup_called_with_sharing_across_transforms(self):
+    class A(nn.Module):
+      def setup(self):
+        self.foo = self.param(
+            'foo', nn.initializers.zeros, (2, 2), jnp.float32)
+      def __call__(self, x):
+        return self.foo
+    class B(nn.Module):
+      a: Any
+      @nn.compact
+      def __call__(self, x):
+        return self.a(x)
+    class C(nn.Module):
+      def setup(self):
+        self.a = A()
+        self.b = nn.jit(B)(self.a)
+      def __call__(self, x):
+        b = self.b(x)
+        a = self.a(x)
+        return a + b
+    k = random.PRNGKey(0)
+    x = random.randint(k, (2, 2), minval=0, maxval=10)
+    vs = C().init(k, x)
+    y = C().apply(vs, x)
+    outer_expect = jax.tree_map(jnp.shape,
+        freeze({'params': {'a': {'foo': jnp.zeros((2, 2))}}}))
+    self.assertEqual(jax.tree_map(jnp.shape, vs), outer_expect)
+
+
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.6.9/tests/linen/partitioning_test.py` & `flax-0.7.0/tests/linen/partitioning_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/linen/summary_test.py` & `flax-0.7.0/tests/linen/summary_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 from jax import random
 import numpy as np
 
 from flax import linen as nn
 from flax.core.scope import Array
 from flax.linen import summary
 from flax import struct
-from flax.configurations import use_regular_dict
+from flax.configurations import temp_flip_flag
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 CONSOLE_TEST_KWARGS = dict(force_terminal=False, no_color=True, width=10_000)
 
 def _get_shapes(pytree):
@@ -177,15 +177,15 @@
     # check no summary is performed
     for row in table:
       self.assertEqual(
         row.module_variables,
         row.counted_variables,
       )
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_module_summary_with_depth(self):
     """
     This test creates a Table using `module_summary` set the `depth` argument to `1`,
     table should have less rows as a consequence.
     """
     batch_size = 32
 
@@ -236,15 +236,15 @@
     self.assertNotEqual(table[2].module_variables, table[2].counted_variables)
 
     # check CNN and Dense_0 output are not summarized
     self.assertEqual(table[0].module_variables, table[0].counted_variables)
     self.assertEqual(table[3].module_variables, table[3].counted_variables)
 
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_tabulate(self):
     """
     This test creates a string representation of a Module using `Module.tabulate`
     and checks that it matches the expected output given the CNN model defined in `_get_tabulate_cnn`.
     """
     batch_size = 32
 
@@ -319,15 +319,15 @@
       method=CNN.cnn_method,
       console_kwargs=CONSOLE_TEST_KWARGS,
     )
 
     self.assertIn("(block_method)", module_repr)
     self.assertIn("(cnn_method)", module_repr)
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_tabulate_function(self):
     """
     This test creates a string representation of a Module using `Module.tabulate`
     and checks that it matches the expected output given the CNN model defined in `_get_tabulate_cnn`.
     """
     batch_size = 32
 
@@ -366,36 +366,35 @@
 
     # total counts
     self.assertIn("Total Parameters", lines[-3])
     self.assertIn("19,850", lines[-3])
     self.assertIn("79.4 KB", lines[-3])
 
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_lifted_transform(self):
     class LSTM(nn.Module):
-      batch_size: int
-      out_feat: int
+      features: int
 
       @nn.compact
       def __call__(self, x):
-          carry = nn.LSTMCell.initialize_carry(
-              random.PRNGKey(0), (self.batch_size,), self.out_feat
+          carry = nn.LSTMCell(self.features).initialize_carry(
+              random.PRNGKey(0), x[:, 0].shape
           )
-          Cell = nn.scan(
+          ScanLSTM = nn.scan(
               nn.LSTMCell,
               variable_broadcast="params",
               split_rngs={"params": False},
               in_axes=1,
               out_axes=1,
           )
-          return Cell(name="ScanLSTM")(carry, x)
+          return ScanLSTM(self.features, name="ScanLSTM")(carry, x)
 
 
-    lstm = LSTM(batch_size=32, out_feat=128)
+    lstm = LSTM(features=128)
 
     with jax.check_tracer_leaks(True):
       module_repr = lstm.tabulate(
         random.PRNGKey(0),
         x=jnp.ones((32, 128, 64)),
         console_kwargs=CONSOLE_TEST_KWARGS)
 
@@ -403,36 +402,35 @@
 
     self.assertIn("LSTM", lines[5])
     self.assertIn("ScanLSTM", lines[9])
     self.assertIn("LSTMCell", lines[9])
     self.assertIn("ScanLSTM/ii", lines[13])
     self.assertIn("Dense", lines[13])
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_lifted_transform_no_rename(self):
     class LSTM(nn.Module):
-      batch_size: int
-      out_feat: int
+      features: int
 
       @nn.compact
       def __call__(self, x):
-          carry = nn.LSTMCell.initialize_carry(
-              random.PRNGKey(0), (self.batch_size,), self.out_feat
+          carry = nn.LSTMCell(self.features).initialize_carry(
+              random.PRNGKey(0), x[:, 0].shape
           )
-          Cell = nn.scan(
+          ScanLSTM = nn.scan(
               nn.LSTMCell,
               variable_broadcast="params",
               split_rngs={"params": False},
               in_axes=1,
               out_axes=1,
           )
-          return Cell()(carry, x)
+          return ScanLSTM(self.features)(carry, x)
 
 
-    lstm = LSTM(batch_size=32, out_feat=128)
+    lstm = LSTM(features=128)
 
     with jax.check_tracer_leaks(True):
       module_repr = lstm.tabulate(
         random.PRNGKey(0),
         x=jnp.ones((32, 128, 64)),
         console_kwargs=CONSOLE_TEST_KWARGS)
 
@@ -440,15 +438,15 @@
 
     self.assertIn("LSTM", lines[5])
     self.assertIn("ScanLSTMCell_0", lines[9])
     self.assertIn("LSTMCell", lines[9])
     self.assertIn("ScanLSTMCell_0/ii", lines[13])
     self.assertIn("Dense", lines[13])
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_module_reuse(self):
     class ConvBlock(nn.Module):
       @nn.compact
       def __call__(self, x):
         x = nn.Conv(32, [3, 3])(x)
         x = nn.BatchNorm(use_running_average=True)(x)
         x = nn.Dropout(0.5, deterministic=True)(x)
@@ -522,15 +520,15 @@
     module_repr = module.tabulate({}, console_kwargs=CONSOLE_TEST_KWARGS)
     lines = module_repr.splitlines()
 
     self.assertIn('4.141592', lines[5])
     self.assertIn('x: 3.141592', lines[7])
     self.assertIn('4.141592', lines[7])
 
-  @use_regular_dict()
+  @temp_flip_flag('return_frozendict', False)
   def test_partitioned_params(self):
 
     class Classifier(nn.Module):
       @nn.compact
       def __call__(self, x):
         hidden = nn.Dense(
           features=1024,
@@ -581,8 +579,8 @@
     x = jnp.ones((16, 9))
     rep = Foo().tabulate(jax.random.PRNGKey(0), x, console_kwargs=CONSOLE_TEST_KWARGS)
     lines = rep.splitlines()
     self.assertIn('Total Parameters: 50', lines[-2])
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.6.9/tests/linen/toplevel_test.py` & `flax-0.7.0/tests/linen/toplevel_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/run_all_tests.sh` & `flax-0.7.0/tests/run_all_tests.sh`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/serialization_test.py` & `flax-0.7.0/tests/serialization_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/struct_test.py` & `flax-0.7.0/tests/struct_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/tensorboard_test.py` & `flax-0.7.0/tests/tensorboard_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/traceback_util_test.py` & `flax-0.7.0/tests/traceback_util_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.6.9/tests/traverse_util_test.py` & `flax-0.7.0/tests/traverse_util_test.py`

 * *Files identical despite different names*

