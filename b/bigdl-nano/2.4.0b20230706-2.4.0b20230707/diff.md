# Comparing `tmp/bigdl_nano-2.4.0b20230706-py3-none-win_amd64.whl.zip` & `tmp/bigdl_nano-2.4.0b20230707-py3-none-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,228 +1,228 @@
-Zip file size: 2849788 bytes, number of entries: 226
--rw-------  2.0 unx      956 b- defN 23-Jul-06 09:50 bigdl/__init__.py
--rw-------  2.0 unx     1554 b- defN 23-Jul-06 09:50 bigdl/nano/__init__.py
--rw-------  2.0 unx      684 b- defN 23-Jul-06 09:50 bigdl/nano/openvino.py
--rw-------  2.0 unx      734 b- defN 23-Jul-06 09:50 bigdl/nano/automl/__init__.py
--rw-------  2.0 unx      633 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/__init__.py
--rw-------  2.0 unx     1888 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/backend.py
--rw-------  2.0 unx     8486 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/callgraph.py
--rw-------  2.0 unx    10528 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/config.py
--rw-------  2.0 unx    20683 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/decorator.py
--rw-------  2.0 unx     6431 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/search.py
--rw-------  2.0 unx    21436 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/space.py
--rw-------  2.0 unx      622 b- defN 23-Jul-06 09:50 bigdl/nano/automl/hpo/visualization.py
--rw-------  2.0 unx      624 b- defN 23-Jul-06 09:50 bigdl/nano/automl/pytorch/__init__.py
--rw-------  2.0 unx     5540 b- defN 23-Jul-06 09:50 bigdl/nano/automl/pytorch/_helper.py
--rw-------  2.0 unx    10273 b- defN 23-Jul-06 09:50 bigdl/nano/automl/pytorch/hposearcher.py
--rw-------  2.0 unx    11262 b- defN 23-Jul-06 09:50 bigdl/nano/automl/pytorch/objective.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/__init__.py
--rw-------  2.0 unx    16110 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/mixin.py
--rw-------  2.0 unx     4530 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/objective.py
--rw-------  2.0 unx     2379 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/keras/Model.py
--rw-------  2.0 unx     2876 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/keras/Sequential.py
--rw-------  2.0 unx      648 b- defN 23-Jul-06 09:50 bigdl/nano/automl/tf/keras/__init__.py
--rw-------  2.0 unx      648 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/__init__.py
--rw-------  2.0 unx     2128 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/edict.py
--rw-------  2.0 unx     1936 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/parallel.py
--rw-------  2.0 unx     1029 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/parallel_worker.py
--rw-------  2.0 unx     1263 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/proxy.py
--rw-------  2.0 unx     4318 b- defN 23-Jul-06 09:50 bigdl/nano/automl/utils/register_modules.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/__init__.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/automl/__init__.py
--rw-------  2.0 unx     2460 b- defN 23-Jul-06 09:50 bigdl/nano/deps/automl/hpo_api.py
--rw-------  2.0 unx     6876 b- defN 23-Jul-06 09:50 bigdl/nano/deps/automl/optuna_backend.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/horovod/__init__.py
--rw-------  2.0 unx     5304 b- defN 23-Jul-06 09:50 bigdl/nano/deps/horovod/distributed_utils_horovod.py
--rw-------  2.0 unx      934 b- defN 23-Jul-06 09:50 bigdl/nano/deps/horovod/horovod_api.py
--rw-------  2.0 unx     1125 b- defN 23-Jul-06 09:50 bigdl/nano/deps/horovod/horovod_worker.py
--rw-------  2.0 unx     2761 b- defN 23-Jul-06 09:50 bigdl/nano/deps/horovod/multiprocs_backend.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/__init__.py
--rw-------  2.0 unx     9510 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/ipex_api.py
--rw-------  2.0 unx     8545 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/ipex_inference_bf16_model.py
--rw-------  2.0 unx    14090 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/ipex_inference_model.py
--rw-------  2.0 unx     4444 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py
--rw-------  2.0 unx     8087 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ipex/ipex_quantization_model.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/__init__.py
--rw-------  2.0 unx     3907 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/inc_api.py
--rw-------  2.0 unx     9975 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/inc_api_2.py
--rw-------  2.0 unx     1013 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/core/__init__.py
--rw-------  2.0 unx     1856 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/core/base_metric.py
--rw-------  2.0 unx    10631 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/core/quantization.py
--rw-------  2.0 unx     1036 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/__init__.py
--rw-------  2.0 unx     1039 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/metric.py
--rw-------  2.0 unx     1101 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/quantization.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/pytorch/__init__.py
--rw-------  2.0 unx     1111 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/pytorch/metric.py
--rw-------  2.0 unx     2856 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/pytorch/quantization.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/tensorflow/__init__.py
--rw-------  2.0 unx     1146 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/tensorflow/metric.py
--rw-------  2.0 unx     2621 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/onnx/tensorflow/quantization.py
--rw-------  2.0 unx      632 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/pytorch/__init__.py
--rw-------  2.0 unx      902 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/pytorch/metric.py
--rw-------  2.0 unx     2871 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/pytorch/quantization.py
--rw-------  2.0 unx     3756 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/pytorch/quantized_model.py
--rw-------  2.0 unx     2233 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/pytorch/utils.py
--rw-------  2.0 unx      635 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/tensorflow/__init__.py
--rw-------  2.0 unx     1088 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/tensorflow/metric.py
--rw-------  2.0 unx     4219 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/tensorflow/model.py
--rw-------  2.0 unx     3007 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/tensorflow/quantization.py
--rw-------  2.0 unx      729 b- defN 23-Jul-06 09:50 bigdl/nano/deps/neural_compressor/tensorflow/utils.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/__init__.py
--rw-------  2.0 unx     4990 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/onnxruntime_api.py
--rw-------  2.0 unx      932 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/core/__init__.py
--rw-------  2.0 unx     3655 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/core/onnxruntime_model.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/pytorch/__init__.py
--rw-------  2.0 unx     9530 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/tensorflow/__init__.py
--rw-------  2.0 unx     7852 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxruntime/tensorflow/model.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxsim/__init__.py
--rw-------  2.0 unx     1105 b- defN 23-Jul-06 09:50 bigdl/nano/deps/onnxsim/onnxsim_api.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/__init__.py
--rw-------  2.0 unx     6986 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/openvino_api.py
--rw-------  2.0 unx      872 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/core/__init__.py
--rw-------  2.0 unx     2129 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/core/metric.py
--rw-------  2.0 unx    15163 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/core/model.py
--rw-------  2.0 unx     4075 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/core/utils.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/pytorch/__init__.py
--rw-------  2.0 unx     1519 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/pytorch/dataloader.py
--rw-------  2.0 unx     1668 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/pytorch/metric.py
--rw-------  2.0 unx    14656 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/pytorch/model.py
--rw-------  2.0 unx     2616 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/pytorch/utils.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/tf/__init__.py
--rw-------  2.0 unx     1845 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/tf/dataloader.py
--rw-------  2.0 unx     1499 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/tf/metric.py
--rw-------  2.0 unx    10317 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/tf/model.py
--rw-------  2.0 unx     1676 b- defN 23-Jul-06 09:50 bigdl/nano/deps/openvino/tf/utils.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ray/__init__.py
--rw-------  2.0 unx     1265 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ray/ray_api.py
--rw-------  2.0 unx     1530 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ray/ray_backend.py
--rw-------  2.0 unx    18650 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ray/ray_distributed.py
--rw-------  2.0 unx     3705 b- defN 23-Jul-06 09:50 bigdl/nano/deps/ray/ray_envbase.py
--rw-------  2.0 unx      618 b- defN 23-Jul-06 09:50 bigdl/nano/k8s/__init__.py
--rw-------  2.0 unx    13183 b- defN 23-Jul-06 09:50 bigdl/nano/k8s/bigdl_submit.py
--rwx------  2.0 unx  5460184 b- defN 23-Jul-06 11:32 bigdl/nano/libs/libjemalloc.so
--rwx------  2.0 unx   393904 b- defN 23-Jul-06 11:32 bigdl/nano/libs/libtcmalloc.so
--rwx------  2.0 unx  1182232 b- defN 23-Jul-06 11:32 bigdl/nano/libs/libturbojpeg.so.0.2.0
--rw-------  2.0 unx     2113 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/__init__.py
--rw-------  2.0 unx     5657 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/context_manager.py
--rw-------  2.0 unx     3245 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/dispatcher.py
--rw-------  2.0 unx     5818 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/lightning.py
--rw-------  2.0 unx     3063 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/model.py
--rw-------  2.0 unx    18936 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/torch_nano.py
--rw-------  2.0 unx      706 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/algorithms/__init__.py
--rw-------  2.0 unx     1235 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/algorithms/selective_backprop/__init__.py
--rw-------  2.0 unx    12005 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/algorithms/selective_backprop/selective_backprop.py
--rw-------  2.0 unx      609 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/amp/__init__.py
--rw-------  2.0 unx      805 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/amp/amp_api.py
--rw-------  2.0 unx     9569 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/amp/bfloat16.py
--rw-------  2.0 unx      640 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/encryption/__init__.py
--rw-------  2.0 unx     6480 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/encryption/encryption.py
--rw-------  2.0 unx      661 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/inference/__init__.py
--rw-------  2.0 unx     4231 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/inference/multi_instance.py
--rw-------  2.0 unx    88306 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/inference/optimizer.py
--rw-------  2.0 unx     5107 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/inference/pipeline.py
--rw-------  2.0 unx     3430 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/low_precision/jit_int8_api.py
--rw-------  2.0 unx     9886 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/low_precision/jit_int8_model.py
--rw-------  2.0 unx      646 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/optim/__init__.py
--rw-------  2.0 unx     8351 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/optim/sparseadam.py
--rw-------  2.0 unx      804 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/__init__.py
--rw-------  2.0 unx      627 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/dtype_patching/__init__.py
--rw-------  2.0 unx     4206 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/dtype_patching/dtype_patching.py
--rw-------  2.0 unx      637 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/encryption_patching/__init__.py
--rw-------  2.0 unx     1854 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/encryption_patching/encryption_patching.py
--rw-------  2.0 unx      650 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/gpu_cpu/__init__.py
--rw-------  2.0 unx     7863 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/patching/gpu_cpu/gpu_cpu.py
--rw-------  2.0 unx      828 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/__init__.py
--rw-------  2.0 unx    13954 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/ddp_spawn.py
--rw-------  2.0 unx     6781 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/ddp_subprocess.py
--rw-------  2.0 unx     4706 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/ipex_strategy.py
--rw-------  2.0 unx     8908 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/k8s.py
--rw-------  2.0 unx     2059 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/strategies/worker.py
--rw-------  2.0 unx    23922 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/trainer/Trainer.py
--rw-------  2.0 unx      617 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/trainer/__init__.py
--rw-------  2.0 unx      875 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/datasets/__init__.py
--rw-------  2.0 unx     7094 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/datasets/datasets.py
--rw-------  2.0 unx     4528 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/datasets/oxfordpet_datasets.py
--rw-------  2.0 unx      672 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/models/__init__.py
--rw-------  2.0 unx     2133 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/models/_utils.py
--rw-------  2.0 unx     2455 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/models/classifier.py
--rw-------  2.0 unx     8900 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/models/vision.py
--rw-------  2.0 unx      613 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/transforms/__init__.py
--rw-------  2.0 unx    31603 b- defN 23-Jul-06 09:50 bigdl/nano/pytorch/vision/transforms/transforms.py
--rw-------  2.0 unx     1618 b- defN 23-Jul-06 09:50 bigdl/nano/tf/__init__.py
--rw-------  2.0 unx     4369 b- defN 23-Jul-06 09:50 bigdl/nano/tf/dispatcher.py
--rw-------  2.0 unx     4894 b- defN 23-Jul-06 09:50 bigdl/nano/tf/model.py
--rw-------  2.0 unx     1294 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/Model.py
--rw-------  2.0 unx     1076 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/Sequential.py
--rw-------  2.0 unx      845 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/__init__.py
--rw-------  2.0 unx    12580 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/customized_training_utils.py
--rw-------  2.0 unx     4929 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/distributed_utils.py
--rw-------  2.0 unx    11245 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/inference_utils.py
--rw-------  2.0 unx     1244 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/inheritance_utils.py
--rw-------  2.0 unx     5393 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/training_utils.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/activations/__init__.py
--rw-------  2.0 unx      609 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/amp/__init__.py
--rw-------  2.0 unx      798 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/amp/amp_api.py
--rw-------  2.0 unx     2036 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/amp/bfloat16.py
--rw-------  2.0 unx     1858 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/inference/_worker.py
--rw-------  2.0 unx    48489 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/inference/optimizer.py
--rw-------  2.0 unx      647 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/layers/__init__.py
--rw-------  2.0 unx     4057 b- defN 23-Jul-06 09:50 bigdl/nano/tf/keras/layers/embeddings.py
--rw-------  2.0 unx      664 b- defN 23-Jul-06 09:50 bigdl/nano/tf/optimizers/__init__.py
--rw-------  2.0 unx     6441 b- defN 23-Jul-06 09:50 bigdl/nano/tf/optimizers/sparse_adam.py
--rw-------  2.0 unx      586 b- defN 23-Jul-06 09:50 bigdl/nano/utils/__init__.py
--rw-------  2.0 unx     1556 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/__init__.py
--rw-------  2.0 unx     1782 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/affinity.py
--rw-------  2.0 unx      746 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/backend.py
--rw-------  2.0 unx     2072 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/checker.py
--rw-------  2.0 unx     3522 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/cpuinfo.py
--rw-------  2.0 unx     1270 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/decorator.py
--rw-------  2.0 unx     3429 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/env.py
--rw-------  2.0 unx      790 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/function.py
--rw-------  2.0 unx     1057 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/inspect.py
--rw-------  2.0 unx     1378 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/log4Error.py
--rw-------  2.0 unx     1248 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/log4Warning.py
--rw-------  2.0 unx     2640 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/model.py
--rw-------  2.0 unx     6189 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/schedule.py
--rw-------  2.0 unx     3278 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/subprocess.py
--rw-------  2.0 unx     1688 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/version.py
--rw-------  2.0 unx      967 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/__init__.py
--rw-------  2.0 unx     3536 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/acceleration_option.py
--rw-------  2.0 unx     3970 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/format.py
--rw-------  2.0 unx     4422 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/latency.py
--rw-------  2.0 unx      709 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/metric.py
--rw-------  2.0 unx     8336 b- defN 23-Jul-06 09:50 bigdl/nano/utils/common/optimizer/optimizer.py
--rw-------  2.0 unx     1939 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/__init__.py
--rw-------  2.0 unx     1100 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/attributes.py
--rw-------  2.0 unx     4527 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/channel_last.py
--rw-------  2.0 unx     1807 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/check_deps.py
--rw-------  2.0 unx     3358 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/convert.py
--rw-------  2.0 unx     6197 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/dataloader.py
--rw-------  2.0 unx     1321 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/dataset.py
--rw-------  2.0 unx     3881 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/input_sample.py
--rw-------  2.0 unx     4952 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/inspect.py
--rw-------  2.0 unx     2595 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/jit_method.py
--rw-------  2.0 unx     6270 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/load.py
--rw-------  2.0 unx     5951 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/metadata.py
--rw-------  2.0 unx     1391 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/metric.py
--rw-------  2.0 unx     5531 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/model_info.py
--rw-------  2.0 unx     2878 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/save.py
--rw-------  2.0 unx     1189 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/version.py
--rw-------  2.0 unx     1012 b- defN 23-Jul-06 09:50 bigdl/nano/utils/pytorch/xpu.py
--rw-------  2.0 unx     1057 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/__init__.py
--rw-------  2.0 unx     4300 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/attributes.py
--rw-------  2.0 unx     2816 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/backend.py
--rw-------  2.0 unx     7421 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/data.py
--rw-------  2.0 unx     2582 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/preprocess.py
--rw-------  2.0 unx     1347 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/subprocess_worker.py
--rw-------  2.0 unx      823 b- defN 23-Jul-06 09:50 bigdl/nano/utils/tf/version.py
--rwxrwxr-x  2.0 unx    13425 b- defN 23-Jul-06 09:50 bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init
--rwxrwxr-x  2.0 unx      909 b- defN 23-Jul-06 09:50 bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init.ps1
--rwxrwxr-x  2.0 unx      293 b- defN 23-Jul-06 09:50 bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env
--rwxrwxr-x  2.0 unx      127 b- defN 23-Jul-06 09:50 bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env.ps1
--rw-------  2.0 unx    10169 b- defN 23-Jul-06 11:32 bigdl_nano-2.4.0b20230706.dist-info/METADATA
--rw-------  2.0 unx       98 b- defN 23-Jul-06 11:32 bigdl_nano-2.4.0b20230706.dist-info/WHEEL
--rw-------  2.0 unx       54 b- defN 23-Jul-06 11:32 bigdl_nano-2.4.0b20230706.dist-info/entry_points.txt
--rw-------  2.0 unx        6 b- defN 23-Jul-06 11:32 bigdl_nano-2.4.0b20230706.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    21965 b- defN 23-Jul-06 11:32 bigdl_nano-2.4.0b20230706.dist-info/RECORD
-226 files, 8063353 bytes uncompressed, 2814242 bytes compressed:  65.1%
+Zip file size: 2850272 bytes, number of entries: 226
+-rw-r--r--  2.0 unx      956 b- defN 22-Oct-25 01:39 bigdl/__init__.py
+-rw-r--r--  2.0 unx     1554 b- defN 23-Feb-09 11:06 bigdl/nano/__init__.py
+-rw-r--r--  2.0 unx      684 b- defN 22-Oct-25 01:39 bigdl/nano/openvino.py
+-rw-r--r--  2.0 unx      734 b- defN 22-Oct-25 01:39 bigdl/nano/automl/__init__.py
+-rw-r--r--  2.0 unx      633 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/__init__.py
+-rw-r--r--  2.0 unx     1888 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/backend.py
+-rw-r--r--  2.0 unx     8486 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/callgraph.py
+-rw-r--r--  2.0 unx    10528 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/config.py
+-rw-r--r--  2.0 unx    20683 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/decorator.py
+-rw-r--r--  2.0 unx     6431 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/search.py
+-rw-r--r--  2.0 unx    21436 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/space.py
+-rw-r--r--  2.0 unx      622 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/visualization.py
+-rw-r--r--  2.0 unx      624 b- defN 22-Oct-25 01:39 bigdl/nano/automl/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5540 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/_helper.py
+-rw-r--r--  2.0 unx    10273 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/hposearcher.py
+-rw-r--r--  2.0 unx    11262 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/objective.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/__init__.py
+-rw-r--r--  2.0 unx    16110 b- defN 23-Feb-09 11:06 bigdl/nano/automl/tf/mixin.py
+-rw-r--r--  2.0 unx     4530 b- defN 23-Feb-09 11:06 bigdl/nano/automl/tf/objective.py
+-rw-r--r--  2.0 unx     2379 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/Model.py
+-rw-r--r--  2.0 unx     2876 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/Sequential.py
+-rw-r--r--  2.0 unx      648 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/__init__.py
+-rw-r--r--  2.0 unx      648 b- defN 22-Oct-25 01:39 bigdl/nano/automl/utils/__init__.py
+-rw-r--r--  2.0 unx     2128 b- defN 22-Oct-25 01:39 bigdl/nano/automl/utils/edict.py
+-rw-r--r--  2.0 unx     1936 b- defN 23-Apr-11 11:07 bigdl/nano/automl/utils/parallel.py
+-rw-r--r--  2.0 unx     1029 b- defN 23-Apr-11 11:07 bigdl/nano/automl/utils/parallel_worker.py
+-rw-r--r--  2.0 unx     1263 b- defN 23-Feb-09 11:06 bigdl/nano/automl/utils/proxy.py
+-rw-r--r--  2.0 unx     4318 b- defN 23-Feb-09 11:06 bigdl/nano/automl/utils/register_modules.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/automl/__init__.py
+-rw-r--r--  2.0 unx     2460 b- defN 22-Oct-25 01:39 bigdl/nano/deps/automl/hpo_api.py
+-rw-r--r--  2.0 unx     6876 b- defN 23-Feb-09 11:06 bigdl/nano/deps/automl/optuna_backend.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/__init__.py
+-rw-r--r--  2.0 unx     5304 b- defN 23-Feb-09 11:06 bigdl/nano/deps/horovod/distributed_utils_horovod.py
+-rw-r--r--  2.0 unx      934 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/horovod_api.py
+-rw-r--r--  2.0 unx     1125 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/horovod_worker.py
+-rw-r--r--  2.0 unx     2761 b- defN 23-Jul-07 11:31 bigdl/nano/deps/horovod/multiprocs_backend.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ipex/__init__.py
+-rw-r--r--  2.0 unx     9826 b- defN 23-Jul-07 11:31 bigdl/nano/deps/ipex/ipex_api.py
+-rw-r--r--  2.0 unx     8545 b- defN 23-Apr-18 11:06 bigdl/nano/deps/ipex/ipex_inference_bf16_model.py
+-rw-r--r--  2.0 unx    14091 b- defN 23-Jul-07 11:31 bigdl/nano/deps/ipex/ipex_inference_model.py
+-rw-r--r--  2.0 unx     4444 b- defN 23-Apr-07 11:07 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py
+-rw-r--r--  2.0 unx     8914 b- defN 23-Jul-07 11:31 bigdl/nano/deps/ipex/ipex_quantization_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/__init__.py
+-rw-r--r--  2.0 unx     3907 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/inc_api.py
+-rw-r--r--  2.0 unx     9975 b- defN 23-Apr-13 11:07 bigdl/nano/deps/neural_compressor/inc_api_2.py
+-rw-r--r--  2.0 unx     1013 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/__init__.py
+-rw-r--r--  2.0 unx     1856 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/base_metric.py
+-rw-r--r--  2.0 unx    10631 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/quantization.py
+-rw-r--r--  2.0 unx     1036 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/onnx/__init__.py
+-rw-r--r--  2.0 unx     1039 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/metric.py
+-rw-r--r--  2.0 unx     1101 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/quantization.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1111 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/pytorch/metric.py
+-rw-r--r--  2.0 unx     2856 b- defN 22-Dec-06 11:06 bigdl/nano/deps/neural_compressor/onnx/pytorch/quantization.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Nov-10 05:41 bigdl/nano/deps/neural_compressor/onnx/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     1146 b- defN 22-Nov-10 05:41 bigdl/nano/deps/neural_compressor/onnx/tensorflow/metric.py
+-rw-r--r--  2.0 unx     2621 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/onnx/tensorflow/quantization.py
+-rw-r--r--  2.0 unx      632 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/pytorch/__init__.py
+-rw-r--r--  2.0 unx      902 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/pytorch/metric.py
+-rw-r--r--  2.0 unx     2871 b- defN 23-May-06 11:33 bigdl/nano/deps/neural_compressor/pytorch/quantization.py
+-rw-r--r--  2.0 unx     3756 b- defN 23-Jun-14 11:32 bigdl/nano/deps/neural_compressor/pytorch/quantized_model.py
+-rw-r--r--  2.0 unx     2233 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/pytorch/utils.py
+-rw-r--r--  2.0 unx      635 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     1088 b- defN 23-Feb-01 11:07 bigdl/nano/deps/neural_compressor/tensorflow/metric.py
+-rw-r--r--  2.0 unx     4219 b- defN 23-Mar-13 11:07 bigdl/nano/deps/neural_compressor/tensorflow/model.py
+-rw-r--r--  2.0 unx     3007 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/tensorflow/quantization.py
+-rw-r--r--  2.0 unx      729 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/tensorflow/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/__init__.py
+-rw-r--r--  2.0 unx     4990 b- defN 23-Mar-07 11:07 bigdl/nano/deps/onnxruntime/onnxruntime_api.py
+-rw-r--r--  2.0 unx      932 b- defN 23-Feb-09 11:06 bigdl/nano/deps/onnxruntime/core/__init__.py
+-rw-r--r--  2.0 unx     3655 b- defN 23-Apr-13 11:07 bigdl/nano/deps/onnxruntime/core/onnxruntime_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/pytorch/__init__.py
+-rw-r--r--  2.0 unx     9530 b- defN 23-Apr-18 11:06 bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     7852 b- defN 23-Mar-13 11:07 bigdl/nano/deps/onnxruntime/tensorflow/model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 12:02 bigdl/nano/deps/onnxsim/__init__.py
+-rw-r--r--  2.0 unx     1105 b- defN 22-Oct-25 12:02 bigdl/nano/deps/onnxsim/onnxsim_api.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/__init__.py
+-rw-r--r--  2.0 unx     6986 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/openvino_api.py
+-rw-r--r--  2.0 unx      872 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/core/__init__.py
+-rw-r--r--  2.0 unx     2129 b- defN 22-Dec-29 11:06 bigdl/nano/deps/openvino/core/metric.py
+-rw-r--r--  2.0 unx    15163 b- defN 23-Jun-09 11:32 bigdl/nano/deps/openvino/core/model.py
+-rw-r--r--  2.0 unx     4075 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/core/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1519 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/pytorch/dataloader.py
+-rw-r--r--  2.0 unx     1668 b- defN 23-Mar-03 11:07 bigdl/nano/deps/openvino/pytorch/metric.py
+-rw-r--r--  2.0 unx    14656 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/pytorch/model.py
+-rw-r--r--  2.0 unx     2616 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/pytorch/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/tf/__init__.py
+-rw-r--r--  2.0 unx     1845 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/tf/dataloader.py
+-rw-r--r--  2.0 unx     1499 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/tf/metric.py
+-rw-r--r--  2.0 unx    10317 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/tf/model.py
+-rw-r--r--  2.0 unx     1676 b- defN 23-Jan-06 02:27 bigdl/nano/deps/openvino/tf/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ray/__init__.py
+-rw-r--r--  2.0 unx     1265 b- defN 23-Feb-09 11:06 bigdl/nano/deps/ray/ray_api.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Feb-09 11:06 bigdl/nano/deps/ray/ray_backend.py
+-rw-r--r--  2.0 unx    18650 b- defN 23-Mar-01 03:16 bigdl/nano/deps/ray/ray_distributed.py
+-rw-r--r--  2.0 unx     3705 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ray/ray_envbase.py
+-rw-r--r--  2.0 unx      618 b- defN 22-Oct-25 01:39 bigdl/nano/k8s/__init__.py
+-rw-r--r--  2.0 unx    13183 b- defN 23-Feb-09 11:06 bigdl/nano/k8s/bigdl_submit.py
+-rwxr--r--  2.0 unx  5460184 b- defN 23-Jul-07 11:33 bigdl/nano/libs/libjemalloc.so
+-rwxr--r--  2.0 unx   393904 b- defN 23-Jul-07 11:34 bigdl/nano/libs/libtcmalloc.so
+-rwxr--r--  2.0 unx  1182232 b- defN 23-Jul-07 11:34 bigdl/nano/libs/libturbojpeg.so.0.2.0
+-rw-r--r--  2.0 unx     2113 b- defN 23-Mar-09 11:07 bigdl/nano/pytorch/__init__.py
+-rw-r--r--  2.0 unx     6152 b- defN 23-Jul-07 11:31 bigdl/nano/pytorch/context_manager.py
+-rw-r--r--  2.0 unx     3245 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/dispatcher.py
+-rw-r--r--  2.0 unx     5818 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/lightning.py
+-rw-r--r--  2.0 unx     3063 b- defN 23-Apr-13 11:07 bigdl/nano/pytorch/model.py
+-rw-r--r--  2.0 unx    18936 b- defN 23-Jun-13 11:32 bigdl/nano/pytorch/torch_nano.py
+-rw-r--r--  2.0 unx      706 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/algorithms/__init__.py
+-rw-r--r--  2.0 unx     1235 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/algorithms/selective_backprop/__init__.py
+-rw-r--r--  2.0 unx    12005 b- defN 23-Feb-15 11:07 bigdl/nano/pytorch/algorithms/selective_backprop/selective_backprop.py
+-rw-r--r--  2.0 unx      609 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/amp/__init__.py
+-rw-r--r--  2.0 unx      805 b- defN 22-Nov-22 07:23 bigdl/nano/pytorch/amp/amp_api.py
+-rw-r--r--  2.0 unx     9569 b- defN 23-Mar-14 11:07 bigdl/nano/pytorch/amp/bfloat16.py
+-rw-r--r--  2.0 unx      640 b- defN 23-Jan-09 11:06 bigdl/nano/pytorch/encryption/__init__.py
+-rw-r--r--  2.0 unx     6480 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/encryption/encryption.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Feb-23 11:07 bigdl/nano/pytorch/inference/__init__.py
+-rw-r--r--  2.0 unx     4231 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/inference/multi_instance.py
+-rw-r--r--  2.0 unx    89541 b- defN 23-Jul-07 11:31 bigdl/nano/pytorch/inference/optimizer.py
+-rw-r--r--  2.0 unx     5107 b- defN 23-Apr-24 11:06 bigdl/nano/pytorch/inference/pipeline.py
+-rw-r--r--  2.0 unx     3430 b- defN 23-Apr-18 11:06 bigdl/nano/pytorch/low_precision/jit_int8_api.py
+-rw-r--r--  2.0 unx     9886 b- defN 23-Apr-19 11:06 bigdl/nano/pytorch/low_precision/jit_int8_model.py
+-rw-r--r--  2.0 unx      646 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/optim/__init__.py
+-rw-r--r--  2.0 unx     8351 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/optim/sparseadam.py
+-rw-r--r--  2.0 unx      804 b- defN 23-Jan-06 11:06 bigdl/nano/pytorch/patching/__init__.py
+-rw-r--r--  2.0 unx      627 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/patching/dtype_patching/__init__.py
+-rw-r--r--  2.0 unx     4206 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/patching/dtype_patching/dtype_patching.py
+-rw-r--r--  2.0 unx      637 b- defN 23-Jan-06 11:06 bigdl/nano/pytorch/patching/encryption_patching/__init__.py
+-rw-r--r--  2.0 unx     1854 b- defN 23-Jan-11 11:06 bigdl/nano/pytorch/patching/encryption_patching/encryption_patching.py
+-rw-r--r--  2.0 unx      650 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/patching/gpu_cpu/__init__.py
+-rw-r--r--  2.0 unx     7863 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/patching/gpu_cpu/gpu_cpu.py
+-rw-r--r--  2.0 unx      828 b- defN 22-Dec-26 11:06 bigdl/nano/pytorch/strategies/__init__.py
+-rw-r--r--  2.0 unx    13954 b- defN 23-Mar-01 03:16 bigdl/nano/pytorch/strategies/ddp_spawn.py
+-rw-r--r--  2.0 unx     6781 b- defN 23-Apr-11 11:07 bigdl/nano/pytorch/strategies/ddp_subprocess.py
+-rw-r--r--  2.0 unx     4706 b- defN 23-Feb-13 11:07 bigdl/nano/pytorch/strategies/ipex_strategy.py
+-rw-r--r--  2.0 unx     8908 b- defN 23-Mar-01 03:16 bigdl/nano/pytorch/strategies/k8s.py
+-rw-r--r--  2.0 unx     2059 b- defN 23-Apr-11 11:07 bigdl/nano/pytorch/strategies/worker.py
+-rw-r--r--  2.0 unx    23922 b- defN 23-Mar-01 07:03 bigdl/nano/pytorch/trainer/Trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/trainer/__init__.py
+-rw-r--r--  2.0 unx      875 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/vision/datasets/__init__.py
+-rw-r--r--  2.0 unx     7094 b- defN 23-Feb-21 06:40 bigdl/nano/pytorch/vision/datasets/datasets.py
+-rw-r--r--  2.0 unx     4528 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/datasets/oxfordpet_datasets.py
+-rw-r--r--  2.0 unx      672 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/__init__.py
+-rw-r--r--  2.0 unx     2133 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/_utils.py
+-rw-r--r--  2.0 unx     2455 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/classifier.py
+-rw-r--r--  2.0 unx     8900 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/vision.py
+-rw-r--r--  2.0 unx      613 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/transforms/__init__.py
+-rw-r--r--  2.0 unx    31603 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/vision/transforms/transforms.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Feb-18 11:06 bigdl/nano/tf/__init__.py
+-rw-r--r--  2.0 unx     4369 b- defN 23-Feb-09 11:06 bigdl/nano/tf/dispatcher.py
+-rw-r--r--  2.0 unx     4894 b- defN 23-Mar-13 11:07 bigdl/nano/tf/model.py
+-rw-r--r--  2.0 unx     1294 b- defN 23-Jan-11 11:06 bigdl/nano/tf/keras/Model.py
+-rw-r--r--  2.0 unx     1076 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/Sequential.py
+-rw-r--r--  2.0 unx      845 b- defN 23-Apr-04 11:07 bigdl/nano/tf/keras/__init__.py
+-rw-r--r--  2.0 unx    12580 b- defN 23-Apr-19 11:06 bigdl/nano/tf/keras/customized_training_utils.py
+-rw-r--r--  2.0 unx     4929 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/distributed_utils.py
+-rw-r--r--  2.0 unx    11245 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/inference_utils.py
+-rw-r--r--  2.0 unx     1244 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/inheritance_utils.py
+-rw-r--r--  2.0 unx     5393 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/training_utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/activations/__init__.py
+-rw-r--r--  2.0 unx      609 b- defN 23-Jan-10 11:06 bigdl/nano/tf/keras/amp/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 23-Feb-02 11:06 bigdl/nano/tf/keras/amp/amp_api.py
+-rw-r--r--  2.0 unx     2036 b- defN 23-Mar-24 11:07 bigdl/nano/tf/keras/amp/bfloat16.py
+-rw-r--r--  2.0 unx     1858 b- defN 23-Feb-14 11:07 bigdl/nano/tf/keras/inference/_worker.py
+-rw-r--r--  2.0 unx    48489 b- defN 23-Mar-24 11:07 bigdl/nano/tf/keras/inference/optimizer.py
+-rw-r--r--  2.0 unx      647 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/layers/__init__.py
+-rw-r--r--  2.0 unx     4057 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/layers/embeddings.py
+-rw-r--r--  2.0 unx      664 b- defN 22-Oct-25 01:39 bigdl/nano/tf/optimizers/__init__.py
+-rw-r--r--  2.0 unx     6441 b- defN 23-Feb-19 03:08 bigdl/nano/tf/optimizers/sparse_adam.py
+-rw-r--r--  2.0 unx      586 b- defN 23-Feb-09 11:06 bigdl/nano/utils/__init__.py
+-rw-r--r--  2.0 unx     1556 b- defN 23-Apr-12 06:44 bigdl/nano/utils/common/__init__.py
+-rw-r--r--  2.0 unx     1782 b- defN 23-Feb-17 11:06 bigdl/nano/utils/common/affinity.py
+-rw-r--r--  2.0 unx      746 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/backend.py
+-rw-r--r--  2.0 unx     2072 b- defN 23-Apr-11 11:07 bigdl/nano/utils/common/checker.py
+-rw-r--r--  2.0 unx     3522 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/cpuinfo.py
+-rw-r--r--  2.0 unx     1270 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/decorator.py
+-rw-r--r--  2.0 unx     3429 b- defN 23-Feb-27 07:35 bigdl/nano/utils/common/env.py
+-rw-r--r--  2.0 unx      790 b- defN 23-Apr-12 06:44 bigdl/nano/utils/common/function.py
+-rw-r--r--  2.0 unx     1057 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/inspect.py
+-rw-r--r--  2.0 unx     1378 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/log4Error.py
+-rw-r--r--  2.0 unx     1248 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/log4Warning.py
+-rw-r--r--  2.0 unx     2640 b- defN 23-Feb-21 03:10 bigdl/nano/utils/common/model.py
+-rw-r--r--  2.0 unx     6189 b- defN 23-May-11 11:33 bigdl/nano/utils/common/schedule.py
+-rw-r--r--  2.0 unx     3278 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/subprocess.py
+-rw-r--r--  2.0 unx     1688 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/version.py
+-rw-r--r--  2.0 unx      967 b- defN 23-Mar-15 11:07 bigdl/nano/utils/common/optimizer/__init__.py
+-rw-r--r--  2.0 unx     3536 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/acceleration_option.py
+-rw-r--r--  2.0 unx     3970 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/format.py
+-rw-r--r--  2.0 unx     4422 b- defN 23-Mar-15 11:07 bigdl/nano/utils/common/optimizer/latency.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/optimizer/metric.py
+-rw-r--r--  2.0 unx     8336 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/optimizer.py
+-rw-r--r--  2.0 unx     1939 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1100 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/attributes.py
+-rw-r--r--  2.0 unx     4527 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/channel_last.py
+-rw-r--r--  2.0 unx     1807 b- defN 23-Mar-01 07:03 bigdl/nano/utils/pytorch/check_deps.py
+-rw-r--r--  2.0 unx     3358 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/convert.py
+-rw-r--r--  2.0 unx     6197 b- defN 23-Apr-19 11:06 bigdl/nano/utils/pytorch/dataloader.py
+-rw-r--r--  2.0 unx     1526 b- defN 23-Jul-07 11:31 bigdl/nano/utils/pytorch/dataset.py
+-rw-r--r--  2.0 unx     3881 b- defN 23-Feb-10 11:06 bigdl/nano/utils/pytorch/input_sample.py
+-rw-r--r--  2.0 unx     4952 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/inspect.py
+-rw-r--r--  2.0 unx     2595 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/jit_method.py
+-rw-r--r--  2.0 unx     6270 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/load.py
+-rw-r--r--  2.0 unx     5951 b- defN 23-Apr-19 11:06 bigdl/nano/utils/pytorch/metadata.py
+-rw-r--r--  2.0 unx     1391 b- defN 23-Feb-09 11:07 bigdl/nano/utils/pytorch/metric.py
+-rw-r--r--  2.0 unx     5531 b- defN 23-Mar-03 11:07 bigdl/nano/utils/pytorch/model_info.py
+-rw-r--r--  2.0 unx     2878 b- defN 23-Feb-23 11:07 bigdl/nano/utils/pytorch/save.py
+-rw-r--r--  2.0 unx     1189 b- defN 23-Apr-11 11:07 bigdl/nano/utils/pytorch/version.py
+-rw-r--r--  2.0 unx     1012 b- defN 23-Apr-07 11:07 bigdl/nano/utils/pytorch/xpu.py
+-rw-r--r--  2.0 unx     1057 b- defN 23-Mar-09 11:07 bigdl/nano/utils/tf/__init__.py
+-rw-r--r--  2.0 unx     4300 b- defN 23-Feb-19 03:08 bigdl/nano/utils/tf/attributes.py
+-rw-r--r--  2.0 unx     2816 b- defN 23-Apr-04 11:07 bigdl/nano/utils/tf/backend.py
+-rw-r--r--  2.0 unx     7421 b- defN 23-Mar-13 11:07 bigdl/nano/utils/tf/data.py
+-rw-r--r--  2.0 unx     2582 b- defN 23-Mar-09 11:07 bigdl/nano/utils/tf/preprocess.py
+-rw-r--r--  2.0 unx     1347 b- defN 23-Apr-04 11:07 bigdl/nano/utils/tf/subprocess_worker.py
+-rw-r--r--  2.0 unx      823 b- defN 23-Feb-19 03:08 bigdl/nano/utils/tf/version.py
+-rwxr-xr-x  2.0 unx    13425 b- defN 23-May-05 11:33 bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init
+-rwxr-xr-x  2.0 unx      909 b- defN 22-Dec-29 11:06 bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init.ps1
+-rwxr-xr-x  2.0 unx      293 b- defN 23-May-05 11:33 bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env
+-rwxr-xr-x  2.0 unx      127 b- defN 22-Dec-29 11:06 bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env.ps1
+-rw-r--r--  2.0 unx    10169 b- defN 23-Jul-07 11:34 bigdl_nano-2.4.0b20230707.dist-info/METADATA
+-rw-r--r--  2.0 unx       98 b- defN 23-Jul-07 11:34 bigdl_nano-2.4.0b20230707.dist-info/WHEEL
+-rw-r--r--  2.0 unx       54 b- defN 23-Jul-07 11:34 bigdl_nano-2.4.0b20230707.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        6 b- defN 23-Jul-07 11:34 bigdl_nano-2.4.0b20230707.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    21965 b- defN 23-Jul-07 11:34 bigdl_nano-2.4.0b20230707.dist-info/RECORD
+226 files, 8066432 bytes uncompressed, 2814726 bytes compressed:  65.1%
```

## zipnote {}

```diff
@@ -645,35 +645,35 @@
 
 Filename: bigdl/nano/utils/tf/subprocess_worker.py
 Comment: 
 
 Filename: bigdl/nano/utils/tf/version.py
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init
+Filename: bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init.ps1
+Filename: bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init.ps1
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env
+Filename: bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env.ps1
+Filename: bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env.ps1
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.dist-info/METADATA
+Filename: bigdl_nano-2.4.0b20230707.dist-info/METADATA
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.dist-info/WHEEL
+Filename: bigdl_nano-2.4.0b20230707.dist-info/WHEEL
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.dist-info/entry_points.txt
+Filename: bigdl_nano-2.4.0b20230707.dist-info/entry_points.txt
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.dist-info/top_level.txt
+Filename: bigdl_nano-2.4.0b20230707.dist-info/top_level.txt
 Comment: 
 
-Filename: bigdl_nano-2.4.0b20230706.dist-info/RECORD
+Filename: bigdl_nano-2.4.0b20230707.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bigdl/nano/deps/ipex/ipex_api.py

```diff
@@ -43,15 +43,15 @@
 
     return ret
 
 
 def PytorchIPEXJITModel(model, input_sample=None, use_ipex=False,
                         use_jit=False, channels_last=None, thread_num=None,
                         inplace=False, jit_strict=True, jit_method=None,
-                        weights_prepack=None, enable_onednn=True,
+                        weights_prepack=None, enable_onednn=False,
                         example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform.
     :param input_sample: torch tensor indicate the data sample to be used
             for tracing.
     :param use_ipex: if use ipex to optimize the model
     :param use_jit: if use jit to accelerate the model
@@ -63,15 +63,15 @@
            convert a model to TorchScript.
     :param weights_prepack: Whether to perform weight prepack for convolution and linear
            to avoid oneDNN weights reorder. The default value is None. Explicitly setting
            this knob overwrites the configuration set by level knob. Only valid when
            ``use_ipex=True``, otherwise will be ignored.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
            API, which provides a flexible API for aggressive fusion. Default to
-           ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
+           ``False``, only valid when use_jit is ``True``, otherwise will be ignored.
     :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
            to ``torch.jit.trace``. Default to None. Either this argument or input_sample
            should be specified when use_jit is ``True`` and torch > 2.0,
            otherwise will be ignored.
     '''
     from .ipex_inference_model import PytorchIPEXJITModel
     return PytorchIPEXJITModel(model, input_sample=input_sample, use_ipex=use_ipex,
@@ -81,15 +81,15 @@
                                enable_onednn=enable_onednn,
                                example_kwarg_inputs=example_kwarg_inputs)
 
 
 def PytorchIPEXJITBF16Model(model, input_sample=None, use_ipex=False,
                             use_jit=False, channels_last=None, thread_num=None,
                             inplace=False, jit_strict=True, jit_method=None,
-                            weights_prepack=None, enable_onednn=True,
+                            weights_prepack=None, enable_onednn=False,
                             example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform.
     :param input_sample: torch tensor indicate the data sample to be used
             for tracing.
     :param use_ipex: if use ipex to optimize the model
     :param use_jit: if use jit to accelerate the model
@@ -101,15 +101,15 @@
            convert a model to TorchScript.
     :param weights_prepack: Whether to perform weight prepack for convolution and linear
            to avoid oneDNN weights reorder. The default value is None. Explicitly setting
            this knob overwrites the configuration set by level knob. Only valid when
            ``use_ipex=True``, otherwise will be ignored.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
            API, which provides a flexible API for aggressive fusion. Default to
-           ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
+           ``False``, only valid when use_jit is ``True``, otherwise will be ignored.
     :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
            to ``torch.jit.trace``. Default to None. Either this argument or input_sample
            should be specified when use_jit is ``True`` and torch > 2.0,
            otherwise will be ignored.
     '''
     from .ipex_inference_bf16_model import PytorchIPEXJITBF16Model
     return PytorchIPEXJITBF16Model(model, input_sample=input_sample, use_ipex=use_ipex,
@@ -119,15 +119,16 @@
                                    enable_onednn=enable_onednn,
                                    example_kwarg_inputs=example_kwarg_inputs)
 
 
 def PytorchIPEXQuantizationModel(model, calib_data, q_config=None,
                                  input_sample=None, channels_last=None,
                                  thread_num=None, inplace=False,
-                                 jit_strict=True, example_kwarg_inputs=None):
+                                 jit_strict=True, example_kwarg_inputs=None,
+                                 enable_onednn=False):
     '''
     :param model: the model(nn.module) to be transform.
     :param calib_data: calibration data is required for static quantization.
     :param q_config: describes how to quantize a layer or a part of the network
             by providing settings (observer classes) for activations and weights
             respectively.
     :param input_sample: torch tensor indicate the data sample to be used
@@ -136,21 +137,25 @@
     :param thread_num: the thread num allocated for this model.
     :param inplace: whether to perform inplace optimization. Default: ``False``.
     :param jit_strict: Whether recording your mutable container types.
     :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
            to ``torch.jit.trace``. Default to None. Either this argument or input_sample
            should be specified when use_jit is ``True`` and torch > 2.0,
            otherwise will be ignored.
+    :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
+           API, which provides a flexible API for aggressive fusion. Default to
+           ``False``.
     '''
     from .ipex_quantization_model import PytorchIPEXQuantizationModel
     return PytorchIPEXQuantizationModel(model, calib_data, q_config=q_config,
                                         input_sample=input_sample, channels_last=channels_last,
                                         thread_num=thread_num, inplace=inplace,
                                         jit_strict=jit_strict,
-                                        example_kwarg_inputs=example_kwarg_inputs)
+                                        example_kwarg_inputs=example_kwarg_inputs,
+                                        enable_onednn=enable_onednn)
 
 
 def PytorchIPEXPUModel(model, thread_num=None, precision="fp32", use_ipex=False):
     '''
     :param model: the model(nn.module) to be transform.
     :param thread_num: the thread num allocated for this model.
     '''
```

## bigdl/nano/deps/ipex/ipex_inference_model.py

```diff
@@ -227,15 +227,15 @@
                                                                       None),
                                    from_load=from_load,
                                    thread_num=thread_num,
                                    inplace=inplace,
                                    jit_strict=status.get('jit_strict', True),
                                    jit_method=status.get('jit_method', None),
                                    weights_prepack=status.get('weights_prepack', None),
-                                   enable_onednn=status.get('enable_onednn', True),
+                                   enable_onednn=status.get('enable_onednn', False),
                                    compression=status.get('compression', "fp32"),)
 
     def _save_model(self, path, compression="fp32"):
         if self.use_jit:
             if compression == "bf16":
                 # for jit, if we want to compress its precision at saving,
                 # we need to save original model's state dict with compression precision
```

## bigdl/nano/deps/ipex/ipex_quantization_model.py

```diff
@@ -25,15 +25,15 @@
 import torch
 
 
 class PytorchIPEXQuantizationModel(AcceleratedLightningModule):
     def __init__(self, model: torch.nn.Module, calib_data, q_config=None,
                  input_sample=None, channels_last=None, thread_num=None,
                  from_load=False, inplace=False, jit_strict=True,
-                 example_kwarg_inputs=None):
+                 example_kwarg_inputs=None, enable_onednn=False):
         """
         This is the accelerated model for pytorch and ipex/jit.
         All the external API is based on InferenceOptimizer, so what we have here is
         basically internal APIs and subject to change.
 
         This PytorchIPEXQuantizationModel will serve for int8 and ipex>1.9 models.
         :param model: the model(nn.module) to be transform if from_load is False
@@ -55,32 +55,41 @@
         :param from_load: this will only be set by _load method.
         :param inplace: whether to perform inplace optimization. Default: ``False``.
         :param jit_strict: Whether recording your mutable container types.
         :param example_kwarg_inputs: keyword arguments of example inputs that will
                be passed to ``torch.jit.trace``. Default to ``None``. Either this
                argument or ``input_sample`` should be specified when ``use_jit`` is
                ``True`` and torch > 2.0, otherwise will be ignored.
+        :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN
+               Graph API, which provides a flexible API for aggressive fusion. Default to
+               ``False``. For more details, please refer https://github.com/
+               pytorch/pytorch/tree/master/torch/csrc/jit/codegen/
+               onednn#pytorch---onednn-graph-api-bridge.
         """
         super().__init__(model)
         if from_load:
             self.channels_last = channels_last
             self.jit_strict = jit_strict
+            self.enable_onednn = enable_onednn
             self._nano_context_manager = generate_context_manager(accelerator="jit",
                                                                   precision="int8",
-                                                                  thread_num=thread_num)
+                                                                  thread_num=thread_num,
+                                                                  enable_onednn=enable_onednn)
             return
         self.channels_last = channels_last
         self.original_state_dict = model.state_dict()
         self.jit_strict = jit_strict
+        self.enable_onednn = enable_onednn
         self.original_model = model
         if self.channels_last:
             self.model = self.model.to(memory_format=torch.channels_last)
         self._nano_context_manager = generate_context_manager(accelerator="jit",
                                                               precision="int8",
-                                                              thread_num=thread_num)
+                                                              thread_num=thread_num,
+                                                              enable_onednn=enable_onednn)
         self.thread_num = thread_num
         if q_config is None:
             # default qconfig
             self.q_config = ipex.quantization.default_static_qconfig
         else:
             self.q_config = q_config
 
@@ -140,15 +149,16 @@
 
     @property
     def status(self):
         status = super().status
         status.update({"channels_last": self.channels_last,
                        "checkpoint": "ckpt.pth",
                        "thread_num": self.thread_num,
-                       "jit_strict": self.jit_strict})
+                       "jit_strict": self.jit_strict,
+                       "enable_onednn": self.enable_onednn})
         return status
 
     @staticmethod
     def _load(path, model, inplace=False):
         status = PytorchIPEXQuantizationModel._load_status(path)
         checkpoint_path = path / status['checkpoint']
         model = torch.jit.load(checkpoint_path)
@@ -160,11 +170,12 @@
             thread_num = int(status['thread_num'])
         return PytorchIPEXQuantizationModel(model,
                                             calib_data=None,
                                             channels_last=status['channels_last'],
                                             from_load=from_load,
                                             thread_num=thread_num,
                                             inplace=inplace,
-                                            jit_strict=status["jit_strict"])
+                                            jit_strict=status["jit_strict"],
+                                            enable_onednn=status.get('enable_onednn', False))
 
     def _save_model(self, path, compression="fp32"):
         self.model.save(path / "ckpt.pth")
```

## bigdl/nano/pytorch/context_manager.py

```diff
@@ -21,24 +21,30 @@
 
 class BaseContextManager(object):
     """
     No grad context manager for Pytorch Model Inference.
 
     This context manager is used for providing no_grad context only.
     """
-    def __init__(self, thread_num=None, accelerator=None, enable_onednn=True):
+    def __init__(self, thread_num=None, accelerator=None, enable_onednn=False):
+        self.infer_mode_string = "inference_mode"
         self.infer_mode = torch.inference_mode(mode=True)
         self.thread_num = thread_num
         self.accelerator = accelerator
         self.enable_onednn = enable_onednn
         self.original_thread_num = torch.get_num_threads()
 
     def __enter__(self):
         if self.thread_num is not None:
             torch.set_num_threads(self.thread_num)
+        if not hasattr(self, "infer_mode"):
+            if self.infer_mode_string == "inference_mode":
+                self.infer_mode = torch.inference_mode(mode=True)
+            else:
+                self.infer_mode = torch.no_grad()
         self.infer_mode.__enter__()
         if self.accelerator == "jit" and self.enable_onednn is True:
             if compare_version("torch", operator.ge, "1.12.0"):
                 # onednn fusion be added to torch from version 1.12
                 if not torch.jit.onednn_fusion_enabled():
                     torch.jit.enable_onednn_fusion(True)
 
@@ -46,28 +52,35 @@
         self.infer_mode.__exit__(exc_type, exc_value, exc_tb)
         if self.accelerator == "jit" and self.enable_onednn is True:
             if compare_version("torch", operator.ge, "1.12.0"):
                 # onednn fusion be added to torch from version 1.12
                 torch.jit.enable_onednn_fusion(False)
         torch.set_num_threads(self.original_thread_num)
 
+    def __getstate__(self):
+        state = self.__dict__
+        if 'infer_mode' in state.keys():
+            del state['infer_mode']
+        return state
+
 
 class AutocastContextManager(BaseContextManager):
     """
     Autocast context manager for Pytorch Model Inference.
 
     This context manager is used for providing no grad and autocast context,
     which is used for bf16 model.
     """
-    def __init__(self, thread_num=None, accelerator=None, enable_onednn=True):
+    def __init__(self, thread_num=None, accelerator=None, enable_onednn=False):
         super().__init__(thread_num=thread_num, accelerator=accelerator,
                          enable_onednn=enable_onednn)
         if compare_version("torch", operator.lt, "1.13.0"):
             # In torch1.12, torch.inference_mode(mode=True) will cause bug for jit+bf16
             self.infer_mode = torch.no_grad()
+            self.infer_mode_string = "no_grad"
         self.autocast = torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16)
 
     def __enter__(self):
         super().__enter__()
         if self.accelerator == "jit" and self.enable_onednn is True:
             if compare_version("torch", operator.lt, "1.14"):
                 # onednn fusion for bf16 only work for torch version > 1.13
@@ -100,24 +113,24 @@
         super().__enter__()
 
     def __exit__(self, exc_type, exc_value, exc_tb):
         super().__exit__(exc_type, exc_value, exc_tb)
 
 
 def generate_context_manager(accelerator=None, precision="fp32", thread_num=None,
-                             enable_onednn=True, use_xpu=False):
+                             enable_onednn=False, use_xpu=False):
     '''
     generate correct context manager according to different situation
     :param acclerator: str, the accelerator to use, we support "onnxruntime", "openvino",
            "jit", and None for pytorch framework.
     :param precision: str, the precision to use, we support "fp32", "bf16" and "int8".
     :param thread_num: int, the thread number to allocate, None for no limit.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
            API, which provides a flexible API for aggressive fusion. Default to
-           ``True``, only valid when accelerator="jit", otherwise will be ignored.
+           ``False``, only valid when accelerator="jit", otherwise will be ignored.
     :param use_xpu: if xpu model is used.
     '''
     if (precision != "bf16" and use_xpu is False) or precision == "fp32":
         return BaseContextManager(thread_num=thread_num, accelerator=accelerator,
                                   enable_onednn=enable_onednn)
     elif not use_xpu:
         return AutocastContextManager(thread_num=thread_num, accelerator=accelerator,
```

## bigdl/nano/pytorch/inference/optimizer.py

```diff
@@ -57,15 +57,16 @@
 import os
 os.environ['LOGLEVEL'] = 'ERROR'  # remove parital output of inc
 
 
 class TorchAccelerationOption(AccelerationOption):
     def optimize(self, model, training_data=None, input_sample=None,
                  thread_num=None, dynamic_axes=True, logging=False,
-                 sample_size_for_pot=100, output_tensors=True):
+                 sample_size_for_pot=100, output_tensors=True,
+                 jit_strict=True):
         accelerator = self.get_accelerator()
         if self.get_precision() == "fp32":
             if accelerator is None and self.ipex is False and \
                     self.channels_last is False:
                 return model
             # trace
             acce_model = \
@@ -74,15 +75,16 @@
                                          input_sample=input_sample,
                                          thread_num=thread_num,
                                          channels_last=self.channels_last,
                                          use_ipex=self.ipex,
                                          dynamic_axes=dynamic_axes,
                                          # remove output of openvino
                                          logging=logging,
-                                         output_tensors=output_tensors)
+                                         output_tensors=output_tensors,
+                                         jit_strict=jit_strict)
         else:
             # quantize
             ort_method: str = self.method
             acce_model = \
                 InferenceOptimizer.quantize(model=model,
                                             precision=self.get_precision(),
                                             accelerator=accelerator,
@@ -92,15 +94,16 @@
                                             input_sample=input_sample,
                                             method=ort_method,
                                             thread_num=thread_num,
                                             dynamic_axes=dynamic_axes,
                                             sample_size=sample_size_for_pot,
                                             # remove output of openvino
                                             logging=logging,
-                                            output_tensors=output_tensors)
+                                            output_tensors=output_tensors,
+                                            jit_strict=jit_strict)
         return acce_model
 
 
 class InferenceOptimizer(BaseInferenceOptimizer):
 
     # acceleration method combinations, developers may want to register some new
     # combinations here
@@ -162,14 +165,16 @@
                  input_sample: Union[torch.Tensor, Dict, Tuple[torch.Tensor], None] = None,
                  metric: Optional[Callable] = None,
                  direction: str = "max",
                  thread_num: Optional[int] = None,
                  accelerator: Optional[Tuple[str]] = None,
                  precision: Optional[Tuple[str]] = None,
                  use_ipex: Optional[bool] = None,
+                 jit_strict: Optional[bool] = True,
+                 enable_onednn: Optional[bool] = False,
                  search_mode: str = "default",
                  dynamic_axes: Union[bool, dict] = True,
                  logging: bool = False,
                  output_tensors: bool = True,
                  latency_sample_num: int = 100,
                  includes: Optional[List[str]] = None,
                  excludes: Optional[List[str]] = None,
@@ -263,14 +268,23 @@
                within the specified accelerator tuple.
         :param precision: (optional) A string tuple that specifies the precision to search.
                The optional precision are: 'int8', 'bf16', and 'fp32'. Defaults to None which
                represents no precision limit. If not None, then will only traverse corresponding
                methods whose precision falls within the specified precision tuple.
         :param use_ipex: (optional) if not None, then will only try methods with/without
                this specific ipex setting.
+        :param jit_strict: Whether recording your mutable container types. This parameter will be
+               passed to ``torch.jit.trace``. if ``accelerator != 'jit'`` or
+               ``jit_method='script'``, it will be ignored. Default to True.
+        :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph API,
+                which provides a flexible API for aggressive fusion. Default to
+                ``False``, only valid when accelerator='jit', otherwise will
+                be ignored. For more details, please refer https://github.com/
+                pytorch/pytorch/tree/master/torch/csrc/jit/codegen/
+                onednn#pytorch---onednn-graph-api-bridge.
         :param search_mode: Here are three modes for optimization:
 
                | 1. default: This mode only traverses a subset of all combinations. This subset
                | is a collection of methods that we select based on experience and think have
                | better acceleration effect in general. This mode allows you to quickly obtain a
                | good acceleration method, but it is not necessarily the global optimal. Default
                | to this mode if you don't specify accelerator/precision/use_ipex.
@@ -445,21 +459,23 @@
                 result_map[method]["status"] = "lack dependency"
             else:
                 print(f"----------Start test {method} model "
                       f"({idx+1}/{len(available_dict)})----------")
                 option: AccelerationOption = self.ALL_INFERENCE_ACCELERATION_METHOD[method]
                 _precision = option.get_precision()
                 try:
-                    acce_model = option.optimize(model, training_data=training_data,
+                    acce_model = option.optimize(model,
+                                                 training_data=training_data,
                                                  input_sample=input_sample,
                                                  thread_num=thread_num,
                                                  dynamic_axes=dynamic_axes,
                                                  logging=logging,
                                                  output_tensors=output_tensors,
-                                                 sample_size_for_pot=sample_size_for_pot)
+                                                 sample_size_for_pot=sample_size_for_pot,
+                                                 jit_strict=jit_strict)
                 except Exception:
                     traceback.print_exc()
                     result_map[method]["status"] = "fail to convert"
                     print(f"----------Failed to convert to {method}----------")
                     continue
 
                 result_map[method]["status"] = "successful"
@@ -886,15 +902,16 @@
                 """
                 If accelerator==None, quantized model returned should be an object of PytorchModel
                 which is defined by neural-compressor containing a `GraphModule` for inference.
                 Otherwise accelerator=='onnxruntime', it returns an ONNXModel object. A supported
                 model which is able to run on Pytorch or ONNXRuntime can be fetched by
                 `quantized_model.model`.
                 """
-                inc_quantize_arguments = {"model": model, "dataloader": inc_calib_dataloader,
+                inc_quantize_arguments = {"model": model,
+                                          "dataloader": inc_calib_dataloader,
                                           "eval_func": eval_func,
                                           "metric": metric, "thread_num": thread_num,
                                           "framework": framework, "conf": conf,
                                           "approach": approach,
                                           "tuning_strategy": tuning_strategy,
                                           "accuracy_criterion": accuracy_criterion,
                                           "timeout": timeout,
@@ -915,15 +932,16 @@
                         return PytorchIPEXQuantizationModel(model,
                                                             inc_calib_dataloader,
                                                             q_config=q_config,
                                                             input_sample=input_sample,
                                                             channels_last=channels_last,
                                                             thread_num=thread_num,
                                                             inplace=inplace,
-                                                            jit_strict=jit_strict)
+                                                            jit_strict=jit_strict,
+                                                            enable_onednn=enable_onednn)
             elif accelerator == 'openvino':
                 model_type = type(model).__name__
                 if not model_type == 'PytorchOpenVINOModel':
                     if input_sample is None:
                         # input_sample can be a dataloader
                         input_sample = calib_dataloader
                     # For CPU: fp32 -> int8, for GPU/VPUX: fp16 -> int8
```

## bigdl/nano/utils/pytorch/dataset.py

```diff
@@ -34,12 +34,16 @@
 def remove_batch_dim_fn(loader):
     def warpper_fn(batch):
         data = default_collate(batch)
 
         def recusive_remove(data):
             if isinstance(data, torch.Tensor):
                 return data.squeeze(0)
-            else:
+            elif isinstance(data, list) or isinstance(data, tuple):
                 return tuple([recusive_remove(x) for x in data])
+            elif isinstance(data, dict):
+                for key in data:
+                    data[key] = data[key].squeeze(0)
+                return data
         return recusive_remove(data)
     loader.collate_fn = warpper_fn
     return loader
```

## Comparing `bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init` & `bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init`

 * *Files identical despite different names*

## Comparing `bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init.ps1` & `bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init.ps1`

 * *Files identical despite different names*

## Comparing `bigdl_nano-2.4.0b20230706.dist-info/METADATA` & `bigdl_nano-2.4.0b20230707.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: bigdl-nano
-Version: 2.4.0b20230706
+Version: 2.4.0b20230707
 Summary: High-performance scalable acceleration components for intel.
 Home-page: https://github.com/intel-analytics/BigDL
 Author: BigDL Authors
 Author-email: bigdl-user-group@googlegroups.com
 License: UNKNOWN
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
```

## Comparing `bigdl_nano-2.4.0b20230706.dist-info/RECORD` & `bigdl_nano-2.4.0b20230707.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -32,19 +32,19 @@
 bigdl/nano/deps/automl/optuna_backend.py,sha256=uiYeGc4GXv4bUiZxt69UqjuZPp3nsYwghiDm77Ulm8w,6876
 bigdl/nano/deps/horovod/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/horovod/distributed_utils_horovod.py,sha256=VIwjFWYc3km0Tz1Mq5cLuJuqFzgvhzUcBPA-mkRgZjg,5304
 bigdl/nano/deps/horovod/horovod_api.py,sha256=JTTEzkhGU6UpKitUrLXJyd3e_9rgItn-dh9i1o8eYb8,934
 bigdl/nano/deps/horovod/horovod_worker.py,sha256=StZxIukKUbvW1aKn-c6WAzVHkFZLiyplgyyDWq8GJGA,1125
 bigdl/nano/deps/horovod/multiprocs_backend.py,sha256=87TCvg4jgH-Pn883kmuydGcRRtdZ8BflEEzB3YunH14,2761
 bigdl/nano/deps/ipex/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
-bigdl/nano/deps/ipex/ipex_api.py,sha256=oR8uIcoolORKtpaeLponC0Et3I2j9PJ_zYA-96ZmO0Q,9510
+bigdl/nano/deps/ipex/ipex_api.py,sha256=0BX3fw-qWAYpR-oitzIWDJ7jEK_EdcbwB1SSZdoZFZA,9826
 bigdl/nano/deps/ipex/ipex_inference_bf16_model.py,sha256=dftb484lCmON7bXgY4JGkhM-Pd9DkiPUgAlJ9JnSASo,8545
-bigdl/nano/deps/ipex/ipex_inference_model.py,sha256=2g-M7pE2trpoxH-QBMBc6WJMFjgD3FYjj_1wsoYGyx4,14090
+bigdl/nano/deps/ipex/ipex_inference_model.py,sha256=6reG61wjh2dinPKpeqxwGCerMxrPj5-Ovhi8zMkcPaA,14091
 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py,sha256=kI0-5pCtOKxE-9moZvEv_ZX75ZrOus7hsoLSKc5LKEU,4444
-bigdl/nano/deps/ipex/ipex_quantization_model.py,sha256=9nd-r8bAx1Vdhym1NpBfBHpBOmW8w2ngjjNit2rlUI0,8087
+bigdl/nano/deps/ipex/ipex_quantization_model.py,sha256=OaNiH-KS0JW5C-Y1vDTfaW4GOv-kuxdGnDfA6aqun6s,8914
 bigdl/nano/deps/neural_compressor/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/neural_compressor/inc_api.py,sha256=dV79vfNV4oxXZxlJbTsEojZanuO6ji296xTpCm5UaP8,3907
 bigdl/nano/deps/neural_compressor/inc_api_2.py,sha256=Rs5XGKuX2rF_vo0tMxIFyDTf_4AYEBB-NVVHxwJpXWg,9975
 bigdl/nano/deps/neural_compressor/core/__init__.py,sha256=x-EqdMB3cq0-KGFPSfUq2CpFzBYoGbNFSBXKfoRkc4w,1013
 bigdl/nano/deps/neural_compressor/core/base_metric.py,sha256=qaMc4sZzffzBvagXHgOMWxLCDY0PaABKRZkBnc32kAQ,1856
 bigdl/nano/deps/neural_compressor/core/quantization.py,sha256=sNqYHv7iL0LJc0CMcA4wsVWVQQvM425UrUui1gfSfok,10631
 bigdl/nano/deps/neural_compressor/onnx/__init__.py,sha256=OTLYPpPF1-_8Dx1CENZVd_e6HG2ypQnee6P2fgMs7lU,1036
@@ -99,30 +99,30 @@
 bigdl/nano/deps/ray/ray_envbase.py,sha256=Q_F76GckR7_W8atNR-f4cD6OJvwFC-gstX0XmxgdlCo,3705
 bigdl/nano/k8s/__init__.py,sha256=Sjb3PUJAupart-VZ3ccAh6j7tXC1mNrv_BTyLlpwpaA,618
 bigdl/nano/k8s/bigdl_submit.py,sha256=yKu4bvdsiYSGNmxPiCg7d6KXAeZcdVzTEZgQYtDRncw,13183
 bigdl/nano/libs/libjemalloc.so,sha256=gBgVkr0vI-5w7KdT38ez485J6RvRlZE1-Xe3JnLe4h8,5460184
 bigdl/nano/libs/libtcmalloc.so,sha256=sGmZ-oxCTzm9O7gBFY_Q5tInlih1tBIFfkOhvrQu2zU,393904
 bigdl/nano/libs/libturbojpeg.so.0.2.0,sha256=zIzbcFVMbPL8PdgdHXvMW626xVH2UMGggkN2kidDVbM,1182232
 bigdl/nano/pytorch/__init__.py,sha256=n7XTQzUJUobba0FvyblZgDThnPmQgIoyOSPV_ccxOc0,2113
-bigdl/nano/pytorch/context_manager.py,sha256=Qbs8DZdTHATUBjLe7uIshpsy1Dbny5rsJ1PlcrxvHwo,5657
+bigdl/nano/pytorch/context_manager.py,sha256=mwiT1QqbCKzRu7LKMGim1sbHjLFnxBCqDObDF6L6040,6152
 bigdl/nano/pytorch/dispatcher.py,sha256=Mf0W83D1XcwVyRBp_PwIbd11DMsOtLrWsEeoOysSnag,3245
 bigdl/nano/pytorch/lightning.py,sha256=Qly7-YaqQQSvssUPv23pRa4H4qx-cmNwWqzZ-XdTZeA,5818
 bigdl/nano/pytorch/model.py,sha256=AFPJnDw1S5Mwhy3OmCzAZ1GItlV9aQUAN8LPIp0711E,3063
 bigdl/nano/pytorch/torch_nano.py,sha256=p9pW-1mtnLOFCOKRlasH5JCdilkyV2pgMbmFhaUj-dc,18936
 bigdl/nano/pytorch/algorithms/__init__.py,sha256=Xugd2uc-waymnAZMtret72KnHHZa3SauFUBRHy7QYuo,706
 bigdl/nano/pytorch/algorithms/selective_backprop/__init__.py,sha256=70KqiUKOyuHFUPlmxE8lsUTN4ewmhXPCKwU_R3CqYrc,1235
 bigdl/nano/pytorch/algorithms/selective_backprop/selective_backprop.py,sha256=Myrp4t53gYDZRuVGGym7Tz_5uYOah4I_d73ld6dX_io,12005
 bigdl/nano/pytorch/amp/__init__.py,sha256=5T_pNFHbS-EdK7nlbJJqb4JlCvUnPQpO-sxVZuaP6Ys,609
 bigdl/nano/pytorch/amp/amp_api.py,sha256=RSZda43RuQZJB3sfvMR58WIHrj_RyHQHn3NOc0O_HSQ,805
 bigdl/nano/pytorch/amp/bfloat16.py,sha256=Qw9ZXUxDeIgBFfPNedS9PwSi7FPl1bG-9HrloKyz_lw,9569
 bigdl/nano/pytorch/encryption/__init__.py,sha256=4e0eEjjj3iNwnCRzLtzVjS27oH-CA1ws6TJXnuUwNg8,640
 bigdl/nano/pytorch/encryption/encryption.py,sha256=UIiNtbyZ0GFlkirOHX2hOzxKXtYJ6g56SQ2lJvO40kY,6480
 bigdl/nano/pytorch/inference/__init__.py,sha256=PlFS2XblVRBXTai86gEV79sQWEJLgaOyDnjdLYNKxTA,661
 bigdl/nano/pytorch/inference/multi_instance.py,sha256=XCMQRg1BHfuQWxNTtsHpplZfSNSj0rEO71if67OsR-I,4231
-bigdl/nano/pytorch/inference/optimizer.py,sha256=2p9_ayUCN5piN08oshL7IugGWBXLyDWsoLjj6d0M0RY,88306
+bigdl/nano/pytorch/inference/optimizer.py,sha256=88tA4NGt7-KXaN1bw_bsA1EKLheVfWVv_UKoEzNyjXo,89541
 bigdl/nano/pytorch/inference/pipeline.py,sha256=ZFWld1F7ezYT5SaXXcO6gM-U-zDAQn6AcjYRkJWYIr8,5107
 bigdl/nano/pytorch/low_precision/jit_int8_api.py,sha256=HJeNprNAa6DyCx7PLZvM26j63kEtr1luo-hjVJK-ypk,3430
 bigdl/nano/pytorch/low_precision/jit_int8_model.py,sha256=7XJNYiNznJDLeLyiKDOZCjB7fHLaU-hN3WMLuVKCF4s,9886
 bigdl/nano/pytorch/optim/__init__.py,sha256=JWc3YVybOcyixkTxAB2QVN9l4KZC7BXg8D2myUs2P-w,646
 bigdl/nano/pytorch/optim/sparseadam.py,sha256=22GGm0Kh2idtAgEj5mn5asMYc3CoKbbE-gwPqtXwrgA,8351
 bigdl/nano/pytorch/patching/__init__.py,sha256=LGg-BpIbWHdj_LNe4Px1m2lU-Y9LFAmWtctRF9wbCFM,804
 bigdl/nano/pytorch/patching/dtype_patching/__init__.py,sha256=P99b-m_UygHUv-c1jsfE2Q0dtGkbM_mt_GFio7bBFlQ,627
@@ -193,15 +193,15 @@
 bigdl/nano/utils/common/optimizer/optimizer.py,sha256=PgOem9vDwbhNY9rrzb9VoWBxZFA8y19kUiD6KwlrQH4,8336
 bigdl/nano/utils/pytorch/__init__.py,sha256=GQN0YyzbnQTb-sUyhrZmv_zUQhdPBZGgnbvR4NtCIog,1939
 bigdl/nano/utils/pytorch/attributes.py,sha256=vFxq074vDnw7OWuiQH-9ERJRk3CU719GxBHmWAB_3kQ,1100
 bigdl/nano/utils/pytorch/channel_last.py,sha256=iDbh_s8yFIFq0i1JRyCCYIJf67FRrf7FNU5j1js05NY,4527
 bigdl/nano/utils/pytorch/check_deps.py,sha256=oFq4BvkXQgJdpQBVwYwCd1ZN64Gfu16rmC55AO5KU-0,1807
 bigdl/nano/utils/pytorch/convert.py,sha256=kTQRSN7RZSoPXm7Q77wxUtGVBZ6KKz6pXNuFf36mobA,3358
 bigdl/nano/utils/pytorch/dataloader.py,sha256=D0vO2NdWtM-8aQQ5EsGWZqgIqBqFi8B03zEOvaWvYsk,6197
-bigdl/nano/utils/pytorch/dataset.py,sha256=8niaxA5SMmet_nVFiO9NMeL4hesCJsJxjhNJwRxfouc,1321
+bigdl/nano/utils/pytorch/dataset.py,sha256=Q_x6FsHL_lf82fuipQhD1lSaVCUYmy2pf4HzlA8A56o,1526
 bigdl/nano/utils/pytorch/input_sample.py,sha256=1wecir3d9uNn39o_StO9GZHr3TMCRC5G4eK4ykIkF-Q,3881
 bigdl/nano/utils/pytorch/inspect.py,sha256=z013opp8CfvOHFFDju2tW9v6vhdjLwQ5huychXYKpiU,4952
 bigdl/nano/utils/pytorch/jit_method.py,sha256=YuSi4LIY828eIFcVJ7VtnLhhkRzVRq_rI0AYMrTQLrk,2595
 bigdl/nano/utils/pytorch/load.py,sha256=LxJ5pQd9Ijfc-KreuihtF1p__Gl25O5a2Noqcm1UHJA,6270
 bigdl/nano/utils/pytorch/metadata.py,sha256=bD1iTrQHntNvefJMNx2umRBGA1CDp0q0Oe3pZmRhx_M,5951
 bigdl/nano/utils/pytorch/metric.py,sha256=7TFBw2F5t_sh-2p0XbbUgUujZULBIwBMFIRt5FV4ATA,1391
 bigdl/nano/utils/pytorch/model_info.py,sha256=IT4RYtgB-YukrEudgu5j3BKDnCBlOIuKo_EyFQuG0G4,5531
@@ -211,16 +211,16 @@
 bigdl/nano/utils/tf/__init__.py,sha256=wxh1M-4H8NwLSNhiipdZcXs8gtycV3Cs2DcVPRUnuAE,1057
 bigdl/nano/utils/tf/attributes.py,sha256=AkRSYw0bmdpLwnWZjRXsaIVrDW9H2TJpBB3Tq-RVoGg,4300
 bigdl/nano/utils/tf/backend.py,sha256=CfI_33A-2Na8R462k592F0FlAkwu0lTKzedf_SXyUgQ,2816
 bigdl/nano/utils/tf/data.py,sha256=HyrSXuDeSC-b2X8yP7RBTTKgvLDhV5OqXf7eFOte6nc,7421
 bigdl/nano/utils/tf/preprocess.py,sha256=DHukkdJUa-3z0jK63okaJ0vhbRAJipAoBUpsyS4P93k,2582
 bigdl/nano/utils/tf/subprocess_worker.py,sha256=LitCgfBQdnMqOmGuio4y58Mh9xCoxvn3JVS0INek5mE,1347
 bigdl/nano/utils/tf/version.py,sha256=IN7ZGJJPKSUAJN2LmiJgO_C6cZM8d9023DjcwZ0x9sg,823
-bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init,sha256=C4Zqhf4zcR-IAHzTBdEaq1v2ZIMUE-QyEa8hWhF8JS0,13425
-bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-init.ps1,sha256=kX83Ue2ZJgibvddidAHzQ5zhbGlHntKlTcK5lvNmkV4,909
-bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env,sha256=gFA_hwK6WoWBA2HBz0vSzfpZT6K5Fjub0EV4MGvwrjw,293
-bigdl_nano-2.4.0b20230706.data/scripts/bigdl-nano-unset-env.ps1,sha256=amIge9q2-1oumygXfW00qeYejEdVMESNUferj5B7txU,127
-bigdl_nano-2.4.0b20230706.dist-info/METADATA,sha256=oiW6f4do6uAKw5KtL4n0pnbeRAtA7mASHN9tSO7JVPY,10169
-bigdl_nano-2.4.0b20230706.dist-info/WHEEL,sha256=bC8mYJUOJCh5KnyEeT6W_BCQYi3v39D3z64Vy_sFvVg,98
-bigdl_nano-2.4.0b20230706.dist-info/entry_points.txt,sha256=NJqjgi9adpmsUUYPXsd7LzSBdoN1nYjeOyFi9Wih_oU,54
-bigdl_nano-2.4.0b20230706.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
-bigdl_nano-2.4.0b20230706.dist-info/RECORD,,
+bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init,sha256=C4Zqhf4zcR-IAHzTBdEaq1v2ZIMUE-QyEa8hWhF8JS0,13425
+bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-init.ps1,sha256=kX83Ue2ZJgibvddidAHzQ5zhbGlHntKlTcK5lvNmkV4,909
+bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env,sha256=gFA_hwK6WoWBA2HBz0vSzfpZT6K5Fjub0EV4MGvwrjw,293
+bigdl_nano-2.4.0b20230707.data/scripts/bigdl-nano-unset-env.ps1,sha256=amIge9q2-1oumygXfW00qeYejEdVMESNUferj5B7txU,127
+bigdl_nano-2.4.0b20230707.dist-info/METADATA,sha256=vyCmM5fnZhM95R0hbNcQa0p4P3oXN539dfw6bA7lpJo,10169
+bigdl_nano-2.4.0b20230707.dist-info/WHEEL,sha256=i9qQj8KaD8_YEW0Vc2oS56fKju23RkQ-FVz-QmzVakQ,98
+bigdl_nano-2.4.0b20230707.dist-info/entry_points.txt,sha256=NJqjgi9adpmsUUYPXsd7LzSBdoN1nYjeOyFi9Wih_oU,54
+bigdl_nano-2.4.0b20230707.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
+bigdl_nano-2.4.0b20230707.dist-info/RECORD,,
```

